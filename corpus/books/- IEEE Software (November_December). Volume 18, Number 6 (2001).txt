***- IEEE Software (November_December). Volume 18, Number 6 (2001)***




































C o p y r i g h t  ©  2 0 0 1  S t e v e n  C .  M c C o n n e l l .  A l l  R i g h t s  R e s e r v e d . N o v e m b e r / D e c e m b e r  2 0 0 1 I E E E  S O F T W A R E 7

from the editor

Raising Your Software
Consciousness
Steve McConnell

E d i t o r  i n  C h i e f :  S t e v e  M c C o n n e l l  � C o n s t r u x  S o f t w a r e  � s o f t w a r e @ c o n s t r u x . c o m

I
n 1970, Charles Reich published a best-
selling book called The Greening of
America.1 In it, Reich identifies three
kinds of awareness or consciousness,
which he calls Consciousness I, Con-
sciousness II, and Consciousness III. 

Consciousness I (“Con I”) is the
pioneer mentality. People who op-
erate at Con I place great value on
independence and self-satisfaction.
They don’t easily tolerate other
people telling them what to do.
They are highly self-reliant and
self-sufficient. Reich believes that
Con I dominated the American
psyche during America’s first cen-
turies and that this focus on self-
reliance was a significant factor in

America’s development. 
Consciousness II is the gray flannel suit

mentality—corporation man. People who op-
erate at Con II understand the importance of
getting along with others and playing by the
rules. They believe rules are good for society,
and they think everyone should follow them.
Reich believes that Con II became more dom-
inant than Con I in the mid-twentieth century. 

Consciousness III is the mentality of en-
lightened independence. The Con III person
operates on the basis of principles, with little
regard for the rules that predominate in Con II
and without the selfishness that predominates
in Con I. By the time Greening was published,
Reich argued that Con II’s time was over. He
believed Con III was in its ascendancy and
would soon replace Con II. 

Although The Greening of America struck
a resonant chord when it was published, his-

tory has not been kind to the book. In 1999,
Slate magazine’s readers voted it the silliest
book of the 20th century. Reich’s Con III was
a hippie nirvana, and the “greening” Reich
predicted was a nationwide movement to-
ward the hippie culture of the 1960s and
1970s—psychedelic drugs, bell-bottom pants,
and all. As the hippie culture faded into ob-
scurity in the 1980s, so did the credibility of
Reich’s predictions. 

Can’t get no satisfaction
Reich’s political predictions may not have

withstood the test of time, but his classifica-
tion of Con I, Con II, and Con III provides a
useful model for the software industry today. 

Con I in software is associated with a focus
on self-reliance. Software experts often refer to
software developers operating at this level of
awareness as mavericks, cowboy program-
mers, Lone Rangers, and prima donnas. Soft-
ware developers at this level tend to have little
tolerance for other people’s ideas. They like to
work alone. They don’t like following stan-
dards. The “Not Invented Here” syndrome
thrives. 

Con I’s advantage is that little training is
needed, and the lone-wolf approach works ad-
equately in environments that employ only
small numbers of programmers who work in-
dependently on small projects. Con I’s disad-
vantage is that it scales poorly to projects that
need teams of programmers rather than iso-
lated individuals. 

Con II in software is associated with a fo-
cus on rules. Many software developers even-
tually discover the limitations of Con I’s self-
reliant development style and see the



8 I E E E  S O F T W A R E N o v e m b e r / D e c e m b e r  2 0 0 1

FROM THE EDITOR

advantages of working in groups.
Over time, they learn rules that allow
them to coordinate their work with
others. Some groups of developers
create their own informal rules
through trial and error, and these
groups can be highly effective. Other
groups buy a prebuilt methodology.
Sometimes the rules are provided by
consultants, as in the classic “17
three-ring binders” methodologies.
Other times, the rules are taken from
books, such as The Rational Unified
Process,2 the Extreme Programming
series,3 or my own Software Project
Survival Guide.4 Developers at this
level of awareness tend to focus on the
details of adhering to the rules. They
argue about which interpretations of
the rules are correct and focus on
“following the methodology.”

The advantage of Con II is that a
developer needs to be trained to use
only a single approach. If a good ap-
proach is chosen, the developer can
leverage a relatively small amount of
training across many projects. The dis-
advantage is that a Con II developer is
ill-equipped to succeed on projects
that fall outside the specific metho-
dology in which the developer was
trained. 

Con III in software is associated
with a focus on principles. At this level
of awareness, developers understand
that the rules of any prepackaged
methodology are, at best, approxima-
tions of principles. Those approxima-
tions might apply most of the time, but
they won’t apply all of the time. The
disadvantage of Con III is that exten-
sive education and training are needed
to introduce a developer to the princi-
ples underlying effective software de-
velopment, and that training is not eas-
ily obtained. Con III’s advantage is
that, once that training has been ob-
tained, the developer is equipped with
a full range of software engineering
tools that support success on a wide
range of projects. 

Love the one you’re with
The software industry has a long

history of trying and ultimately reject-
ing “one size fits all” methodologies.
These methodologies are Con II soft-

ware approaches, and they fail outside
narrowly defined areas of applicability
—predictably—precisely because they
are Con II. The world of software is far
too varied to be addressed by a single
set of rules. 

For example, compare the practices
you would use to develop a heart pace-
maker control to those you would use
to develop a video store management
program. If a software malfunction
caused you to lose one video out of
1,000, it might affect the store’s prof-
itability by a fraction of a percent, but
the impact is negligible. If a malfunc-
tion caused one pacemaker out of
1,000 to fail, however, you’ve got a
real problem. Generally speaking,
widely distributed products must be
developed more carefully than nar-
rowly distributed ones. Products whose
reliability is important must be devel-
oped more carefully than products
whose reliability doesn’t much matter. 

These different kinds of software
require different development prac-
tices. Practices that would be consid-
ered to be overly rigorous, bureau-
cratic, and cumbersome for video
store management software might be
considered irresponsibly quick and
dirty—or reckless—for an embedded
pacemaker control. The Con III de-
veloper will use different practices to
develop a heart pacemaker control
than to develop an video inventory
tracking system. The Con II developer
will try to apply a one-size-fits-all
methodology to both projects, with
the likelihood that the methodology
won’t work particularly well for ei-
ther one. 

Are you experienced?
Reich identified the three levels of

consciousness as the zeitgeists of dif-
ferent eras, but I see Con I, Con II, and
Con III as three distinct steps along a
path of personal software engineering
maturity. Most software developers
begin their careers at Con I and even-
tually journey to Con II. In many envi-
ronments, Con II supports effective
work, and no further development is
needed. In some environments, how-
ever, a further progression toward Con
III is needed. 

DEPARTMENT EDITORS

Bookshelf: Warren Keuffel, 
wkeuffel@computer.org

Country Report: Deependra Moitra, Lucent Technologies
d.moitra@computer.org

Design: Martin Fowler, ThoughtWorks,
fowler@acm.org

Loyal Opposition: Robert Glass, Computing Trends,
rglass@indiana.edu

Manager: Don Reifer, Reifer Consultants,
dreifer@sprintmail.com

Quality Time: Jeffrey Voas, Cigital, 
voas@cigital.com

STAFF

Senior Lead Editor 
Dale C. Strok

dstrok@computer.org

Group Managing Editor
Crystal Chweh

Associate Editors
Jenny Ferrero and 

Dennis Taylor

Staff Editors
Shani Murray, Scott L. Andresen,

and Kathy Clark-Fisher

Magazine Assistants
Dawn Craig

software@computer.org

Pauline Hosillos

Art Director
Toni Van Buskirk

Cover Illustration
Dirk Hagner

Technical Illustrator
Alex Torres

Production Artists
Carmen Flores-Garvey and Larry Bauer

Acting Executive Director
Anne Marie Kelly

Publisher
Angela Burgess

Assistant Publisher
Dick Price

Membership/Circulation
Marketing Manager
Georgann Carter

Advertising Assistant
Debbie Sims

CONTRIBUTING EDITORS

Greg Goth, Denise Hurst, Anne Lear, Keri Schreiner,
and Margaret Weatherford

Editorial: All submissions are subject to editing for clarity,
style, and space. Unless otherwise stated, bylined articles
and departments, as well as product and service descrip-
tions, reflect the author’s or firm’s opinion. Inclusion in
IEEE Software does not necessarily constitute endorsement
by the IEEE or the IEEE Computer Society.

To Submit: Send 2 electronic versions (1 word-processed
and 1 postscript or PDF) of articles to Magazine Assistant,
IEEE Software, 10662 Los Vaqueros Circle, PO Box 3014,
Los Alamitos, CA 90720-1314; software@computer.org. Ar-
ticles must be original and not exceed 5,400 words including
figures and tables, which count for 200 words each. 



N o v e m b e r / D e c e m b e r  2 0 0 1 I E E E  S O F T W A R E 9

FROM THE EDITOR

The by-the-book methodologies of
Con II seem to be a reasonable learn-
ing path for developers at Con I who
are not yet well versed in a wide range
of software practices. The specific de-
tails of the rules-based practices prob-
ably don’t matter all that much. People
who are trying to raise themselves
from Con I to Con II simply need to
take a first step away from the chaos
of a completely unmanaged project.
They must learn a set of rules and get
some experience applying those rules
before they can advance to the Con III
level, where they understand software
project dynamics well enough to break
the rules when needed. This whole
process is part of the natural progres-
sion from apprentice to journeyman to
master.

References
1. C. Reich, The Greening of America, Random

House, New York, 1970.
2. P. Kruchten, The Rational Unified Process: An

Introduction, 2nd ed., Addison-Wesley, Read-
ing, Mass., 2000.

3. K. Beck, Extreme Programming: Embrace
Change, Addison-Wesley, Reading, Mass.,
2000.

4. S. McConnell, Software Project Survival
Guide, Microsoft Press, Redmond, Wash.,
1998.

EDITOR IN CHIEF: 
Steve McConnell

10662 Los Vaqueros Circle
Los Alamitos, CA 90720-1314

software@construx.com

EDITOR IN CHIEF EMERITUS:
Alan M. Davis, Omni-Vista

ASSOCIATE EDITORS IN CHIEF

Design: Maarten Boasson, Quaerendo Invenietis
boasson@quaerendo.com

Construction: Terry Bollinger, Mitre Corp.
terry@mitre.org

Requirements: Christof Ebert, Alcatel Telecom
christof.ebert@alcatel.be

Management: Ann Miller, University of Missouri, Rolla
millera@ece.umr.edu

Quality: Jeffrey Voas, Cigital
voas@cigital.com

Experience Reports: Wolfgang Strigel, 
Software Productivity Center; strigel@spc.ca

EDITORIAL BOARD

Don Bagert, Texas Tech University
Richard Fairley, Oregon Graduate Institute

Martin Fowler, ThoughtWorks
Robert Glass, Computing Trends

Andy Hunt, Pragmatic Programmers
Warren Keuffel, independent consultant
Brian Lawrence, Coyote Valley Software

Karen Mackey, Cisco Systems
Deependra Moitra, Lucent Technologies, India

Don Reifer, Reifer Consultants
Suzanne Robertson, Altantic Systems Guild

Dave Thomas, Pragmatic Programmers
Karl Wiegers, Process Impact

INDUSTRY ADVISORY BOARD

Robert Cochran, Catalyst Software (chair) 
Annie Kuntzmann-Combelles, Q-Labs

Enrique Draier, PSINet
Eric Horvitz, Microsoft Research

David Hsiao, Cisco Systems
Takaya Ishida, Mitsubishi Electric Corp.

Dehua Ju, ASTI Shanghai
Donna Kasperson, Science Applications International

Pavle Knaflic, Hermes SoftLab
Günter Koch, Austrian Research Centers

Wojtek Kozaczynski, Rational Software Corp.
Tomoo Matsubara, Matsubara Consulting

Masao Matsumoto, Univ. of Tsukuba
Dorothy McKinney, Lockheed Martin Space Systems

Nancy Mead, Software Engineering Institute
Stephen Mellor, Project Technology

Susan Mickel, AgileTV
Dave Moore, Vulcan Northwest

Melissa Murphy, Sandia National Laboratories
Kiyoh Nakamura, Fujitsu

Grant Rule, Software Measurement Services
Girish Seshagiri, Advanced Information Services

Chandra Shekaran, Microsoft
Martyn Thomas, Praxis

Rob Thomsett, The Thomsett Company 
John Vu, The Boeing Company

Simon Wright, Integrated Chipware
Tsuneo Yamaura, Hitachi Software Engineering

MAGAZINE OPERATIONS COMMITTEE

Sorel Reisman (chair), James H. Aylor, Jean Bacon,
Thomas J. Bergin, Wushow Chou, William I.

Grosky, Steve McConnell, Ken Sakamura, Nigel
Shadbolt, Munindar P. Singh, Francis Sullivan, 

James J. Thomas, Yervant Zorian

PUBLICATIONS BOARD

Rangachar Kasturi (chair), Angela Burgess (pub-
lisher), Jake Aggarwal, Laxmi Bhuyan, Mark Chris-

tensen, Lori Clarke, Mike T. Liu, Sorel Reisman,
Gabriella Sannitti di Baja, Sallie Sheppard, Mike

Williams, Zhiwei Xu

January/February ’02: 
Building Systems Securely 

from the Ground Up

March/April ’02: 
The Software Engineering 

of Internet Software 

May/June ’02: 
Knowledge Management

in Software Engineering

July/August ’02: 
The Business of 
Software Engineering

U p c o m i n g  T o p i c s



10 I E E E  S O F T W A R E N o v e m b e r / D e c e m b e r  2 0 0 1 0 7 4 0 - 7 4 5 9 / 0 1 / $ 1 0 . 0 0  ©  2 0 0 1  I E E E

design
E d i t o r :  M a r t i n  F o w l e r  � T h o u g h t Wo r k s  � f o w l e r @ a c m . o r g

design

S
oftware is an odd medium in which
to construct something. Because few
physical forces make you design one
way or another, many design deci-
sions sadly resist any form of objec-
tive analysis. Where design counts is

often not in how the software runs but in
how easy it is to change. When how it runs
is important, ease of change can be the
biggest factor in ensuring good performance.

This drive toward change-
ability is why it’s so important
for a design to clearly show
what the program does—and
how it does it. After all, it’s hard
to change something when you
can’t see what it does. An inter-
esting corollary of this is that
people often use specific designs
because they are easy to change,
but when they make the pro-
gram difficult to understand, the

effect is the reverse of what was intended. 

Attributes and dictionaries
Let’s say we want a person data structure.

We can accomplish this by having specific
fields, as Figure 1 shows. Of course, to make
this work, we must define the variables in the
person class. Like many modern languages,
Ruby provides a dictionary data structure
(also knows as a map, associative array, or
hash table). We could use Ruby instead to
define the person class, using the approach in
Figure 2. (This is slower, but let’s assume this
section of code isn’t performance critical.)

Using a dictionary is appealing because it
lets you change what you store in the person
without changing the person class. If you
want to add a telephone number, you can do
it without altering the original code.

Despite this, the dictionary doesn’t make it
easier to modify the code. If I’m trying to use
the person structure, I can’t tell what is in it.
To learn that someone’s storing the number of
dependents, I must review the entire system. If
the number of dependents is declared in the

To Be Explicit
Martin Fowler

Figure 1. Explicit
fields (using Ruby).

class Person

attr_accessor :lastName, :firstName, :numberOfDependents

end

def frag1

martin = Person.new

martin.firstName = “Martin”

martin.lastName = “Fowler”

martin.numberOfDependents = 1

print (martin.firstName, “ “, martin.lastName, “ has “, 

martin.numberOfDependents, “ dependents”)

end



class, then I only have to look in the
person class to see what it supports.

The key principle is that explicit
code is easier to understand—which
makes the code easier to modify. As
Kent Beck puts is, the explicit code is
intention revealing.

This dictionary example is small
in scale, but the principle holds at al-
most every scale of software design.

Events and explicit calls
Here’s another example, on a

slightly bigger scale. Many platforms
support the notion of events to com-
municate between modules. Say we
have a reservation module that, when
canceled, needs to get a person mod-
ule to send email to that person. 

We can do this using an event, as Fig-
ure 3 shows. We can define interesting

events that affect a reservation, and any
object that wants to do anything when
an event occurs can build a handler to
react when it occurs. This approach is
appealing because you need not modify

the reservation class to get something
else to happen when you cancel a reser-
vation. As long as other objects put
handlers on the event, it’s easy to extend
the behavior at these points.

Figure 2. Dictionary fields (using
Ruby). class Person

attr_accessor :data

def initialize()

@data = {}

end

end

def frag2

martin = Person.new

martin.data[“firstName”] = “Martin”

martin.data[“lastName”] = “Fowler”

martin.data[“numberOfDependents”] = 1

print (martin.data[“firstName”],“ “,

martin.data[“lastName”], “ has “, 

martin.data[“numberOfDependents”], 

“ dependents”)

end

Figure 3. Cancellation using events (using C#).

public delegate void ReservationHandler (IReservation source);

public class Reservation ...

public String Id;

public event ReservationHandler Cancelled;

public Person client {

get {

return client;

}

set {

value.AddReservation(this);

}

}

public void Cancel(){

Cancelled (this);

}

public class Person ...

public String EmailAddress;

public readonly ArrayList reservations;

public void SendCancellationMessage(Reservation arg) {

// send a message

}

public void AddReservation(Reservation arg) {

//invoke SendCancellationMessage when the cancelled event occurs on arg

arg.Cancelled += 

new ReservationHandler(SendCancellationMessage);

}

N o v e m b e r / D e c e m b e r 2 0 0 1 I E E E  S O F T W A R E 11



However, there is a cost to using
events—I can’t see what happens at
cancellation by reading the code in the
cancellation method. To find out what
happens, I have to search for all the
code that has a handler for the event.
The explicit code for this (see Figure 4)
clearly shows in the cancel method the
consequences of cancellation, at the
cost of modifying the reservation class
when I need to change the behavior. 

I’ve seen a few code examples that
use events heavily, and the problem is

that it’s hard to determine what the
program does when you call a
method. This becomes particularly
awkward when you’re debugging,
because behavior pops up suddenly
in places you don’t expect.

I’m not saying that you shouldn’t
use events. They let you carry out be-
havior without changing a class,
which makes them useful when
working with library classes you
can’t modify. They are also valuable
because they don’t create a depen-

dency from the class triggering the
event to the one that needs to react.
This lack of a dependency is valuable
when the two classes are in different
packages and you don’t want to add
a dependency. The class case of this is
when you want to modify a window
in a presentation when some domain
object changes. Events let you do this
while preserving the vital separation
of the presentation and domain.

Those forces both suggest events,
but in their absence, the lack of explic-
itness of events becomes more domi-
nant. So, I would be reluctant to use
events between two application classes
that can be aware of each other.

As you can see, explicitness is not
always the dominant force in design
decisions. In this example, packaging
and dependency forces are also im-
portant. People often underestimate
the value of explicitness. There are
times when I would add a dependency
to make code more explicit, but, as al-
ways with design, each situation has
its own trade-offs to consider.

Data-driven code and explicit
subclasses

My final example is on yet a bigger
scale. Consider a discounting scheme
for orders that uses different discount-
ing plans. The blue plan gives you a
fixed discount of 150 if you buy
goods from a particular group of sup-
pliers and the value of your order is
over a certain threshold. The red plan
gives you a 10 percent discount when
delivering to certain US states.  

Figure 5 presents explicit code for
this. The order has a discounter with
specific subclasses for the blue and
red cases. The data-driven version in
Figure 6 uses a generic discounter
that is set up with data when the or-
der is created. 

The generic discounter’s advantage
is that you can create new kinds of dis-
counters without making new classes
by writing code—if the new classes fit
in with the generic behavior. For the
sake of argument, let’s assume they
can. Is the generic case always the best
choice? No, again because of explicit-
ness. The explicit subclasses are easier
to read and they make it easier to

DESIGN



N o v e m b e r / D e c e m b e r 2 0 0 1 I E E E  S O F T W A R E 13

Figure 4. An explicit reaction to
cancel (using C#). public class Reservation ...

public String Id;

public Person client;

public void Cancel(){

client.SendCancellationMessage(this);

}
Figure 5. Explicitly programmed
discount logic (using C#).

public class Order ...

public Decimal BaseAmount;

public String Supplier;

public String DeliveryState;

public Discounter Discounter;

public virtual Decimal Discount {

get {

return Discounter.Value(this);

}

}

}

abstract public class Discounter {

abstract public Decimal Value (Order order);

}

public class BlueDiscounter : Discounter {

public readonly IList DiscountedSuppliers = new ArrayList();

public Decimal Threshold = 500m;

public void AddDiscountedSupplier(String arg) {

DiscountedSuppliers.Add(arg);

}

public override Decimal Value (Order order) {

return (DiscountApplies(order)) ? 150 : 0;

}

private Boolean DiscountApplies(Order order) {

return DiscountedSuppliers.Contains(order.Supplier) && 

(order.BaseAmount > Threshold);

}

}

public class RedDiscounter : Discounter {

public readonly IList DiscountedStates = new ArrayList();

public void AddDiscountedState (String arg) {

DiscountedStates.Add(arg);

}

public override Decimal Value (Order order) {

return (DiscountedStates.Contains(order.DeliveryState)) ? 

order.BaseAmount * 0.1m : 0;

}

}

// to set up a blue order

BlueDiscounter bluePlan = new BlueDiscounter();

bluePlan.AddDiscountedSupplier(“ieee”);

blue = new Order();

blue.Discounter = bluePlan;

blue.BaseAmount = 500;

blue.Supplier = “ieee”;



14 I E E E  S O F T W A R E N o v e m b e r / D e c e m b e r  2 0 0 1

DESIGN

Figure 6. Data-programmed discount logic (using C#).

public class GenericOrder : Order ...

public Discounter Discounter;

public override Decimal Discount {

get {

return Discounter.Value(this);

}

}

public enum DiscountType {constant, proportional};

public class Discounter ...

public DiscountType Type;

public IList DiscountedValues;

public String PropertyNameForInclude;

public String PropertyNameForCompare;

public Decimal CompareThreshold;

public Decimal Amount;

public Decimal Value(GenericOrder order) {

if (ShouldApplyDiscount(order)) {

if (Type == DiscountType.constant) 

return Amount;

if (Type == DiscountType.proportional) 

return Amount * order.BaseAmount;

throw new Exception (“Unreachable Code reached”);

} else return 0;

}

private Boolean ShouldApplyDiscount(Order order) {

return PassesContainTest(order) && 

PassesCompareTest(order);

}

private Boolean PassesContainTest(Order order) {

return DiscountedValues.Contains

(GetPropertyValue(order, PropertyNameForInclude));

}

private Boolean PassesCompareTest(Order order){

if (PropertyNameForCompare == null) return true;

else {

Decimal compareValue = 

(Decimal) GetPropertyValue(order, PropertyNameForCom-

pare);

return compareValue > CompareThreshold;

}

}

private Object GetPropertyValue (Order order, String propertyName) {

FieldInfo fi = typeof(Order).GetField(propertyName);

if (fi == null) 

throw new Exception(“unable to find field for “ + property-

Name);

return fi.GetValue(order);

}

}



N o v e m b e r / D e c e m b e r 2 0 0 1 I E E E  S O F T W A R E 15

DESIGN

understand the behavior. With the
generic case, you must look at the
generic code and setup code, and it’s
harder to see what’s happening—and
even harder for more complicated bits
of behavior. Of course, we can extend
the generic order without “program-
ming,” but I’d argue that configuring
that data is a form of programming.
Debugging and testing are often both
difficult and overlooked with data-dri-
ven behavior. 

The generic case works when you
have dozens of discounters. In such
cases, the volume of code becomes a
problem, while greater volumes of
data are less problematic. Sometimes
a well-chosen data-driven abstrac-
tion can make the logic collapse into
a much smaller and easier-to-main-
tain piece of code.

Ease of deploying new code is also a
factor. If you can easily add new sub-
classes to an existing system, explicit
behavior works well. However, generic
behavior is a necessity if new code
means long and awkward compile and
link cycles.

There’s also the option of combin-
ing the two, using a data-driven
generic design for most of the cases
and explicit subclasses for a few hard
cases. I like this approach because it
keeps the generic design much sim-
pler, but the subclasses give you a lot
of flexibility when you need it.

E xplicitness is not an absolute in de-sign, but clever designs often be-come hard to use because they
aren’t explicit enough. In some cases,
the cost is worth it, but it’s always
something to consider. In the last few
years, I’ve tended to choose explicit de-
signs more often because my views of

what makes good design have evolved
(hopefully in the right direction). 

Martin Fowler is the chief scientist for ThoughtWorks, an
Internet systems delivery and consulting company. Contact him
at fowler@acm.org.

//to set up a blue order

GenericDiscounter blueDiscounter = new GenericDiscounter();

String[] suppliers = {“ieee”};

blueDiscounter.DiscountedValues = suppliers;

blueDiscounter.PropertyNameForInclude = “Supplier”;

blueDiscounter.Amount = 150;

blueDiscounter.PropertyNameForCompare = “BaseAmount”;

blueDiscounter.CompareThreshold = 500m;

blueDiscounter.Type = DiscountType.constant;

blue = new Order();

blue.BaseAmount = 500;

blue.Discounter = blueDiscounter;

If you have significant academic or industrial experience in developing large
software systems and you are committed to improving the practice of software
engineering, we want to talk to you about a faculty appointment at Boston Uni-
versity in Computer Systems Engineering.  Our graduate program teaches the
engineering skills necessary for the effective development of large-scale com-
puter systems in which software provides essential functionality.  We teach stu-
dents to apply engineering principles to the design of a full range of computer
products from embedded systems, to data communication networks, to soft-
ware products.  Three types of appointments are available in the Department of
Electrical and Computer Engineering (ECE) starting in September 2002:
•  Research oriented tenure-track and tenured appointments.
• Non-tenure track positions, which require extensive experience in 

practicing software engineering.
•  Adjunct (part-time) positions for Boston-area experts who are interested in 

teaching their specialty at the graduate level.
All positions require a commitment to excellence in teaching at the undergrad-
uate and graduate levels.  For additional information on the College of Engi-
neering and ECE Department visit the College’s homepage at
http://www.bu.edu/eng/.

To learn more about opportunities for a non-tenure track OR adjunct posi-
tions, please e-mail:  besaleh@bu.edu and a faculty member will call to discuss
our opportunities.  For tenure-track OR tenured appointments, send your Cur-
riculum Vita to:  Professor Bahaa E. A. Saleh, Chair, Department of Electrical and
Computer Engineering, Boston University, 8 Saint Mary’s Street, Boston, MA
02215.

Boston University



focus

0 7 4 0 - 7 4 5 9 / 0 1 / $ 1 0 . 0 0  ©  2 0 0 1  I E E E N o v e m b e r / D e c e m b e r  2 0 0 1 I E E E  S O F T W A R E 17

experience reports and give our readers a fo-
rum to share their own learning experiences
with others. If you are interested in submitting

an experience report, please refer to www.
computer.org/software/genres.htm for author
guidelines.

Reports from the Field
Using Extreme Programming 
and Other Experiences

Wolfgang Strigel, Software Productivity Center

L
earning from the successes and failures of others is a quick way to
learn and enlarge our horizon. Our own experience can only cover 
a narrow path though the wealth of existing knowledge. Last June,
the IEEE Software Editorial Board decided to make more room for 

guest editor’s introduction



By lucky coincidence, we had a large
backlog of experience reports and were able
to include six of them in this issue. On an on-
going basis, we hope to publish two or three
shorter experience reports per issue. I think
you’ll enjoy these interesting stories that are
typical of the challenges we all face in this in-
dustry. Even if you were to pick only one gem
from the experience of others, it might help
you, your project, and your company.

The first four articles address the topic of
Extreme Programming; the final two address
a different set of experiences from the field.

Extreme Programming in the real
world

Many methodologies have come and
gone. Only time will tell if one of the more
recent methodology innovations, Extreme
Programming, will have a lasting impact on
our way to build software systems. Like
other methodologies, XP is not the ultimate
silver bullet that offers an answer to all de-
velopment problems. But it has gained sig-
nificant momentum and an increasing num-
ber of software teams are ready to give it a
try. Our first article is not really an experi-
ence report but an interesting comparison of
XP with the more established Capability

Maturity Model. As one of the foremost ex-
perts on CMM, Mark Paulk offers an opin-
ion on XP as a lightweight methodology
from the perspective of the heavyweight
CMM. From my perspective, the difference
is not so much the “weight” of the method-
ology than the way they are introduced in
an organization. XP tends to be a grassroots
methodology. Developers and development
teams typically drive its introduction. This
becomes quite clear from reading the subse-
quent experience reports. CMM, on the
other hand, is typically introduced at the
corporate level and then deployed to devel-
opment teams. As in past “methodology
wars,” there are heated debates about the
pros and cons of the respective approaches.
I agree with Paulk that CMM and XP can
be considered complementary. To establish
lasting success, methodologies need buy-
in from management as well as from the 
developers.

Martin Fowler offers a few links to fur-
ther information about XP and agile meth-
ods in the “Web Resources” sidebar.

Two more reports
The last two articles in the set cover dis-

similar experiences, but they have one thing
in common: an account of our continuous
struggle to make software development
more efficient. The first article presents a
typical example of survival struggles in a
rapidly growing company and its attempts
to use process to get development activities
under control. The second article describes
a technique, called defect logging and defect
data analysis, that aims to decrease pro-
grammers’ repetitive errors. The author
picked one element of the Personal Software
Process and made it easier to apply.

1 8 I E E E  S O F T W A R E N o v e m b e r / D e c e m b e r  2 0 0 1

Martin Fowler, ThoughtWorks

Short introduction to XP:
www.cutter.com/ead/ead0002.html
www.rolemodelsoft.com/articles/xpCorner/xpDistilled.htm

XP portals:
www.xprogramming.com
www.extremeprogramming.org
www.rolemodelsoft.com/xp/index.htm
www.jera.com/techinfo/xpfaq.html

XP mailing lists:
http://groups.yahoo.com/group/extremeprogramming/
news:comp.software.extreme-programming

Introduction to agile methods:
http://martinfowler.com/articles/newMethodology.html

(All URLs current 15 Oct. 2001)

Extreme Programming and Agile Methods:
Web Resources

About the Author

Wolfgang Strigel is the founder and
president of Software Productivity Center, a con-
sulting and products company, and of QA Labs, a
contract testing company. His interests include
collaborative software development, process im-
provement, project estimation, testing, and soft-
ware engineering economics. He has a BSc in
mathematics from the Technical University, Mu-
nich, Germany, an MSc in computer science from

McGill University, and an MBA from Simon Fraser University. Contact him at
strigel@spc.ca.



focus

0 7 4 0 - 7 4 5 9 / 0 1 / $ 1 0 . 0 0  ©  2 0 0 1  I E E E N o v e m b e r / D e c e m b e r  2 0 0 1 I E E E  S O F T W A R E 19

In this article, I summarize both XP
and the SW-CMM, show how XP can help
organizations realize the SW-CMM goals,
and then critique XP from a SW-CMM
perspective.

The Software CMM
The Software Engineering Institute at

Carnegie Mellon University developed the
SW-CMM as a model for building organi-
zational capability, and it has been widely
adopted in the software community and be-
yond. As Table 1 shows, the SW-CMM is a
five-level model that describes good engi-
neering and management practices and pre-
scribes improvement priorities for software
organizations.

Although the SW-CMM is described in a
book of nearly 500 pages, the requirements
for becoming a Level 5 organization are
concisely stated in 52 sentences—the 52
goals of the model’s 18 key process areas
(KPAs). The practices, subpractices, and ex-
amples that flesh out the model can guide

software professionals in making reason-
able, informed decisions about a broad
range of process implementations.

The SW-CMM informative materials fo-
cus primarily on large projects and large or-
ganizations. With minor tailoring and com-
mon sense, however, the model can be
applied in radically different environments,
ranging from two- to three-person projects
in small start-up companies to 500-person
projects building hard real-time, life-critical
systems.2,3 The SW-CMM’s rating compo-
nents are intentionally abstract, capturing
“universal truths” about high-performance
software organizations. As a glance at Table
2 shows, the KPAs are clearly important to
all types of software organizations.

With the exception of software subcon-
tract management, which applies only to
organizations that do subcontracting, the
KPAs and their goals can apply to any soft-
ware organization. Companies that focus
on innovation more than operational excel-
lence might downplay the role of consis-

Extreme Programming
from a CMM Perspective

Mark C. Paulk, Software Engineering Institute

XP has good
engineering
practices that
can work well
with the CMM
and other highly
structured
methods. 
The key is 
to carefully
consider XP
practices and
implement them
in the right
environment.

E
xtreme Programming is an “agile methodology” that some people
advocate for the high-speed, volatile world of Internet and Web
software development. Although XP is a disciplined process, some
have used it in arguments against rigorous software process im-

provement models such as the Software Capability Maturity Model.1

reports from the field



tency, predictability, and reliability, but per-
formance excellence is important even in
highly innovative environments.

Extreme Programming
The XP method is typically attributed to

Kent Beck, Ron Jeffries, and Ward Cun-
ningham.4,5 XP’s target is small to medium-
sized teams building software with vague or
rapidly changing requirements. XP teams
are typically colocated and have fewer than
10 members.

XP’s critical underlying assumption is that
developers can obviate the traditional high
cost of change using technologies such as ob-
jects, patterns, and relational databases, re-
sulting in a highly dynamic XP process. Beck’s
book is subtitled Embrace Change, and XP
teams typically deal with requirements
changes through an iterative life cycle with
short cycles.

The XP life cycle has four basic activities:
coding, testing, listening, and designing. Dy-
namism is demonstrated through four values:

� continual communication with the cus-
tomer and within the team;

� simplicity, achieved by a constant focus
on minimalist solutions;

� rapid feedback through mechanisms
such as unit and functional testing; and

� the courage to deal with problems
proactively.

Principles in practice
Most of XP’s principles—minimalism, sim-

plicity, an evolutionary life cycle, user involve-
ment, and so forth—are commonsense prac-
tices that are part of any disciplined process.
As Table 3 summarizes, the “extreme” in XP
comes from taking commonsense practices to
extreme levels. Although some people may in-
terpret practices such as “focusing on a mini-
malist solution” as hacking, XP is actually a
highly disciplined process. Simplicity in XP
terms means focusing on the highest-priority,
most valuable system parts that are currently
identified rather than designing solutions to
problems that are not yet relevant (and might
never be, given that requirements and operat-
ing environments change).

Although developers might use many dif-
ferent XP practices, the method typically
consists of 12 basic elements:

� Planning game: Quickly determine the
next release’s scope, combining business
priorities and technical estimates. The cus-
tomer decides scope, priority, and dates
from a business perspective, whereas tech-
nical people estimate and track progress.

� Small releases: Put a simple system into
production quickly. Release new ver-
sions on a very short (two-week) cycle.

� Metaphor: Guide all development with
a simple, shared story of how the over-
all system works.

Table 1
An overview of the Software CMM

Level Focus Key process areas

5: Optimizing Continual process improvement Defect prevention
Technology change management
Process change management

4: Managed Product and process quality Quantitative process management
Software quality management

3: Defined Engineering processes and organizational support Organization process focus
Organization process definition
Training program
Integrated software management
Software product engineering
Intergroup coordination
Peer reviews

2: Repeatable Project management processes Requirements management
Software project planning
Software project tracking and oversight
Software subcontract management
Software quality assurance
Software configuration management

1: Initial Competent people (and heroics)

2 0 I E E E  S O F T W A R E N o v e m b e r / D e c e m b e r  2 0 0 1



� Simple design: Design as simply as pos-
sible at any given moment.

� Testing: Developers continually write
unit tests that must run flawlessly; cus-
tomers write tests to demonstrate that
functions are finished. “Test, then code”
means that a failed test case is an entry
criterion for writing code.

� Refactoring: Restructure the system
without changing its behavior to re-
move duplication, improve communica-
tion, simplify, or add flexibility.

� Pair programming: All production code
is written by two programmers at one
machine.

� Collective ownership: Anyone can im-

prove any system code anywhere at
any time.

� Continuous integration: Integrate and
build the system many times a day
(every time a task is finished). Continual
regression testing prevents functionality
regressions when requirements change.

� 40-hour weeks: Work no more than 40
hours per week whenever possible; never
work overtime two weeks in a row.

� On-site customer: Have an actual user on
the team full-time to answer questions.

� Coding standards: Have rules that empha-
size communication throughout the code.

These basic practices work together to cre-

N o v e m b e r / D e c e m b e r  2 0 0 1 I E E E  S O F T W A R E 21

Table 2
The Software CMM key process areas and their purposes

Key process area Purpose

Maturity Level 2: Repeatable

Requirements management Establish a common understanding between the customer and software project team about the customer’s 
requirements.

Software project planning Establish reasonable plans for software engineering and overall project management.
Software project tracking and oversight Provide adequate visibility into actual progress so that management can act effectively when the software 

project’s performance deviates significantly from the software plans.
Software subcontract management Select qualified software subcontractors and manage them effectively.
Software quality assurance Provide management with appropriate visibility into the product and the software process.
Software configuration management Establish and maintain the integrity of software products throughout the project’s software life cycle.

Maturity Level 3: Defined

Organization process focus Establish organizational responsibility for software process activities that improve the organization’s overall 
software process capability.

Organization process definition Develop and maintain a usable set of software process assets that improve process performance across the
projects and provide a basis for cumulative, long-term organizational benefits.

Training program Develop individuals’ skills and knowledge so that they can perform their roles effectively and efficiently.
Integrated software management Integrate the software engineering and management activities into a coherent, defined software process 

based on the organization’s standard software process and related process assets.
Software product engineering Consistently use a well-defined engineering process that integrates all the software engineering activities to 

produce correct, consistent software products effectively and efficiently.
Intergroup coordination Establish a way for the software engineering group to participate actively with other engineering groups so 

that the project can effectively and efficiently satisfy customer needs.
Peer reviews Remove defects from the software work products early and efficiently. An important corollary effect is to 

develop a better understanding of the software products and the preventable defects.

Maturity Level 4: Managed

Quantitative process management Quantitatively control the performance of the software project’s process. Software process performance 
represents the actual results achieved from following a software process.

Software quality management Quantify the quality of the project’s software products and achieve specific quality goals.

Maturity Level 5: Optimizing

Defect prevention Identify the cause of defects and prevent them from recurring.
Technology change management Identify new technologies (such as tools, methods, and processes) and introduce them into the organiza-

tion in an orderly manner.
Process change management Continually improve the organization’s software processes with the goal of improving software quality, 

increasing productivity, and decreasing the product-development cycle time.



ate a coherent method. XP characterizes the
full system functionality using a pool of “sto-
ries,” or short feature descriptions. For the
planning game and small releases, the cus-
tomer must select a subset of stories that char-
acterize the most desirable work for develop-
ers to implement in the upcoming release.
Because the customer can add new stories to
the pool at any time, requirements are highly
volatile. However, volatility is managed by
implementing functionality in two-week
chunks. Having a customer onsite supports
this ongoing cycle of two-week releases.

XP developers generate a metaphor to
provide the project’s overarching vision. Al-
though you could view this as a high-level
architecture, XP emphasizes design, while at
the same time minimizing design documen-
tation. Some people have characterized XP
as not allowing documentation outside
code, but that is not quite accurate. Because
XP emphasizes continual redesign—using
refactoring whenever necessary—there is lit-
tle value to detailed design documentation
(and maintainers rarely trust anything other
than the code anyway).

XP developers typically throw away design
documentation after the code is written, al-
though they will keep it if it’s useful. They also
keep design documentation when the cus-
tomer stops coming up with new stories. At
that point, it’s time to put the system in moth-
balls and write a five- to 10-page “mothball
tour” of the system. A natural corollary of the
refactoring emphasis is to always implement
the simplest solution that satisfies the immedi-
ate need. Requirements changes are likely to
supersede “general solutions” anyway.

Pair programming is one of XP’s more
controversial practices, mainly because it
has resource consequences for the very man-
agers who decide whether or not to let a
project use XP. Although it might appear

that pair programming consumes twice the
resources, research has shown that it leads
to fewer defects and decreased cycle time.6

For a jelled team, the effort increase can be
as little as 15 percent, while cycle time is re-
duced by 40 to 50 percent. For Internet-time
environments, the increased speed to mar-
ket may be well worth the increased effort.
Also, collaboration improves problem solv-
ing, and increased quality can significantly
reduce maintenance costs. When considered
over the total life cycle, the benefits of pair
programming often more than pay for
added resource costs.

Because XP encourages collective owner-
ship, anyone can change any piece of code in
the system at any time. The XP emphasis on
continuous integration, continual regression
testing, and pair programming protects
against a potential loss of configuration con-
trol. XP’s emphasis on testing is expressed in
the phrase “test, then code.” It captures the
principle that developers should plan testing
early and develop test cases in parallel with
requirements analysis, although the tra-
ditional emphasis is on black-box testing.
Thinking about testing early in the life cycle
is standard practice for good software engi-
neering, though it is too rarely practiced.

The basic XP management tool is the
metric, and the metric’s medium is the “big
visible chart.” In the XP style, three or four
measures are typically all a team can stand
at one time, and those should be actively
used and visible. One recommended XP
metric is “project velocity”—the number of
stories of a given size that developers can
implement in an iteration.

Adoption strategies
XP is an intensely social activity, and not

everyone can learn it. There are two conflict-
ing attitudes toward XP adoption. XP is gen-

2 2 I E E E  S O F T W A R E N o v e m b e r / D e c e m b e r  2 0 0 1

Table 3
The “extreme” in Extreme Programming

Commonsense XP extreme XP implementation practice

Code reviews Review code all the time Pair programming
Testing Test all the time, even by customers Unit testing, functional testing
Design Make design part of everybody’s daily business Refactoring
Simplicity Always work with the simplest design that supports the system’s The simplest thing that could possibly work

current functionality
Architecture Everybody works to refine the architecture all the time Metaphor
Integration testing Integrate and test several times a day Continuous integration
Short iterations Make iterations extremely short—seconds, minutes, and hours Planning game

rather than weeks, months, and years



erally viewed as a system that demonstrates
emergent properties when adopted as a
whole. As the discussion thus far shows, there
are strong dependencies between many XP
practices, such as collective ownership and
continuous integration.

Nonetheless, some people recommend
adopting XP one practice at a time, focusing
on the team’s most pressing current prob-
lem. This is consistent with the attitude to-
ward change that XP is “just rules” and the
team can change the rules anytime as long
as they agree on how to assess the change’s
effects. Beck, for example, describes XP
practices as “etudes”: They help developers
master the techniques, but experienced
users can modify them as necessary.

XP and the CMM
The SW-CMM focuses on both the man-

agement issues involved in implementing ef-
fective and efficient processes and on system-
atic process improvement. XP, on the other
hand, is a specific set of practices—a “method-
ology”—that is effective in the context of
small, colocated teams with rapidly changing
requirements. Taken together, the two meth-
ods can create synergy, particularly in con-
junction with other good engineering and
management practices. I’ll now illustrate this
by discussing XP practices in relation to the
CMM KPAs and goals outlined in Table 2.

XP and Level 2 practices
XP addresses Level 2’s requirements man-

agement KPA through its use of stories, an
onsite customer, and continuous integration.
Although system requirements might evolve
dramatically over time, XP integrates feed-
back on customer expectations and needs 
by emphasizing short release cycles and con-
tinual customer involvement. “Common un-
derstanding” is established and maintained
through the customer’s continual involvement
in building stories and selecting them for the
next release (in effect, prioritizing customer
requirements).

XP addresses software project planning
in the planning game and small releases.
XP’s planning strategy embodies Watts
Humphrey’s advice, “If you can’t plan well,
plan often.” The first three activities of this
KPA deal with getting the software team in-
volved in early planning. XP integrates the
software team into the commitment process

by having it estimate the effort involved to
implement customer stories; at the level of
two-week releases, such estimates are typi-
cally quite accurate. The customer maintains
control of business priorities by choosing
which stories to implement in the next release
with the given resources. By definition, the
XP life cycle is both incremental and evolu-
tionary. The project plan is not detailed for
the project’s whole life cycle, although the
system metaphor does establish a vision for
project direction. As a result, developers can
identify and manage risks efficiently.

XP addresses software project tracking
and oversight with the “big visual chart,”
project velocity, and commitments (stories)
for small releases. XP’s commitment process
sets clear expectations for both the customer
and the XP team at the tactical level and max-
imizes flexibility at the project’s strategic
level. The emphasis on 40-hour weeks is a
general human factors concern; although
CMM does not address it, having “rational
work hours” is usually considered a best
practice. XP also emphasizes open work-
spaces, a similar “people issue” that is outside
CMM’s scope. XP does not address software
subcontract management, which is unlikely to
apply in XP’s target environment.

While an independent software quality
assurance group is unlikely in an XP cul-
ture, SQA could be addressed by the pair-
programming culture. Peer pressure in an
XP environment can achieve SQA’s aim of
assuring conformance to standards, though
it does not necessarily give management vis-
ibility into nonconformance issues. Dealing
with process and product assurance using
peer pressure can be extraordinarily effec-
tive in a small team environment. However,
larger teams typically require more formal
mechanisms for objectively verifying adher-
ence to requirements, standards, and proce-
dures. Also, peer pressure might be ineffec-
tive when the entire team is being pushed,
just as a software manager might be vulner-
able to external pressure. This vulnerability
should be addressed at the organizational
level when considering SQA.

Although not completely and explicitly
addressed, software configuration manage-
ment is implied in XP’s collective ownership,
small releases, and continuous integration.
Collective ownership might be problematic
for large systems, where more formal com-

N o v e m b e r / D e c e m b e r  2 0 0 1 I E E E  S O F T W A R E 23

Taken together,
the two methods

can create
synergy,

particularly 
in conjunction

with other good
engineering and

management
practices.



munication channels are necessary to pre-
vent configuration management failures.

XP and Level 3 practices
At Level 3, XP addresses organization

process focus at the team rather than organi-
zational level. A focus on process issues is
nonetheless implied in adopting XP one prac-
tice at a time, as well as in the “just rules” phi-
losophy. Because XP focuses on the software
engineering process rather than organiza-
tional infrastructure issues, organizations
adopting XP must address this and other or-
ganization-level processes, whether in a
CMM-based context or not.

Similarly, the various XP-related books,
articles, courses, and Web sites partially ad-
dress the organization process definition
and training program KPAs, but organiza-
tional assets are outside the scope of the XP
method itself. As a consequence, XP cannot
address integrated software management
because there may not be any organiza-
tional assets to tailor.

Several XP practices effectively address
software product engineering: metaphor,
simple design, refactoring, the “mothball”
tour, coding standards, unit testing, and
functional testing. XP’s de-emphasis of de-
sign documentation is a concern in many en-
vironments, such as hard real-time systems,

large systems, or virtual teams. In such envi-
ronments, good designs are crucial to suc-
cess, and using the refactoring strategy
would be high-risk. For example, if develop-
ers performed refactoring after a technique
such as rate-monotonic analysis proved that
a system satisfied hard real-time require-
ments, they’d have to redo the analysis. Such
an environment invalidates XP’s fundamen-
tal assumption about the low cost of change.

XP’s emphasis on communication—
through onsite customers and pair program-
ming—appears to provide as comprehensive
a solution to intergroup coordination as in-
tegrated product and process development.
In fact, XP’s method might be considered an
effective IPPD approach, although the soft-
ware-only context ignores multidiscipline
environments.

Pair programming addresses peer reviews,
and is arguably more powerful than many
peer review techniques because it adopts pre-
ventive concepts found in code reading and
literate programming. However, pair pro-
gramming’s relative lack of structure can
lessen its effectiveness. Empirical data on pair
programming is currently sparse but promis-
ing.6 To make informed trade-off decisions,
we’ll need more empirical research that con-
trasts and compares pair programming and
peer review techniques, especially more rig-
orous techniques such as inspections.

Beyond Level 3
XP addresses few of the Level 4 and 5

KPAs in a rigorous statistical sense, al-
though feedback during rapid cycles might
partially address defect prevention. Table 4
summarizes XP’s potential to satisfy CMM
KPAs, given the appropriate domain.

Many of the KPAs that XP either ignores
or only partially covers are undoubtedly ad-
dressed in real projects. XP needs manage-
ment and infrastructure support, even if it
does not specifically call for it.

Discussion
As the earlier comparison shows, XP gen-

erally focuses on technical work, whereas the
CMM generally focuses on management is-
sues. Both methods are concerned with “cul-
ture.” The element that XP lacks that is crucial
for the SW-CMM is the concept of “institu-
tionalization”—that is, establishing a culture
of “this is the way we do things around here.”

2 4 I E E E  S O F T W A R E N o v e m b e r / D e c e m b e r  2 0 0 1

Table 4
XP satisfaction of key process areas, given the 

appropriate environment
Level Key process area Satisfaction

2 Requirements management ++
2 Software project planning ++
2 Software project tracking and oversight ++
2 Software subcontract management —
2 Software quality assurance +
2 Software configuration management +
3 Organization process focus +
3 Organization process definition +
3 Training program —
3 Integrated software management —
3 Software product engineering ++
3 Intergroup coordination ++
3 Peer reviews ++
4 Quantitative process management —
4 Software quality management —
5 Defect prevention +
5 Technology change management —
5 Process change management —

+ Partially addressed in XP
+ + Largely addressed in XP (perhaps by inference)
— Not addressed in XP



Although implicit in some practices, such as
the peer pressure arising from pair program-
ming, XP largely ignores the infrastructure
that the CMM identifies as key to institution-
alizing good engineering and management
practices. Table 5 summarizes XP’s coverage
of institutionalization in its domain.

The CMM’s KPAs share common fea-
tures that implement and institutionalize
processes. Each KPA’s institutionalization
practices map to the area’s goals; a naïve XP
implementation that ignored these infra-
structure issues would fail to satisfy any
KPA. XP ignores some of these practices,
such as policies. XP addresses others, such
as training and SQA, by inference. It ad-
dresses still others—project-specific prac-
tices such as management oversight and
measurement—to a limited degree. As an
implementation model focused on the devel-
opment process, these issues are largely out-
side XP’s focus, but they are arguably cru-
cial for its successful adoption.

Size matters
Much of the formalism that characterizes

most CMM-based process improvement is
an artifact of large projects and severe relia-
bility requirements, especially for life-critical
systems. The SW-CMM’s hierarchical struc-
ture, however, is intended to support a range
of implementations through the 18 KPAs
and 52 goals that comprise the requirements
for a fully mature software process.

As systems grow, some XP practices be-
come more difficult to implement. XP is, af-
ter all, targeted toward small teams working
on small to medium-sized projects. As proj-
ects become larger, emphasizing a good 
architectural “philosophy” becomes increas-
ingly critical to project success. Major invest-
ment in product architecture design is one of
the practices that characterizes successful In-
ternet companies.7

Architecture-based design, designing for
change, refactoring, and similar design
philosophies emphasize the need to manage
change systematically. Variants of the XP
bottom-up design practices, such as architec-
ture-based design, might be more appropri-
ate in large-project contexts. In a sense, ar-
chitectural design that emphasizes flexibility
is the goal of any good object-oriented
methodology, so XP and object orientation
are well suited to one another. Finally, large

projects tend to be multidisciplinary, which
can be problematic given that XP is aimed at
software-only projects.

Why explore XP?
Modern software projects should capture

XP values, regardless of how radically their
implementation differs from XP’s. Organiza-
tions might call communication and simplic-
ity by other names, such as coordination and
elegance, but without these values, nontrivial
projects face almost insurmountable odds.

XP’s principles of communication and
simplicity are also fundamental for organi-
zations using the SW-CMM. When defining
processes, organizations should capture the
minimum essential information needed,
structure definitions using good software
design principles (such as information hid-
ing and abstraction), and emphasize useful-
ness and usability.2

For real-time process control, rapid feed-
back is crucial. Previous eras have captured
this idea in aphorisms such as “don’t throw
good money after bad”; in a quantitative
sense, we can view this as the soul of the
CMM’s Level 4. One of the consequences of
the cultural shift between Levels 1 and 2 is
the need to demonstrate the courage of our
convictions by being realistic about esti-
mates, plans, and commitments.

False opposition
The main objection to using XP for

process improvement is that it barely
touches the management and organizational
issues that the SW-CMM emphasizes. Im-
plementing the kind of highly collaborative
environment that XP assumes requires en-
lightened management and appropriate or-
ganizational infrastructure.

N o v e m b e r / D e c e m b e r  2 0 0 1 I E E E  S O F T W A R E 25

Table 5
XP and institutionalization practices

Common feature (in each KPA) Practice Satisfaction

Commitment to perform Policy —
Leadership and sponsorship —

Ability to perform Organizational structures +
Resources and funding +
Training +

Measurement and analysis Measurement +
Verifying implementation Senior management oversight —

Project management oversight ++
Software quality assurance +

+ Partially addressed in XP
+ + Largely addressed in XP (perhaps by inference)
— Not addressed in XP



The argument that CMM’s ideal of a rig-
orous, statistically stable process is antithet-
ical to XP is unconvincing. XP has disci-
plined processes, and the XP process itself is
clearly well defined. We can thus consider
CMM and XP complementary. The SW-
CMM tells organizations what to do in gen-
eral terms, but does not say how to do it.
XP is a set of best practices that contains
fairly specific how-to information—an im-
plementation model—for a particular type
of environment. XP practices can be com-
patible with CMM practices (goals or
KPAs), even if they do not completely ad-
dress them.

M ost of XP consists of good prac-tices that all organizations shouldconsider. While we can debate the
merits of any one practice in relation to
other options, to arbitrarily reject any of
them is to blind ourselves to new and po-
tentially beneficial ideas.

To put XP practices together as a
methodology can be a paradigm shift simi-
lar to that required for concurrent engineer-
ing. Although its concepts have been around
for decades, adopting concurrent engineer-
ing practices changes your product-building
paradigm. XP provides a systems perspec-
tive on programming, just as the SW-CMM
provides a systems perspective on organiza-
tional process improvement. Organizations
that want to improve their capability should
take advantage of the good ideas in both,
and exercise common sense in selecting and
implementing those ideas.

Should organizations use XP, as pub-
lished, for life-critical or high-reliability 
systems? Probably not. XP’s lack of design 
documentation and de-emphasis on architec-
ture are risky. However, one of XP’s virtues is
that you can change and improve it for dif-
ferent environments. That said, when you
change XP, you risk losing the emergent
properties that provide value in the proper
context. Ultimately, when you choose and
improve software processes, your emphasis
should be to let common sense prevail—and
to use data whenever possible to offer insight
on challenging questions.

Acknowledgments
I gratefully acknowledge Kent Beck, Steve Mc-

Connell, and Laurie Williams for their comments. I
presented an earlier version of this article at XP Uni-
verse in July 2001.

References
1. M.C. Paulk et al., The Capability Maturity Model:

Guidelines for Improving the Software Process, Addi-
son-Wesley, Reading, Mass., 1995.

2. M.C. Paulk, “Using the Software CMM with Good
Judgment,” ASQ Software Quality Professional, vol. 1,
no. 3, June 1999, pp. 19–29.

3. D.L. Johnson and J.G. Brodman, “Applying CMM Pro-
ject Planning Practices to Diverse Environments,” IEEE
Software, vol. 17, no. 4, July/Aug. 2000, pp. 40–47.

4. K. Beck, Extreme Programming Explained: Embrace
Change, Addison-Wesley, Reading, Mass., 1999.

5. “eXtreme Programming Pros and Cons: What Ques-
tions Remain?” IEEE Computer Soc. Dynabook, J. 
Siddiqi, ed., Nov. 2000; www.computer.org/seweb/
dynabook/index.htm (current 24 Sept. 2001).

6. L. Williams et al., “Strengthening the Case for Pair Pro-
gramming,” IEEE Software, vol. 17, no. 4, July/Aug.
2000, pp. 19–25.

7. A. MacCormack, “Product-Development Practices that
Work: How Internet Companies Build Software,” MIT
Sloan Management Rev., no. 42, vol. 2, Winter 2001,
pp. 75–84.

For more information on this or any other computing topic, please visit our
Digital Library at http://computer.org/publications/dlib.

2 6 I E E E  S O F T W A R E N o v e m b e r / D e c e m b e r  2 0 0 1

Mark C. Paulk is a senior member of the
technical staff at the Software Engineering Insti-
tute. His current interests include high-maturity
practices and statistical control for software
processes. He was “book boss” for Version 1.0 of
the Capability Maturity Model for Software and
project leader during the development of Soft-
ware CMM Version 1.1. He is also involved with
software engineering standards, including ISO

15504, ISO 12207, and ISO 15288. He received his bachelor’s degree in
mathematics and computer science from the University of Alabama in
Huntsville and his master’s degree in computer science from Vanderbilt Uni-
versity. Contact him at the Software Engineering Inst., Carnegie Mellon
Univ., Pittsburgh, PA 15213; mcp@sei.cmu.edu.

About the Author



focus

0 7 4 0 - 7 4 5 9 / 0 1 / $ 1 0 . 0 0  ©  2 0 0 1  I E E E N o v e m b e r / D e c e m b e r  2 0 0 1 I E E E  S O F T W A R E 27

Through a systems engineering effort, DPS
divided the system into subsystems, each of
which was later designed by a development
team. I was brought in to help one of the
teams start its part of the project using iter-
ative development, use cases, and object-
oriented design techniques.

Shortly after starting this project, I spent
a week in XP Immersion I, a one-week in-
tensive training class in the techniques and
philosophy of Extreme Programming.1 En-
thused by XP, I talked to the DPS team
about applying some XP practices to our
work. The company had already decided to
try iterative development and OO tech-
niques, departing from the division’s stan-
dard process. They were game for some-
thing different, but how different? We
decided to take the idea of incorporating
XP practices to the director of engineering.

What we were facing
The standard practice in the division was

created in response to past and current
problems. Most of the developers had one
to three years’ experience. The more senior
developers had the role of reviewing and
approving the work of the less experienced
developers. To their credit, the review team
took the formal review process very seri-
ously. They were good at it: they captured
issues and defects on the Web, prepared re-
viewers, and held crisp review meetings.

In the world of Big Design Up Front and
phase containment, these guys were good. I
could end this article right now except for
one problem: all this process added over-
head to the development of software. To de-
sign something, they needed to figure out
the design, document it in Rose, schedule a
review meeting, and distribute materials to

Launching Extreme
Programming at a Process-
Intensive Company 

James Grenning, Object Mentor

A company that
has traditional
formal
processes
launched a
project using
many Extreme
Programming
practices. The
author covers
how XP was
proposed to
management,
how the project
seed began and
grew, and some
of the issues 
the team faced
during its first
six months.

T
his is a story about starting a project using an adaptation of XP in
a company with a large formal software development process. De-
fined Process Systems is a big company (the name is fictitious). The
division I worked with was developing safety-critical systems and

was building a new system to replace an existing legacy product. The project
was an embedded-systems application running on Windows NT and was part
of a network of machines that had to collaborate to provide services. 

reports from the field



review. Then reviewers had to review the
materials and enter issues on the Web, have
a review meeting, document the meeting
outcome, repair the defects and then close
them on the Web, fix documents, and have
the changes reviewed again.

However, all this process work was not
keeping bugs out of the product. Unrealistic
deadlines and surprises late in the project
were taking their toll. Products were deliv-
ered late. Engineers were just getting their
skills to a decent technical depth, but they
were also burning out and heading for sys-
tems engineering or management.

The team struggled with how to begin the
new project. Its requirements were in prose
format and fully understood only by the per-
son who wrote them. 

To summarize: the existing process had a
lot of overhead, deadlines were tight, engi-
neers were running away, requirements were
partially defined, and they had to get a project
started. With all these issues, something had to
change. Did they need something extreme like
XP? I believed and hoped XP would help.

Choosing your battles
The DPS culture values up-front require-

ments documents, up-front designs, reviews,
and approvals. Writing the product features
on note cards, not doing any up-front de-
sign, and jumping into coding were not go-
ing to be popular ideas. To people unfamil-
iar with XP, this sounded a lot like hacking.
How did we get by these objections?

Having been introduced to XP, the group
understood what the main objections would
be as we tried to sell XP to the management
team. Like good lawyers, we prepared an-
ticipated questions along with their answers
for our presentation. We expected that the
decision makers would consider some of the
practices dangerous and unworkable at
DPS. The need for documentation was in-
grained in the culture, so we expected con-
cern over XP’s lack of formal documenta-
tion. Can the code be the design? Can we
really build a product without up-front de-
sign? What if there is thrashing while refac-
toring? What about design reviews? 

To paraphrase Kent Beck, one of XP’s
originators, “do all of XP before trying to
customize it.” I think that is great advice,
but for this environment we would never
have gotten the okay to mark up the first in-

dex card. We decided to choose our battles.
We needed to get some of the beneficial
practices into the project and not get hurt
by leaving other practices behind. We did
not omit practices that we didn’t feel like
doing; we tried to do as many as we could.
We used the practices and their interactions
as ways to sell around the objections.

We started by identifying the project’s
goals—to build a working product with re-
liable operation and timely delivery, with
enough documentation to enable effective
maintenance (no more, no less), and with
understandable source code. This was as
objectionable as motherhood and apple pie.
The standard process would identify the
same objectives.

We all agreed that a reliable working
product was a critical output of the project.
This was particularly important, as this was
a safety-critical system. A tougher question
was, what was enough documentation? This
was where it got interesting. This applica-
tion was not your typical XP target applica-
tion—it was part of a larger system that mul-
tiple groups were developing at multiple
sites. These other groups were using the
standard, waterfall-style DPS process, not
XP or short-iteration development. We had
a potential impedance mismatch between
the XP team and the rest of the project. 

How much documentation?
Proposing no documentation would end

the conversation. We decided to keep the
conversation going and answer a question
with a question. What did we want from
our documentation? We needed

� enough documentation to define the
product requirements, sustain technical
reviews, and support the system’s main-
tainers;

� clean and understandable source code;
and

� some form of interface documentation,
due to the impedance mismatch be-
tween groups. 

These answers did not all align with XP out of
the book, but they kept the conversation go-
ing. XP is not antidocumentation; it recog-
nizes that documentation has a cost and that
not creating it might be more cost-effective.
This, of course, violates conventional wisdom.

2 8 I E E E  S O F T W A R E N o v e m b e r / D e c e m b e r  2 0 0 1

The need for
documentation
was ingrained
in the culture,

so we expected
concern over

XP’s lack 
of formal

documentation.



After acknowledging and addressing the
project’s objectives, I led the team through
the cost-of-change pitch from Beck’s Ex-
treme Programming Explained.1 The direc-
tor, the manager, and some senior technol-
ogists agreed that XP addressed many of
their current development problems. They
also thought XP right out of the book
would not work for them. What did we
want to do differently?

Documentation and reviews were going
to be the big roadblocks. We heard, “Re-
quirements on note cards!?!” “I can’t give a
stack of note cards to the test team.” “Bob
in firmware needs the cards too.” “Someone
will lose the cards.” I noticed that the com-
pany’s “standard” process allowed use cases
in the form described by Alistair Cock-
burn.2 This is a text-based method, similar
to user stories but with more details. Other
DPS groups needed to look at the use cases,
so we decided not to fight that battle—we
had enough lined up already. We decided to
use use cases.

Another objection was “We need docu-
mentation for the future generations of en-
gineers that will maintain the product we
are building. We need our senior people to
look at the design to make sure your design
will work.” Our answer was that a good
way to protect future software maintainers
is to provide them with clean and simple
source code, not binders full of out-of-date
paper. Maintainers always go to the source
code; it cannot lie, as documents can. XP re-
lies on the source code being simple and ex-
pressive and uses refactoring to keep it that
way.3 The source code is the most important
part of the design. At some point, the main-
tainers will need a high-level document to
navigate the system.

A follow-on objection was that what one
person thinks is readable source code is not
to another. XP addresses this through pair
programming. If a pair works hard at mak-
ing the source readable, there is a really
good chance that a third programmer who
sees the code will find it readable, too. Plus,
with collective code ownership, anyone can
change the source code if need be.

Another expected objection was that code
was not enough—we needed a documenta-
tion or transition strategy for whenever a
project is put on the shelf or transferred to
another team. Ideally, the documentation

given to the maintainers describes the state
of the software at the time it was shelved or
transferred. This document can and should
be written in a way that avoids needing fre-
quent modification. As maintenance pro-
ceeds, the document will likely be neglected.
Make the document high-level enough so
that the usual maintenance changes and bug
fixes do not affect it. Let the documentation
guide the future developers to the right part
of the code—then they can use the high-
quality, readable, simple source code to
work out the details. 

Following this strategy will not result in a
huge document. Remember, you have some
of the best detailed documentation available
in the form of automated unit tests—work-
ing code examples of exactly how to use
each object in the system. XP does not pro-
hibit documentation; just realize it has a cost
and make sure it is worth it. You can plan
documentation tasks into any iteration. The
advice here is to document what you have
built, not what you anticipate building.

The next follow-on objection was that
we’d never do the document at the end of
the project. My reply: So, you would rather
do a little bit at a time, and have to keep
changing and rewriting it? Doesn’t that
sound like it would take a lot of time? It
does! So the management team must stick to
its guns and do the high-level documenta-
tion task at the end of the project. Pay for it
with less time wasted during development. 

Reviews
The director did not completely buy into

documenting only at the end of the project
and hence made a final objection: “I still
have my concerns. What if the design is no
good? Do I have to wait until the end of the
project to find out?” Pair programming was
not reason enough for the management team
to give up its review process. The big prob-
lem with DPS’s development process was that
it guaranteed that development would go
slowly. The process went something like this:
create a design, document it in Rose, sched-
ule a review meeting, distribute the materials,
have everyone review the materials, have the
review meeting, collect the issues, fix the is-
sues, maybe do another review, then finally
write some code to see if the design works.
This made sure the cost of change was high.
Because of the safety-critical nature of the 

N o v e m b e r / D e c e m b e r  2 0 0 1 I E E E  S O F T W A R E 29

Documentation
and reviews

were going to
be the big

roadblocks.



application, the management team was not
willing to give up on reviews.

I proposed a more efficient way to do the
reviews: the ask-for-forgiveness (rather than
ask-for-permission) design process. We let
the development team work for a month at
a time on the system. At the end of the
month, they assembled and reviewed a de-
sign-as-built review package. This took the
review off the critical path, so the review
process did not slow down the team. We
agreed to document the significant designs
within the iteration and review them with
the review team. We gave the remaining is-
sues that the reviewers found to the next it-
eration as stories. The idea here was to
spend a small amount of time in the itera-
tion documenting the design decisions that
month. As it turned out, we really did not
have to ask for forgiveness at all.

It’s not about XP
It’s about building better software pre-

dictably and faster. We compromised on a
number of issues, but we did have agreement
to use most of the XP practices: test-first pro-
gramming, pair programming, short itera-
tions, continuous integration, refactoring,
planning, and team membership for the cus-
tomer. Table 1 describes the XP practices we
used and how we modified them to suit our
needs. We added some process and formality:
use cases, monthly design reviews, and some
documentation. This adaptation of XP was a
significant change in how DPS developed
software, and we hoped to prove it was an
improvement. The director thought XP of-
fered a lot of promise for a better way to
work that could lead to improved quality,
faster development, better predictability, and
more on-the-job satisfaction (“smiles per
hour”). He said we were “making footprints
in the sand.” If we could improve one devel-
opment factor—quality, job satisfaction, pro-
ductivity, or predictability—the director
thought it might be worth doing XP. If we
could improve any two of those factors, he
thought there would be a big payoff.

The problems of poor quality, delivery de-
lays, long delivery cycles, and burned-out en-
gineers plague the software industry. XP
struck a cord with our team leaders. The tech-
niques appeared to address some of the prob-
lems the team was facing, and the focus on
testing and pair programming could help it

build a high-quality product. XP’s iterative na-
ture could help the team determine how fast it
could go and give management the feedback it
needed. So, it’s about getting the job done. XP
is a set of techniques that seemed promising.

Projects can get stalled in the fuzzy front
end.4 This is a problem, especially in water-
fall projects, where you must define all the
requirements prior to starting the design
process. In XP, as soon as you have defined
a couple weeks’ worth of user stories, devel-
opment can start. Think of how hard it is to
shave a month off the end of a project. Now
think of how easy it would be to save that
month just by starting development as soon
as you have identified a month’s worth of
stories. Throughout the rest of the project,
story development occurs concurrently with
story implementation. 

The first iteration
The team had three people—a customer

and two developers, including me. We started
by getting the unit test tool CppUnit set up
and integrated with our development envi-
ronment, VC++. This did not take long—the
tools are relatively easy to use.

The project’s customer (our systems engi-
neer) gave us a requirements document. As
we identified a functional requirement, we
wrote it on a card. Each card named a use
case. We did not bother to elaborate the use
cases, just name them; in a few days, we had
identified 125 use cases. Picking the most im-
portant ones was relatively easy using this list.

In XP, the customer chooses the most
valuable user stories and discusses them
with the programmers. We were using use
cases, a similar idea; our customer chose the
most valuable use cases and elaborated
them. For the early iterations, we decided to
ignore use case extensions (which hold the
special cases or variations) and keep the
product definition simple.2 We just assumed
there were no special cases or error cases;
because we were using XP, we believed we
could ignore the details and not be penal-
ized later. We also did not bother using use
case diagrams, because they did not add any
value to the development team. Our main
goal in the first iteration was to build a lit-
tle bit of the product, get some experience,
and build some skill and confidence.

At the beginning of a project, you need to
believe that the design can and will evolve.

3 0 I E E E  S O F T W A R E N o v e m b e r / D e c e m b e r  2 0 0 1

At the beginning
of a project, 
you need to

believe that the
design can and

will evolve.



Otherwise, the desire to do up-front specifica-
tion work will put the team into analysis
paralysis. Knowing that the design can evolve
sets the team free to start building the system
as soon as some functionality is identified.
Simplifying assumptions keep complexity out
of the code, at least temporarily. John Gall
wrote, “A complex system that works is in-
variably found to have evolved from a simple
systems that works.”5 Thus, it really helps to
make these simplifying and scope-limiting de-
cisions in each iteration. This lets the core fea-
tures drive the design.

We had a planning meeting and discussed
the features that were to be included in the
first iteration. My partner and I had OOD
experience but no real XP experience (except
for the week I spent in class). We wanted a
guide to our first iteration, so we spent

about half a day with a whiteboard looking
at design ideas. Because we were unsure of
XP, we were not sure if we could really start
coding without doing some design. We did a
little bit of design, or as we called it a Little
Design Up Front (LDUF). Ron Jeffries calls
this a Quick design session.6

In our LDUF session, we found a group
of collaborating objects we thought would
meet the needs of our first stories. We
worked from hand-drawn copies, not both-
ering with a diagramming or CASE tool. We
knew things would change, and we did not
want to spend our time making it look
pretty. It gave us confidence and a vision of
where we were going.

During the iteration planning meeting
and our LDUF session, we identified some of
the interfaces we needed to support the iter-

N o v e m b e r / D e c e m b e r  2 0 0 1 I E E E  S O F T W A R E 31

Table 1
Summary of XP practices used

XP practice Adoption status Our experience

Planning game Partially adopted The team practiced scope limiting, task breakdown, and task sign-up techniques. We 
used use cases rather than user stories. We wrote the use cases from an existing 
requirements document and stored them in a database. 

Small releases Adopted Iterations were one month long each.
Metaphor Not adopted A metaphor had not yet evolved, and we didn’t develop one. Instead, a high-level 

design evolved and was recorded in a small set of UML diagrams and explanatory 
text. It played the role of our metaphor.

Simple design Adopted The design did not have anticipatory elements. A monthly design-as-built review
let the senior people monitoring the project see the team’s design decisions.

Functional testing Adopted We developed functional tests in a custom scripting language. The tests demonstrated 
that the application logic met the customer’s need. However, the team got behind on 
automated acceptance tests. This is not recommended.

Test-first design Adopted We wrote the first line of production code using test-first design. We wrote the program in 
C++ and used CppUnit 1.5 as our unit test tool.

Refactoring Adopted We refactored regularly, and the design evolved smoothly.
Pair programming Adopted We could develop tests, interfaces, and simulations on our own but used pair programming 

to create production code.
Collective ownership Adopted We collectively owned the code. On one occasion, new code that required special 

knowledge resulted in a module owned by one programmer. Development slowed on 
that part of the system.

Continuous integration Adopted During our first iteration, continuous integration was no problem. As soon as we added a 
second pair to the project, the team had integration problems. We quickly learned how 
to avoid collisions and do merges.

40-hour week (also known as Adopted The team worked at a sustainable pace. A couple times, we put in overtime to meet the 
sustainable pace) iteration goals.
On-site customer Partially adopted A systems engineer acted as the on-site customer. We derived our acceptance tests from the

use cases. The customer was not directly responsible for these tests. The team scared the
customer a couple times by delivering his use cases in less time than it took him to define
them.

Coding standards Adopted The main coding standard was “make the code look like the code that is already there.” 
The team used header and C++ file source templates to provide the company-required 
comment blocks. The comment blocks were mainly noise that hid the code.

Open workspace Not adopted There was no open workspace. Workstations were in the corners, making pair 
programming awkward. The roadblocks to building a team workspace were political.



ation 1 features. These acted as placeholders
for the real hardware. We then added a sim-
ulation of the interface by creating a Mock
Object (see Figure 1).7 This facilitated devel-
opment and also kept volatile entities such
as the database schema, GUI, hardware de-
pendencies, and protocol from creeping into
the application logic. We witnessed an ex-
ample of designing for testability leading to
reduced coupling in the application.

So we sat down to write our first line of
code. We picked a candidate class from our
hand-drawn LDUF diagram and followed
the test-first design process. Figure 2 repre-
sents the test-first design process, XP’s in-
nermost feedback loop. Our first line of
code was a test. The test did not compile.
We fixed the compile. We ran the test. It
failed. We fixed the test. We were finally in
maintenance!

We also established pair-programming
guidelines. We could develop test cases, sim-
ulators, and interface classes on our own
but had to do all other production code in
pairs. The first iteration had significant
downtime, considering only one pair was

working on the project. Meetings were a
real productivity killer. Pair programming
was fun and intense; we were able to stay on
task. If one partner lost track of where we
were going, the other partner quickly re-
synched their efforts. We taught each other
about the tools and learned new skills.

The coding standard is a team agreement
to make the code look the same—it is the
team’s code, so it should all look the same. It
turned out that my partner and I had a com-
patible coding style. As more people joined
the team, we established a self-documenting
coding standard: “Make the new or modified
code look like the code that is already there.”
Cool, a one-line coding standard! However,
there was pressure to use the division’s coding
standard. From a “choosing your battles”
point of view, we gave into using the standard
comment blocks in front of each function. 

Later iterations
We planted a seed of functionality at the

center of this subsystem application and
simulated its interactions with its environ-
ment. We brought in new stories that made
the seed grow, incrementally adding new
functionality and complexity. From its sim-
ple beginning of a half dozen classes and a
few simulations, the clean, loosely coupled
design evolved to about 100 classes.

Unit test volume grew. The unit tests saved
us numerous times from unexpected side-ef-
fect defects. We could fix the defects immedi-
ately because we just added the code that
broke the tests. 

We created our own acceptance test script-
ing language to drive transactions into the sys-
tem and used text file comparisons of simula-
tion output to confirm system operation. We
were able to design simulations that stressed
the system beyond the limits expected in the
field. Unfortunately, the team got behind in ac-
ceptance testing. I do not recommend this.

Evolutionary design
Evolutionary design relieved a lot of

pressure from the team. We didn’t have to
create the best design of all time for things
we were not quite sure about—only the best
design for what we knew about at that mo-
ment. We made good design decisions one
at a time. Our automated tests and refactor-
ing gave us the confidence that we could
continue to evolve the system. 

3 2 I E E E  S O F T W A R E N o v e m b e r / D e c e m b e r  2 0 0 1

Application
logic

Service Interface
class

Service
implementation

Simulated (mock)
service

implementation

Hardware
API

<<extends>>

Figure 1. Simulating
the interface by 
creating a Mock 
Object, shown in 
Unified Modeling 
Language.

Write a
test for new

feature

Compile

Fix
compile
errors

Write
the

code

Run the
test and

see it fail

Run the
test and

see it pass

Refactor
as needed

Figure 2. The 
test-first design
process. 



Object-oriented design is an important
supporting practice of XP. OO program-
ming languages let you build software in
small independent pieces, a practice that
test-first programming promotes.8

Project manager surprises
Not only were the programmers happy

with their creation, but the project manager
was as well. After the fourth iteration, he said,
“I’d only have three documents by now! In-
stead I have a piece of the system that works!”

The manager discovered another benefit.
Usually a project manager coordinating a
team’s work with other teams spends a lot of
time juggling priorities and figuring out task
dependencies. On the XP team, dependencies
between features were almost nonexistent.
We built features in the order of customer
priority, not internal software framework or-
der dictated by a BDUF (Big Design Up Front).
The team was agile and able to adapt to the
other subsystems’ changing needs.

Building the team
We built the team slowly, while we were

developing skills. We felt we could absorb one
or two people per iteration. We did not let
newcomers take tasks right away, but used
them mainly as pair partners during their first
iteration. Then, as they got to know the sys-
tem and our practices, they started to take on
tasks at the iteration planning meetings. We
didn’t assume that team velocity would in-
crease when we added a new person to the
team—we measure velocity, not predict it.

The DPS way of developing software
made up for programmer inexperience by
having senior engineers review the less ex-
perienced engineers’ work. In XP projects,
you must still address the spread of exper-
tise; for instance, it is critical to have at least
one senior engineer on the team. We don’t
give senior people a big title or special role,
but we need them. They help spread the
wealth of knowledge, and both they and
their pair partners learn.

The end of the story
Unfortunately, I cannot present the story

of how the project completed, because it was
mothballed due to changing market needs.
This is in the spirit of one of XP’s mantras:
Work on the most important thing first. Nev-
ertheless, the team and the managers were

impressed with our results in terms of pro-
ductivity and quality. Because of this project,
two other pilot projects were started.

I n my experience, when the engineerswant XP, the management doesn’t, andif management wants XP, the engineers
don’t. Where is the trust between manage-
ment and engineering?

To managers: Try XP on a team with
open-minded leaders. Make it okay to try
new things. Encourage the XP practices. Pro-
vide good coaching. Challenge your team to
go against the status quo. Recruit a team that
wants to try XP rather than force a team to
use XP. Make sure the organization sees that
no one will be punished for trying something
different. When hand-picking XP practices,
you might compromise the self-supporting
nature of XP. Try as much of XP as you can.
Iterations are short. Feedback comes often.

To engineers: Develop a sales pitch. Iden-
tify problems that you might solve. Identify
the benefits, identify the risks. Do a pilot
project. Iterations are short. Feedback
comes often.

Acknowledgments
I thank the real client that provided the experience

to write this article (who wished to remain anony-
mous). I also want to thank Chris Biegay and Jennifer
Kohnke for a job well done in helping me prepare this
article. I presented a earlier version of this article at XP
Universe in July 2001.

References
1. K. Beck, Extreme Programming Explained, Addison-

Wesley, Reading, Mass., 1999.
2. A. Cockburn, Writing Effective Use Cases, the Crystal

Collection for Software Professionals, Addison-Wesley,
Reading, Mass., 2000.

3. M. Fowler et al., Refactoring: Improving the Design 
of Existing Code, Addison-Wesley, Reading, Mass.,
1999.

4. S. McConnell, Rapid Development, Microsoft Press, Red-
mond, Wash., 1996.

5. J. Gall, Systemantics: How Systems Really Work and
How They Fail, 2nd ed., General Systemantics Press,
Ann Arbor, Mich., 1986.

6. R.E. Jeffries, Extreme Programming Installed, Addison-
Wesley, Reading, Mass., 2001.

7. T. Mackinnon, S. Freeman, and P. Craig, Endo-Testing:
Unit Testing with Mock Objects, XP Examined, Addi-
son-Wesley, Reading, Mass., 2001.

8. R. Martin, “Design Principles and Design Patterns,”
www.objectmentor.com/publications/
Principles%20and%20Patterns.PDF (current 12 Oct.
2001).

For more information on this or any other computing topic, please visit our
Digital Library at http://computer.org/publications/dlib.

N o v e m b e r / D e c e m b e r  2 0 0 1 I E E E  S O F T W A R E 33

About the Author

James
Grenning
is the director
of consulting
at Object 
Mentor and 
is currently 
practicing 
and coaching 

Extreme Programming.  Areas of interest
are organizational change, software proc-
ess improvement, and the business impact
of agile software development. He helped
create the Manifesto for Agile Software
Development (http://AgileAlliance.org)
and is a signatory. He is also a member of
the ACM. He earned a BS in electrical en-
gineering and computer science from the
University of Illinois, Chicago. Contact 
him at Object Mentor, 565 Lakeview Park-
way, Ste. 135, Vernon Hills, IL 60061; 
grenning@objectmentor.com.



3 4 I E E E  S O F T W A R E N o v e m b e r / D e c e m b e r  2 0 0 1 0 7 4 0 - 7 4 5 9 / 0 1 / $ 1 0 . 0 0  ©  2 0 0 1  I E E E

requirements—gathered by the previous con-
sultancy—that had outlived their build-by
date. On the other side (the hard place), was a
client that, acknowledging that it had
switched horses midstream, was willing to
scale back on functionality but refused to
budge on delivery date. In short, Thought-
Works found itself with a mere two months to
both turn the project around and deliver a
business-ready application. For the purposes
of this article, we’ll say that the delivery date
was February 2nd, Groundhog Day.

The application under construction was
Web-enabled and powered by Enterprise Java
Beans. Although the system had a smart n-
tier architecture, its innards were seriously ill.
The business objects served as little more
than mappers to the database. The session
bean methods—the application’s brain
trust—were superlong tendrils of procedural
code. The written code—its style, semantics,

and legibility—varied wildly depending on
the area of the application one happened to
be nosing about in. Tests, where they existed,
were expected to fail. Building and deploy-
ment were a near mystery to more than half
the development staff.

Possibly because of the problem’s sheer
size and pervasiveness, ThoughtWorks was
slow to realize how fundamentally bad things
were. Even when the danger became appar-
ent to the consultants on site, the team lead
and project manager had to compete with
several other projects for overstretched re-
sources. However, once the situation was
clearly stated and understood, Thought-
Works responded with a swift and sustained
effort. The problem areas in the application
were identified, and developers with relevant
expertise were brought in. After two long
months, the project delivered on time. But we
didn’t stop there.

focus
Recovery, Redemption, 
and Extreme Programming

Peter Schuh, ThoughtWorks

The author
recounts the tale
of a development
project’s
rejuvenation
through XP: the
successes, the
shortcomings,
and, ultimately,
the lessons
learned.

I
n the autumn of the New Economy, my employer, ThoughtWorks,
recognized that one of its projects had maneuvered itself into a bit of
a jam. In the software development industry it is well understood—
and occasionally admitted—that such situations occur. What is less

well-known, however, is how to address them. For this particular project,
the prospects were not good. On one side (the rock), was a development
team both behind schedule and coding to more than a year’s worth of 

reports from the field



Damage control
I was one of four “ThoughtWorkers” who

began flying to the client site in early Decem-
ber—just two months prior to go-live—to as-
sist the six developers already on site. Our
mandate was to slap the application into
shape no matter the cost. Full of vigor and
with little appreciation for the hundred miles
of bad road ahead of us, we saw this as an op-
portunity to test-drive Extreme Programming
in a project recovery setting.

By mid-January, conference-room dinners
were the norm, all-nighters customary, and
weekends worked through. Although we
bought, distributed, and even read and
quoted Extreme Programming Explained,1

we disregarded practically every XP princi-
ple—from the 40-hour workweek, to testing,
to continuous integration.

As the severity of the matter quickly be-
came apparent, we were forced to concede
that the impending deadline did not allow us
the luxury of XP. Instead, we cut up the appli-
cation and parceled it out by areas of expert-
ise: servlets, documentation generation, busi-
ness engines, database, and so on. Deemed
untenable, the servlet package was all but
rewritten. Elsewhere, refactoring became an
opportunist’s endeavor. If a developer decided
that refactoring would take less time than
complementing ugly code with more of the
same, then that developer refactored. Other-
wise, we sucked it up and coded ugly. This was
a humbling exercise that engendered such
memorable comment tags as “It’s 3:30 in the
morning. Please forgive me.” This was found
atop a 150-line switch statement.

Coding standards and test writing were
goals we aspired to and occasionally met. In
lieu of an object model or static documenta-
tion, one developer scrubbed the database
clean and reverse-engineered it into a data
model. The model was maintained regularly,
and it served as the application’s most reli-
able high-level specifications.

The 70-plus-hour workweeks would not
have persisted had developer pampering not
been administered by both the project man-
ager and the top levels of ThoughtWorks man-
agement. Conference-room dinners featured
catered sushi, the project manager graciously
made 2 a.m. ice cream runs, and the company
paid for last-minute, full-fare tickets for hur-
ried trips home. ThoughtWorks’ management
made it clear, through words, acts, and expen-

ditures, that it understood and genuinely ap-
preciated the sacrifices the team was making.

In the end, it all came together the last week
of January. The application did its part by
bursting nearly every seam on the 30th, forc-
ing a 24-hour go/no-go decision. The team re-
doubled its efforts. Things improved. Func-
tional tests passed. We performed the last
build six hours prior to go-live. Four hours
later, 5,000 rows of data were sneaked into the
database. After eight weeks, we delivered the
system—minus some requirements and plus a
few too many bugs—on time. One member of
the team best expressed the general consensus:
“I’m glad I did that once. I never want to do it
again.” 

The nuts and bolts of code reform
On 3 February, the system was live, and we

were all well aware of its shortcomings. The
client had a long list of functionality for us to
add, not to mention the requirements it had
dropped to meet go-live. Meanwhile, the
team was determined to refactor the applica-
tion into something we weren’t embarrassed
to leave our names on. The team lead and
project manager negotiated with the client for
time to apply “ease of maintenance” to the
system (a senior developer proposed the term
in response to the suggestion that “refactor-
ing” not appear on timesheets). With the
worst behind us and an easier road ahead,
several team members (but by no means all)
decided that this refactoring phase would be
an opportune time to begin adopting XP.

Walking out of the starting gate
The switch to XP was a slow, unsteady

process. Not only was the current code base a
reluctant conspirator, but only perhaps a
third of the team really supported adopting
XP. Another third was impartial, and, as
might be expected, the final third was quietly
but vehemently wishing that all this extreme
nonsense would just go away. What began as
lone developers and pairs taking some first
steps toward agile processes—with patience
and determination—became a teamwide push
to adopt XP.

Testing and building both benefited from
single-handed accomplishments. The few ex-
isting tests were unreliable, largely because
they depended on nonrestorable data that had
either been altered or dropped from the data-
base. One developer set out to base the exist-

N o v e m b e r / D e c e m b e r  2 0 0 1 I E E E  S O F T W A R E 35



ing tests on data that could be replenished,
then bundled those tests into a JUnit-based
test suite (see the JUnit Web site, www.ju-
nit.org). Another developer streamlined the
build process, reducing it to a few simple
steps that could be quickly learned, letting
every developer on the team perform a build
before checking in—our first step toward
continuous integration. 

Some developers paired up to take on
more traditional development tasks. We at-
tacked the buggiest sections of the application
first. Because we knew new functional re-
quests were not a long away off, refactoring,
for the most part, was pursued gently.
Mediocre code was improved upon when
convenient, while truly untenable code was
gutted and rewritten. From a teamwide per-
spective, the senior developers advocated a
unified coding standard, JavaDoc-based com-
menting (see the JavaDoc Tool homepage,
http://java.sun.com/j2se/javadoc/index.html),
and unit tests for all refactored or new code.

Learning from successes and building on mo-
mentum

Shortly after Groundhog Day, two devel-
opers began applying a constants pattern to
the application. Because the constants, as they
are wont to be, were used throughout the ap-
plication, the switch was neither smooth nor
painless. The general consensus was that
refactoring was necessary and that the pattern
was solid for both current use and extensibil-
ity. The team agreed, however, that we needed
better communication for future refactoring.
The result was an increase in email “advi-
sories,” pick-up development discussions,
and regularly scheduled code reviews.

The team’s analysts had readily accepted
the story card as their new document, both as
a way to distribute functionality requests to
the developers and as a basis on which to ne-
gotiate with the client. When the analysts
handed the first batch of cards to develop-
ment, some pairs and some individuals began
cleaning up the portions of the application as-
sociated with their cards. Tests began to ap-
pear in the newly refactored areas of code,
and these were added to the main suite. One
developer made the build process portable, so
everyone could build locally prior to checking
in. The build machine was moved from an oc-
cupied developer’s space to an otherwise
empty cube. The number of builds per day in-

creased, and the number of broken builds
plummeted.

The one insurmountable impediment we
encountered after Groundhog Day was the
client’s lack of cooperation in XP. Without
objecting to the shift, the client nonetheless
refused to participate in the process in any
meaningful way. The team’s project manager
and analyst (both ThoughtWorkers) assumed
the role of the XP customer by writing story
cards, providing domain knowledge on de-
mand, and communicating decisions. How-
ever, the client’s absence in these activities
certainly affected us. Without a decision
maker to sit in the same room with the team,
we could never fully implement the planning
game, nor could we convince the customer to
make official iterations more frequent than
every three months. The irony of all this was
that ThoughtWorks was doing development
on site. The client was paying to fly multiple
ThoughtWorkers out every week, only to
house them several office buildings down the
road from any potential users.

Making it up as you go along
By April, nearly all the functionality we

had originally promised the client had been
coded into the application and had passed
user acceptance testing. Once it was clear the
project was no longer in imminent danger, a
few team members turned their attention to
the more fundamental aspects of develop-
ment. Innovations in these areas furthered
our adoption of XP.

The first crucial change was the mandate
of a nightly greenbar. (A greenbar is the
graphically displayed result of a successful
test run in JUnit; conversely, a redbar is the re-
sult of any test that fails.) It took weeks to
whittle the error log down and see our first
greenbar. When this occurred, we tacked a
calendar to the wall alongside the build ma-
chine and recorded the result of each day’s
last build with a red or green sticky note.
With this highly visible performance measure
serving as a reminder, developers began to
work toward nightly greenbars. Then, about
a month after the calendar was posted, it
veered dangerously into the red, just as a ma-
jor delivery date was approaching. After five
days of consistent redbars, one developer
sounded the alarm, emailing a teamwide plea
for a greenbar. Having been alerted to the sit-
uation, the analysts and project manager be-

We attacked 
the buggiest
sections of 

the application
first.

3 6 I E E E  S O F T W A R E N o v e m b e r / D e c e m b e r  2 0 0 1



gan pressuring development to promote only
builds that greenbarred, and the calendar
moved back into the green. 

In the end, the calendar served two pur-
poses. First, by providing a simple, straight-
forward metric, it gave development a clear
and attainable performance goal. Second, be-
cause it was viewable and easily understood
by the rest of the team, it served as a failsafe
mechanism for development. When the devel-
opers—in a heads-down coding frenzy—
failed to follow their own rules, the other
team members could push them back in line. 

Once again, we improved the build
process, this time simplifying it to a push-of-
a-button procedure that eased the adoption of
continuous integration.2 Possibly as impor-
tant, we automated build promotion—from
the development environment, through inter-
nal testing, to user acceptance testing (UAT).
Automation started with code checkout and
ended with the emailing of unit test results.
This meant that even analysts could promote
builds, and they did. Guided by the auto-
mated test results, which ran on every build,
an analyst could promote the latest greenbar
build into a testing environment. This saved
development the hassle of being on call to
perform the task and resulted in quicker feed-
back on new functionality and bug fixes.

Finally, several developers teamed up to de-
vise and code a test-data generator, dubbed
ObjectMother.3 Through a handful of simple
method calls, this utility provided a complete,
valid, and customizable structure of business
objects (think of an invoice: its lines, all related
charges, remit to, bill to). ObjectMother
yielded numerous benefits. Because it made the
test suite database-independent, we could swap
UAT and even production databases in and out
of the development environment, letting devel-
opers code and debug against real data. More-
over, ObjectMother drastically simplified the
creation of test data within code, resulting in
three major benefits. First, developers were
much less likely to cheat and base their tests on
supposedly persistent data in the test database.
Second, it became much easier to convert ex-
isting tests that did rely on persistent data. Fi-
nally, developers began writing more tests.

Past the finish line and still running
To reduce travel costs, the project began

rolling off some of its more experienced per-
sonnel in late March, little more than three

months after the cavalry’s arrival. Junior de-
velopers who had proven their mettle now
took greater responsibility, and fresh, impres-
sionable recruits came on board. Good prac-
tices were passed along, and XP-based devel-
opment gained more momentum. Within six
months, the project had metamorphosed its
well-deserved infamy within ThoughtWorks
into high repute.

If we had to do it all over again
Notwithstanding everything I’ve said,

what saved the project was not XP. Instead, it
was a well-financed and tremendously suc-
cessful death march. The client’s refusal to
budge on the delivery date was the single
greatest contributing factor to this outcome.
Groundhog Day meant that the team could
not step back and reassess the situation. It
meant that we could adjust the course of de-
velopment only by degrees, not by turning it
on its head. The developers new to agile
processes had no time to adopt a coding stan-
dard or collective ownership, build a continu-
ous integration process, or learn to program
in pairs. More often than not, we didn’t have
time to refactor bad code, so the hack fre-
quently won out over the simplest thing that
could possibly work. The irony of it, however,
was that Groundhog Day took the code live,
and XPers prefer to work with live code.

Although XP wasn’t the team’s immediate
salvation, it did, in the end, make the appli-
cation sustainable beyond Groundhog Day.
Our gradual adoption of XP recovered, re-
tooled, and rejuvenated the code base. Be-
cause of the project’s nature, I believe we
were correct to put many agile processes on
hold during the first months of rehabilita-
tion. However, we could have introduced
some principles—such as improving the
build and test processes—much earlier. This
section distills our experiences with XP into a
list of steps that might have best served our
team (or any project in similar or less dire
straits). Table 1 details our overall experience
with each XP practice.

What you should do right away
So, it’s two months to go-live, the team

methodology to date has been waterfall, the
project is a month and a half behind sched-
ule, and Beelzebub is banging at the front
door. What do you do? Lock the door.
Okay, what next?

N o v e m b e r / D e c e m b e r  2 0 0 1 I E E E  S O F T W A R E 37

Within six
months, the
project had

metamorphosed
its well-
deserved

infamy within
ThoughtWorks

into high
repute.



Before considering XP or any best practices.
For starters, I cannot stress enough how es-
sential ThoughtWorks’ support was to the
project’s initial success. First, I do not believe
the project would have ever met its original
goal without serious moral and financial com-
mitments. Employees simply will not give up
their lives for two months and deliver the near
impossible, gift-wrapped, if they don’t have
constant reminders of how important the mat-
ter is and how valuable they are. Second,
ThoughtWorks contributed to our initial suc-
cess by making intelligent staffing decisions.
When a project is in danger and requires extra
developers, additional resources must target
the project’s specific needs. Finally, reputation
matters. ThoughtWorks has long asserted that
its employees are its greatest asset, and it has
continuously backed those words with deeds.
(Forgive me if this comes off as a shameless
corporate pitch—it is not.)

Target the build process. This is one of the
first things we should have done. Developers
are keen to things that yield the greatest ben-
efit from the least effort. A long, arduous
build process discourages developers from
taking responsibility for the code they check
in. Conversely, the simpler it is to perform a
build—the less time it takes—the more likely
it is that a developer will want to know that

new code integrates successfully. Making the
build process portable, so it can run on devel-
oper machines, further encourages responsi-
ble check-ins. Assuming the process is swift
and easy, what developer would not check
out the latest code, perform a clean compile,
and check in with confidence? 

The build process doesn’t have to be true
push-of-a-button at this stage, but it must be
streamlined to a handful of steps. The benefit
to the team is obvious and measurable: few
things in software development are more dis-
couraging than spending an entire day trying
to make a clean build.

Organize a test suite. Even if it is the first
working test class to go into the code base,
write an AllTests class and run it at the end of
every build. Add any existing tests that do
pass, or can easily be made to pass, to All-
Tests. Treat a build that redbars no differently
than one that fails to compile. (You’ll have to
sell this idea to the analysts—or the client—as
well.) Encourage developers to write tests and
add them to the test suite, but don’t shove test
writing down their throats (at least not yet).
Finally, I recommend against including old
tests that fail, even if the team intends to get
them working some time in the future. Seeing
a greenbar has a particular psychological
value. A redbar that you consider a virtual

3 8 I E E E  S O F T W A R E N o v e m b e r / D e c e m b e r  2 0 0 1

Table 1
Adoption of XP practices

Practice Adoption status The experience

Planning game Partially adopted The client never really got involved in the planning game (similar to the absence of an on-site 
customer; see below). Nor did the entire development team participate. 

Small releases Not adopted We could never get client buy-in.
Metaphor Not adopted The one XP practice we overlooked. We did not attempt to implement metaphor.
Simple design Fully adopted This went hand-in-hand with refactoring. It became a serious endeavor and permeated its way 

through the application.
Testing Fully adopted Acceptance tests were written and used. There was, however, some kicking and screaming 

regarding unit tests. Some developers were behind them, some neutral, and some had to be 
shamed into writing them. In the end, even unit-testing gained team-wide support.

Refactoring Fully adopted At least half the team had refactoring in mind moments after they landed on the project, but serious
projects were put off until after Groundhog Day. There was no difficulty mustering team-wide 
support after that.

Pair programming Partially adopted We began pairing seriously, but slacked off and settled in at perhaps 30 percent of all developer time.
Collective ownership Fully adopted Easily accepted by the team. Developers still had their favored areas, but moved about frequently, 

and anyone could change any portion of the code.
Continuous integration Fully adopted The process arose from our automated build scripts; the team quickly embraced it.
40-hour week Not adopted After Groundhog Day, some weeks may have been 40, but some were 60. This depended on where 

we were in the iteration cycle.
On-site customer Partially adopted Although we were on site, the client never provided an XP-like customer. The project manager and 

team analyst stood in, with some success, in the role of the customer.
Coding standards Partially adopted A single standard was proposed and influenced some team members. Peer pressure had an 

effect at times. JavaDoc was incorporated into the build process.



greenbar because “those test never pass any-
way” isn’t the same.

Write an ObjectMother. Writing an object gen-
erator is not a trivial task, but it pays for itself
by shortening the time spent writing new tests
and fixing broken ones. The utility reduces the
effort-versus-benefit ratio for test writing. De-
velopers are much more likely to write a test
when they can acquire an invoice and all its as-
sociated objects from one simple method call;
they are much less likely to write the same test
when the invoice, its lines and charges, the cus-
tomer, the bill-to address, and perhaps the as-
sociated assets all have to be instantiated and
bound together before calling get-total. An Ob-
jectMother also makes it easier to maintain
tests when their associated business objects
change, because it consolidates the instantia-
tions into one space instead of letting them be
spread across the application.

Practices to phase in early
So you’ve taken care of the most crucial el-

ements. Now you can move on to phase two.

Continuous integration. If a project has a sim-
ple build process and a serviceable test suite,
the next step is to combine these and tackle
continuous integration. There are two ways
to approach this. The first is the standard se-
rialized XP practice based on a build machine
or build token, where developers queue up
one at a time to build, test, and integrate their
changes.1 The second alternative, a variation
we use at ThoughtWorks, is for developers to
build and test on their own machines before
checking in. Backed up by multiple builds per
day (to keep everybody honest), this has
proved a successful practice.2 Either way, con-
tinuous integration drastically reduces the
time developers would otherwise waste con-
verging code. It had a noticeable effect on our
team as soon as we implemented it.

Gentle refactoring. Refactoring is good, but at
this stage in a project’s recovery, it must be tem-
pered for several reasons. First, it may be diffi-
cult to get client buy-in. Second, the worse the
code base is and the less likely it is to follow the
principles of object-oriented abstraction, the
more difficult it will be to isolate portions for
retooling. Third, at least in the beginning, you
probably won’t have either a quick build
process or dependable test results as success in-

dicators. Nonetheless, do pursue gentle refac-
toring from the start. Remove insufferable por-
tions of code and undertake any refactoring
task that offers low risk and high value. 

Simply simplicity. Simple design is straightfor-
ward to those in the know and aggravatingly
intangible for anyone anywhere else. For our
team, as we wended our way along trails of
bad code in the first months of project recov-
ery, we could easily pick out issues such as
overdesign and logic duplication; however, be-
fore Groundhog Day, we had no time to ad-
dress these issues. After Groundhog Day, as
refactoring and new functionality became
everyday business, we remembered the squalor
we had wallowed through and consistently
strove for simplicity. We pushed logic that per-
meated through the session beans back into the
business layer and again refactored the servlets.
Weekly code reviews were initiated, and sim-
plicity regularly rolled off nearly every tongue.

Coding standard. This is easy to encourage
without spending too much time or effort. In
our case, we were fortunate to have two stan-
dards to fall back on. First, we had a proposed
in-house coding standard that one team mem-
ber had helped author several months earlier.
This document was both a good primer for the
greener developers and a good starting point
for further discussion and debate. Second, we
relied on the JavaDoc standard for code com-
menting, which we further leveraged by incor-
porating the generation of API docs directly
into our build process. This practice is easiest
to encourage when you have an in-house or
open standard (such as JavaDoc) that you can
simply pass or email about and then occasion-
ally refer to during discussions.

Stand-up meetings. We never introduced
these, and I believe it was a major mistake.
Quick, daily, face-to-face meetings keep de-
velopers informed about what others on the
team are doing. They help stop people from
stepping on each other’s toes. They keep the
team lead informed about who’s ahead and
who’s behind. They air new ideas and prevent
people from duplicating work.

Mind the database. Okay, this isn’t an XP
process, but it’s definitely as important as one.
The database is an essential component of
nearly every business application—neglect it

N o v e m b e r / D e c e m b e r  2 0 0 1 I E E E  S O F T W A R E 39

Refactoring is
good, but at this

stage in a
project’s

recovery, it
must be

tempered for
several
reasons.



at your peril. Nothing good ever comes of a
database architected without a mind to con-
version or reporting. Similarly, a database that
updates schema for new attributes and entities
but not deleted ones—a database that lets test
data pile up and atrophy—will be cantanker-
ous to develop with, hard to test on, and diffi-
cult to alter. Conversely, a well-architected and
maintained database, through intelligent and
efficient data organization, can guide good de-
velopment. Finally, as mentioned earlier, in
lieu of an object model or other documenta-
tion, a data model can provide an extremely
handy overview of the application.

What to do when the pressure lets up
Groundhog Day has come and gone, and

you’ve phased in several XP practices. So
what’s next? 

Step back and relax. Once the project has met
some of its immediate goals (a major delivery
or go-live date), the development team should
step back and get everything into perspective.
If the time has come for serious refactoring,
what parts of the application should be put
through the grinder first? How is the adop-
tion of XP coming along—who’s resisting and
who’s welcoming it? How is the team’s men-
tal health? If the last couple months have been
a bloodbath, should some exhausted team
members be rolled off the project? Would the
project benefit from new recruits and fresh
perspective?

Iterations and small releases. I didn’t list this
practice earlier because the principles in the
first two groups weigh in as either more im-
portant or easier to accomplish. Why? Be-
cause a project plan is already in place, and
negotiating for a formal change to that plan
expends the scarce resources of both time and
political capital. However, even without for-
mal customer consent, a team can institute in-
ternal iterations. We did this. They were not
the same as true iterations because the cus-
tomer wasn’t bearing down on us, but break-
ing the work out internally was definitely bet-
ter than delivering once every three months.

Get an on-site customer. This is also on my
wish list of XP practices to phase in early. I
fear, however, that any project that falls to the
level of depravity that ours did does so partly
because there has been no real participation

on the customer’s part. Therefore, attempting
to foster customer input early in the recovery
process may well be a Sisyphean endeavor. If
your project can actually produce a real cus-
tomer earlier, then Tyche has shined on you.

Roll in the rest of XP. As the pace of the proj-
ect returns to something akin to normal, the
team should address the remaining elements
of XP. As a rule, when you add functionality
to poorly written areas of the application,
refactor the code. Start looking at patterns.
What parts of the application might benefit
from their use? Encourage pair programming
and make changes to the workspace to facili-
tate it if necessary. Story cards and the plan-
ning game should become the means by
which the functionality is proposed, deliber-
ated, and built into the application. We never
got serious about metaphor or the 40-hour
workweek; they should, however, be given se-
rious consideration.

Communicate, communicate, communicate.
If you have so far managed to avoid institut-
ing stand-up meetings, put them in place now.
Whenever possible, XP principles should
propagate from the bottom up rather than be-
ing imposed from the top down. Ideally, this
means that the team as a whole decides what
XP principles it will introduce and get serious
about first. Involve the entire team in estima-
tion. All these things foster a sense of collec-
tive ownership, not only of the code but also
of the project’s general well-being.

H ad our client been willing, manymembers of our team would haverazed the code base and started
again from step one. But few clients are so
giving and few projects so fortunate. Fur-
thermore, who is to say a project will not fal-
ter again—for similar or wholly different
reasons? In such situations, little is achieved
without a lot of hard work.

XP had no hand in recovering our project
but weighed in heavily on its redemption. Be-
fore Groundhog Day, although we discussed
XP and made occasional, nearly clandestine
incursions into agile development, we simply
could see no way to rewrite our development
process in the time we had. Rather, we
stepped in to shore up and compensate for the

Quick, daily,
face-to-face

meetings keep
developers

informed about
what others on
the team are

doing.

4 0 I E E E  S O F T W A R E N o v e m b e r / D e c e m b e r  2 0 0 1



current process until we had the time to ad-
dress the many issues in earnest. After
Groundhog Day, we made a sincere and sus-
tained effort to go XP, which was successful
on many levels. In the end, we landed some-
where within the realm of agile development
without quite having achieved XP—but it
was quite a good place to be.

I cannot stress enough how the team com-
position factored into our success. If we did
not have so many stars and suckers—those ca-
pable of great things and those willing to put
their lives on hold to make great things hap-
pen—the project would have never attained
the success it did. Ultimately, our project’s re-
demption rested on two strong foundations:
the processes drawn from Extreme Program-
ming and the team that applied them.

Acknowledgments
First and foremost, credit goes to the fellow

ThoughtWorkers with whom I endured the worst
death march I ever wish to be a party to. I am quite
grateful to have worked with such a team. Many
thanks, also, go to Martin Fowler for suggesting the
topic for the article and providing the guidance neces-
sary to ensure that it was written. I presented an ear-
lier version of this article at XP2001 in May 2001.

References
1. K. Beck, Extreme Programming Explained, Addison-

Wesley, Reading, Mass., 1999.
2. M. Fowler and M. Foemmel, “Continuous Integration,”

www.martinfowler.com/articles/continuousIntegration.
html (current 16 Oct. 2001). 

3. P. Schuh and S. Punke, “ObjectMother: Easing Test Ob-
ject Creation in XP,” www.thoughtworks.com/cl_
library.html (current 24 Oct. 2001).

For more information on this or any other computing topic, please visit our
Digital Library at http://computer.org/publications/dlib.

About the Author

Peter Schuh is a team lead at ThoughtWorks,
Inc., a provider of custom e-business application de-
velopment and advanced system integration services
to Global 1000 companies. He has written and spo-
ken about XP, the adoption of agile processes, agile
development’s impacts upon database administra-
tion, and the ObjectMother pattern. With a BA in
English literature and an MA in international rela-
tions, he is an unapologetic interloper in the soft-

ware development industry. Contact him at ThoughtWorks, Ste. 600, 651 W.
Washington, Chicago, IL 60661; peschuh@thoughtworks.com.

Possible topics include but are not limited to the following:
• Coding for network (grid-based) applications
• Coding for high-availability applications
• Coding for compatibility and extensibility
• Coding for network interoperability
• Effective use of standards by programmers
• Lessons learned from game programming
• Techniques for writing virus-proof software
• Simple and commonsense techniques for making   

multithreading and concurrency easier and more reliable
• Coding implications of continually increasing processing 

and memory resources
• Agents: When, where, and how to use them
• PDAs and the future of “wearable” software 
• Is “agile” programming fragile programming? 
• Prestructuring versus restructuring of code
• Integration of testing and construction

HAVE YOU EVER HAD AN EXPERIENCE in constructing software that gave you un-
expected insights into the larger problem of software engineering and development
of high-quality software? If so, IEEE Software encourages you to submit your 
experiences, insights, and observations so that others can also benefit from them.
A unique aspect of software engineering is that techniques used at the smallest
scale of individual software construction are often relevant even for the largest
software systems—and vice-versa. We are looking for articles that encourage a
better understanding of this commonality between programming in the small and
programming in the large, and especially ones that explore the larger implications
of hand-on software construction experiences.

• Aspect-oriented programming
• Impacts of language choice on application cost, stability, 

and durability
• Proliferation of Internet-based computing languages (for

instance, Perl, Python, Ruby)
• “Throw-away” programming and languages: Good, bad,

or just ugly?
• Code ownership in an Internet economy
• When, how, and where to use open source
• choosing languages for your applications
• Lessons learned in buying and using tools
• XML: Where to next? What will follow it?
• How to keep documentation from diverging
• Personal-level configuration management
• Personal-level defect analysis
• Build processes for the new millennium
• Deciding when to branch code bases

CALL FO
R

ARTICLE
S: 

Software
Construc

tion

Construction Editor: Terry Bollinger, terry@mitre.org Submissions are accepted at any time; contact software@computer.org



4 2 I E E E  S O F T W A R E N o v e m b e r / D e c e m b e r  2 0 0 1 0 7 4 0 - 7 4 5 9 / 0 1 / $ 1 0 . 0 0  ©  2 0 0 1  I E E E

in Iona’s engineering team, we often ig-
nored good engineering processes and prac-
tices. As a result, the code base’s overall
health degenerated and was salvaged only
through two successful reengineering ef-
forts and a series of infrastructure projects
designed to improve overall engineering
practices. It wasn’t until later that we real-
ized how closely our efforts were tied to Ex-
treme Programming.

The problems 
When Iona moved from a start-up to a

market-leading provider of middleware
products in 1999, it faced four major prob-
lem areas: processes and practices, testing,
code entropy, and team morale. An initial
reengineering project that started in 1997
was concluding, and although it focused a
tremendous amount of resources on reengi-
neering, improving testing, and making nec-

essary changes to the code base to comply
with the latest version of the Corba specifi-
cation, it did not sufficiently address the
four problem areas; it merely skimmed the
surface of what needed to be done. 

Processes and practices
In early 1999, you could have asked two

different engineers from the same Orbix
team how they did their jobs, and each
would have replied differently. This re-
flected the team’s lack of process documen-
tation and visibility and its failures to make
process a part of each engineer’s personal
software practices. Also, there was no focus
on process improvement. The junior engi-
neers assigned to maintenance and en-
hancement had no experience with good
engineering practices and had only a rudi-
mentary understanding of process. To com-
pound the problems, the maintenance and

focus
Using Extreme
Programming in a
Maintenance Environment

Charles Poole and Jan Willem Huisman, Iona Technologies

The authors
review 
how efforts 
to introduce
industry-level
best practices 
led to Extreme
Programming,
which improved
the team’s 
ability to deliver 
quality support
and product
enhancements.

D
uring the 1990s, Iona Technologies’ flagship product was a Corba-
based middleware product called Orbix. We have since created a
newer version of Orbix, but here we discuss how we developed
and, more importantly, now maintain the older version, which rep-

resents the Corba specification’s early evolution. We also review the inherent
problems of code entropy due to specification instability and time-to-market
pressures. Over the years, because of these pressures and the rapid growth 

reports from the field



enhancement team used disparate source
control elements across two globally distrib-
uted development sites and lacked a well-
defined and tested interface between config-
uration units. Dependency management
was a nightmare.

Testing
In general, quality was never one of Orbix’s

strong points. We didn’t document test cover-
age very well, so good metrics were not avail-
able. Our test suites were cumbersome, diffi-
cult to run, and impossible to accurately
report. Also, the test suite used to provide in-
teroperability testing with other Iona products
and product components—or against other
Corba middleware products—was not auto-
mated across the platform set to which we de-
livered our product. Thus, system testing our
product releases was difficult. Furthermore,
instead of having the development and main-
tenance teams monitor quality, a team that
was detached from the engineering effort
monitored it through checklists and forms.

Code entropy
By the end of 1997, we had already

patched and repatched the Orbix code hun-
dreds of times to address numerous customer
issues as well as the changing Corba specifi-
cation. These patches often used band-aid ap-
proaches to resolve problems or add func-
tionality and were a major factor in the code’s
rapid entropy. Many of these unreviewed
changes were made to a code base that was
never designed to withstand the punishment
meted out by many of Iona’s larger customer
deployment environments. Customers de-
manded fixes and faster resolve times. 

The code’s degradation further acceler-
ated when Iona removed most of the senior
development staff from the maintenance
team so that they could develop new prod-
ucts. Finally, there was limited acceptance
of the well-documented style guides that
Iona’s chief architect specified for all code
development, making it difficult to read the
code. Poor structuring of the source code’s
directory space also made it difficult to
quickly become familiar with the overall
code structure. 

Team morale
People indicated in reviews that they didn’t

feel cohesiveness in the team. Many reported

poor visibility of the projects on which people
worked. In general, we felt underappreciated
and overworked.

Initial history of change
In 1999, prior to the release of Kent

Beck’s Extreme Programming Explained,1

Iona undertook several projects to address
its problem areas. It is in the context of these
projects that it introduced Extreme Pro-
gramming, realizing that many of the things
it was implementing were elements of XP. 

Reengineering
The second reengineering effort initially

focused on resolving problems—which many
customers reported—related to poorly imple-
mented modules. Bug clusters highlighted in
our defect-tracking system clearly identified
these problem areas. Today, this reengineer-
ing effort continues as a part of our XP ef-
fort, which helps engineers resolve problems.
An additional outcome of the initial effort
was reducing the code’s complexity by strip-
ping out unused code and implementing sev-
eral patterns that have made it easier to
maintain, test, and understand the code.
We’ve reduced code size by over 40 percent. 

Improving engineering practices
Jim Watson, one of Iona’s distinguished

engineers, led the reengineering project. In
addition to providing strong technical in-
sight into coding problems, he was also in-
strumental in promoting stronger engineer-
ing practices. He initiated several practices
to encourage growth in the engineering
team, including weekly presentations that
gave everyone an opportunity to present
technical topics of relevance or interest such
as merge strategies, patterns, and estima-
tion. He emphasized code reviews, adher-
ence to source-code management guidelines,
and ownership of and responsibility for
code style. He made engineers consider how
they could constantly improve the quality of
their work in areas such as test coverage,
and he established a more proactive ap-
proach to problem solving. His practices
still strongly influence the team.

Automating everything
We established a separate project to

clearly understand the build and test depend-
encies across the older product set’s various

N o v e m b e r / D e c e m b e r  2 0 0 1 I E E E  S O F T W A R E 43



elements and to fully automate the build and
test process. This is an ongoing project, but
so far we’ve been able to build and unit test
the complete product set on a nightly basis.
Efforts continue to better automate the inter-
operability and system tests, which still re-
quire some manual intervention. 

Team consolidation
Before starting the reengineering projects,

over 70 engineers maintained and enhanced
the Orbix products. By the time we com-
pleted the major portion of those projects,
we had reduced the team to around 40 but
were servicing three times the number of
customers. Today, the team size is down to
25. In addition to the personnel consolida-
tion, the team managed a single mainline of
code using a well-defined set of common
rules that govern how to merge fixes and en-
hancements into the consolidated mainline.

Extreme maintenance
The second reengineering effort resulted

in a remarkable transformation—we were
suddenly on the road to a consistent and au-
tomated build, test, and release infrastruc-
ture for nightly product builds and tests.
The reengineering and refactoring efforts
eliminated much of the code complexity and
stagnation, resulting in a clean, well-struc-
tured code base that conformed to Iona
code standards. So, if we saw so much im-
provement, why consider XP? 

Despite our progress, we had yet to resolve
issues of testing, visibility, morale, and per-
sonal work practices. From a management
standpoint, we wanted to do more with less:
higher productivity, coupled with increased
quality, decreased team size, and improved
customer satisfaction. The engineers wanted
more time to do a good job instead of always
feeling pressured to deliver fix after fix. So we
started looking at XP and learned that many
of its elements come naturally to teams work-
ing the maintenance game. According to Kent
Beck, “Maintenance is really the normal state
of an XP project.”1

In our earlier projects, we created a set of
maintenance processes that describe how a
bug is created, prioritized, analyzed, fixed,
and delivered to the customer. The Genera-
tion 3 team also had a set of processes that
explain how to request, describe, prioritize,
and implement incremental enhancements

and deliver them to customers. Clearly, these
processes already incorporated important
XP elements—extreme maintenance means
following the XP model while a product is in
the mainstream of its product life cycle.2

The synergy was so great that we started
to use the term XP to describe the set of
processes and practices the Generation 3 el-
ement and product teams used. We then
started the XP experiment in earnest in an
attempt to resolve our outstanding issues.
Table 1 presents all of the practices pre-
sented in Extreme Programming Explained1

and describes how we approached them. 

Metaphor
When customers report a bug, they de-

scribe it in the form of a short story that ex-
plains what they are trying to do and how
the problem manifests itself. We then note
this story in our defect-tracking system and
make it available to the customer and every-
one working with that customer. 

When we receive an enhancement request
or functional requirement, we present it as a
specification on our requirements Web page.
Because of the need for global visibility, and
because customer service representatives and
engineers must be able to update the system,
unlike the bug reports, these requirements are
not presented as index cards and don’t read as
stories (yet). However, they do contain essen-
tially the same information as traditional re-
quirements. For the purpose of our XP exper-
iment, we left the enhancement request until a
later improvement phase and instead focused
on tracking the bug-fixing stories in a well-
structured Web-based queuing system, visible
to the internal customer (or Customer Ser-
vice), engineers, and engineering managers. 

We liked the idea of using index cards to
track tasks and decided to implement a story-
board for each of the element teams compris-
ing our consolidated Generation 3 team. Our
goal was to improve the poor visibility of
each engineer’s work effort. We wrote each
task, regardless of whether it was a customer
issue or internal project task, on a color-coded
card (see Figure 1). Nirmalya Sengupta, our
Generation 3 operations lead, then sat down
with our customer (or Customer Service rep-
resentative), so he could prioritize his list of
bug and internal tasks. The customer could
also view internal tasks and ask to have his
tasks escalated above internal tasks. Cur-

We were
suddenly 

on the road 
to a consistent
and automated

build, test, 
and release

infrastructure
for nightly

product builds
and tests. 

4 4 I E E E  S O F T W A R E N o v e m b e r / D e c e m b e r  2 0 0 1



rently, only customer issues are rigidly priori-
tized, but we intend to integrate internal tasks
into the prioritization process as well. (Kent
Beck suggests having a Web-accessible digital
camera positioned so that people can regu-
larly look at and zoom in on the story-
boards—but that’s for a future project.)

An engineer estimates the completion date
after 24 hours of initial analysis. When the

task is completed, the engineer records the ac-
tual date of completion, and if there were de-
lays, she notes reasons for the delays, observa-
tions, and lessons learned on the back of the
task card. We remove all closed tasks from the
board, extract the data, record it in a spread-
sheet, and store the task card in a task log. 

In general, bug-fixing tasks should be com-
pleted as two-week pieces of effort. If, after

N o v e m b e r / D e c e m b e r  2 0 0 1 I E E E  S O F T W A R E 45

Table 1
Extreme Programming practice adoption

Extreme Programming Adoption status The Orbix Generation 3 experience

The planning game Partial adoption We haven’t found it necessary to follow a strict iteration planning process. However, to 
understand and manage capacity, it is still critical to monitor estimates of ideal time and actual 
time to completion. Most bugs in our product take about two weeks to fix, so we fell into two-
week patch release iterations. Our customer service and sales organizations act as our customer
proxies, providing a coordinated twice-weekly prioritization of bug issues for each patch. They
also run the test cases that are our acceptance tests for each customer story against each 
patch delivered to a customer. We still get interruptions and midcycle reprioritization, but it’s 
unavoidable.

Small releases Followed religiously Short release cycle requirements drive our support efforts, so our customer patch cycle is two 
weeks. Our point release cycle has moved from months down to weeks. Without automated 
nightly regression and unit testing, we wouldn’t have come this far. Without a single button 
integration test capability, engineers wouldn’t have been able to ensure that changes had not 
broken code. Without optimizing our test suites around test time, we wouldn’t have been able to
provide the necessary timely feedback. 

Metaphor Adopted from start Our metaphor is the customer story as detailed in our bug-tracking system. Our customer
supplies our acceptance test for each story. 

Simple design Partial adoption There hasn’t been as much focus on simple design, because the product is complex, and we 
continue to use high to midlevel design documents to help people quickly understand the over
all system and some system components. In our maintenance environment, there is a clear 
story on which to focus. We tend not to generate design documents for fixes but instead focus 
on implementing the story and passing the acceptance and regression tests. 

Testing Adopted from start Test first is naturally a part of the maintenance process in the Generation 3 team. We don’t work
on bugs unless a test case is developed and all test cases are run nightly. We also use code
reviews to back up the lack of great test coverage. 

Refactoring Adopted from start We took this on as one of the cornerstones of our efforts in improving code maintainability 
and stability. More often than not, the call is to focus on refactoring as a part of an engineer’s
personal practices. Sometimes in refactoring, we have found ourselves doing wholesale 
reengineering, but this doesn’t happen often. 

Pair programming Sparingly adopted Although we’ve tried it, it is not something the team widely practices. We’ve used code reviews 
to try and address code standards, design of fixes, and so forth. 

Collective ownership Adopted from start We have people changing code everywhere. Some have a stronger knowledge of certain areas, 
but we have fewer people maintaining the code and we can’t afford not to have people working 
on all parts of the code. For the most part, we’ve used pairing to gain an edge. We also have a 
strong set of code standards we enforce not only at code reviews but also as a part of the 
check-in process in our source-control system. 

Continuous integration Adopted from start It took us 18 months to build an automated testing system that was comprehensive but also easy
to use and fast. Also, we don’t use integration stations. Instead we use a mutex file (requiring 
check in and check out) in our source control system to ensure that no one else is integrating 
while we are merging into the mainline. Our merge process is strong and followed quite well. 

40-hour week Not adopted We haven’t felt courageous enough to tackle this. 
On-site customer Adopted from start We use our customer service team and sales force to act in the role as a customer. They set 

priorities and generate acceptance tests.
Coding standards Adopted from start We had already started this one when we began using XP. 



analyzing an issue, an engineer estimates more
than two weeks’ worth of work, she splits the
story into several tasks. Anything more is a
refactoring project and should be structured
more along the lines of an enhancement with
clear incremental delivery of functionality
based on tasks extracted from the original cus-
tomer story.

We also incorporated the daily stand-up
meeting into the team (see Figure 2)—an XP
element. Each team took 15 to 30 minutes
to review their progress, and each person
took a couple of minutes to talk about what
he or she was working on that day and the
previous day. The focus was not only on im-
proving visibility but also on encouraging
communication between team members.
The qualitative results were immediate.
Some engineers found it difficult to explain
why they spent so much time on issues or
why they looked at things that weren’t high
priorities on the team’s list of tasks. 

Testing
Automation is essential, and one of the

first initiatives we undertook in 1999 was to
automate our test suites and speed them up.
We completed an extension of that effort in
2000, which lets developers build and test
the entire product set on any platform com-
bination we support by clicking a button
from a Web-based interface or using the
workstation command line. These initia-
tives let us test the entire product set against
the Corba specification, the Orbix API, and
customer-supplied test cases on a nightly ba-
sis or at the engineer’s discretion on the 17
platforms we support. Both the stories de-
scribing the functionality that the software
supports and those describing the bugs that
customers encountered have tests associated
with them. Engineering requires each cus-
tomer to provide a test case for each re-
ported bug prior to doing any work on the
issue. This test case is automatically in-
cluded in the test suite’s nightly runs. 

Any engineer must be able to build the
entire product or system and run the test
suite to get immediate feedback on changes
made. Nightly builds are a reasonable inter-
mediate step to this test-on-demand require-
ment, and again the processes we decided to
follow map well to the testing idea at the XP
model’s foundation—test before you imple-
ment, and test everything often.

Pairing
Pair programming in XP is critical to im-

proving collaboration and greatly facilitates
mentoring and improvements in engineering
practice. However, convincing engineers that
it is useful and should be incorporated into
their work practices is extremely difficult. 

Our pair programming experiment came
about accidentally in late 2000. The Gener-
ation 3 team worked on a particularly diffi-
cult set of customer issues. We had a cus-
tomer engineer working onsite with us.
(Having customers onsite is another XP
principle, although normally we treat our
onsite product managers and customer serv-
ice representatives as our customers.) At dif-
ferent times, the customer teamed up with
various members of the Generation 3 team
to do some pair programming. Likewise,
members of the team gravitated toward a
pair-programming approach as they pro-
gressed with the issues on which they were
working for this particular customer. 

In the end, the qualitative feedback from
the team was positive. They felt that they
worked more effectively and with a higher
level of communication as opposed to work-
ing independently on individual issues. Also,
the senior staff enjoyed some of the mentoring
that naturally occurred as they interacted with
the junior members of the group. The overall
productivity during this period was also high
and morale improved. Engineers got a lot of
positive reenforcement from each other, and
engineering practices and quality improved as
we actively noted coaching interactions be-
tween engineers. We also got no patch rejec-
tions for the work these engineers did. 

We are currently trying to formally imple-
ment pair programming. Until we do, we have
asked each engineer to take on one or two
tasks each quarter in which they pair with an-
other team member. We’ll note the tasks in
which they worked in pairs and will show
them the data at the end of the quarter to

4 6 I E E E  S O F T W A R E N o v e m b e r / D e c e m b e r  2 0 0 1

Pair
programming 

in XP is critical
to improving
collaboration
and greatly
facilitates

mentoring and
improvements
in engineering

practice.

Figure 1. The color-
coded task card.

Date queue   Date active     Expect finish   Date closed

Task:
Area:                Details:

Customer:

Engineer:



(hopefully) reinforce the results we got with
our initial pair-programming experiment.

Small releases
In addition to describing functionality, XP

focuses on increments of functionality that
engineers can quickly merge into the main-
line. They merge in bug fixes quite rapidly,
and with the improved nightly build and test
and the one-button test, they can make sys-
tems available as a temporary or permanent
patch within one day. Any initial effort
should focus on releasing a patch every itera-
tion for a large, complex system—and au-
tomation and improved testing enable that
end. For projects approaching XP at midlife,
we encourage focusing on automation at any
costs. Rapid turnaround time can only im-
prove customer satisfaction. Unfortunately,
because we just recently started collecting
metrics on mainline breakage, we don’t have
statistics to show what is currently a qualita-
tive analysis of Iona’s improvement. 

In addition, because the code base we were
working with was a legacy code base, we did
not have much to say about how it was put to-
gether. However, we needed to make it main-
tainable and add functionality if required. As
mentioned earlier, one of the improvement ef-
forts we undertook was a major reengineering
of elements of the Orbix product. This effort
incorporated stories that typically took three
to four weeks to implement. By keeping these
increments relatively short, we could more ef-
fectively manage the delivery schedule of our
last minor point release by modifying the pro-
ject’s scope but not its delivery date, because
we were almost always at a point where we
could deliver a functional product. 

Refactoring
With regards to refactoring, analyzing

bug statistics is important in identifying ar-
eas of code that might need refactoring or
reengineering. This form of customer feed-
back is an important part of targeting im-
provements and stopping code entropy.
Refactoring should always be on an engi-
neer’s mind when analyzing a problem or
enhancement request: “How can I make this
area of the code work better?” as opposed
to “How can I quickly fix this problem (us-
ing the band-aid approach)?”

We addressed refactoring in our effort to
improve personal practices. Recent code re-

views have revealed that refactoring has be-
come part of the engineers toolkit when ap-
proaching a problem. Along with analyzing
a problem, they now identify whether the
area of code they are working on is a candi-
date for refactoring, and they follow up after
delivery with a refactoring task on the story-
board. Sometimes this extends into a reengi-
neering effort if the scope of improvement is
increased, and for some customer issues, we
can’t afford the time to do the refactoring ef-
fort due to commitments to restore times.
However, for the most part, we’ve seen
fewer problems in our new releases, a signif-
icant decrease in code entropy (as measured
by comments from code reviewers), no patch
rejections over the last few months, and re-
duced code complexity (again, as measured
by comments from code reviewers). Unfor-
tunately, we have not been able to collect
complexity metrics to verify this. 

Facilities strategy
A maintenance team environment should

combine personal and team space to en-
hance interactions and collaborations that
should occur in the team. Although not an
explicit XP principle, our facilities strategy
continues to be key to XP’s success. The pair
programming could not have happened
without changing the engineers’ workspace,
and the collective ownership, testing, and
continuous integration would have suffered. 

Prior to initiating our XP project, Iona’s en-
gineering workspace consisted of the typical
floor plan—it had multiple bays, each con-

N o v e m b e r / D e c e m b e r  2 0 0 1 I E E E  S O F T W A R E 47

Figure 2. A stand-up
meeting at the 
storyboard.



taining six shoulder-height cubicles (see Figure
3a). The engineers and engineering managers
initiated a project to restructure the space in
which they worked to enhance team commu-
nication. They created a large central area by
eliminating 20 percent of the individual cubi-
cles and establishing nondelineated group
workspaces. The group workspaces consist of
either a round or rectangular meeting table or
workstation area. In addition, white boards
and flipcharts on tripods were made available
in each work area. They also purchased a
couch, and a catering service now delivers
snacks on a weekly basis (see Figure 3b).

The results are noticeable. Code reviews
happen in the group area; pair programming
happens at the workstations. People discuss
their ideas on the whiteboards and large
flipcharts. There is a measurable increase in
the visibility of what everyone is doing on the
team and the number of conversations and

collaborations that occur. These changes to
the environment were instrumental to our
success in improving overall team morale. We
also have had a visible increase in the number
of team interactions—not only on code re-
views but also regarding ongoing tasks. 

Qualitative improvements
Our metrics (although not particularly

extensive) seem to show some quantitative
improvements in productivity during the pe-
riod in which we used pair programming to
address specific deliverables or bug fixes for
a major customer. The productivity is based
on a constant work force with no change in
overall work habits in terms of hours spent
fixing bugs. This productivity increase has
continued beyond the initial experiment and
is perhaps attributable to more people using
pair programming and open collaborative
spaces created during that period. 

4 8 I E E E  S O F T W A R E N o v e m b e r / D e c e m b e r  2 0 0 1

Figure 3. Orbix 
Generation 3 cubicles
(a) prior to initiating
the Extreme 
Programming project
and (b) after.

20

18

16

14

12

10

8

6

4

2

0

7-
Se

pt
-0

0

14
-S

ep
t-0

0

21
-S

ep
t-0

0

28
-S

ep
t-0

0

5-
Oc

t-0
0

12
-O

ct
-0

0

19
-O

ct
-0

0

26
-O

ct
-0

0

2-
No

v-
00

9-
No

v-
00

16
-N

ov
-0

0

23
-N

ov
-0

0

30
-N

ov
-0

0

7-
De

c-
00

14
-D

ec
-0

0

4-
Ja

n-
01

11
-J

an
-0

1

18
-J

an
-0

1

25
-J

an
-0

1

1-
Fe

b-
01

8-
Fe

b-
01

15
-F

eb
-0

1

22
-F

eb
-0

1

1-
M

ar
-0

1

8-
M

ar
-0

1

15
-M

ar
-0

1

22
-M

ar
-0

1

29
-M

ar
-0

1

5-
Ap

r-
01

13
-A

pr
-0

1

20
-A

pr
-0

1

29
-A

pr
-0

1

6-
M

ay
-0

1

Week ending

2
3

2
1

2

0

2
1

9
8

7

2
3

1

4

1
0

3

0

4

00

8

4
3

6

9

3 3

2

18

10
9

22 2
1

00

4 4
3

6
5 5 5

7 7

3 3

5 5
6 6

7

2 2
1

6
5 5 5

4

11
12

New bugs

Closed bugs

8 week moving average (closed)

Nu
m

be
r o

f b
ug

s

Figure 4. Team bug-
fixing productivity.

(a) (b)



As Figure 4 shows, the month of Novem-
ber (the period of this effort) saw a measur-
able peak in productivity over and above
the previous several months. This was an
improvement of 67 percent, based on the
next most productive five-week period, even
though the total number of people-weeks
missed due to holidays during the two most
productive periods is relatively equivalent
(Christmas in the later half of December
and the Indian holiday Diwali at the end of
October and early November). Looking at
the trend line, we see continued improve-
ment, even though the overall team size was
reduced in March 2001 from 36 to 25.

Figure 4 shows several things. First, the
new bugs represent issues that the team
must address. An eight-week running aver-
age trend line would show that this averages
out to four issues a week. Second, the num-
ber of closed issues represents the number of
issues that are resolved in a patch to the cus-
tomer. This patching cycle represents our
XP iteration and is on the order of two to
three weeks (note the closure peaks). 

One of the greatest success stories is im-
provements in visibility. This is the greatest
benefit to the team. Having a storyboard on
which to daily prioritize tasks and discuss
progress encourages best practices, lets peo-
ple see what others are doing, and lets man-
agement gauge progress. Figure 5 shows the
dramatic improvements in our workflow
queues as a result of using the storyboard.
When we installed the board in February
2001, it focused people’s attention on issues
that went unverified for significant amounts
of time. We also saw a dramatic increase in
the number of issues that people started to

actively work on. Visibility alone was a
strong motivational factor in this turn-
around. In the future, we hope to scale this
practice across multiple development sites.

H ow can XP help us further improve?First, improving the pair program-ming initiative can improve our
lack of cross-training among the code base’s
many modules. It is not a practice for which
this is intended but is a useful side benefit to
a real problem as the team grows smaller.
Engineers have accepted the benefits, but we
are still in the process of structuring a more
general approach to ensuring that the prac-
tice of pairing becomes part of the Iona cul-
ture. Earlier this year, we proposed having
an individual sign up for each new bug, en-
hancement request, or refactoring effort,
making that individual responsible for grab-
bing a partner and working through the is-
sue. Our technical leads felt comfortable
with this approach and they are now using
it as a part of the standard process.

Second, metrics are critical to the replan-
ning game, but getting engineers to contribute
is difficult. To plan how long it will take to
complete a story, you must know how long it
took to complete a similar piece of work based
on the estimates of the engineer to whom the
work is assigned. “As the basis for your plan-
ning, assume you’ll do as much this week as
you did last week.”3 Kent Beck and Martin
Fowler call this rule Yesterday’s Weather, and
we think it is an appropriate analogy. 

We are currently running an internal proj-
ect to improve our ability to enter metrics into
our defect tracking system and report on them

N o v e m b e r / D e c e m b e r  2 0 0 1 I E E E  S O F T W A R E 49

70

60

50

40

30

20

10

0

8-
Ja

n-
01

15
-J

an
-0

1

22
-J

an
-0

1

29
-J

an
-0

1

5-
Fe

b-
01

12
-F

eb
-0

1

19
-F

eb
-0

1

22
-F

eb
-0

1

1-
M

ar
-0

1

8-
M

ar
-0

1

15
-M

ar
-0

1

22
-M

ar
-0

1

29
-M

ar
-0

1

5-
Ap

r-
01

13
-A

pr
-0

1

20
-A

pr
-0

1

29
-A

pr
-0

1

6-
M

ay
-0

1

Week ending

6

To
ta

l n
um

be
r o

f i
ss

ue
s

7

11

31

4

10

12

32

2

10

10

33

3

9

10

33

2

9

10

33

3
6

8

32

3

7

8

32

7

11

13

6

13
8

11
14

14
9

5 5

13

11

11

4

12

17

11

2

10

15

10

9

12

18

15

5

7

19

13

8

10

20

13

6

10

22

10

11

10

12

8

23

Implemented

Active

Verified

Submitted

Figure 5. Workflow
queues.



automatically. The volume of issues makes it
difficult to track using a simple spreadsheet,
but we do it anyway. Our Operations Lead
speaks to each developer and notes how they
have progressed on the work they are doing.
The limited metrics we have indicate that it
takes an engineer on average two weeks to fix
a bug.3 This matches the suggested length of
time to implement a story in the XP model. By
measuring the calendar time with the defect
tracking system, we can measure the calendar
time it took to resolve an issue and ultimately
the team’s velocity.3 This information is in-
valuable in planning what we will do in the
next patch or release iteration.

Acknowledgments
The authors thank Kent Beck for working with the

Orbix Generation 3 team and the invaluable insights
into the development process he provided that have mo-
tivated our teams to adopt XP. We also acknowledge the
influence of ideas presented in Refactoring: Improving
the Design of Existing Code (M. Fowler, Addison-Wes-
ley, 1999) and Extreme Programming Installed (R. Jef-
fries, A. Anderson, and C. Hendrickson, Addison-Wes-
ley, 2000). We presented an earlier version of this article
at XP Universe in July 2001.

References
1. K. Beck, Extreme Programming Explained: Embrace

Change, Addison-Wesley, Reading, Mass., 1999.
2. G.A. Moore, Crossing the Chasm: Marketing and Sell-

ing High Tech Products to Mainstream Customers,
Harper Collins, New York, 1999, p. 47. 

3. K. Beck and M. Fowler, Planning Extreme Program-
ming, Addison-Wesley, Reading, Mass., 2000.

For more information on this or any other computing topic, please visit our
Digital Library at http://computer.org/publications/dlib.

I E E E  S O F T W A R E N o v e m b e r / D e c e m b e r  2 0 0 1

About the Authors

Charles Poole is a senior engineering
manager at Iona Technologies.  His research in-
terests include Extreme Programming in large
distributed development environments and self-
assembling software paradigms.  He received his
BSE in aerospace engineering from the Univer-
sity of Michigan and his MSE in astronautical en-
gineering from the Air Force Institute of Technol-
ogy. Contact him at charles.poole@iona.com.

Jan Willem Huisman
is currently working with Iona Technologies in
Dublin as an engineering manager responsible
for the maintenance team for the Orbix Genera-
tion 3 product. He has extensive experience in
software quality control and software develop-
ment. He has also been involved in the entire
software development cycle in the areas of pro-
gramming, technical design, testing, project man-
agement, and maintenance. Contact him at jan.willem.huisman@iona.com.

IEEE Computer Society
10662 Los Vaqueros Circle

Los Alamitos, California 90720-1314
Phone: +1 714  821 8380

Fax: +1 714  821 4010
http://computer.org

advertising@computer.org

Advertising Personnel

Advertiser                Page Number

ADVERTISER INDEX
N O V E M B E R  /  D E C E M B E R  2 0 0 1

James A. Vick
IEEE Staff Director, Advertising
Businesses
Phone: +1 212 419 7767
Fax: +1 212 419 7589
Email: jv.ieeemedia@ieee.org

Marion Delaney
IEEE Media, Advertising Director
Phone: +1 212 419 7766
Fax: +1 212 419 7589
Email: md.ieeemedia@ieee.org

Marian Anderson
Advertising Coordinator
Phone: +1 714 821 8380
Fax: +1 714 821 4010
Email: manderson@computer.org

Sandy Brown
IEEE Computer Society,
Business Development Manager
Phone: +1 714 821 8380
Fax: +1 714 821 4010
Email: sb.ieeemedia@ieee.org

Debbie Sims
Assistant Advertising Coordinator
Phone: +1 714 821 8380
Fax: +1 714 821 4010
Email: dsims@computer.org

Atlanta, GA
C. William Bentz III
Email: bb.ieeemedia@ieee.org
Gregory Maddock
Email: gm.ieeemedia@ieee.org
Sarah K. Huey
Email: sh.ieeemedia@ieee.org
Phone: +1 404 256 3800
Fax: +1 404 255 7942

San Francisco, CA
Matt Lane
Email: ml.ieeemedia@ieee.org
Telina Martinez-Barrientos
Email: Telina@husonusa.com
Phone: +1 408 879 6666
Fax: +1 408 879 6669

Chicago, IL (product)
David Kovacs
Email: dk.ieeemedia@ieee.org
Phone: +1 847 705 6867
Fax: +1 847 705 6878

Chicago, IL (recruitment)
Tom Wilcoxen
Email: tw.ieeemedia@ieee.org
Phone: +1 847 498 4520 
Fax: +1  847 498 5911

New York, NY
Dawn Becker
Email: db.ieeemedia@ieee.org
Phone: +1 732 772 0160
Fax: +1 732 772 0161

Boston, MA
David Schissler
Email: ds.ieeemedia@ieee.org
Phone: +1 508 394 4026
Fax: +1 508 394 4926

Dallas, TX
Royce House
Email: rhouse@houseco.com
Phone: +1 713 668 1007
Fax: +1 713 668 1176

Japan
German Tajiri
Email: gt.ieeemedia@ieee.org
Phone: +81 42 501 9551
Fax: +81 42 501 9552

Europe
Glesni Evans
Email: ge.ieeemedia@ieee.org
Phone: +44 193 256 4999
Fax: +44 193 256 4998

Boston University 15

California State University Northridge 6

Compaq Cover 4

Eaton 12

John Wiley Inside Front Cover

Monmouth University 16

ParaSoft 5

Requirements Engineering Conference 2002 1



focus

0 7 4 0 - 7 4 5 9 / 0 1 / $ 1 0 . 0 0  ©  2 0 0 1  I E E E N o v e m b e r / D e c e m b e r  2 0 0 1 I E E E  S O F T W A R E 51

This article reports our work with im-
provement initiatives in a fast-moving soft-
ware organization called Linq. Since the
company’s start in 1996, it has grown from
five to 340 employees and undergone major
changes in organization, technology, and
strategy. Adapting improvement ideas was
challenging because commitment and re-
sponsiveness to improvement fluctuated de-
pending on the organization’s preoccupa-
tion with other challenges. 

The key to addressing this issue lies in the
emerging cultures of such organizations.
The culture is the result of the organization’s
attempts to deal effectively with its environ-
ment;7 it is not explicitly created. Rather, it
emerges through behavioral responses to
challenges and problems. We can express
such behaviors as survival patterns.8 These
patterns are activated in our daily work, and
they help us make priorities, solve problems,
and do things, but they can collide when

new work practices challenge traditions.
From this context, we examine how to un-
derstand and facilitate improvement in dy-
namic software organizations while preserv-
ing their capacity for innovation.

A fast-moving software company
Linq sprang from the idea of using collab-

orative software to support workflow and
projects in knowledge companies.9 Although
the company changed from consulting to soft-
ware product development, the basic business
idea remained the same—and it profoundly
affected the way the company conducted SPI.

Creation
In January 1996, Michael Mandahl and

Jan Morath founded Linq. They wanted to
start a company that would help its customers
make their employees contribute more value
to the organization by working together. The
company had a simple structure,10 with Man-

Survival Patterns in 
Fast-Moving Software
Organizations

Lena Holmberg, Sydney Systems

Lars Mathiassen, Aalborg University

Fast-moving
software
organizations
must respond
quickly to
changing
technological
options and
market needs.
They must also
deliver high-
quality products
and services 
at competitive
prices. The
authors
describe how to
deal effectively
with such
dilemmas and
opportunities.

S
oftware practices change. Many managers adopt software process
improvement initiatives to increase their organizations’ ability to
develop high-quality services and institutionalize state-of-the-art
disciplines.1–3 At the same time, approaches such as open

source4,5 and Extreme Programming6 introduce new and innovative ways to
develop software and force most organizations to choose between improv-
ing present practices and supporting innovation.

reports from the field



dahl as CEO and the rest working on projects. 
Linq grew steadily, and although most

employees came fresh from universities, ex-
perienced IT consultants also joined. A cus-
tomer-specific solution turned into a prod-
uct, although the major part of the business
still focused on consulting.

The founders soon discovered that al-
though customer satisfaction was high, effi-
ciency was too low. The organization
changed from a simple structure into one
composed of four teams headed by a team
manager,10 but employees still conducted
projects in an ad hoc fashion, and learning
from experience was difficult.

Professionalization
During the summer of 1997, the founders

realized that they needed help to accomplish
process improvement, so they hired an SPI
consultant. Top management committed to
SPI, forming four task forces to improve
project start, requirements management,
testing, and customer management and ap-
pointing an SPI manager to coordinate the
groups. Although all the task forces pro-
duced results, implementation was slow.

To make the improvements more visible,
management set a clear objective: Linq would
perform at Software Capability Maturity
Model (CMM) Level 2 by September 2000.
An internal assessment in September 1998
started the initiative. The results, although
devastating, encouraged new commitments.
Four new task forces formed (after the initial
four completed their missions): project
method development, formal reviews, elec-
tronic project room, and training and diffu-
sion. Carefully selected project members from
all parts of the organization joined the groups
to ensure a broad reach. This approach to im-
proved project practices—based on the Linq
business idea—was named LinQing. Linq in-
ternally developed the LinQing framework
for cooperation in software projects. The pur-
pose was to create a collaborative space for
innovation and learning through joint use of
simple control mechanisms. One of this arti-
cle’s authors, Lars Mathiassen of Aalborg
University, helped develop it. 

The process started with Steve McConnell’s
Software Project Survival Guide.11 The task
forces presented the resulting templates and
instructions in Lotus Notes databases, and
training started in spring 1999. From the start,

two emphases characterized LinQing: 

� collaboration and competence transfer
between Linq and the customer, and 

� using the customer and IT to support
the project process. 

Implementing LinQing was never manda-
tory under the SPI recommendations, but
performing in accordance to CMM Level 2
was an objective. Unfortunately, the innova-
tion-oriented founders did not always use
LinQing in their own projects, which created
a mixed message. In spring 1999, many em-
ployees expressed their frustration with the
way projects were accomplished and de-
manded more structure, which resulted in
further diffusion and finalizing of LinQing.

From September 1997 to June 1999,
Linq grew from 30 to 100 employees. The
projects involved larger and more demand-
ing customers, and the company reached a
higher level of professionalism—with for-
mal contracts, formal project plans, and sys-
tematic tracking and oversight.

Transformation
In spring 1999, a window of opportunity

opened for Linq—namely, to produce Linq-
Portal, a corporate portal product based on
Microsoft technology. In June 1999, the
company reorganized, separating product
development from consulting.10 Simultane-
ously, plans for a larger and faster European
expansion emerged, and a search for in-
vestors began. The product, the CEO’s en-
trepreneurship, the tight upper-management
team, and the company’s performance im-
pressed investors. The investors also stressed
the existence and practical use of LinQing as
one of the organization’s key assets. The
company grew from 100 employees to 340,
and new offices opened in several countries.

Although the consultants still worked
with customers, the company focused more
on designing and delivering a product for a
perceived market need and on building a
sales force. A major R&D project started in
the summer: developing a mobile version of
LinqPortal.

When starting the new product division,
the chief technology officer decided that all
projects should use LinQing. The SPI man-
ager formed and headed a formal SPI unit in
the product division. The team’s five mem-

This approach
to improved

project
practices—
based on the
Linq business
idea—was

named LinQing. 

5 2 I E E E  S O F T W A R E N o v e m b e r / D e c e m b e r  2 0 0 1



bers worked part-time in product develop-
ment or as consultants to ensure diffusion
of the results and development of the right
relevance criteria.

Because LinQing was designed for consult-
ing, the SPI unit started working on special
editions for product development. The team
incorporated training into new-employee ori-
entation, and Linq initiated a simple metrics
program that emphasized the packaging of
relevant LinQing features. The SPI unit pro-
duced product information sheets, put to-
gether physical folders in addition to the in-
formation presented on the intranet, and
introduced a special strategy for corporate
portal projects: instant deployment. The SPI
initiative was thus quickly tailored to the or-
ganization’s specific needs—to support prod-
uct development and sales of LinqPortal.

In early spring 2000, the SPI unit was
dissolved and diffused on the SPI manager’s
initiative into the rest of the organization. It
had delivered special editions of LinQing,
and the organization needed to focus on ap-
plying them. Members of the former SPI
group continued diffusion work by arrang-
ing training courses and presenting informa-
tion at meetings. 

Epilogue
The focus then changed to developing

and selling a new product. Management
considered producing customer satisfaction
and delivering a product that could meet
market demand to be vital.

Major changes occurred in the organiza-
tion. Sales separated from consulting, product
management separated from product develop-
ment, and the number of consultants decreased
to reduce costs (which was necessary to attract
new investors).10 The business and product di-
visions started deciding how to best use legacy
practices to improve production. At this point,
Linq had the infrastructure and competence
needed to perform at CMM Level 2, but actu-
ally using it would require increased commit-
ment throughout the organization. 

On 23 April 2001, Linq filed for liquida-
tion in Sweden. Although LinqPortal received
recognition as one of the best of its kind, the
market had not evolved as predicted. The in-
vestors quickly decided not to go through with
their long-term plans, and liquidation was the
only alternative. The business was split into
parts and sold to other companies. The

founders and approximately 50 other employ-
ees now work in a new software company that
focuses on LinqPortal’s mobilility.

Survival patterns
Two survival patterns drove Linq’s behav-

ior and management priorities: innovation
and improvement (see Table 1). Each one is
characterized by the behavior of the employ-
ees, the organization’s requirements, and as-
sumptions about the nature of the environment.

Innovation
The innovation pattern is strategy-driven.

A fast-moving software organization’s envi-
ronment is extremely dynamic: technology
and market conditions change constantly,
inviting or forcing the organization to adapt
or change its behavior. Investing in infra-
structures does not pay for the organization
because they make it difficult to respond ef-
fectively to new environmental conditions.
To facilitate learning, foster new ideas, and
create the dynamics needed to respond
quickly to new opportunities and demands,
all members of the organization must interact
with each other, customers, and external
players with relevant knowledge and experi-
ence. In other words, to create innovations at
a reasonable speed, networking is important.

Throughout Linq’s rather short history, it
underwent major changes as a result of re-
sponses to internal and external opportunities
and challenges. The shift from Lotus Notes to
Microsoft-based solutions was one such ex-
ample of market-driven considerations. Simi-
larly, moving from focusing on projects for
specific customers to emphasizing internally
developed products for corporate portal solu-
tions was another major change. The com-
pany needed many major innovations to de-
velop new management practices in response
to its fast growth, gradually transform into an
international rather than a national player,
and successfully develop LinQing. The inno-
vation culture emerged from the start, with
the behavior of the two founders, and it flour-
ished and continued to develop in response to
a highly dynamic environment.

N o v e m b e r / D e c e m b e r  2 0 0 1 I E E E  S O F T W A R E 53

Table 1
Two complementary survival patterns

Survival pattern

Level Innovation Improvement
Operational (behavior) Network Deliver
Tactical (organization) Flexible Supportive
Strategic (environment) Dynamic Stable



5 4 I E E E  S O F T W A R E N o v e m b e r / D e c e m b e r  2 0 0 1

Improvement
Software people want to do a good job—as

professionals, they want to deliver high-qual-
ity solutions in response to customer or mar-
ket needs. The organization must develop so-
lutions that satisfy its customers and generate
sufficient revenue—or it won’t survive. At the
operational level, a mission to deliver satisfac-
tory solutions drives this pattern. To achieve
this, the organization must offer a supportive
infrastructure that makes it easy (and possible)
to reuse successes from one project to the next
and a management tradition that encourages
(rather than hinders) professional practices. To
build such a supportive infrastructure, you
must make certain assumptions about the
types of projects, technologies, and solutions
to support. In this way, we see certain parts of
the environment as being stable.

The founders imported an improvement
culture in response to problems experienced
with projects. It also had to change from a
simple structure to one composed of teams
and team managers, and an infrastructure
developed to make better projects. This im-
provement initiative combined Linq’s col-
laboration and networking techniques with
state-of-the-art ideas on SPI. Initiated by de-
sign, the improvement pattern grew to be-
come an integral part of Linq’s culture.

Dynamics
The innovation and improvement patterns

are complementary, but tensions easily arise
between them. The innovation pattern gener-
ates a pull toward minimal and highly flexi-
ble infrastructures; the improvement pattern
generates a contradictory pull toward sup-
portive and more elaborate infrastructures.

The innovation culture naturally dominates
in the beginning with its ad hoc structures and
mutual adjustment as key coordination mech-
anisms.10 As the software organization grows
and matures, more elaborate structures de-
velop and different forms of standardization
occur to exploit past successes and increase
management control.10 The defining property
of quickly evolving software organizations is,
however, their strategic drive to respond effec-
tively to the opportunities and challenges gen-

erated through their environments. We should
therefore expect a constant struggle between
the innovation and improvement cultures,
with changing patterns of domination but the
innovation paradigm having the upper hand.

Although Linq experienced both patterns,
their role and relationship changed (see Table
2). During the creation phase, innovation
values nearly exclusively drove the behavior.
The Linq concept was developed and imple-
mented through intensive collaborations
with customers, but little attention was paid
to improvement values (beyond each individ-
ual project) and few resources were used to
develop organizational infrastructures.

Driven by the company’s experiences and
pressure to improve, this picture changed
dramatically as Linq moved into its profes-
sionalization phase. During this period,
management initiated and heavily supported
improvement activities, and most members
of the organization took an active part in at-
tempts to build supportive infrastructures.

In response to new business opportuni-
ties, Linq entered the transformation phase
to pursue corporate portal technologies and
emphasize product development. Manage-
ment heavily downsized the improvement
efforts, new SPI processes were not devel-
oped, and the emphasis was solely on main-
taining the current position.

Lessons learned
Each software organization has its own

history and needs to make strategic decisions
that fit its unique environment. Linq’s lessons
are therefore not directly transferable to
other software organizations. We have, how-
ever, learned certain lessons that might in-
spire other fast-moving software organiza-
tions in their ongoing struggle to cope with a
dynamic environment while simultaneously
trying to improve professional practices.

Appreciate the survival game 
Everyone in a dynamic software organiza-

tion must realize the reciprocal relationship
between innovation and improvement. Both
values and practices must be actively sup-
ported and cultivated to create a sustainable
software business. Both need top manage-
ment support in terms of resources and
recognition, and the different talents and dis-
ciplines involved must constantly be devel-
oped and maintained.

Table 2
The dynamics of Linq’s survival patterns

Pattern Creation Professionalization Transformation

Innovation 95% 30% 80%
Improvement 5% 70% 20%



Protect the improvement culture 
Fast-moving software organizations are

constantly on the move—not because they
find this behavior particularly attractive,
but simply because their raison d’être is to
constantly adapt to an extremely turbulent
environment. Recruiting resources to work
with improvement and creating the neces-
sary commitment toward improvement is
therefore difficult. Innovation always re-
ceives more hype, and the urgency and en-
ergy involved in innovative activities easily
become an excuse for giving low priority to
improvements. When innovation domi-
nates, protecting and maintaining the im-
provement culture is particularly important.

Create innovative improvements 
To keep up with the organization’s inno-

vation, the people working with SPI must be
agile and creative. They must anticipate the
possible next steps in technology, software
development, and customer relations and
constantly evaluate the consequences these
might have for the organization. SPI activi-
ties should adapt to changing requirements,
and the SPI organization should be minimal
and adaptive. Key practices should be based
on active networking in which software de-
velopers, managers, and customers partici-
pate actively in creating and implementing
new improvements.

Improve the ability to innovate
Improvements must be conceived as rele-

vant and useful in the software organization.
A conventional approach to SPI that starts by
addressing the six key process areas on CMM
Level 2 will have little chance of creating the
necessary commitment in dynamic software
organizations. Classical key process areas
should be considered, but they must be com-
plemented with other ideas that focus on the
needs and practices of an innovative software
culture. Otherwise there is little chance of suc-
cess with SPI. This is why Linq used LinQing
as the framing device for SPI. LinQing unifies
the basic business idea of supporting collabo-
ration between professionals using modern
information technology with state-of-the-art
disciplines in software project management. 

Don’t specialize 
For the SPI organization, understanding

the business is vital, so those involved in SPI

must actively take part in the core processes.
SPI people should develop double careers:
one in SPI and one in software development
or management. In that way, they build a
good sense of what it takes to be fast mov-
ing. Management is well advised to make
participation in SPI activities an important
career step and to avoid having a small
group of professionals specialize in SPI. 

T hese basic lessons can help dynamicsoftware organizations face theirbasic paradox. SPI is particularly
important in such organizations—other-
wise, they have little chance of surviving. At
the same time, however, fast-moving organ-
izations are the most difficult ones to im-
prove in a sustainable way.

References
1. B. Fitzgerald and T. O’Kane, “A Longitudinal Study of

Software Process Improvement,” IEEE Software, vol.
16, no. 3, May/June 1999, pp. 37–45.

2. K. Wiegers, “Software Process Improvement in Web
Time,” IEEE Software, vol. 16, no. 4, July/Aug. 1999,
pp. 78–86.

3. K. Kautz, “Making Sense of Measurement for Small
Organizations,” IEEE Software, vol. 16, no. 2,
Mar./Apr. 1999, pp. 14–20.

4. E.S. Raymond, The Cathedral & the Bazaar, O’Reilly,
Sebastopol, Calif., 1999.

5. J. Ljungberg, “Open Source Movements as a Model for
Organizing,” European J. Information Systems, no. 9,
2000, pp. 208–216.

6. K. Beck, Extreme Programming Explained: Embrace
Change, Addison-Wesley, Reading, Mass., 1999.

7. E.K. Schein, Organizational Culture and Leadership: A
Dynamic View, Jossey-Bass, San Francisco, 1985.

8. G.M. Weinberg, Becoming a Technical Leader: An Or-
ganic Problem-Solving Approach, Dorset House, New
York, 1986.

9. T.H. Davenport and L. Prusak, Working Knowledge:
How Organizations Manage What They Know, Har-
vard Business School Press, Boston, 1998.

10. H. Mintzberg, Structure in Fives: Designing Effective
Organizations, Prentice-Hall, Upper Saddle River, N.J.,
1983.

11. S. McConnell, Software Project Survival Guide, Mi-
crosoft Press, Redmond, Wash., 1998.

N o v e m b e r / D e c e m b e r  2 0 0 1 I E E E  S O F T W A R E 55

About the Authors

Lena Holmberg is the managing director of Sydney Systems, a family enterprise focus-
ing on knowledge management. After her PhD in educational research at Göteborg University,
she joined Linq. Over five years, she held various positions such as Chief Knowledge Officer,
HR Director, and consultant, and was responsible for the Software Process Improvement initia-
tive. Contact her at Sydney Systems, N. Skattegård, Upplid 330 17 Rydaholm, Sweden; lena_
sydney@hotmail.com.

Lars Mathiassen is a professor of computer science at Aalborg University, Denmark.
His research is in software engineering and information systems, most of it based on close col-
laboration with industry. He has coauthored many books including Computers in Context
(Blackwell, 1993), Object Oriented Analysis and Design (Marko Publishing, 2000), and Im-
proving Software Organizations: From Principles to Practice (Addison-Wesley, 2001). Contact
him at the Dept. of Computer Science, Aalborg Univ., Fredrik Bajers Vej 7E, 9220 Aalborg Øst,
Denmark; larsm@cs.auc.dk; www.cs.auc.dk/~larsm.



5 6 I E E E  S O F T W A R E N o v e m b e r / D e c e m b e r  2 0 0 1 0 7 4 0 - 7 4 5 9 / 0 1 / $ 1 0 . 0 0  ©  2 0 0 1  I E E E

takes years to learn a certain rule—positive or
negative—about his or her behavior. As a re-
sult, programmers might turn to the Personal
Software Process to help decrease mistakes.

We show how to accelerate this process
of learning from mistakes for an individual
programmer, no matter whether learning is
currently fast, slow, or very slow, through
Defect Logging and Defect data Analysis
techniques.

The Personal Software Process
One successful approach to improving

individual learning is the PSP, developed by
Watts Humphrey.1 PSP provides effort esti-
mation and time-planning techniques and a
complete framework for process definition,
process measurement and control, and con-
tinuous process improvement. IEEE Soft-
ware published a report on the PSP course’s
success.2

PSP techniques for improving learning
from mistakes are based on defect logs.
Programmers record each defect they find
during design, design inspection, coding,
code inspection, testing, or operation. They
also record the defect’s description, type
classification and origin, the cause of the
human error leading to the defect (if
known), and the time required for locating
and repairing it. When programmers collect
enough defect data, they analyze the data
for recurring mistakes; they then convert
these insights into entries on inspection
checklists and changes to the development
process.

However, learning the PSP methodology
requires an immense effort. The standard
form (aimed at undergraduate or graduate
software engineering courses) requires 15
complete working days (spread over 15
weeks). Many project managers are inter-

focus
Accelerating Learning 
from Experience: 
Avoiding Defects Faster

Lutz Prechelt, Universität Karlsruhe

Defect logging 
and defect data
analysis aims 
to decrease
programmers’
repetitive errors.
DLDA is inspired
by the Personal
Software Process
but has a 
much lower
learning cost. 
A controlled
experiment
validated the
technique. 

A
ll programmers learn from experience. A few are rather fast at it
and learn to avoid repeating mistakes after once or twice. Others
are slower and repeat mistakes hundreds of times. Most pro-
grammers’ behavior falls somewhere in between:  They reliably

learn from their mistakes, but the process is slow and tedious. The proba-
bility of making a structurally similar mistake again decreases slightly dur-
ing each of some dozen repetitions. Because of this a programmer often 

reports from the field



ested in PSP—until they hear about the
learning effort. Most industrialists consider
even a reduced PSP variant (such as in
Humphrey’s Introduction to the Personal
Software Process)3 far too expensive to learn
in terms of time. Furthermore, the volume of
bookkeeping the PSP proposes is so large,
the data quality might become dubious.4

The problem with PSP is that program-
mers cannot learn it just by listening to a
few presentations and then applying these
techniques to their daily work. Under nor-
mal work conditions, few programmers can
continuously follow all the cumbersome
PSP techniques, especially before they expe-
rience their advantages. Programmers only
experience the benefits if they practice the
techniques for some time. That is why the
course is required—for providing a pres-
sure-free playground in which to learn
about the effectiveness of PSP techniques.

DLDA technique
It is possible to learn and apply defect log-

ging and defect data analysis (DLDA) in iso-
lation, without attending a PSP course. The
instruction time consists of only a half day,
and the self-discipline requirements are mod-
erate. DLDA techniques apply not only to
coding but to most software process phases
and activities. Our controlled experiment in-
dicated that a programmer’s first DLDA ap-
plication results in significant improvement.
This suggests that it is possible to achieve
benefits similar to PSP techniques without
substantial investment in a PSP course.

If a programmer commits a mistake, it is
an error; a possible result of that error is a

defect in the software document. Once a de-
fect is turned into code, its execution can re-
sult in failure of the software. Thus, errors
(by human beings) and failures (by ma-
chines) are events, but defects are structural
deficiencies. Much of the software process is
concerned either with avoiding or detecting
and removing such deficiencies. An explicit
focus on avoidance and early detection is a
characteristic of a mature process. DLDA
aims to improve these activities, but it can
be applied in otherwise immature software
processes.

As its name suggests, DLDA consists of
two separate phases. The first, defect log-
ging, is performed during all software con-
struction activities where programmers
might find defects in the product, such as
during requirements definition, require-
ments review, design, design review, imple-
mentation, code review, testing, and mainte-
nance. The second, defect data analysis, is a
process improvement activity that program-
mers perform only rarely, when they have
collected sufficient defect data (after several
weeks).

Defect logging
First, programmers create a protocol en-

try whenever they detect a new defect in a
software document. The entry starts with a
time stamp recording. When the program-
mer localizes, understands, and repairs the
defect, an additional time stamp and de-
scriptive information completes the entry.
The description can include the defect’s ex-
act location, its type according to a fixed
taxonomy, the phase when it was presum-

N o v e m b e r / D e c e m b e r  2 0 0 1 I E E E  S O F T W A R E 57

1999-04-13 13:09:37 bts
1999-04-13 13:12:23 be
1999-04-13 13:22:24 ee cd we ty # multiplied instead of adding
1999-04-13 14:39:34 be
1999-04-13 14:46:35 ee cd ma om # forgot to reset stdDeviation after first calc

Figure 1. Two defect log entries. The first defect (bbee, “begin error”) was detected at 13:12 hours, three
minutes after the start of the testing phase (bbttss, “begin test”), and was located and removed (eeee, “end
error”) 10 minutes later at 13:22. It had been introduced in the coding phase (ccdd), is of the structural
type “wrong expression” (wwee), and the reason for making it was a typo (ttyy). The second defect was of
type “missing assignment” (mmaa) and was produced owing to omission (oomm—that is, in principle, the 
programmer knew that it had to be done). A typical DLDA describes defects between two to 10 phases,
10 to 50 defect types, and seven defect reasons (omission, ignorance, commission, typo, missing 
education, missing information, or external).



ably created, a hypothesis as to why it was
created (such as lack of information, other
defects, trivial omission, or trivial mistake),
and possibly a verbal description (see Figure
1). After programmers have practiced this
technique, and when using a compact for-
mat, recording becomes simpler and faster if
the programmer performs it with a tool that
records the time stamps and does some sim-
ple consistency checking.

Defect data analysis
In the defect data analysis phase, pro-

grammers cluster the defects into related
groups according to appropriate criteria.
Programmers group defects by the catego-
rizations used in the defect log. They then
analyze these groups to pinpoint the most
frequent and costliest mistakes to try to
determine why they make those mistakes
(root cause analysis). There is no pre-
scribed method for doing this. The analysis
is entirely data-driven and relates to what-
ever understanding programmers have of
their own software process. Defect data
tabulations, which are categorized by work
phase, defect type, repair cost, and so on,
are created automatically and are used to
aid analysis.

Unfortunately, defect logging requires
quite a bit of discipline—most programmers
cannot keep up with it along with their nor-
mal work. Because of this, coaching is the
third important ingredient of DLDA. When
programmers start applying DLDA, they
need to be instructed how the technique
works and should be motivated by at least
one convincing example of an insight gained
by DLDA—ideally an insight of someone
they know well. During the first few days of
learning the defect-logging technique, a
coach must stop by from time to time, ask
for the most recent defects, remind the pro-
grammer about logging, and discuss any
how-to questions. Coaching is effective only
with programmers who want to apply
DLDA—there is no point in trying to force
the technique on programmers who aren’t
willing to give it a chance. Obviously, the
coach should be somebody who not only
knows how to do DLDA but also is con-
vinced of its benefits. When programmers
have collected enough defect data (about
one hundred defects is a good start), their
coach should counsel them during defect

data analysis. The coach should explain
how to summarize the data and help with
the actual conclusions if the programmer
does not find them.

The experiment
We validated the DLDA technique in a con-

trolled experiment with participants divided
into two groups, with one using DLDA.

The programming tasks
We prepared two algorithmic program-

ming tasks. Task 1 consisted of computing
the “skyline” for a set of “buildings,” which
were represented as rectangles described by
2D coordinates. Task 2 consisted of com-
puting the convex hull for a set of 2D
points, described by their coordinates. In
both cases, the algorithms that program-
mers used were given in coarse pseudocode
(four statements for Task 1, seven state-
ments for Task 2). Both tasks involve com-
putations with 2D coordinates. The result-
ing programs had 80 to 206 lines of code
for Task 1 and 93 to 160 lines of code for
Task 2.

Each experiment participant worked
alone and solved both tasks, starting with
Task 1. There were two groups: the experiment
group, which used DLDA, and the control
group, which did not. We asked both groups
to record the time required for each work
phase (design, coding, compilation, and test)
and to track and subtract the interruption
time. Participants finished Task 1 in 3.6 to
10.7 hours and Task 2 in 1.8 to 6 hours.

We gave the experiment group a one-
page defect-logging instruction sheet before
they started Task 1. After reading it, we
gauged their understanding of the method
through a one-page scenario on which they
filled in the defect log entries. The coach ex-
plained mistakes made in that test. The per-
son who carried out the experiment also
acted as the coach for all participants. We
gave the participants a defect data record
template, a two-page template for the defect
data analysis, and one page of concrete in-
structions. In the experiment, we used a
scaled-down DLDA variant because of time
constraints. Besides a short description, pro-
grammers recorded the defect’s repair time,
the inject and remove phase of the defect,
and the error reason class (not the defect
type class). Consequently, defect data analy-

We validated
the DLDA
technique 

in a controlled
experiment.

5 8 I E E E  S O F T W A R E N o v e m b e r / D e c e m b e r  2 0 0 1



sis was simplified accordingly. However, be-
cause the tasks were small, we asked partic-
ipants to log even simple defects such as
syntactical defects and missing declarations.

During the experiment, we observed the
participants unobtrusively. If a participant
failed to fill in the time log or filled out the
defect log incorrectly, the coach reminded
him. If necessary, the coach reiterated how
to apply the method. The coach painstak-
ingly watched participants to help them
with DLDA techniques, but he did not offer
help with the programming task. We ac-
cepted a finished program only when it
passed a fixed set of tests.

The participants
The participants were 18 male graduate

or senior undergraduate students of com-
puter science, electrical engineering, and
computer engineering from the University
of Massachusetts at Dartmouth. We paired
participants with similar experience levels,
thus ensuring a reasonable balance between
the groups. We randomly assigned members
of each pair to the groups. That left 10
participants, with five in each group.
Seven of these 10 had one or several years
of experience as professional program-
mers. Several participants dropped out
during the experiment; three were too in-
experienced and could not solve the first
task, and five were unwilling to invest the
time for the second task after they finished
the first. 

We saved the participants’ source code of
each program version compiled along with
a time stamp. We used this data to deter-
mine the number of defects introduced into
a program and the time required for elimi-
nating each of them.

Results
With only five participants in each

group, you might expect that obtaining
meaningful results was impossible. But, as
we discovered, this was not the case.

Basic metrics
From the data collected during the exper-

iment, we computed these basic metrics for
each participant’s task: 

� the total required time for the task, 
� the resulting program size in lines of

code (LOC), 
� the number of compilations, 
� the number of defects inserted in any

program version, and 
� the number of defects removed during

testing. 

From these, we then computed the defect
density—that is, the number of defects in-
serted divided by the program’s size. The
test defect density accordingly considers the
defects removed during test only. The defect
latency expresses a defect’s average lifetime
from its insertion to its removal as visible in
the compiled program versions.

N o v e m b e r / D e c e m b e r  2 0 0 1 I E E E  S O F T W A R E 59

X
X

X

X

X X X

X X

X

Def.density(L)

Def.density(-)

TDef.density(L)

TDef.density(-)

Def.latency(L)

Def.latency(-)

Time(L)
Time(-)

LOC(L)
LOC(-)

Improvements from Task 1 to Task 2 in percent

100

50

0

-50

-150

Figure 2. Performance
changes of the 
defect-logging group
(L) versus the 
nonlogging group (–)
for several metrics: 
defect density, test
defect density, defect
latency (average 
defect lifetime), 
total work time,
and program length.
In each box plot, the
whiskers indicate the
value of the best and
worst participant, the
box indicates those
of the second best
and second worst,
and the middle line
those of the median.



Because of our small group sizes, it
would be dangerous for us to assume the
exact equivalence of the two groups and to
compare their performance on one task di-
rectly. Because the two tasks are obviously
different, it is also not meaningful to com-
pare Task 1 to Task 2 within one group. So,
the most sensible approach to evaluating
our data is to compare one group’s per-
formance changes from Task 1 to Task 2 to
that of the other group. Therefore we com-
puted the improvements (in percent) for the
above metrics by comparing Task 1 results
to Task 2 results for each group. 

Figure 2 shows the most interesting re-
sults. Each box and the median line indicate
improvements of the middle three subjects
of one group for one metric. The whiskers
indicate the other two subjects. With at
most one exception per group, there was al-
ways an improvement from the first task to
the second. Only Subject 1 (from the log-
ging group) and Subject 8 (from the control
group) performed worse in Task 2.

Improvement analysis
The improvement differences between

the experiment group and the control group
are not huge, but nevertheless are substan-
tial. Are they real or accidental? To answer
this question, we used a statistical hypothe-
sis test (see the sidebar).

A one-sided Wilcoxon Rank Sum test in-
forms us that the reductions in median de-

fect density and test defect density each
have a probability of only 11 percent of be-
ing accidental—reasonable evidence that
DLDA improves defect prevention. This is
corroborated by the corresponding com-
parison of the mean reductions using a
bootstrap-based test, which indicates a 9
and 10 percent probability for defect den-
sity and test defect density, respectively,
that the observed differences are random,
nonsystematic events. For the defect la-
tency, the same tests indicate a 35 and 54
percent chance, respectively, that there is no
real improvement, which suggests that
DLDA did not cause faster defect removal,
at least in this experiment.

There is a 5 percent chance that the me-
dian work time reduction is accidental (3
percent for the mean). With a probability of
0.8, the improvement percentage is larger
by at least 16 in the experiment group. We
should be aware that part of this improve-
ment is because the defect logging is getting
faster itself—in the first task, the DLDA
participants were undoubtedly slowed by
their unfamiliarity with the technique.
However, on average, they logged only 25
defects in six hours, which is a minor effort
in any case.

The difference in program length is quite
interesting. During the defect data analysis,
several participants recognized that they
should have spent more effort on properly
designing their program. The more careful
design in Task 2 then resulted in more com-
pact programs. There was a probability of
only 5 percent that the median difference is
accidental (2 percent for the mean).

We determined that despite the small
group sizes, we can be reasonably sure of
the evidence found in the experiment.
DLDA results in better defect prevention
and thus increases productivity.

Some skeptics might feel these results do
not transfer to professional programmers. It
is plausible that our inexperienced partici-
pants had so much more room for improve-
ment than an experienced software engineer
that DLDA might be worthless in practice,
in spite of our results. However, we found
evidence to the contrary in our data. For the
test defect density, our data shows a clear
trend that the more experienced partici-
pants actually obtained a larger improve-
ment than the others. This is true no matter

6 0 I E E E  S O F T W A R E N o v e m b e r / D e c e m b e r  2 0 0 1

A statistical hypothesis test is a mathematical procedure for comparing
two data samples (sets of related values). For instance, the Wilcoxon
Rank Sum test compares the medians of two samples, the t-Test compares
the means, and a Bootstrap-based means-differences test compares the
means but does not require assuming a normal distribution. On the basis
of the variation in the observed data, such a procedure will compute the
probability (called the p-value) that the apparent differences between the
two samples are not real but rather merely due to chance. 

In our case, if that probability is small, we are willing to believe that
the observed additional improvements in the Defect Logging and Defect
data Analysis group are not accidental and will consider the experiment
good evidence of DLDA’s usefulness. Comparing the medians indicates
whether we should expect an improvement for a single individual. In con-
trast, comparing the means indicates whether we should expect an im-
provement when averaging across a team (a group of individuals).

Statistical Hypothesis Tests



whether we measure experience in years of
professional programming experience or by
the length of the largest program the partic-
ipants ever wrote.

D LDA appears to be a viable tech-nique for accelerating learning fromexperience: programmers learn to
prevent mistakes faster than usual. For most
programmers, learning DLDA will require
an experienced coach, but apart from that,
the learning cost is low. In a controlled ex-
periment we found that a group using
DLDA for only one small programming
task (a half day) solved a second task faster
and with smaller defect density than a con-
trol group—even though the experiment
used only a simplified version of DLDA ow-
ing to time constraints.

We believe in DLDA’s validity; we urge
practitioners to try it with a few colleagues.
It is a low-cost, low-risk technique with
considerable potential benefits; you can
usually find a champion to be the coach.

Our research raises a number of ques-
tions. Our experience with teaching PSP in-
dicates that more than half of all program-
mers are unable to keep up the self-discipline
required for defect logging. This seems to be
a personality issue. How can we quickly and
safely determine whether training a pro-
grammer in DLDA will lead to actual usage?
Merely asking the candidate solves only
about half of the problem.

What tools provide the best support for
defect logging? The Web page www.ipd.ira.
uka.de/PSP provides tools for defect logging
and defect data summarization, as well as a
defect classification standard.

What defect classification categories are
most useful, and in which contexts? What
criteria are best for defect analysis? How far
can defect analysis be standardized and sim-
plified before it loses value? Our experiment
indicates that for most (but not all) people,
more value comes directly from the logging
rather than from the analysis. However, a
useful defect classification scheme might
play an important role even then.

What minimum intervention by the
coach is sufficient? How can the coach best
adapt the interventions to the needs of the

specific trainee? Should DLDA be combined
with pair programming?

Answering these questions will lead to
faster learning in software programming.

Acknowledgments
Georg Grütter performed this experiment. We

thank Michael Philippsen for commenting on a draft
of this article and our experimental subjects for their
participation.

References
1. W.S. Humphrey, A Discipline for Software Engineering,

Addison-Wesley, Reading, Mass., 1995. 
2. W.S. Humphrey, “Using a Defined and Measured Per-

sonal Software Process,” IEEE Software, vol. 13, no. 1,
May 1996, pp. 77–88.

3. W.S. Humphrey, Introduction to the Personal Software
Process, Addison-Wesley, Reading, Mass., 1997.

4. P.M. Johnson and A.M. Disney, “The Personal Software
Process: A Cautionary Case Study,” IEEE Software,
vol. 15, no. 6, Nov./Dec. 1998, pp. 85–88.

For more information on this or any other computing topic, please visit our
Digital Library at http://computer.org/publications/dlib.

N o v e m b e r / D e c e m b e r  2 0 0 1 I E E E  S O F T W A R E 61

About the Author

Lutz Prechelt is the head of process man-
agement, training, and technical documentation
at abaXX Technology, Stuttgart. He previously
was a senior researcher at the School of Infor-
matics at the University of Karlsruhe, Stuttgart.
His research interests include software engineer-
ing (particularly using an empirical research ap-
proach), compiler construction for parallel ma-
chines, measurement and benchmarking, and

research methodology. He has performed several controlled experiments on
design patterns, inheritance hierarchies, and the Personal Software Process.
He received his diploma and his PhD in informatics from the University of
Karlsruhe. He is a member of the IEEE Computer Society, the ACM, and the
German Informatics Society, and is the editor of the Forum for Negative Re-
sults, part of the Journal of Universal Computer Science. Contact him at
prechelt@computer.org.Abstract—Defect



62 I E E E  S O F T W A R E N o v e m b e r / D e c e m b e r  2 0 0 1 0 7 4 0 - 7 4 5 9 / 0 1 / $ 1 0 . 0 0  ©  2 0 0 1  I E E E

T
he worst thing that can happen in re-
quirements engineering is that your set
of requirements, however expressed,
doesn’t accurately represent your
users’ needs and consequently leads
your team down the wrong develop-

ment path. The whole point of requirements
engineering is to steer your development to-
ward producing the right software. If you
don’t get the requirements right, how well
you execute the rest of the project doesn’t
matter because it will fail. So how are we led
astray? The risk is greatest at several points.

Overlooking a crucial requirement
Perhaps the greatest risk in RE is missing

a critical functional or attribute requirement.
If you overlook an important user class,
you’ll probably have to do a big job—usu-
ally larger than stakeholders care to toler-
ate—to add in what that user needs. Missing
a critical quality or performance attribute is
typically even worse. Often, the only way
adapt a software-based system to accommo-
date an important attribute is to re-architect.
An example that many software developers
have encountered recently is scalability in e-
commerce. If designers don’t keep scalability
in mind when choosing their architectures
(and many don’t), they find themselves in a
tough position when the usage load on their
software leaps to a thousand or more times
what they were expecting. System perfor-
mance is inextricably tied to system architec-
ture. In most instances, the only way to im-
prove performance is to choose another
one—that is, to start over from scratch. This
is not popular when your senior managers

promised you’d deliver next week. It’s even
worse if you make the discovery when your
software is already in the field.

Inadequate customer
representation 

One of the central activities in RE is nego-
tiating agreement on requirements. To achieve
this agreement, you must find out what your
customers really need. Not much of a negoti-
ation will take place if you never actually in-
teract with them. “Take it or leave it” hap-
pens all too frequently when we assume our
design ideas suit our customers and don’t
bother to check if this assumption is really
true. Customers only discover if we had the
right idea when they attempt to use our soft-
ware. If our confidence was misplaced, that’s
a late time to discover it. For example, a cor-
porate IT development team told Karl that
they recently rolled out a new application for
internal use, but that they developed the sys-
tem with virtually no customer input. The
first time the users saw it was on delivery, and
they immediately rejected the system as com-
pletely unacceptable. On the day you proudly
unveil your new baby to the world, you don’t
want to hear, “Your baby is ugly!” 

Modeling only functional
requirements

Both the requirements literature and our
practices have historically focused on func-
tional requirements—the things our software
systems are supposed to do. Functional re-
quirements are the most obvious ones to the
user, so most elicitation discussions focus on
them. Perhaps more important, though, is

requirements

The Top Risks of 
Requirements Engineering
Brian Lawrence, Karl Wiegers, and Christof Ebert

E d i t o r :  S u z a n n e  R o b e r t s o n  � T h e  A t l a n t i c  S y s t e m s  G u i l d  � s u z a n n e @ s y s t e m s g u i l d . c o m



N o v e m b e r / D e c e m b e r  2 0 0 1 I E E E  S O F T W A R E 63

REQUIREMENTS

gaining agreement on quality attribute
requirements—the characteristics you
intend your software to exhibit. Old
standbys include reliability, perfor-
mance, security, robustness, ease of
use; others are scalability, innovation,
coolness, or fun. Functional models,
such as use cases, frequently gloss
over the attribute requirements alto-
gether. The attribute requirements are
the heart and soul of why your cus-
tomers will value your software. They
determine why using your software is
better than whatever they did before
to achieve the same end.

For example, a system that fails to
handle exceptions effectively will not
be robust and will crash when unex-
pected conditions occur. It does you
no good to simply record the require-
ment that “the system shall be ro-
bust.” A skillful requirements analyst
knows to ask the prompting ques-
tions that will elicit the user’s implicit
expectations about various attrib-
utes, explore what the user has in
mind when he or she says “robust,”
and negotiate the inevitable trade-
offs among conflicting attributes.

Not inspecting requirements
The evidence is overwhelming and

long known that the cost to remove
defects in requirements increases geo-
metrically with time. Once your soft-
ware hits the field, removing a re-
quirements defect costs at least a
hundred times as much, assuming
you can fix it at all. Inspecting your
requirements models is the most ef-
fective way to identify ambiguities,
unstated assumptions, conflicting re-
quirements, and other defects at the
earliest possible point. Of course, to
hold an inspection, you must have
something inspectable. And you have
to believe that your set of require-
ments has defects that you need to
identify. Personally, in all the years
I’ve (Brian) seen requirements for
countless software projects, I’ve
never seen a defect-free set. If there’s
an ironclad rule in software develop-
ment, it’s “Always inspect your re-
quirements.” And choose inspection
teams with a broad constituency, in-
cluding testers. One company we

know measured a 10-to-1 return on
investment from performing inspec-
tions on requirements specifications,
sustained over five years. For more
on inspecting requirements, check
out Karl Wiegers’ article “Inspecting
Requirements” at StickyMinds.com.

Attempting to perfect
requirements before
beginning construction

The time when we could know
everything we needed to know before
starting software construction is long
past. Today, we live in an emergent
world—some information simply isn’t
available early in our projects—and
only emerges later.  We can’t possibly
know everything we’d like to know
before we start development. It’s safer
to assume that our requirements are
going to change than that they won’t.
Yet for some projects, participants feel
as though they must completely un-
derstand the requirements before any
other work begins. For most projects,
this is a mistake. You need to do some
design and construction before you
can tell how hard the job will be and
how much each part will cost. As this
kind of information becomes evident,
it could well affect your views about
your requirements, further changing
them. Do the best job you can early to
get a good set of requirements, but
don’t be discouraged if everything 
isn’t absolutely certain. Identify those
areas of uncertainty and move on, en-
suring that someone is responsible for
closing those gaps in your knowledge
before construction is complete. Track
the uncertain requirements carefully
as your project proceeds.

Representing requirements in
the form of designs

Possibly the subtlest risk in re-
quirements engineering is letting de-
signs creep into, and then remain in,
your requirements specifications.
When you view designs as require-
ments, several things happen. You
run the risk of choosing a particular
solution that might not be the best
one to implement. Also, you under-
mine your ability to validate your
system, because you are specifying

the problem you hope to solve in
terms of how you intend to solve it.
All you can really do is verify that
you built what you said you would.
One surefire indicator that you’re
falling into this trap is highlighted by
references to technology. Any time a
technology is specified, you’re using
a design to represent the underlying
requirement. Using designs as re-
quirements is a subtle risk because al-
though you might have specific infor-
mation about what you want, it
doesn’t represent the underlying need
and is consequently vulnerable to
mistaken assumptions.

W hereas these risks are pretty seri-ous, the greatest threat to pro-ject success is not performing 
requirements engineering at all. Re-
quirements are your project’s foun-
dation. They define the level of qual-
ity you need, facilitate decision
making, provide a basis for tracking
progress, and serve as the basis for
testing. If you let them remain un-
stated, you have no opportunity to
examine and negotiate them with
your customer and no way to tell
when your project has met its objec-
tives. Without clear requirements,
how will you know when you’re
ready to release your product?

Brian Lawrence is a principal at Coyote Valley Soft-
ware, a software consulting firm in Silicon Valley, California. He
helps software organizations model and manage requirements,
plan projects, and conduct peer reviews. Contact him at brian@
coyotevalley.com.

Karl Wiegers is the principal consultant at Process Im-
pact, a software process education and consulting company in
Portland, Oregon. He has written books about peer review in
software and software requirements. Contact him at kwiegers@
acm.org.

Christof Ebert is director of software coordination and
process improvement at Alcatel in Paris. He is also the IEEE Soft-
ware associate editor for requirements. Contact him at christof.
ebert@alcatel.be.



64 I E E E  S O F T W A R E N o v e m b e r / D e c e m b e r  2 0 0 1 0 7 4 0 - 7 4 5 9 / 0 1 / $ 1 0 . 0 0  ©  2 0 0 1  I E E E

S
oftware quality stems from many fac-
tors, including implementation deci-
sions, software architecture, and re-
quirements. In this column, we focus
on software architecture, which can
enable or inhibit many of a software

system’s qualities. Because the cost of ad-
dressing quality concerns is a function of
how late you address them (the later, the

more costly), addressing them in the archi-
tectural or requirements phase makes sense.

Architectural quality
Researchers have begun to recognize that

we can view software architecture as a for-
mal model for requirements specification.
This recognition expands the possibilities
for evaluating quality attributes early dur-
ing development. In this context, when
evaluating architecture properties, our
goals include

� providing an early opportunity to correct
requirements defects and 

� ensuring that the software architecture

provides an accurate blueprint for system
implementers. 

In the first case, you could use a software ar-
chitecture evaluation to check qualities such
as safety, liveness, and completeness. In the
second case, you might predict qualities in-
cluding maintainability, performance, and
reliability. 

First things first
You must establish an architecture’s cor-

rectness before using it as a system blue-
print. An incorrect architecture will not pro-
duce quality evaluations of merit. So, the
blueprint must be correct before you can
build a system targeting quality goals. Cor-
rectness evaluations also mitigate the effort
and cost of performing simulations or other
evaluations to predict qualities such as per-
formance and reliability. 

Many techniques evaluate correctness
properties of software architectures; how-
ever, no existing single technique can evalu-
ate all correctness aspects. For example,
model checking works well to evaluate
safety and liveness but is not useful for eval-
uating completeness. Conversely, simulation
helps detect completeness errors, but you
can’t use it to prove the absence of safety or
liveness errors. 

Automated translation of a software ar-
chitecture specification into a form suit-
able for a model checker or simulation
tool removes the necessity of having the
software architect be an expert in model
checking or simulation techniques. For the

quality time

Software Architecture 
Correctness
K. Suzanne Barber, University of Texas at Austin
Jim Holt, Motorola

E d i t o r :  J e f f r e y  Vo a s  � C i g i t a l  � v o a s @ c i g i t a l . c o m



N o v e m b e r / D e c e m b e r  2 0 0 1 I E E E  S O F T W A R E 65

QUALITY TIME

same reason, it’s desirable to auto-
matically collect and present the re-
sults of model checking or simula-
tion. Automation is reasonably
straightforward, provided the spec-
ification contains declarative ele-
ments, integration details, and be-
havioral information. Declarative
elements typically take the form of
sets of services collected into class
interfaces, integration details define
what data and events are exchanged
between interfaces, and behavioral
information outlines pre- and post-
conditions for state changes. You
can use this set of information to
translate the specification into a set
of communicating processes defined
in the model checker or simulation
tool semantics.

Architectural completeness
A model checker can unambigu-

ously detect safety and liveness er-
rors. Completeness errors, however,
are associated with unexpected sys-
tem behavior, so a software archi-
tecture could exhibit an absence of
safety and liveness errors but still
lack a certain required functionality
(a form of completeness error). 

Completeness errors typically
manifest in sequences of service exe-
cutions (scenarios) that

� are missing expected service exe-
cutions, 

� contain unexpected service execu-
tions, 

� contain unexpected paths, or 
� are missing paths. 

At times, completeness errors have
gone undetected during requirements
elicitation and have propagated into
the software architecture. Such omis-
sions or inaccuracies in the require-
ments are not unlikely, given the
challenges of the knowledge acquisi-
tion and modeling process used to
gather requirements. In fact, the
knowledge contained in the require-
ments is a function of many vari-
ables, including the

� spectrum of expertise held by do-
main experts,

� time spent with each expert,
� knowledge acquisition approach,
� ability of knowledge engineers

and experts to conceptualize, and 
� degree to which experts can ex-

press knowledge and offer neces-
sary detail. 

So how does the software architect
or domain expert use simulation re-
sults to evaluate the architecture’s
completeness?

Simulations can produce visual-
izations of the architecture’s execu-
tion. These visualizations are par-
ticularly useful for identifying
completeness errors by inspection—
you can visually spot errors in
threads of execution the simulation
captures. Unfortunately, simula-
tions have two limitations: they do
not provide an exhaustive means of
evaluating an architecture model,
and they require a human to inter-
pret their output. So, simulations
are best for showing the presence of
completeness errors, but cannot
prove their absence.

A scenario space can help archi-
tects and domain experts visualize
completeness errors. A scenario
space is a directed graph that repre-
sents possible threads of execution
composed of services in the soft-
ware architecture. A vertex in the
graph represents a service execu-
tion state, and an edge represents a
path showing reachability from one
service execution state to another.
This visualization technique pro-
vides a high-level view of software

architecture execution through par-
tial-order reduction of simulation
data to merge similar threads of ex-
ecution produced over many simu-
lation runs. Creating such a visual-
ization can help evaluate whether
executing the architecture will re-
sult in threads of execution that
support the anticipated scenarios
for the application domain. 

A human approach
Regardless of method, the state of

the art in evaluating correctness
qualities of software architectures
still requires human involvement.
This invariably means human ex-
perts familiar with the application
domain inspecting an artifact. In-
specting requirements specifications
or static software architecture dia-
grams of large systems is daunting.
However, cost savings provides clear
motivation to perform these evalua-
tions early. A formal software archi-
tecture representation allows for ac-
curate, repeatable model checking
and simulation for correctness evalu-
ations, thus mitigating human effort
and potential inaccuracies. 

K. Suzanne Barber is an associate professor in the
Electrical and Computer Engineering Department at the Univer-
sity of Texas at Austin. She is also the director of the Laboratory
for Intelligent Processes and Systems, where ongoing research
projects address distributed, autonomous agent-based systems
and formal software engineering methods and tools. Contact her
at barber@mail.utexas.edu.

Jim Holt is a senior verification engineer for the Motorola
Semiconductor Products Sector and a PhD candidate in the De-
partment of Electrical and Computer Engineering at the Univer-
sity of Texas at Austin under the supervision of K. Suzanne Bar-
ber. His work at Motorola involves creation of advanced tools for
automated code generation and verification of Motorola micro-
processors and systems-on-a-chip. Contact him at jim.holt@
motorola.com.

Simulations can 
produce visualizations 
of the architecture’s

execution. These
visualizations are

particularly useful for
identifying completeness

errors by inspection.



66 I E E E  S O F T W A R E N o v e m b e r / D e c e m b e r  2 0 0 1 0 7 4 0 - 7 4 5 9 / 0 1 / $ 1 0 . 0 0  ©  2 0 0 1  I E E E

T
he mandate came from our member
companies: “Help us manage knowl-
edge better.” So, in 1999, the Software
Productivity Consortium—a not-for-
profit alliance of industry, government,
and academia—asked our members to

indicate which technological advances they
need most urgently. Most respondents
stressed the need to better leverage an in-
creasingly vast and complex array of intel-
lectual assets. Such assets represent today’s
new capital, marking a profound shift from
more traditional types of capital.

To address this urgent need, the Consor-
tium (see www.software.org) launched a
knowledge management program to develop
our competency in KM so that we can better
serve our members and to provide products
and services that will help members develop
their own KM competencies. We focused
first on making access to Consortium assets
easier through an enterprise portal. Then, to
address the larger KM issues, we also part-
nered with George Washington University
and its new Institute for Knowledge Man-
agement (http://km.gwu.edu/km/index.cfm),
which seeks to establish a sound theoretical
foundation for KM. 

Here, we recap the lessons we have
learned in pursuing our KM mandate and
set forth what we believe are the keys to
KM’s future success.

The promise and challenge of
knowledge management 

Knowledge is to the Information Age as
oil was to the Industrial Age. Today, the
main asset of production is intellectual capi-

tal as opposed to the tangible assets that pre-
viously drove manufacturing-based markets. 

“In the [new] age, a company’s value is
largely determined by its ability to con-
vert…knowledge into net worth,” according
to a briefing by the National Knowledge &
Intellectual Property Management Taskforce.
To convert intellectual capital into market
value, however, an organization must foster
knowledge creation, utilization, and sharing. 

KM is about leveraging relevant intellec-
tual capital to achieve organizational mis-
sions. While this premise might appear self-
evident, successful outcomes are elusive.
What is critical is how we leverage knowl-
edge to measurably enhance effectiveness,
efficiency, and innovation. 

Knowledge management at the
Consortium 

Like most organizations, the Consortium
faces daunting change. Member diversifica-
tion has grown dramatically over the past
four years, increasing demand for a wider
range of products and services. Providing in-
novative support for information access and
member collaboration has become an in-
creasingly important aspect of our value
proposition. KM offers a partial solution to
these challenges. Given this situation, and in
response to our board of directors, we held
a workshop for 33 member representatives.
Attendees exchanged ideas, declared needs,
and came to consensus: to help them better
manage knowledge, our KM program
should focus first on technology transfer. 

To improve technology transfer, we
could, for example, 

manager

Knowledge Management:
Insights from the Trenches
Linda Larson Kemp, Kenneth E. Nidiffer, Louis C. Rose, Robert Small, and Michael Stankosky

E d i t o r :  D o n a l d  J .  R e i f e r  � R e i f e r  C o n s u l t a n t s  � d . r e i f e r @ i e e e . o r g



N o v e m b e r / D e c e m b e r  2 0 0 1 I E E E  S O F T W A R E 67

MANAGER

� update our business
model, putting KM at
the core; 

� reengineer our product-
service lines, emphasiz-
ing intellectual capital; 

� hold frequent member
forums to boost out-
reach; 

� invest in distance-learn-
ing capabilities to share
more information more
quickly to more people;
or 

� implement a portal with
such features as intelligent
search and support for
communities of practice. 

We quickly concluded
that a portal was the key to
our short-term investment
strategy. We selected among
candidate features and tools
based on need, cost, and
feasibility. Last fall, we pi-
loted a portal prototype in-
ternally. Its functionality includes con-
cept search, content spidering, and
topic maps. In the fall of 2001, we are
rolling out the portal to members as
part of the Consortium’s Web site up-
date. The update will include features
such as concept-based searching using
advanced search mechanisms, a tai-
lorable user interface for a more effec-
tive engagement, and capabilities to
facilitate communities of practice.
Now we look forward to the next it-
eration of the portal’s evolving func-
tionality, support for communities of
practice. 

Lessons learned
During nearly two years of effort,

we have learned many things. For ex-
ample, as with any other major insti-
tutional change initiative, several fa-
miliar factors proved critical to our
KM program’s success: 

� Clear goals. Investment in KM
must have specific ends defined in
measurable terms and be aligned
with business aims. Knowledge
needs should be prioritized, and
policies, systems, and infrastruc-

tures should be designed to satisfy
them.

� Strong sponsorship. A KM pro-
gram requires constant executive
commitment and support. Every-
one should grasp KM’s impor-
tance to the organization and see
their role in it. 

� Realistic expectations. KM is no
silver bullet. The complex chal-
lenge to identify and leverage rele-
vant intellectual capital requires
more than simplistic solutions.

� Balance. “Too much emphasis on
technology without incorporating
the other critical elements (i.e.,
leadership, organizational struc-
ture, and learning) can easily re-
sult in a failed system.”1

� Iterative approach. Despite poten-
tial impatience with incremental
gains, prototyping is important to
clarify requirements, test capabili-
ties, and gain buy-in. Moreover,
continual system evolution is es-
sential to keep pace with a dy-
namic business environment. 

We also have learned about another
set of critical factors, those that consti-

tute the essential compe-
tencies in KM. For exam-
ple, we discovered that
few, including ourselves,
know enough about how
to convert knowledge into
net worth. Such competen-
cies are not yet well devel-
oped within the theory and
practice of KM. The KM
community at large needs
to address these essentials
immediately. 

� A systems approach.
Typically, KM programs
focus on narrow solutions
(such as technology or
learning). The roots of an
organization’s overall mis-
sion receive little attention.
KM must address the or-
ganization as a whole; we
need to use system tech-
niques tied to business
goals.
� A flexible frame-

work. KM must be custom fit to
each organization. While certain
fundamental principles should al-
ways apply, no unified theory ex-
ists to explain these principles and
enable diverse organizations to
craft adept solutions to individual
situations.

� An evolutionary process. Cur-
rently, no accepted process model
supports the continual evolution of
KM capabilities into the organiza-
tion. We need to define and test a
process model to meet these needs.

� Integrated measurement. We need
guidance on how to identify those
measures that are most vital for
leveraging intellectual capital,
which is the primary objective of
any KM program.

� A capability model. Many KM ap-
proaches and tools exist, many
claims are made, but what consti-
tutes the essential core set of capa-
bilities that truly represent KM
proficiency? No industry-accepted
model exists to assess the maturity
of an organization’s KM program.

� Technical maturity. KM tool fea-
tures need to converge; for exam-



68 I E E E  S O F T W A R E N o v e m b e r / D e c e m b e r  2 0 0 1

MANAGER

ple, support for moderated chat
rooms and single sign-on is diffi-
cult to find in a set of compatible
tools. Tool quality also must im-
prove; for example, browser com-
patibility and support for inte-
grated third-party technologies
can be problematic.

In addition, there is growing frus-
tration within industry and govern-
ment over the KM community’s
fragmentation, along with growing
awareness of the need for a com-
mon, holistic approach to leverag-
ing intellectual capital. A sound
framework, containing all of the es-
sential elements of a successful KM
program and based on a strong the-
oretical foundation, would improve
consistency within the KM commu-
nity and advance the practice of
KM. 

To develop KM’s theoretical foun-dation and provide consistentpractical guidance, the Institute
and the Consortium recently part-
nered to develop and deploy the re-
quired interdisciplinary theory, for
which they are well suited. The In-
stitute contributes expertise across
many fields; the Consortium is al-
ready in the business of transform-
ing theory into practice.

Together, we will review the KM
landscape and facilitate industry con-
sensus on the essential elements. We
will scan the breadth of disciplines
contributing to KM—from cultural
anthropology to Web technologies—
trying to consolidate the best and lat-
est thinking in each area. From this
baseline, we plan to develop a unified
framework for KM with associated
definitions, criteria, and practices.
We want to enable organizations to
better envision and deal with the re-
alities of the new economy and fol-
low through with the hard choices in-
volved in adapting to change and
aligning resources. 

The proof will be in the pudding.
The framework will need to be vali-
dated, tested, and evolved. You can
help! We welcome your comments,
inquiries, and participation. 

Acknowledgments  
The authors thank the Consortium’s Infor-

mation Support Group for their support of the
portal, and Susan Polen and Deanna West for
their immense dedication, creativity, and pa-
tience in rolling it out.

Reference  
1. M. Stankosky and C. Baldanza, “A Systems

Approach to Engineering a Knowledge Man-
agement System,” Knowledge Management: A
Catalyst for Electronic Government, R.C. Bar-
quin, A. Bennet, and S.G. Remez, eds., Man-
agement Concepts, Vienna, Va, 2001.

Linda Larson Kemp is a senior member of the Con-
sortium’s technical staff. Contact her at 2214 Rock Hill Rd., Hern-
don, VA 20170-4227; kemp@software.org; www.software.org.

Kenneth E. Nidiffer is vice president of new business
at the Consortium. Contact him at 2214 Rock Hill Rd., Herndon,
VA 20170-4227; nidiffer@software.org; www.software.org.

Louis C. Rose is manager of strategic initiatives at the
Consortium. Contact him at 2214 Rock Hill Rd., Herndon, VA
20170-4227; rosel@software.org; www.software.org.

Robert Small is a principal member of the Consortium’s techni-
cal staff and former co-chairman of the Consortium’s Technical
Advisory Board. Contact him at 2214 Rock Hill Rd., Herndon, VA
20170-4227; small@software.org; www.software.org.

Michael Stankosky is an associate professor of engineering
management and systems engineering at George Washington
University, founding director of the Institute for Knowledge Man-
agement, and creator of GWU’s KM program. Contact him at
Melvin Gelman Library, 2130 H St. NW Suite 704, Washington,
DC 20052; mstanko@seas.gwu.edu; http://km.gwu.edu/km/
stankosky.cfm.

How to 
Reach Us

Writers
For detailed information on submitting articles,
write for our Editorial Guidelines (software@
computer.org) or access http://computer.org/
software/author.htm.

Letters to the Editor
Send letters to

Letters Editor, IEEE Software
10662 Los Vaqueros Circle
Los Alamitos, CA 90720
dstrok@computer.org

Please provide an email address or 
daytime phone number with your letter.

On the Web
Access http://computer.org/software for
information about IEEE Software.

Subscribe
Visit http://computer.org/subscribe. 

Subscription Change of Address
Send change-of-address requests for magazine
subscriptions to address.change@ieee.org. 
Be sure to specify IEEE Software.

Membership Change of Address
Send change-of-address requests for IEEE
and Computer Society membership to 
member.services@ieee.org.

Missing or Damaged Copies
If you are missing an issue or you received 
a damaged copy, contact help@computer.org.

Reprints of Articles
For price information or to order reprints, 
send email to software@computer.org or fax
+1 714 821 4010.

Reprint Permission
To obtain permission to reprint an article, 
contact William Hagen, IEEE Copyrights and
Trademarks Manager, at whagen@ieee.org.

How to 
Reach Us



0 7 4 0 - 7 4 5 9 / 0 1 / $ 1 0 . 0 0  ©  2 0 0 1  I E E E N o v e m b e r / D e c e m b e r  2 0 0 1 I E E E  S O F T W A R E 69

design techniques? I try to show here how I
believe this can be achieved.

Let us take a quick overview of one rep-
resentative methodology each for WUI and
GUI design.

WUI design methodology
A popular Web design technique3 is

based on building an information architec-
ture, following these basic steps:

1. Define the site’s mission and vision.
Also, envision your intended audience.

2. Determine the site’s content and func-
tionality.

3. Define the organization of information
on the site, including navigation, label-
ing, and search systems. Specify these in
terms of Web page hierarchy diagrams,
or information architecture blueprints.

4. Build and test. Preferably, test the site’s
usability by observing members of your
intended audience perform specific tasks
on the site.

Analyzing the Web design technique a bit
more closely, we can see a few gaps in the
scheme. At the second step, the link between

audience and design breaks. While the de-
sign is guided by the expected audience’s
characteristics, there is no direct way of
defining user requirements, especially in
terms of content. This invariably leads to an
almost imperceptible shift in the focus from
what the user would like to find on the site
to what the owner would like to display.
Similarly, at the third step, the organization
of content has no explicit reference to user
requirements or expectations. It usually in-
volves a logical partitioning of content and a
drill-down architecture for details. For ex-
ample, on an entertainment site, music and
movies typically appear on separate pages
because they are different types of items.
However, would it be better if the movie’s
soundtrack appeared on the movie page?
We need to understand the user to answer
this question. Indeed, an expert site designer
recognizes this and often flouts the “logical
partitioning” rule. But this leaves us to grap-
ple with the question, “If not logical, then
what?” GUI design gives us some pointers,
as I shall reveal later. At the fourth step, the
user focus explicitly returns to the design cy-
cle—a bit late to avoid major design flaws in
the earlier steps.

feature
Web User Interface Design: 
Forgotten Lessons

Uttara Nerurkar, Infosys Technologies 

The methods used to design Web sites and graphical user interfaces differ

The methods
used to design
Web sites and
graphical user
interfaces differ
considerably,
but the author
argues that it 
is possible to
improve Web
design methods
by applying
well-established
GUI design
principles.

A
number of prescriptions are in vogue for Web user interface de-
sign (let’s call it WUI, in line with GUI). However, Web site us-
ability continues to be a serious issue.1 WUI design methods are
quite different from their GUI precursors. The latter have ma-

tured considerably over some time and have been proven to yield highly us-
able interfaces.2 While the Web paradigm is apparently quite different from
the GUI one, are there similarities that we can exploit to improve WUI 

inter face design



7 0 I E E E  S O F T W A R E N o v e m b e r / D e c e m b e r  2 0 0 1

GUI design methodology
For GUIs, a proven method for improved

interface usability is user-centered design.3

It broadly involves the following steps:

1. Set up user types.
2. Define tasks, or operational user scenar-

ios, for each user type.4

3. Design the user interface by specifying pre-
sentation elements to complete these tasks.

The GUI methodology’s main strength lies
in the mapping between users and tasks and
that between tasks and presentation elements.
These two mappings provide not only a direct
aid in focusing the designer’s attention on the
user, but also help in discovering the applica-
tion’s usability, such as by revealing the num-
ber of steps required to complete a task.

How different are these paradigms?
Let’s step back a bit from the interface

design techniques and analyze the differ-
ences between the WUI and GUI application
paradigms themselves. Major differences
between them include the following:

� User characterization is more difficult
on the Web because it typically ad-
dresses an unknown audience. For ap-
plications, however, the user community
is usually well defined.

� Applications are typically task-centric,
whereas the Web is still largely informa-
tion-centric.

� Customers are not stuck with a Web site
the way they might be with an applica-
tion or product they have purchased. At
the slightest difficulty, the surfer tends
to move on to another site, maybe never
to return again.

� Navigation between Web pages is much
more flexible than that between forms (or
screens) of an application. Thus, you can
reach a particular Web page by any num-
ber of navigation paths provided on the
site. In contrast, in the GUI application, the
routes to a particular screen are extremely
limited. Indeed, most screens have only
one path to them.

The first distinction reveals a hurdle in
defining the Web paradigm’s user types.
Web design experts have suggested the cre-
ation of personas or detailed characteriza-

tions of users belonging to different demo-
graphic profiles to predict the surfer’s be-
havior on the site. Other design experts de-
fine relevant user characteristics and predict
user behavior based on various combina-
tions of these characteristics, such as expert
versus novice. 

The second issue pertains to a problem in
defining tasks for a Web site. In the infor-
mation-centric paradigm, search is the key
operation. It does not seem to fit the defini-
tion of a task because it is too flexible and
vague. For this reason, the Web techniques
assume that search will be made simpler if
the content is organized logically and proper
labeling and navigation schemes are pro-
vided. So why do so many content organiza-
tions fail on this account? I contend that or-
ganizations focus on the content they want
to share with the world and not on what the
world would like to know about them. I
suggest that we take each of the user types
defined earlier and set up tasks for them
such as “Find product information” and
“Find company profile” based on our
guesstimate of user interest. Not only will
such a scheme create a mapping between
users and their tasks, but it will also provide
direct guidance to how to organize content
on the site. If users cannot accomplish the
“Find product information” task within
three page visits, the site’s design must be im-
proved. Evidence that we can enumerate the
search operation in this way comes from the
WUI design method’s fourth step. We can
define the same tasks given to the tester to
perform on the site before we design the site. 

Difference three (customer loyalty) has
two aspects. One, aesthetic appeal, is outside
this discussion’s scope. The other aspect is
usability, which is the subject of this article. 

Navigation flexibility is undoubtedly a
nonissue in the traditional approach. Form
navigation in older applications was nor-
mally so simple that it was unnecessary to
model it explicitly using navigation maps.
On the other hand, a way to model page
navigation is critical for any Web design.

I suggest bringing together the best of
both worlds to yield a new and improved
Web design methodology, thus:

1. Define the site’s mission and vision. 
2. Envision the intended audience and

model it as a finite number of user types.

The GUI
methodology’s
main strength

lies in the
mapping

between users
and tasks and
that between

tasks and
presentation

elements.



N o v e m b e r / D e c e m b e r  2 0 0 1 I E E E  S O F T W A R E 71

3. Specify the tasks you expect each user
type to perform. Include here the infor-
mation they would be interested in view-
ing as “Find <content>” tasks.

4. Design the Web pages, including content
and presentation elements, by building a
mapping between the page and the task.
The search or “Find <content>” tasks
will then define the content required on
the pages. Some tasks will span across a
number of pages, thus automatically cre-
ating a page hierarchy and navigation
path. Complete the page hierarchy and
navigation paths for all remaining pages. 

5. Design the labeling and search system so
that it is optimized for the tasks.

6. Test the design.

T he essentials of a user-centered in-terface design approach are defi-nitely applicable for the Web. How-
ever, WUI gurus advocate a design path that

is totally divorced from the GUI approach.
Applying some “ancient wisdom” from the
GUI era could lead to more usable Web
sites.

References
1. K. Scoresby, “Win Consumers with Better Usability,” 

e-Business Advisor, vol. 18, no. 6, June 2000.
2. A.R. Puerta et al., “MOBILE: User-Centered Interface

Building,” Proc. Conf. Human Factors and Computing
Systems, ACM Press, New York, 1999.

3. L. Rosenfeld and P. Morville, Information Architecture
for the World Wide Web, O’Reilly, New York, 1998.

4. B. Schneiderman, Designing the User Interface, Addison
Wesley Longman, New York, 1998.

About the Author

Uttara Nerurkar is a senior research associate at Infosys Technologies Limited, a CMM
Level 5 software services and consultancy company headquartered in Bangalore, India. She
has worked in the software development area for the last nine years. Her research interests in-
clude software architecture, security, and user interfaces. She has a BTech in chemical engi-
neering from Indian Institute of Technology. She is also a professional member of the ACM.
Contact her at Software Concept Laboratory, Infosys Technologies Ltd., Electronics City, Hosur
Rd., Bangalore 561 229, India; uttara@acm.org.

C A L L  F O R  A R T I C L E S

In i t ia t ing Sof tware Product  L ines

Su
bm

is
si

on
 D

ea
dl

in
e:

 1 
M

ar
ch

 2
00

2
Pu

bl
ic

at
io

n 
Da

te
: S

ep
te

m
be

r/
Oc

to
be

r 
20

02

Guest Editors:
John D. McGregor 
Clemson University 
johnmc@cs.clemson.edu

The product line approach to software development is emerging as a technique for organizing the large-scale production of a 
number of related software products. Adopting the product line perspective on software development affects the usual way of 
doing business at the organizational and technical management levels as well as at the software engineering level.

Many companies are investigating the potential benefits, establishing pilot projects, and baselining these initial efforts. They 
face a number of questions:

• What types of planning are necessary prior to initiating the product line? 
• How can existing products be incorporated into the product line? 
• What levels of resources are required to support the early phases of a product line project? 
• Are there lightweight approaches that can streamline the migration to product lines? 
• How much of the requirements and the architecture must be determined initially? 
• What must be done to existing development methods and processes to adapt them to the product line environment? 
• How long does it take to realize the benefits of a product line approach? 

We seek original articles on methods, tools, experiences, and techniques related to initiating software product lines. A full list 
of suggested topics is at computer.org/software/edcal.htm. Work should be presented in terms that are useful to the software 
community at large, emphasizing approaches taken and lessons learned.

Please submit two electronic copies, one in RTF or Microsoft Word and one in PDF or Postscript, by 1 March 2002 to IEEE Software
at software@computer.org. Articles must not exceed 5,400 words including figures and tables, which count for 200 words each.
The papers we deem within the theme’s scope will be peer-reviewed and are subject to editing for magazine style, clarity, and space. 

Please contact any of the special issue guest editors for more information about the focus or to discuss a potential submission; please
contact IEEE Software (software@computer.org or computer.org/software/author.htm) for author guidelines and submission details.

Klaus Pohl
University of Essen
pohl@informatik.uni-essen.de

Salah Jarrad
Panasonic Wireless Design Center
sjarrad@email.com

Linda Northrop
Software Engineering Institute 
lmn@sei.cmu.edu



7 2 I E E E  S O F T W A R E N o v e m b e r / D e c e m b e r  2 0 0 1 0 7 4 0 - 7 4 5 9 / 0 1 / $ 1 0 . 0 0  ©  2 0 0 1  I E E E

will normally implement measurement pro-
grams,5 such as the Goal-Question-Metric
method.6 However, organizations at vary-
ing maturity levels do not always need to
follow the same steps. Focusing on effort
usage—the working hours spent on imple-
mentation—this article will introduce and
outline the Nokiaway metrics program,
show how it differs from GQM, and ex-
plain the differences.

Nokiaway
Nokiaway is a real-world method used in

Nokia’s Fixed Switching Research & Design
Department. FSG R&D, named hereafter
FSG, produces software for the DX200
product line of telephone switches. The
DX200 program library has over 15 million
source lines of code, and Nokia has com-
mitted more than 2,000 engineers to carry
out DX200 development work. DX200 is

possibly Finland’s largest software project. 
The Nokiaway metrics program com-

prises these continuous metrics activities:
metrics definition, data collection, metrics
analysis, and metrics reporting. 

Metrics definition
In FSG, metrics definition is a part of con-

tinuous process improvement—that is, defin-
ing the metrics follows the same organiza-
tional structure as process improvement
does. At the highest organizational level, the
FSG management team steers FSG’s process
improvement (see Figure 1). FSG’s processes
comprise five separate process areas: release,
design, testing, maintenance, and quality as-
surance. The process teams, each of which is
responsible for a specific process area, man-
age these areas. A process development man-
ager—a full-time process developer—heads
each process team. Other team members are

feature
Implementing a Software
Metrics Program at Nokia

Tapani Kilpi, Nokia

Metrics play an
important role in
many software
organizations’
continuous
process
improvement
activities. In 
an organization
with mature
software
processes 
such as Nokia,
carefully
adjusting the
measurement
program to the
environment-
specific needs
can save
considerable
effort.

A
ll software organizations experience chaos. In theory, the solution
is easy: define and implement all key processes clearly enough, and
the chaos is gone. Software process improvement is an approach
for defining, organizing, and implementing software processes.

Well-known SPI methodologies include ISO/IEC 9000 standards,1 the Capa-
bility Maturity Model,2 Bootstrap,3 and ISO/IEC 15504.4 Measuring plays a
large role in these methodologies; to address this, an organization

metrics



part-time process developers whose primary
responsibilities include software develop-
ment, testing, and so on. 

The FSG software quality assurance
(SQA)7 process team has the overall responsi-
bility of coordinating and running the meas-
urement program. Other process teams are re-
sponsible for planning detailed improvement
projects and defining the metrics belonging to
their process area. The management team de-
fines the long-term process improvement road
maps and action plans and accepts and gives
resource commitments to process improve-
ment projects that the process teams propose.
The FSG R&D departments implement the
improvement projects.

Typically, a process team defines and con-
tinuously updates a set of 10 to 20 metrics.
Each metric addresses one of FSG’s approx-
imately 20 metric profiles (including Release
Program, Sw Project, Testing Project, De-
partment, and Section). For instance, each
process team has added one to three metrics
to the Sw Project metrics profile. Many met-
rics are common to several different metrics
profiles, but no profile resembles another.
Typically, a metrics profile contains approx-
imately ten metrics that most of the process
teams (not necessarily all of them in all
cases) define. The different process teams
collectively define the sets of metrics for the
FSG metrics library.

Data collection
Ideally, data collection should be set so

that the metrics save (that is, collect) data at
its inception. This requirement demands a
relatively high level of process maturity. The
Nokiaway metrics program uses four differ-
ent management tools and their data stor-
age for collecting and creating metrics: Re-
source and Project, Fault, Test Case, and
Inspection and Review. FSG project man-
agers, software designers, test engineers,
fault assistants, and people in other corre-
sponding roles routinely use these tools.
From the metrics point of view, this
arrangement provides excellent circum-
stances for data collection. No one must
collect the data after inception, because it
already exists in data storage for each of the
four tools. 

For example, project managers use the
Resource and Project tool to control proj-
ects, so they are eager to feed in the required
data. Other people in the project must also
feed in their data, but it’s such a simple task
that no one really complains. We similarly
apply the other tools: they are a part of daily
processes, and we feed data to them. Quality
engineers then have the relatively easy task
of producing the metrics values by using the
management tools. This minimizes the tradi-
tional manual hard work, and we can direct
attention to analyzing the metrics values.

N o v e m b e r / D e c e m b e r  2 0 0 1 I E E E  S O F T W A R E 73

• Improve coordination

Release Design Testing Maintenance

FSG R&D departments

Improvement
project initiatives

FSG R&D

Quality
assurance

• Improve portfolio
   management

Process teams

Ro
ad

 m
ap

s
Ac

tio
n 

pl
an

s

Headed 
by process

development
managers

Other units
in Nokia

• Master Improvement Plan
   accepted (monthly)
• Milestone decisions
• Resource commitment

R&D management team

Figure 1. The FSG process improvement steering structure.



Metrics analysis
In many real-life metrics programs, metrics

analysis gets little attention while other met-
rics activities are handled reasonably well.
However, people in such organizations prob-
ably seldom ask themselves, “Who should re-
act to the metrics values, and who should en-
sure that the necessary corrections are carried
out?” If no one uses a metric, there’s no point
in producing it.

Defining analysis guidelines and analyzing
responsibilities for metrics play a vital role 
in FSG metrics definition. This ensures that
someone, with some instructions, analyzes a
metric’s results. In FSG, SQA people and the
quality engineers generate the metrics reports.
They have been trained to analyze the metrics
values based on each metric’s analysis guide-
lines. Typically, a quality engineer produces a
set of approximately 10 metrics and analyzes
them within the project’s scope. This quality
engineer also has the expertise to carry out the
local analysis of a metric’s value. In FSG, we
could say that a quality engineer’s metrics du-
ties focus primarily on analysis instead of data
collection. This is not the traditional way, is it?

Metrics reporting
The metrics and analysis results are often

all that most people in an organization see
from a metrics program. These reports are
typically saved to a database or published
on the Web. From both of these places,
everyone in the organization can access the

reports—in theory. What’s missing is the pe-
riodical review carried out by predefined
people. Also missing are the decisions these
people make on the basis of the metrics re-
sults. In a situation such as this, there is no
hope that changes are planned and carried
out on the basis of the metrics.

The FSG metrics profiles are defined to
measure different types of targets. If the
measured target is, for example, a project,
the project’s quality engineer produces the
Project profile metrics. Then, the quality en-
gineer presents the metrics report at a pro-
ject meeting. This works well when the
quality engineer knows the project and peo-
ple involved. This makes it easier to trans-
late the metrics values to an understandable
language and communicate the results to
those who should be most concerned with
the metrics values: the people in the project.

A comparison of GQM and
Nokiaway

GQM is a well-known and widespread
method for planning and implementing a
metrics program.7 It represents a systematic
approach for tailoring and integrating goals
to the software process models, software
products, and quality perspectives of inter-
est—on the basis of the project’s and orga-
nization’s specific needs. (FSG defines the
corresponding goals as part of the Master
Improvement Plan, a yearly process im-
provement plan.) I compare Nokiaway with
the GQM as exemplified by Rini van Solin-
gen and Egon Berghout’s practical interpre-
tation and example effort calculations of
implementing a GQM metrics program.8

General characteristics
Nokiaway is a free interpretation of GQM.

With respect to a standard GQM approach,
it has these essential enhancements:

� It uses a quality metrics library instead
of defining a new set of metrics for each
project.

� It automatically, instead of manually,
collects raw data for the metrics.

� It semiautomatically (instead of manu-
ally) produces the reports’ metrics charts.

� Most people running operative tasks in
the measurement program are part-time
instead of full-time employees.

� It uses the Quality Plan and FSG Met-

7 4 I E E E  S O F T W A R E N o v e m b e r / D e c e m b e r  2 0 0 1

600

500

400

300

200

100

0
GQM
team

Data analysis and interpretation total
Data collection
Develop GQM plan
Conduct GQM interviews
Identify and define goals
GQM program planning

Manager Single
engineer

10
engineers

Total

Pe
rs

on
-h

ou
rs

Figure 2. An example effort model for a routine application of
GQM.8



rics Guidelines instead of a GQM plan
for defining a measurement program’s
exact background and scope.

A fundamental reason for these extensions
is that FSG is a multiproject environment;
that is, several projects of the same type run
in FSG instead of just one project. It is im-
portant that the experiences of all FSG’s
measurement programs are collected and
analyzed to further improve the metrics.
FSG saves this data in the FSG Metrics
Guidelines; that is, the document’s contents
form a metrics library. 

Some GQM purists might criticize
Nokiaway for depending too much on
predefined metrics and losing sight of the
GQM idea of defining goal-driven metrics
separately for each specific case. I have
this response:

� Is there really no point in defining com-
mon metrics that can be applied across
several similar projects in an organization?

� Do independently defined metrics for
different but similar projects really vary
that much from each other?

� Doesn’t management, after all, have the
main responsibility in deciding an orga-
nization’s process improvement strat-
egy—including goals?

� Could an organization provide individ-
uality in different measurement pro-
grams by selecting a subset of metrics
from a predefined, actively updated
metrics library?

� Aren’t collecting raw metrics data and
producing the metrics reports laborious
tasks, which require exhaustive re-
sources when performed manually?

While posing these questions with certain
answers in mind, we must remember that
FSG is a large organization with specific
characteristics. Environment and circum-
stances greatly influence planning an orga-
nization’s measurement program. My hope
is that FSG’s experiences can offer other
large organizations something to think
about when applying GQM or another
measurement program method.

Effort models
Solingen and Berghout presented an ex-

ample effort model (see Figure 2) for the

routine application of a typical GQM meas-
urement program with these characteristics:8

� Measurement program target: one soft-
ware project.

� Project team size: 11 persons (one proj-
ect manager and 10 software engineers).

� Project duration: approximately one
year.

� One major goal.
� An existing tool infrastructure.
� No need for special training; the partic-

ipants are familiar with the program.
� Feedback sessions during the project:

five.

Figure 2 also shows the following cost
structure, which is typical for a routine
GQM application:

� Roughly 30 percent of the effort goes
toward defining the measurement pro-
gram, and 70 percent goes toward con-
tinuing the measurement program,
which is almost completely spent on
feedback sessions.

� On the measurement program, the
GQM team spends 70 percent of the ef-
fort; the project team spends only 30
percent of the total effort.

� The project team’s effort is less than 1
percent of their total working time.

� A typical GQM measurement program
over one calendar year in a project of 11
persons requires three person-months of
effort.8

Table 1 compares GQM and Nokiaway
activities, with Solingen and Berghout’s de-
scription of their effort model in the left col-
umn.8 In GQM, the GQM plan contains the
detailed description of the measurement pro-
gram’s activities, their schedule, and their as-
sociated responsibilities. In FSG, the quality
plan contains the corresponding information
for implementing a measurement program.

Figure 3 presents the total resource use
in routine applications of standard GQM
and Nokiaway. (Analyzing the realized
hours of 10 FSG projects has produced the
estimations for the realized working hours
spent on running a measurement program
in FSG.) The total effort in Nokiaway ap-
pears to be less than that of the GQM
measurement program’s. This is because

N o v e m b e r / D e c e m b e r  2 0 0 1 I E E E  S O F T W A R E 75

In many real-
life metrics
programs,

metrics
analysis gets
little attention
while other

metrics
activities are

handled
reasonably

well. 



Nokiaway uses a high degree of automa-
tion in data collection while GQM does
not. However, because we can apply auto-
mated data collection to GQM, the overall
Nokiaway effort profile differs insignifi-
cantly from an average GQM effort profile.

A well-planned and carefully focusedmeasurement program is a usefulaid for an organization that highly
values continuous process improvement.
Some capable methods, such as GQM, for
planning and implementing a measurement
program tend to offer an independent and
complete set of steps and means for process
improvement. There is nothing wrong in
this, but many organizations already have a
successful history in applying different ap-
proaches, such as ISO/IEC, CMM, and
Bootstrap. As a result of this evolution,

7 6 I E E E  S O F T W A R E N o v e m b e r / D e c e m b e r  2 0 0 1

600

500

400

300

200

100

0
GQM/SQA

team

GQM
FSG

Manager Single
engineer

10
engineers

Total

Pe
rs

on
-h

ou
rs

Figure 3. Effort usage comparison between GQM and 
Nokiaway.

Table 1
Correspondences between the Goal-Question-Metric and Nokiaway activities 

Goal-Question-Metric8 Nokiaway

� Plan GQM program. This includes identifying available input, � Define FSG metrics guidelines. This document defines all quality metrics 
preconditions, and constraints; setting up an infrastructure; selecting that FSG uses, their intended use, directions and criteria for analyzing
an improvement area and a project; initially planning the measurement them, and so on. 
program; and preparing and training the project team. � Prepare a quality plan. This includes definitions for quality organization

and responsibilities, a schedule for quality feedback sessions, training
activities, most common tools and instructions, internal and external
verification activities, quality criteria and metrics (a subset selected from
the FSG metrics guidelines), a schedule and target group for quality
reports, and a status control table for the quality plan follow-up.

� Identify and define GQM goals. This includes characterizing projects � Define measurement key areas. FSG process teams define the process
and organizations, identifying and selecting improvement goals, defining areas’ process improvement goals on the basis of their observations and
measurement and GQM goals, modeling relevant software processes, on the quality metrics used to evaluate these areas.
and identifying artifacts for reuse.

� Conduct GQM interviews. This includes studying documentation; � Apply metrics feedback. All quality metrics that FSG uses have been
identifying, scheduling, and inviting interviewees; briefing project teams; defined in close cooperation with FSG practitioners. The process teams 
and conducting and reporting GQM interviews. and the SQA people collect the feedback between projects to improve the

metrics for the next use.
� Develop GQM deliverables. This includes defining, reviewing, and � Update the FSG metrics guidelines.

refining a GQM plan; defining a measurement plan; identifying and defining � Prepare the quality plan’s measurement parts. This part of the quality 
data collection procedures; reviewing and refining a measurement plan; plan defines a subset of the metrics included in the FSG metrics guidelines
and developing an analysis plan. according to the project-specific process improvement goals and possibly 

defines project-specific directions for applying the metrics.
� Collect data. This includes a trial to test data collection procedures � Collect data. Process-oriented tools automatically collect all data 

and forms; briefing the project team; launching the measurement needed for producing the FSG quality metrics.
program; and collecting, validating, coding, and storing measurement data. � Analyze and interpret data. The FSG SQA personnel carry this out;

� Analyze and interpret data. This includes analyzing the measurement most of them are software or test engineers working in SQA part-time.
data; preparing the presentation material; and planning, conducting,  
and reporting the feedback sessions.



these organizations have already developed
their processes to a relatively high maturity
level and do not need to start over.

The enhancements to GQM I’ve de-
scribed can save approximately 50 per-
cent of the cost of implementing and run-
ning a measurement program that relies
on manual work. These improvements
also help minimize bureaucracy and pre-
vent people from losing their motivation
for measuring.

References
1. ISO 90001, Quality Systems—Model for Quality As-

surance in Design/Development, Production, Installa-
tion and Servicing, Int’l Organization for Standardiza-
tion, Geneva, 1994.

2. B. Curtis, W.E. Hefley, and S. Miller, People Capability
Maturity Model (P-CMM), tech. report CMU/SEI-95-
MM-02, Software Eng. Inst., Carnegie Mellon Univ.,
Pittsburgh, 1995.

3. P. Kuvaja et al., Software Process Assessment and Im-
provement: The Bootstrap Approach, Blackwell, Ox-
ford, UK, 1994.

4. Information Technology: Software Process Assessment,
ISO/IEC tech. report 15504 Type 2, Int’l Organization
for Standardization and Int’l Electrotechnical Commis-
sion, Geneva, 1998.

5. N.E. Fenton and S.L. Pfleeger, Software Metrics: A Rig-
orous and Practical Approach, Int’l Thomson Com-
puter Press, London, 1996.

6. V.R. Basili, C. Caldiera, and H.D. Rombach, “Goal
Question Metric Paradigm,” Encyclopedia of Software
Engineering, vol. 1, J.J. Marciniak, ed., John Wiley &
Sons, New York, 1994.

7. ISO 9000-3, Guidelines for the Application of ISO
9001 to the Development, Supply and Maintenance of
Software: Quality Management and Quality Assurance
Standards—Part 3, Int’l Organization for Standardiza-
tion, Geneva, 1991.

8. R.van Solingen and E. Berghout, The Goal/Question/
Metric Method: A Practical Guide for Quality Improve-
ment of Software Improvement, McGraw-Hill, Cam-
bridge, UK, 1999.

For more information on this or any other computing topic, please visit our
Digital Library at http://computer.org/publications/dlib.

About the Author

Tapani Kilpi is the chief researcher at the Solid Applied Research Center, Oulu, Finland.
His current responsibilities focus on network management and 3G–4G applications. He re-
ceived his MS and PhD in information processing science at the University of Oulu. Contact him
at Solid, Elektroniikkatie 6, 90570 Oulu, Finland; tapani.kilpi@solidtech.com.

PURPOSE The IEEE Computer Society is the world’s
largest association of computing professionals, and is the

leading provider of technical information in the field.

MEMBERSHIP Members receive the monthly
magazine COMPUTER, discounts, and opportunities

to serve (all activities are led by volunteer mem-

bers). Membership is open to all IEEE members,

affiliate society members, and others interested in

the computer field.

B O A R D  O F  G O V E R N O R S
Term Expiring 2001: Kenneth R.Anderson,
Wolfgang K. Giloi, Haruhisa Ichikawa, Lowell G.
Johnson, Ming T. Liu, David G. McKendry, An-
neliese Amschler Andrews

Term Expiring 2002: Mark Grant, James D.
Isaak, Gene F.Hoffnagle, Karl Reed, Kathleen M.
Swigger, Ronald Waxman, Akihiko Yamada  

Term Expiring 2003: Fiorenza C.Albert-
Howard, Manfred Broy, Alan Clements, Richard
A. Kemmerer, Susan A. Mengel, James W. Moore,
Christina M. Schober

Next Board Meeting: 8 Feb 2001, Orlando, FL

I E E E  O F F I C E R S
President: JOEL B. SNYDER

President-Elect: RAYMOND D. FINDLAY

Executive Director: DANIEL J. SENESE

Secretary: HUGO M. FERNANDEZ  VERSTAGEN

Treasurer: DALE C. CASTON

VP, Educational Activities: LYLE D. FEISEL

VP, Publications Activities:JAMES M. TIEN

VP, Regional Activities: ANTONIO BASTOS

VP, Standards Association: MARCO W. MIGLIARO

VP, Technical Activities: LEWIS M. TERMAN

President, IEEE-USA: NED R. SAUTHOFF

EXECUTIVE COMMITTEE

President: BENJAMIN W. WAH*
University of Illinois
Coordinated Sci Lab
1308 W. Main St
Urbana, IL 61801-2307
Phone: +1 217 333 3516 Fax: +1 217 244 7175
b.wah@computer.org

President-Elect: WILLIS K. KING*
Past President: GUYLAINE M. POLLOCK*
VP, Educational Activities: CARL K. CHANG (1ST VP)*
VP, Conferences and Tutorials: GERALD L. ENGEL*
VP, Chapters Activities: JAMES H. CROSS†

VP, Publications: RANGACHAR KASTURI†

VP, Standards Activities: LOWELL G. JOHNSON*
VP, Technical Activities: DEBORAH K. SCHERRER
(2ND VP)*

Secretary:  WOLFGANG K. GILOI*
Treasurer: STEPHEN L. DIAMOND*
2000–2001 IEEE Division V Director:
DORIS L. CARVER†

2001–2002 IEEE Division VIII Director:
THOMAS W. WILLIAMS†

Acting Executive Director: ANNE MARIE KELLY†

* voting member of the Board of Governors

COMPUTER SOCIETY WEB SITE
The IEEE Computer Society’s Web site, at 
http://computer.org, offers information and samples
from the society’s publications and conferences, as
well as a broad range of information about technical
committees, standards, student activities, and more.

COMPUTER SOCIETY O F F I C E S
Headquarters Office

730 Massachusetts Ave.NW  
Washington, DC 20036-1992
Phone:+1 202 371 0101 • Fax:+1 202 728 9614
E-mail:hq.ofc@computer.org

Publications Office
10662 Los Vaqueros Cir., PO  Box 3014
Los Alamitos, CA 90720-1314
Phone:+1 714 8218380
E-mail:help@computer.org
Membership and Publication Orders:
Phone:+1 800 272 6657 Fax:+1 714 821 4641
E-mail: help@computer.org

European Office
13, Ave. de L’Aquilon
B-1200 Brussels, Belgium
Phone: +32  2  770 21 98 • Fax: +32 2 770 85 05
E-mail: euro.ofc@computer.org

Asia/Pacific Office
Watanabe Building
1-4-2 Minami-Aoyama,Minato-ku,
Tokyo107-0062, Japan
Phone: +81 3 3408 3118 • Fax: +81 3 3408 3553
E-mail: tokyo.ofc@computer.org

E X E C U T I V E  S T A F F
Acting Executive Director : ANNE MARIE KELLY
Publisher: ANGELA BURGESS
Acting Director, Volunteer Services:
MARY-KATE RADA
Chief Financial Officer: VIOLET S. DOAN
Director, Information Technology & Services:
ROBERT CARE
Manager, Research & Planning: JOHN C. KEATON

26-OCT-2001



7 8 I E E E  S O F T W A R E N o v e m b e r / D e c e m b e r  2 0 0 1 0 7 4 0 - 7 4 5 9 / 0 1 / $ 1 0 . 0 0  ©  2 0 0 1  I E E E

problem; this impediment became known
as the 500-Language Problem.

In 1998, we realized that we had discov-
ered a breakthrough in solving the 500LP—
so we had something to offer regarding the
Y2K problem. We immediately informed all
the relevant Y2K solution providers and peo-
ple concerned with the Y2K awareness cam-
paign. In answer to our emails, we received a
boilerplate email from Ed Yourdon explain-
ing that the 500LP was a major impediment
to solving the Y2K problem (which we knew,
of course). Ed was apparently so good at cre-
ating awareness that this had backfired on
him: he got 200 to 300 messages a day with
Y2K questions and was no longer able to
read, interpret, and answer his email other
than in “write-only” mode. Although he pre-
sumably missed our input, his response re-
garding the 500LP is worth quoting:

I recognize that there is always a chance that
someone will come up with a brilliant solu-
tion that everyone else has overlooked, but at
this late date, I think it’s highly unlikely. In
particular, I think the chances of a “silver bul-
let” solution that will solve ALL y2k problems
is virtually zero. If you think you have such a
solution, I have two words for you: embedded
systems. If that’s not enough, I have three
words for you: 500 programming languages.
The immense variety of programming lan-
guages (yes, there really are 500!), hardware
platforms, operating systems, and environ-
mental conditions virtually eliminates any
chance of a single tool, method, or technique
being universally applicable.

The number 500 should be taken poeti-
cally, like the 1,000 in the preserving
process for so-called 1,000-year-old eggs,
which last only 100 days. For a start, we

feature
Cracking the 
500-Language Problem

Ralf Lämmel and Chris Verhoef, Free University of Amsterdam

Parser
implementation
effort dominates
the construction
of software
renovation tools
for any of the
500+ languages
in use today. The
authors propose
a way to rapidly
develop suitable
parsers: by
stealing the
grammars. 
They apply this
approach to 
two nontrivial,
representative
languages, PLEX
and VS Cobol II.

A
t least 500 programming languages and dialects are available in
commercial form or in the public domain, according to Capers
Jones.1 He also estimates that corporations have developed some
200 proprietary languages for their own use. In his 1998 book on

estimating Year 2000 costs, he indicated that systems written in all 700 lan-
guages would be affected.2 His findings inspired many Y2K whistle-blowers
to characterize this situation as a major impediment to solving the Y2K 

programming languages



should add the 200 proprietary languages.
Moreover, other estimates indicate that 700
is rather conservative: in 1971, Gerald
Weinberg estimated that by the following
year, programming languages would be in-
vented at the rate of one per week—or
more, if we consider the ones that never
make it to the literature, and enormously
more if we consider dialects.3

Peter de Jager also helped raise aware-
ness of the 500LP. He writes this about the
availability of Y2K tools:4

There are close to 500 programming lan-
guages used to develop applications. Most of
these conversion or inventory tools are di-
rected toward a very small subset of those 500
languages. A majority of the tools are focused
on Cobol, the most popular business program-
ming language in the world. Very few tools, if
any, have been designed to help in the area of
APL or JOVIAL for example.

If everyone were using Cobol and only a
few systems were written in uncommon
languages, the 500-Language Problem
would not be important. So, knowing the
actual language distribution of installed
software is useful. First, there are about
300 Cobol dialects, and each compiler
product has a few versions—with many
patch levels. Also, Cobol often contains
embedded languages such as DMS, DML,
CICS, and SQL. So there is no such thing as
“the Cobol language.” It is a polyglot, a
confusing mixture of dialects and embed-
ded languages—a 500-Language Problem
of its own. Second, according to Jones, the
world’s installed software is distributed by
language as follows:

� Cobol: 30 percent (225 billion LOC)
� C/C++: 20 percent (180 billion LOC)
� Assembler: 10 percent (140 to 220 bil-

lion LOC)
� less common languages: 40 percent (280

billion LOC)

In contrast, there were Y2K search en-
gines for only about 50 languages and auto-
mated Y2K repair engines for about 10 lan-
guages.2 Thus, most languages had no
automated modification support, clarifying
the concerns of Jones, Yourdon, McCabe,
de Jager, and others. These alarming figures
underscored the 500LP’s importance.

What is the 500-Language
Problem?

We entered the new millennium without
much trouble, so you might conclude that
whatever the 500LP was, it is not relevant
now. Of course, the problem existed before
the Y2K gurus popularized it, and it has not
gone away.

Why is the problem still relevant? If you
want tools to accurately probe and manipu-
late source code, you must first convert the
code from text format to tree format. To do
this, you need a so-called syntactic analyzer,
or parser. But constructing a parser is a ma-
jor effort, and the large up-front investment
hampers initiatives for many commercial
tool builders. Indeed, a few years ago Tom
McCabe told us that his company, McCabe
& Associates, had made a huge investment
in developing parsers for 23 languages. Not-
ing that 500 would be insurmountable, he
dubbed this problem “the number one
problem in software renovation.” Thus, the
500LP is the most prominent impediment to
constructing tools to analyze and modify
existing software assets. Because there are
about a trillion lines of installed software
written in myriad languages, its solution is a
step forward in managing those assets.

Solutions that don’t work
A solution sometimes suggested for the

500LP is just to convert from uncommon
languages to mainstream ones for which
tool support is available. However, you
need a full-blown tool suite—including a se-
rious parser—to do this. And obtaining a
parser is part of the 500LP. So language
conversion will not eliminate the problem—
on the contrary, you need a solution for the
500LP to solve conversion problems.

A Usenet discussion on comp.compilers
offered a second suggestion to solve the
500LP: generating grammars from the
source code only, in the same way linguists
try to generate a grammar from a piece of
natural language. In search of solutions, we
studied this idea and consulted the relevant
literature. We did not find any successful ef-
fort where the linguistic approach helped to
create a grammar for a parser in a cost-
effective way. We concluded that the lin-
guistic approach does not lead to useful
grammar inferences from which you can
build parsers.5

N o v e m b e r / D e c e m b e r  2 0 0 1 I E E E  S O F T W A R E 79



Another, more reasonable suggestion is to
reuse the parser from compilers: just tap a
compiler’s parser output and feed it to a
modification tool. Prem Devanbu’s pro-
grammable GENOA/GENII tool can turn a
parser’s idiosyncratic output format into a
more suitable format for code analysis.6

There is, however, one major drawback to
this approach: as Devanbu points out, the
GENOA system does not allow code modifica-
tion. This is not a surprise: a compiler’s
parser removes comments, expands macros,
includes files, minimizes syntax, and thus ir-
reversibly deforms the original source code.
The intermediate format is good enough for
analysis in some cases, but the code can
never be turned into acceptable text format
again. Hence, the approach does not help
regarding mass modifications, for which the
Gartner Group recommends tool support to
handle a larger code volume.7,8 Obviously,
this concerns Y2K and Euro conversions,
code restructuring, language migrations, and
so on. Another real limitation of Devanbu’s
approach is that, even if you only want to do
code analysis, you often cannot get access to
a compiler company’s proprietary source.

How we are cracking the 500LP
Recall that Yourdon claimed that the

large number of programming languages
would virtually eliminate any chance of a
single tool, method, or technique being uni-
versally applicable. Nevertheless, there is a
single, feasible solution for the 500LP. It is

cracked when there is a cheap, rapid, and re-
liable method for producing grammars for
the myriad languages in use so that existing
code can be analyzed and modified. Cheap is
in the US$25,000 ± $5,000 range, rapid is in
the two-week range (for one person), and re-
liable means the parser based on the pro-
duced grammar can parse millions of LOC.

Why is this a solution? A grammar is
hardly a Euro conversion tool or a Y2K an-
alyzer. It is because the most dominant fac-
tor in building renovation tools is con-
structing the underlying parser.

From grammar to renovation tool
Renovation tools routinely comprise the

following main components: preprocessors,
parsers, analyzers, transformers, visualizers,
pretty printers, and postprocessors. In many
cases, language-parameterized (or generic)
tools are available to construct these com-
ponents. Think of parser generators, pretty-
printer generators, graph visualization
packages, rewrite engines, generic dataflow
analyzers, and the like. Workbenches pro-
viding this functionality include Elegant,
Refine, and ASF+SDF, for instance, but
there are many more. 

Figure 1 depicts a grammar-centric ap-
proach to enabling rapid development of
renovation tools. Arrow length indicates the
degree of effort involved (longer arrows im-
ply more effort). As you can see, if you have
a generic core and a grammar, it does not
take much effort to construct parsers, tree
walkers, pretty printers, and so on. Al-
though these components depend on a par-
ticular language, their implementation uses
generic language technology: a parser is pro-
duced using a parser generator, a pretty
printer is created using a formatter genera-
tor,9 and tree walkers for analysis or modifi-
cation are generated similarly.10 All these
generators rely heavily on the grammar.
Once you have the grammar and the rele-
vant generators, you can rapidly set up this
core for developing software renovation
tools. Leading Y2K companies indeed con-
structed generic Y2K analyzers, so that deal-
ing with a new language would ideally re-
duce to constructing a parser. The bottleneck
is in obtaining complete and correct gram-
mar specifications. The longest arrow in Fig-
ure 1 expresses the current situation: it takes
a lot of effort to create those grammars.

8 0 I E E E  S O F T W A R E N o v e m b e r / D e c e m b e r  2 0 0 1

Tool 1

Tool 2

...

...

Tool
n

Pretty
printer

Editor

Analyzer

Parser

Generic
core

Tree
walker

Grammar Grammar

Figure 1. Effort shift for renovation tool development. The
longer the arrow, the more effort is needed. The dashed 
line represents the greater effort needed if the traditional 
approach is used.



Implementing a high-quality Cobol parser
can take two to three years, as Vadim Maslov
of Siber Systems posted on the Usenet news-
group comp.compilers (he has constructed
Cobol parsers for about 16 dialects). Adapt-
ing an existing Cobol parser to cope with
new dialects easily takes three to five months.
Moreover, patching existing grammars using
mainstream parser technology leads to un-
maintainable grammars,11,12 significantly in-
creasing the time it takes to adapt parsers. In
contrast, Table 1 lists the effort expended on
various phases of a typical Cobol renovation
project that used our grammar-centric solu-
tion. Notice that the grammar part of this
project took only two weeks of effort, so the
team could start developing actual renova-
tion tools much more quickly. 

This Cobol renovation project concerned
one of the world’s largest financial enter-
prises, which needed an automatic con-
verter from Cobol 85 back to Cobol 74 (the
8574 Project).13 The Cobol 85 code was ma-
chine-generated from a fourth-generation-
language tool, so the problem to convert
back was fortunately restricted due to the
code generator’s limited vocabulary. It took
some time to solve intricate problems, such
as how to simulate Cobol 85 features like
explicit scope terminators (END-IF, END-
ADD) and how to express the INITIALIZE
statement in the less-rich Cobol 74 dialect.
The developers discussed solutions with the
customer and tested them for equivalence.
Once they solved these problems, imple-
menting the components was not difficult
because they had the generic core assets gen-
erated from a recovered Cobol 85 grammar.
They cut the problem into six separate tools
and then implemented all of them in only
five days. The programming by hand was
limited (fewer than 500 LOC), but compiled
into about 100,000 lines of C code and
5,000 lines of makefile code (linking all

the generated generic renovation functional-
ity). After compilation to six executables
(2.6 Mbytes each), it took 25 lines of code
to coordinate them into a distributed, com-
ponent-based software renovation factory,
which then converted Cobol 85 code to
Cobol 74 at a rate of 500,000 LOC per
hour using 11 Sun workstations.

Measuring this and other projects, it be-
came clear to us that the total effort of writ-
ing a grammar by hand is orders of magni-
tude larger than constructing the renovation
tools themselves. So the dominant factor in
producing a renovation tool is constructing
the parser. Building parsers using our ap-
proach reduces the effort to the same order
of magnitude as constructing the tools. Build-
ing parsers in turn is not hard: use a parser
generator. But the input for the generator is a
grammar description, so complete and cor-
rect grammars are the most important arti-
facts we need to enable tool support. When
we find an effective solution for producing
grammars quickly for many languages, we
have solved the 500LP.

But how do we produce grammars
quickly? For years, we and many others have
been recapturing an existing language’s syntax
by hand: we took a huge amount of sources,
manuals, books, and a parser generator and
started working. But then we realized that this
hand work is not necessary. Because we are
dealing with existing languages, we just steal
and massage the underlying grammars ac-
cording to our needs.

Grammar stealing covers almost all languages
The following exhaustive case distinction

shows that our approach covers virtually all
languages. Let’s look at the coverage diagram
for grammar stealing shown in Figure 2. Be-
cause the software we want to convert al-
ready exists, it can be compiled or interpreted.
We first enter the Compiler Sources diamond.
There are two possibilities: the source code is
or is not available to you. If it is, you just have
to find the part that turns the text into an in-
termediate form. That part now contains the
grammar in some form. You do this by lexi-
cally searching the compiler source code for
the language’s keywords.

Compiler constructors implement a
parser in one of three ways: they hard-code
it, use a parser generator, or do both (in a
complex multilanguage compiler, for in-

N o v e m b e r / D e c e m b e r  2 0 0 1 I E E E  S O F T W A R E 81

When we find 
an effective
solution for
producing
grammars
quickly for

many languages, 
we have solved

the 500LP.

Table 1
Effort for the 8574 Project

Phase Effort

Extract the grammar Two weeks
Generate the parser One day 
Build six tools Five days 
Assemble all the components One hour
Total Three weeks



stance). Figure 2 shows the first two cases—
the third is just a combination. If you start
with a hard-coded grammar, you must re-
verse-engineer it from the handwritten code.
Fortunately, the comments of such code of-
ten include BNF rules (Backus Naur Forms)
indicating what the grammar comprises.
Moreover, because compiler construction is
well-understood (there is a known reference
architecture), compilers are often imple-
mented with well-known implementation al-
gorithms, such as a recursive descent algo-
rithm. So, the quality of a hard-coded parser
implementation is usually good, in which
case you can easily recover the grammar
from the code, the comments, or both. Ex-
cept in one case, the Perl language,14 the
quality of the code we worked with was al-
ways sufficient to recover the grammar.

If the parser is not hard-coded, it is gen-
erated (the BNF branch in Figure 2), and
some BNF description of it must be in the
compiler source code. So, with a simple tool
that parses the BNF itself, we can parse the
BNF of the language that resides in the com-
piler in BNF notation, and then extract it.

When the compiler source code is not
accessible (we enter the Language Refer-
ence Manual diamond in Figure 2), either a
reference manual exists or not. If it is avail-
able, it could be either a compiler vendor
manual or an official language standard.
The language is explained either by exam-

ple, through general rules, or by both ap-
proaches. If a manual uses general rules, its
quality is generally not good: reference
manuals and language standards are full of
errors. It is our experience that the myriad
errors are repairable. As an aside, we once
failed to recover a grammar from the man-
ual of a proprietary language for which the
compiler source code was also available
(so this case is covered in the upper half of
Figure 2). As you can see in the coverage
diagram, we have not found low-quality
language reference manuals containing
general rules for cases where we did not
have access to the source code. That is, to
be successful, compiler vendors must pro-
vide accurate and complete documenta-
tion, even though they do not give away
their compilers’ source code for economic
reasons. We discovered that the quality of
those manuals is good enough to recover
the grammar. This applies not only to com-
piler-vendor manuals but also to all kinds
of de facto and official language standards.

Unusual languages rarely have high-quality
manuals: either none exists (for example, if
the language is proprietary) or the company
has only a few customers. In the proprietary
case, a company is using its in-house lan-
guage and so has access to the source code;
in the other case, outsiders can buy the code
because its business value is not too high.
For instance, when Wang went bankrupt, its

8 2 I E E E  S O F T W A R E N o v e m b e r / D e c e m b e r  2 0 0 1

No

Yes

Yes

Start

Hard-coded
parser

Recover the
grammar

Recover the
grammar

BNF

No cases known

Yes

One case:
perl

No

Language
reference
manual?

No

General
rules

Recover the
grammar

One case:
RPG

Constructions
by example

Yes

No cases
known

No

Compiler
sources?

Quality?

Quality?

Figure 2. Coverage diagram for grammar stealing.



key customers bought the source code for its
operating system and compilers to create
their own platform and dialect migration
tools. This explains why we do not know of
low-quality manuals containing general
rules. In one case, that of RPG, the manual
explains the language through code exam-
ples, and general rules are absent. We can
examine this case in more detail if we are
asked for an RPG renovation project in-
volving a large amount of RPG code. We
think we can systematically extract RPG’s
general rules from the code examples.

In addition, because the manual contains
code examples, there is a good chance that
the compiler has tested these examples. This
means that the manual’s formal content
could be of a much higher quality than you
would expect from such documents.

Finally, we must deal with the case in
which we have no access to the compiler
sources or a reference manual. Capers Jones
mailed us that “for a significant number of
applications with Y2K problems, the com-
pilers may no longer be available either be-
cause the companies that wrote them have
gone out of business or for other reasons.”
He did not come up with actual examples.
Recall Wang’s bankruptcy: key customers
just bought the source code and hence could
solve their problems using the upper half of
Figure 2. Theoretically, we cannot exclude
Jones’s case—for instance, responding emo-
tionally, Wang’s core developers could have
thrown away the sources. You can learn an
important lesson from this: Contracts be-
tween you and the vendor of a business-crit-
ical language should include a solution for
source access in case of bankruptcy or termi-
nated support (for example, giving the sealed
source code to key customers). Summarizing,
our coverage diagram shows that you can re-
cover virtually any grammar, whether you
have the compiler sources or not.

But what about semantics?
Some people think you need up-front, in-

depth knowledge of a language’s semantics
to change code. If you recover the BNF, you
can generate a syntax analyzer that produces
trees, but the trees are not decorated with
extra knowledge such as control flow, data
flow, type annotation, name resolution, and
so on. Some people also think you need a lot
of semantical knowledge to analyze and

modify existing software, but this is not true.
You can try to capture a language’s semanti-
cal knowledge on three levels:

� for all the compilers of a language (dif-
ferent dialects),

� for one compiler product, or
� on a project-by-project basis.

Because we are trying to facilitate the
construction of tools that work on existing
software, there is already a compiler. This
has implications for dealing with the se-
mantical knowledge. Consider the follow-
ing Cobol excerpt:

PIC A X(5) RIGHT JUSTIFIED

VALUE ’IEEE’.

DISPLAY A.

The OS/VS Cobol-compiled code prints the
expected result—namely, “ IEEE”—which is
right justified. However, because of a change
in the 1974 standard, the same code com-
piled with a Cobol/370 compiler displays the
output “IEEE ” with a trailing space, which
is left justified. This is because the RIGHT
JUSTIFIED phrase does not affect VALUE
clauses in the case of the Cobol/370 com-
piler. There are many more such cases, so
trying to deal with the semantics of all com-
pilers in advance is not feasible. Even when
you restrict yourself to one compiler, this
problem does not go away. Consider this
Cobol fragment:

01 A PIC 9999999.

MOVE ALL ’123’ to A.

DISPLAY A.

Depending on the compiler flags used to
compile this code, the resulting executables
display either 3123123 or 1231231. There
are hundreds of such problems, so it is also
infeasible to capture the semantics in ad-
vance for a single compiler. No single se-
mantics is available, and gathering all vari-
ants is prohibitively expensive and error
prone given the semantical differences be-
tween compilers, compiler versions, and
even compiler flags used.

The good news is that you only need spe-
cific ad hoc elements of the semantics on a
per-project basis. We call this demand-driven
semantics. For instance, the NEXT SENTENCE

N o v e m b e r / D e c e m b e r  2 0 0 1 I E E E  S O F T W A R E 83

Some people
think you need

up-front, 
in-depth

knowledge of 
a language’s
semantics to
change code.



phrase in Cobol directs control to the state-
ment after the next separation period (de-
noted with a dot). So, depending on where
people put a dot, the code jumps directly be-
hind the dot. Omitting a dot can lead to dif-
ferent behavior. One of our customers
wanted tools to get rid of this potentially
hazardous implicit jump instruction. Luck-
ily, it turned out that for this project, we
could replace the implicit jump instruction
NEXT SENTENCE with the innocent no-op
CONTINUE. So, after our semantical investi-
gation, we knew we could use a simple
transformation tool to make this change.
However, in another project, this transfor-
mation might break down—for example, if
the NEXT SENTENCE phrase is used in prob-
lematic code patterns. The transformation of
the code in Figure 3a into the code in Figure
3b has changed the program’s meaning: you
cannot turn NEXT SENTENCE into CONTINUE
in the context of Figure 3. Specifically, as-
suming both X and Y are equal to 1, the code
in Figure 3a prints “SENTENCE passed”
while the code in Figure 3b prints first
“Nested IF passed” and then “SENTENCE
passed”. As you can see, you must be utterly
aware of the semantics to find out whether it
is necessary to implement any of it. In most
cases we have seen, implementing this type
of intricate semantical issue was not neces-
sary—but knowing about the potential
problems was necessary, if only to check
whether they were present.

To give you an idea how far you can go
with demand-driven semantics, consider this:
we have developed relatively dumb tools for
some Cobol systems that can wipe out com-
plex GO TO logic.15 You do need to know the
semantics for many different tasks, but it is
not necessary in advance to encode the com-
piler semantics in a parse tree or otherwise.
So, a tool developer can construct (mostly)
syntactic tools taking semantical knowledge
into account on a per-project basis.

Grammar stealing in practice
We—and others from industry and aca-

demia—have applied grammar stealing
successfully to a number of languages, in-
cluding Java, PL/I, Ericsson PLEX, C++,
Ada 95, VS Cobol II, AT&T SDL, Swift
messages, and more. Here, we focus on
PLEX (Programming Language for Ex-
changes), a proprietary, nontrivial, real-
time embedded-system language (for which
the compiler source code was accessible to
us), and, at the other end of the gamut, VS
Cobol II, a well-known business language
(for which no compiler code was avail-
able). Both languages are used in business-
critical systems: the AXE 10 public branch
exchange uses PLEX, and numerous IBM
mainframe systems run VS Cobol II. These
two languages represent the two main
branches in Figure 2.

Our approach uses a unique combination
of powerful techniques:

� automated grammar extraction,
� sophisticated parsing,
� automated testing, and
� automated grammar transformation.

If one of these ingredients is missing, the syn-
ergy is gone. Extraction by hand is error
prone, and basic parsing technology limits
you to work with grammars in severely lim-
ited formats. With powerful parsing technol-
ogy, you can work with arbitrary context-free
grammars and test them regardless of their
format. Without automated testing, you can-
not find many errors quickly. Without tool
support to transform grammar specifications,
analyses are inaccurate and corrections are in-
consistent; without transformations, you can-
not repeat what you have done or change ini-
tial decisions easily. So, to steal grammars,
you need to know about grammars, powerful
parsing techniques, how to set up testing, and
automated transformations.

8 4 I E E E  S O F T W A R E N o v e m b e r / D e c e m b e r  2 0 0 1

IF X=1 THEN IF X=1 THEN

IF Y=1 THEN               IF Y=1 THEN

NEXT SENTENCE    CONTINUE

END-IF                        END-IF

DISPLAY ’Nested IF passed’ DISPLAY ’Nested IF passed’

END-IF. END-IF.

DISPLAY ’SENTENCE passed’. DISPLAY ’SENTENCE passed’.

Figure 3. A code segment (a), transformed inappropriately into the code 
segment in (b). The change in line 3 results in wrong output.

(a) (b)
We recovered

the PLEX
grammar 

in two weeks,
including tool
construction,

parser
generation, 
and testing, 
at a cost of
US$25,000.



Stealing from compiler source code
Ericsson uses the extremely complex pro-

prietary language PLEX to program public
telephone switches. PLEX consists of about
20 sublanguages, called sectors, including
high-level programming sectors, assembly
sectors, finite-state-machine sectors, marshal-
ing sectors, and others. We applied our gram-
mar-stealing approach to PLEX as follows:16

1. Reverse-engineer the PLEX compiler 
(63 Mbytes of source code) on site to 
look for grammar-related files. We 
learned that there were BNF files and a 
hard-coded parser.

2. Find the majority of the grammars in 
some BNF dialect.

3. Find a hand-written proprietary assem-
bly parser with BNF in the comments.

4. Write six BNF parsers (one for each 
BNF dialect used).

5. Extract the plain BNF from the com-
piler sources and convert it to another 
syntax definition formalism (SDF) for 
technical reasons.

6. Find the files containing the lexical an-
alyzer and convert the lexical defini-
tions to SDF.

7. Combine all the converted grammars
into one overall grammar.

8. Generate an overall parser with a so-
phisticated parser generator.

9. Parse the code.

We recovered the PLEX grammar in two weeks,
including tool construction, parser generation,
and testing with 8-MLOC PLEX code, at a cost
of US$25,000. Ericsson told us that a cutting-
edge reengineering company had estimated this
task earlier at a few million dollars. When we
contacted this company, they told us that
US$25,000 was nothing for such a grammar.

To illustrate the limited complexity of the
work, consider the fragment of raw com-

piler source code in Figure 4. A PLEX pro-
gram consists of a header, a list of state-
ments, the phrase END PROGRAM, and a clos-
ing semicolon. The other code in the figure
deals with semantic actions relevant to the
compiler. Our tools converted this to a com-
mon BNF while removing the idiosyncratic
semantic actions:

plex-program ::= program-header

statement-row

’END’ ’PROGRAM’ ’;’

Then our tools converted this into SDF,
which was subsequently fed to a sophisti-
cated parser generator accepting arbitrary
context-free grammars. The output was

Program-header 

Statement-row 

”END” ”PROGRAM” ”;” -> Plex-program

The tools we built automatically recov-
ered most of the 3,000+ production rules in
an afternoon. Then we tested each sector
grammar separately. We used a duplicate
detector to weed out production rules that
were used in more than one sector grammar,
so that we could construct an overall gram-
mar able to parse complete PLEX programs.
One assembly sector parser was hard-coded
(see Figure 2), so we had to recover its
grammar through reverse engineering. The
comments accompanying the code con-
tained reasonably good BNF, so we had no
problem with this task. With all the sector
grammars combined, we generated a parser
to test it with an 8-MLOC PLEX test suite.
The only files that did not parse were com-
piler test files that were not supposed to
parse—the rest passed the test. In addition,
we generated a Web-enabled version of the
BNF description as a basis for a complete
and correct manual.

N o v e m b e r / D e c e m b e r  2 0 0 1 I E E E  S O F T W A R E 85

The tools 
we built

automatically
recovered 
most of the

3,000+
production
rules in an
afternoon.

<plex-program>

= <program-header> <statement-row> ’END’ ’PROGRAM’ ’;’

%% xnsmtopg(1) ; %%

--     <= sect

--        Compound(

--            Reverse(STATEMENT-ROW.stat_list) =>

--               PROGRAM-HEADER.sect.as_prog_stat : ix_stat_list_p

--               PROGRAM-HEADER.sect : ix_sect_node_p)

;

Figure 4. Raw compiler source code for the PLEX language. 



Stealing from reference manuals
Some of our colleagues felt a little fooled

by the PLEX result: “You are not really con-
structing a parser; you only converted an ex-
isting one. We can do that, too. Now try it
without the compiler.’’ Indeed, at first sight,
not having this valuable knowledge source
available seemed to make the work more dif-
ficult. After all, an earlier effort to recover
the PLEX grammar from various online
manuals had failed: they were not good
enough for reconstructing the language.17

Later, we discovered that the manuals lacked
over half of the language definition, so that
the recovery process had to be incomplete by
definition. We also found that our failure
was due not to our tools but to the nature of
proprietary manuals: if the language’s audi-
ence is limited, major omissions can go un-
noticed for a long time. When there is a large
customer base, the language vendor has to
deliver better quality.

In another two-week effort,5 we recovered
the VS Cobol II grammar from IBM’s manual
VS COBOL II Reference Summary, version
1.2. (For the fully recovered VS Cobol II gram-
mar, see www.cs.vu.nl/grammars/vs-cobol-ii.)
Again, the process was straightforward:

1. Retrieve the online VS Cobol II manual
from www.ibm.com.

2. Extract its syntax diagrams.
3. Write a parser for the syntax diagrams.
4. Extract the BNF from the diagrams.
5. Add 17 lexical rules by hand.
6. Correct the BNF using grammar trans-

formations.
7. Generate an error-detection parser.
8. Incrementally parse 2 million lines of 

VS Cobol II code.
9. Reiterate steps 6 through 8 until all errors

vanish.
10. Convert the BNF to SDF.
11. Generate a production parser.

8 6 I E E E  S O F T W A R E N o v e m b e r / D e c e m b e r  2 0 0 1

3.30 SEARCH Statement

   ___ Format 1--Serial Search _____________________________________  
  |                                                                 | 
  | >>__SEARCH__identifier-1______________________________________> | 
  |                           |_VARYING____identifier-2___|         | 
  |                                      |_index-name-1_|           | 
  |                                                                 | 
  | >_____________________________________________________________> | 
  |    |_________END__imperative-statement-1_|                      | 
  |      |_AT_|                                                     | 
  |                                                                 | 
  |    <_______________________________________________             | 
  | >____WHEN__condition-1____imperative-statement-2___|__________> | 
  |                         |_NEXT-SENTENCE__________|              | 
  |                                                                 | 
  | >____________________________________________________________>< | 
  |    |            (1)|                                            | 
  |    |_END-SEARCH____|                                            | 
  |                                                                 | 
  | Note:                                                           | 
x | (1)  END-SEARCH with NEXT SENTENCE is an IBM extension.         | 
  |_________________________________________________________________|

search-statement =

   ”SEARCH” identifier [”VARYING” (identifier | index-name)]

   [[”AT”] ”END” statement-list]

   {”WHEN” condition (statement-list | ”NEXT” ”SENTENCE”)}+

   [”END-SEARCH”]

(a)

(b)

Figure 5. (a) The original syntax diagram for the Search statement; (b) the
same diagram after conversion to BNF and correction.



12. Incrementally parse VS Cobol II code 
to detect ambiguities.

13. Resolve ambiguities using grammar 
transformations.

14. Reiterate steps 11 through 13 until you
find no more ambiguities.

So, apart from some cycles to correct errors
and remove ambiguities, the process is the
same as in the earlier case, where we had ac-
cess to the compiler source. An error-
detection parser detects errors in the grammar
from which it is generated. In this case, we
used an inefficient top-down parser with infi-
nite lookahead. It accepts practically all con-
text-free grammars and does not bother with
ambiguities at all. We use this kind of parser to
test the grammar, not to produce parse trees.
Because we only used compilable code, all the
errors that this parser detects raise potential
grammar problems. In this way, we found all
the omissions, given our Cobol testbed. When
all our test code passed the top-down parser,
we converted the grammar to SDF, generated
a parser that detects ambiguities, and cor-
rected them. This project also took two weeks
of effort, including tool construction and test-
ing. We did this for free, so that we could
freely publish the grammar on the Internet as
a gift for Cobol’s 40th birthday.18

To get an idea of the limited complexity of
this technique, consider the syntax diagram
shown in Figure 5a, taken from the manual.
After conversion to BNF and correction, the
diagram looks like the one in Figure 5b.

We used grammar transformations to re-
move the dash between NEXT and SENTENCE
and to replace the two occurrences of 
imperative-statement with statement-
list. The diagram was overly restrictive,
allowing only one statement. However, in
the manual’s informal text we learned, “A
series of imperative statements can be spec-
ified whenever an imperative statement is
allowed.” Our error-detection parser found
these errors: first, the tool parsed code when
it found NEXT SENTENCE, that is, without a
dash. After inspecting the manual and
grammar, we wrote a grammar transforma-
tion repairing this error. The error-detection
parser also found that, according to the
compiler, more than one statement was cor-
rect whereas the manual insisted on exactly
one statement. We repaired this error with a
grammar transformation.

Next, in a separate phase, we removed
ambiguities. For example, the following
fragment of a syntax diagram is present in
the Cobol CALL statement:

_____identifier__________________

|__ADDRESS__OF__identifier__|

|__file-name________________|

This stack of three alternatives can lead to an
ambiguity. Namely, both identifier and
file-name eventually reduce to the same
lexical category. So, when we parsed a CALL
statement without an occurrence of ADDRESS
OF, the parser reported an ambiguity because
the other alternatives were both valid. With-
out using type information, we cannot sepa-
rate identifier from file-name. This is
the ambiguous extracted BNF fragment:

(identifier 

| ”ADDRESS” ”OF” identifier 

| file-name)

With a grammar transformation, we elimi-
nated the file-name alternative, resulting in

(identifier 

| ”ADDRESS” ”OF” identifier)

The adapted grammar accepts the same
language as before, but an ambiguity is
gone. Note that this approach is much sim-
pler than tweaking the parser and scanner to
deal with types of names. In this way, we re-
covered the entire VS Cobol II grammar and
tested it with all our Cobol code from earlier
software renovation projects and code from
colleagues who were curious about the proj-
ect’s outcome. For the final test, we used
about two million lines of pure VS Cobol II
code. As in the PLEX case, we generated a
fully Web-enabled version of both the cor-
rected BNF and the syntax diagrams that
could serve as the core for a complete and
correct language reference manual.

A part from PLEX and Cobol, wehave recovered several other gram-mars, as have others. From our ef-
forts in solving the 500LP, we learned two
interesting lessons. First, the more uncom-

N o v e m b e r / D e c e m b e r  2 0 0 1 I E E E  S O F T W A R E 87

The more
mainstream 

a language is,
the more likely

that you will
have direct
access to 

a reasonably
good, debugged

language
reference.



mon a language is, the more likely that you
will have direct access to the compiler’s
source code, an excellent starting place for
grammar recovery. Second, the more main-
stream a language is, the more likely that
you will have direct access to a reasonably
good, debugged language reference, also an
excellent source for grammar recovery.

Acknowledgments
Thanks to Terry Bollinger, Prem Devanbu, Capers

Jones, Tom McCabe, Harry Sneed, Ed Yourdon, and
the reviewers for their substantial contributions.

For more information on this or any other computing topic, please visit our
Digital Library at http://computer.org/publications/dlib.

References
1. C. Jones, Estimating Software Costs, McGraw-Hill,

New York, 1998.

2. C. Jones, The Year 2000 Software Problem: Quantify-
ing the Costs and Assessing the Consequences, Addi-
son-Wesley, Reading, Mass., 1998.

3. G.M. Weinberg, The Psychology of Computer Pro-
gramming, Van Nostrand Reinhold, New York, 1971.

4. P. de Jager, “You’ve Got To Be Kidding!” www.year2000.
com/archive/NFkidding.html (current 20 Sept. 2001).

5. R. Lämmel and C. Verhoef, “Semi-automatic Grammar
Recovery,” Software: Practice and Experience, vol. 31,
no. 15, Dec. 2001, pp. 1395–1438; www.cs.vu.nl/~x/
ge/ge.pdf (current 20 Sept. 2001).

6. P.T. Devanbu, “GENOA—A Customizable, Front-End
Retargetable Source Code Analysis Framework,” ACM
Trans. Software Eng. and Methodology, vol. 8, no. 2,
Apr. 1999, pp. 177–212. 

7. B. Hall, “Year 2000 Tools and Services,” Symp./ITxpo
96, The IT Revolution Continues: Managing Diversity
in the 21st Century, Gartner Group, Stamford, Conn.,
1996.

8. N. Jones, Year 2000 Market Overview, tech. report,
Gartner Group, Stamford, Conn., 1998.

9. M.G.J. van den Brand and E. Visser, “Generation of
Formatters for Context-Free Languages,” ACM Trans.
Software Eng. and Methodology, vol. 5, no. 1, Jan.
1996, pp. 1–41.

10. M.G.J. van den Brand, M.P.A. Sellink, and C. Verhoef,
“Generation of Components for Software Renovation
Factories from Context-Free Grammars,” Science of
Computer Programming, vol. 36, nos. 2–3, Mar. 2000,
pp. 209–266; www.cs.vu.nl/~x/scp/scp.html (current 20
Sept. 2001).

11. M.G.J. van den Brand, M.P.A. Sellink, and C. Verhoef,
“Current Parsing Techniques in Software Renovation
Considered Harmful,” Proc. 6th Int’l Workshop Pro-
gram Comprehension, IEEE CS Press, Los Alamitos,
Calif., 1998, pp. 108–117; www.cs.vu.nl/~x/ref/ref.html
(current 20 Sept. 2001).

12. D. Blasband, “Parsing in a Hostile World,” Proc. 8th
Working Conf. Reverse Eng., IEEE CS Press, Los
Alamitos, Calif., 2001, pp. 291–300.

13. J. Brunekreef and B. Diertens, “Towards a User-
Controlled Software Renovation Factory,” Proc. 3rd
European Conf. Maintenance and Reengineering, IEEE
CS Press, Los Alamitos, Calif., 1999, pp. 83–90.

14. L. Wall, T. Christiansen, and R.L. Schwartz, Program-
ming Perl, 2nd ed., O’Reilly & Associates, Cambridge,
Mass., 1996.

15. M.P.A. Sellink, H.M. Sneed, and C. Verhoef, “Restruc-
turing of Cobol/CICS Legacy Systems,” to be published
in Science of Computer Programming; www.cs.vu.nl/~x/
res/res.html (current 20 Sept. 2001).

16. M.P.A. Sellink and C. Verhoef, “Generation of Software
Renovation Factories from Compilers,” Proc. Int’l
Conf. Software Maintenance, IEEE CS Press, Los
Alamitos, Calif., 1999, pp. 245–255; www.cs.vu.nl/~x/
com/com.html (current 20 Sept. 2001).

17. M.P.A. Sellink and C. Verhoef, “Development, Assess-
ment, and Reengineering of Language Descriptions,”
Proc. 4th European Conf. Software Maintenance and
Reengineering, IEEE CS Press, Los Alamitos, Calif.,
2000, pp. 151–160; www.cs.vu.nl/~x/cale/cale.html
(current 20 Sept. 2001).

18. R. Lämmel and C. Verhoef, VS COBOL II Grammar
Version 1.0.3, 1999; www.cs.vu.nl/grammars/vs-cobol-ii
(current 20 Sept. 2001).

8 8 I E E E  S O F T W A R E N o v e m b e r / D e c e m b e r  2 0 0 1

About the Authors

Ralf Lämmel is a lecturer at the Free University of Amsterdam and is affiliated with the
Dutch Center for Mathematics and Computer Science (CWI). His research interests include pro-
gram transformation and programming languages. As a freelancer and consultant, he has de-
signed, implemented, and deployed developer tools, migration tools, and software develop-
ment application generators based on Cobol and relational databases. He received his PhD in
computer science from the University of Rostock, Germany. Contact him at the Free Univ. of
Amsterdam, De Boelelaan 1081-A, 1081 HV Amsterdam, Netherlands; ralf@cs.vu.nl;
www.cs.vu.nl/~ralf.

Chris Verhoef is a computer science professor at the Free University
of Amsterdam and principal external scientific advisor of the Deutsche Bank AG, New York. He
is also affiliated with Carnegie Mellon University’s Software Engineering Institute and has con-
sulted for hardware companies, telecommunications companies, financial enterprises, software
renovation companies, and large service providers. He is an elected Executive Board member
and vice chair of conferences of the IEEE Computer Society Technical Council on Software Engi-
neering and a distinguished speaker of the IEEE Computer Society. Contact him at the Free
Univ. of Amsterdam, Dept. of Mathematics and Computer Science, De Boelelaan 1081-A, 1081
HV Amsterdam, Netherlands; x@cs.vu.nl; www.cs.vu.nl/~x.


