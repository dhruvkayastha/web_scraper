***- IEEE Software (May_June). Volume 19, Number 3 (2002)***




































1 4 I E E E  S O F T W A R E M a y / J u n e  2 0 0 2 0 7 4 0 - 7 4 5 9 / 0 2 / $ 1 7 . 0 0  ©  2 0 0 2  I E E E

A
fter studying knowledge management
in recent years, wondering if it could
help me manage software projects
better, I’ve concluded that indeed it
can. To illustrate this point, I’ve used
Table 1 to list potential knowledge

management benefits and drawbacks orga-
nized around the classical functions software

managers perform. 
Most articles on this topic fo-

cus on strategic, not tactical goals,
describing potential benefits at the
40,000-foot level. But to deter-
mine where you and I as software
managers can use this technology
to deliver products on time and
within budget, the focus must be
closer to the 100-foot level.

You’ll note that the table lists a
“facilitating” function. That’s be-

cause project managers need to bring in the
right people at the right times to get the job
done. Facilitation in this sense is high on any
good manager’s priority list. For example, as
managers, you and I would need to coordi-
nate with our legal departments to have at-
torneys available to review each subcontract.

Now, let’s see how knowledge manage-
ment can make your life easier, too.

Knowledge management—hype or
reality?

After I constructed this table, I was skep-
tical: with these drawbacks, do managers
ever take advantage of this technology? So,
I talked to members of my peer network,
mostly software managers with profit-and-

loss responsibility in big organizations. It’s a
good thing I stayed open minded; most peo-
ple I contacted in organizations that had
mounted knowledge management initiatives
were ecstatic about the results. 

Here are several bits of what I heard:

� “We made a project knowledge reposi-
tory available to all players on our in-
tranet. Once we organized it into collec-
tions, useful information was available
immediately—right when we needed it.
Collaboration was fast and easy. Setting
up and maintaining the collections using
taxonomies that the corporation created
was relatively painless.”

� “We had a lot of data that we needed to
keep track of from old projects. Using ex-
isting knowledge management tools, we
could mine the data quickly by discover-
ing patterns and relationships that weren’t
immediately apparent.”

� “My firm created a knowledge base of in-
formation highlighting critical success
factors for project managers. When I first
viewed them, I said ‘so what?’ But when
I played with the system, I found it intu-
itive and useful. I could take messy data
from my project and make sense of it
once I played it through the system. I
could also reason with the system based
on heuristics derived from relationships.”

Not all the comments were this good. I also
heard the following:

� “Here we go again. Upper management

manager

A Little Bit of Knowledge
Is a Dangerous Thing
Donald J. Reifer

E d i t o r :  D o n a l d  J .  R e i f e r  � R e i f e r  C o n s u l t a n t s  � d . r e i f e r @ i e e e . o r g



M a y / J u n e  2 0 0 2 I E E E  S O F T W A R E 1 5

MANAGER

must have taken a semi-
nar. Another initiative—
more time wasted at the
worker level.”

� “While it sounds good, I
am too busy to look at
knowledge management.
I’ll play the wait-and-see
game and hope it blows
over soon.”

You can imagine some of
the rest. Give a manager a
chance to purge with a peer
and that’s exactly what hap-
pens. But, the comments made
me think twice about how I
could use this technology to
improve my ability to deliver.
My research into the use of
knowledge management had
panned out. Instead of joining
the naysayers, now I can think
of a dozen ways the tech-
nology can improve my per-
formance. 

Making it work
I’m also convinced that I’ll need

help. Not well versed in knowledge
management, my staff had no experi-
ence with the tools and techniques.
My contacts at large companies were
lucky. Their corporate initiatives
brought in experienced people who

developed the processes project man-
agers need to structure the knowl-
edge base of information serving as
their initiative’s foundation. These
experts also developed the training
and incentives needed to put the
technology into practice.

I am also skilled enough to know

that a little bit of knowledge
about knowledge manage-
ment can be dangerous. I’m
not too bashful to seek help
from those who have tackled
transfer of this technology,
building on their shoulders
rather than repeating their
mistakes. 

Y ou won’t get me to say thatknowledge management isa cure-all. But I will recom-
mend that you consider using
it—especially if your firm has
invested in it. 

All too often, I’ve seen
project after project get into
trouble because of miscom-
munication. People are just
too busy to tell each other
what they’re doing. Using
knowledge management, you
can make this information
readily available, especially if
your firm helps you put the
systems and procedures into

place to overcome the challenges as-
sociated with technology transfer. 

Keep an open mind, and check out
this technology. You might like it.

Don Reifer is president of Reifer Consultants and visiting
associate of the Center for Software Engineering at the Univer-
sity of Southern California. Contact him at d.reifer@ieee.org.

Table 1  

Project manager’s assessment of knowledge management technology
Management 
functions Potential upsides Possible limitations 
Project planning Captures organizational knowledge and makes lessons that are Hard to capture lessons learned.

often learned the hard way available for use in an organized and  Few incentives to use knowledge, especially when under pressure 
systematic manner. to get plans finalized.

Organizing Determines patterns of organization that work best based on  Your customer might require an organization that doesn’t adhere 
domain knowledge. to preferred patterns.

Staffing Makes it easy to identify people with the skills and knowledge An individual’s ability to fit into a team is something that only the 
required to fill open slots in the organizational structure. people he or she will work with can fully evaluate.

Directing Makes it easy to deduce leadership and teamwork abilities. Leadership and teamwork are qualities that need to be developed, 
Identifies and streamlines optimal communication patterns not inferred.
in the organization. While communications can be streamlined, breakdowns often 
Exploits lessons learned relative to past experience. occur due to people issues that must be anticipated to address.

Controlling Creates benchmarks to compare performance against, both Benchmarks might be misleading because they might be based 
corporate and competitive. on norms instead of specifics.

Facilitating Identifies linkages between work processes and highlights Good project managers can facilitate problem solving because 
techniques that improve efficiency and effectiveness. they anticipate problems and bring in experts to work on them.



1 6 I E E E  S O F T W A R E M a y / J u n e  2 0 0 2 0 7 4 0 - 7 4 5 9 / 0 2 / $ 1 7 . 0 0  ©  2 0 0 2  I E E E

F
ulfilling your customers’ interests de-
termines your market success, but
how do you find these requirements
effectively and efficiently? As simple
as this question sounds, answering it in
daily practice is difficult. Often, stake-

holders are interviewed about
their requirements or asked to
write them down, but this ap-
proach rarely uncovers the real
requirements that reflect a cus-
tomer’s true interests or needs.
We need a way of getting infor-
mation about the customers’
core desires—conscious, uncon-
scious, even subconscious. The
hottest sellers are products that
fulfill these desires.

Linguistics
Communication between people is never

easy, especially when it’s about requirements
for complex systems or products. To help
overcome communication problems, I rec-
ommend a method from the area of neu-
rolinguistic programming (NLP), which be-
longs to the field of psychotherapy. (For
further research into this topic, please see R.
Bandler and J. Grinder, The Structure of
Magic, Science and Behavior Books, 1990.)
This conversational method tries to decode
what a person means when speaking. 

According to NLP, mistakes in statements
can happen in two places: 

� perception of facts (reality)
� linguistic representation of one of these

perceptions (personal reality)

The goal of semantic language analysis is to
uncover formulations borne of subjective
experience and replace them with clear, ob-
jective formulations. 

Since 1995, our company, Sophist GmbH,
has applied language models such as the
Requirements Engineering (RE)-Metamodel
to areas of computer science. The result has
been simple, easy-to-use rules for prose re-
quirements in project development. Using
these adaptable rules, we can systematically
review requirements formulated in natural
language so that errors or ambiguities never
see the light of day.

The RE-Metamodel
Figure 1 shows the RE-Metamodel, sum-

marizing the most frequently observed lan-
guage phenomena linguistically. Each of
these phenomena indicates linguistic defects.
I will explain an example of each type of de-
fect and how you can recognize them in
your own work. 

Deletion
We all use the process of deletion to re-

duce the world to dimensions that we can
handle. In requirements engineering, how-
ever, we must know exactly what informa-
tion we’ve lost, or we run the risk of a mis-

requirements

Requirements and 
Psychology
Chris Rupp

E d i t o r :  S u z a n n e  R o b e r t s o n  � T h e  A t l a n t i c  S y s t e m s  G u i l d  � s u z a n n e @ s y s t e m s g u i l d . c o m

“Well, I thought you meant...” “No, that’s not what I said...” sounds familiar to any require-
ments engineer. Chris Rupp decided to borrow some ideas from the discipline of psychother-
apy to help pin down what people really mean. Here’s a good example of how we can use 
well-developed ideas from other disciplines to help improve our own. —Suzanne Robertson 



M a y / J u n e  2 0 0 2 I E E E  S O F T W A R E 1 7

REQUIREMENTS

understanding. One way to find
deleted information is to examine a
sentence’s process words, or verbs. 

To be complete, a process word
usually requires an argument or a
noun phrase. Consider the following:
“The system shall report data loss.”
The process word “report” is com-
pletely defined only if the following
questions are answered: Who reports
what to whom? What is reported?
When? How? 

For each requirement, you must be
sure that the process word in it is a sat-
isfactory and clear-without-ambiguity
definition of the actual process. Identify-
ing and roughly specifying the processes
that your system should carry out will
guide you in asking questions that clar-
ify meaning. If dynamics and complex
processing rules play a deciding role in
your system, pay special attention to
process words. They can lead you to a
clear definition of when and in what
condition your system performs its
processes.

Generalizations
In the generalization process, re-

quirements seem to apply to the sys-
tem as a whole, but in fact they usu-
ally apply to just a small piece of it.
Typical indicators of generalization
are universal quantifiers: parts of
statements broadly applied to all of
an occurrence’s incidences. 

Representatives of linguistic quan-
tifiers include concepts such as
“never,” “always,” “no,” “every,”
and “all.” The danger in using them
is that the specified behavior does
not always apply to all the referenced
objects in a group or set. Consider
the following: “Each signal shall be
labeled with a time stamp.” On the

basis of the keyword “each,” a ques-
tion immediately arises: Are there
one or more special cases in which
the time stamp is not required?

You probably won’t find many
universal quantifiers when searching
your text, which is not necessarily a
good sign. Inspect sentences that
don’t contain explicit statements
about the number of objects for
which the specified reaction occurs.
They often contain an implicit as-
sumption that the specified reaction
is valid for all relevant objects. 

Sometimes knowing how much
time and effort to spend on special
cases and exceptions is hard. It all
comes back to assessing the risk of not
asking the questions. In cases where
safety of life and limb is involved, you
can’t afford to miss exceptions.

Distortion
The problem of distortion appears

almost exclusively in the form of
nominalization, which occurs when a
process is reformulated into an event. 

A nominalization can change a
statement’s meaning and cause im-
portant information regarding a
process to be lost. Linguistically,
nominalization is a process word
(verb or predicate) molded into an
event word (noun or argument).
Consider the following: “After a sys-
tem breakdown, a restart shall auto-
matically be initiated.” The processes
behind the nouns “system,” “break-
down,” and “restart” actually con-
sist of a system breaking down and a
system restarting. But, how is the
restart being performed? Who initial-
izes it? What ends it?

Using nominalized terms for a
complex process is fine—if the process

is clearly defined. This definition
should not leave any margin for in-
terpretation of the process and
should clarify its progression as well
as all input and output parameters.
The goal is not necessarily to avoid
nominalizations but to use them only
if the process lying behind them is
clear. Nominalizations often appear
in domains with an extensive techni-
cal language, so check your field’s
terminology. Certainly you will iden-
tify most specialist terms as nominal-
izations that hide a large amount of
specialized knowledge.

Applying the metamodel
Many companies have success-

fully used the RE-Metamodel in var-
ious project areas: Deutsche
Flugsicherung, Swisscontrol, and Eu-
rocontrol (air traffic control sys-
tems); Deutsche Post (logistic sys-
tems); Kreditwerk and Bausparkasse
Schwäbisch Hall (societal systems);
and Stoll GmbH (knitting machines).

In all these different areas, the RE-
Metamodel helped systematize the
engineering of stakeholder needs.
Stakeholders used additional tem-
plates to formulate requirements,
which in turn clearly defined how to
finalize them (see www.sophist.de for
more information). Clear precondi-
tions on how to elicit and accurately
document requirements is especially
helpful in cases where they are not
noted in the stakeholders’ primary
language. Most analysts and stake-
holders love to receive precise work-
ing instructions because specifying a
new system can be a complex and
nonintuitive process.

You can apply the RE-Metamodel
in three ways:

Deletion
➯ Presuppositions
➯ Incomplete comparatives and superlatives
➯ Modal operators of possibility
➯ Modal operators of necessity
➯ Incompletely defined process words

Generalization
➯ Universal quantifiers
➯ Incompletely specified conditions
➯ Nouns without referential indices

Distortion
➯ Nominalizations

Figure 1. The RE-Metamodel, summarizing the most frequently observed linguistic defects.



1 8 I E E E  S O F T W A R E M a y / J u n e  2 0 0 2

REQUIREMENTS

� Dialogue. The analyst immedi-
ately checks each stakeholder
statement for linguistic effects and
studies missing facts that appear
to be important. Thus, interviews
quickly get down to the nitty
gritty, efficiently eliciting the re-
quired knowledge and promptly
revealing knowledge gaps. This
technique requires an experienced
analyst. The interviewee should
not notice that the analyst is sys-
tematically inspecting all state-
ments for linguistic effects. To do
this, the analyst must be experi-
enced enough to identify and ask
essential questions.

� Inspection of written require-
ments. Using linguistic rules, the
analyst inspects already existing
requirements for omissions, lack
of clarity, and so forth according
to the metamodel’s rules.

� Combination. Of course, you can
combine both approaches.

Ultimately, the analyst decides
how to use the RE-Metamodel,
adapting it to each particular situa-
tion on a case-by-case basis. With
some concessions, the RE-Metamodel
can help represent a complete, fault-
less set of requirements using the in-
formal notation of natural language.
Thus, the door is opened for the ana-
lyst and stakeholder to work to-
gether to review and criticize the
problem’s working description.

Usually, the NLP approach is im-
parted at a two- or three-day training
course. Afterward, you can success-
fully apply it immediately because
you don’t have to learn a new way of
thinking, you just have to explore
your knowledge of language. You’re
learning a model to make that knowl-
edge more accessible and some refine-
ments to help you apply it to RE.

For the first application of the
method after training, we recom-
mend using it to analyze already ex-
isting documents. On the basis of ex-
isting requirements, you can test each
statement for linguistic effects and
scrutinize it before interviewing
stakeholders. A few days of practice
will integrate the RE-Metamodel into

your thinking enough for you to ex-
amine and scrutinize your customers’
statements in real time. 

What makes a good analyst?
Knowing that perfect communica-

tions are nearly impossible relieves
you of trying to achieve a perfection
you never could reach. Managing the
incompleteness of communication is
core to mastering the RE process.
The point of developing approaches
such as the RE-Metamodel is to help
systems analysts be more effective in
doing a complex job. 

Good analysts mediate between
worlds. They investigate the stake-
holders’ needs while discovering, for-
mulating, and inventing requirements.
They then translate and transmit this
information to the world of system
development and informatics.

Analysts do much more than
translate stakeholders’ wishes into re-
quirements, however. Excellent com-
municative and therapeutic skills are
expected. In cases of diverging opin-
ions between stakeholders, analysts
must work out a common essence
and develop acceptable solutions for
all people involved. They must over-
come the stakeholders’ hindrances
and fears; every new system carries
the potential to create fear by scruti-
nizing previous positions and habits.
Analysts should have experience in
group dynamics, conflict manage-
ment, and team management apart
from sound IT and domain know-how.

Good systems analysts have a
complete suitcase of elicitation tech-
niques from various sectors. This
suitcase should contain therapeutic-
sector approaches (NLP, family ther-
apy) as well as creativity techniques
(brainstorming), behavioral science
methods (observation techniques),
and computer science approaches
(use-case-driven modeling, prototyp-
ing techniques). 

A good analyst’s working method
is never really predictable. He or she
starts the conversation with a stake-
holder and decides, based on the sit-
uation, which technique to use. Fac-
tors influencing that decision include

� Power and trust relationships
within the team

� Whether the demanded informa-
tion is lodged in the conscious, un-
conscious, or subconscious

� Whether the stakeholder is good
at explaining knowledge (prefers
to work with concrete examples
or abstract concepts)

� Whether the system is concerned
with dynamic–active procedures
or static information

The analyst should develop a flair
for using the right technique in the right
situation. A mixture of many different
techniques can bring revolutionary suc-
cess (see www.systemsguild.com).

T o be a successful requirements en-gineer, be prepared to always be ina learning situation, accept
change, and never do things by rote.
The variety of techniques, stakehold-
ers, and domains make the job of RE
an increasingly challenging and en-
joyable one. 

Chris Rupp is the managing director of Sophist GmbH and
Sophist Technologies GmbH. Contact her at Sophist Gesellschaft
für innovative Softwareentwicklung mbH, Vordere Cramergasse
11-13, 90478 Nuremberg, Germany; chris.rupp@sophist.de;
www.sophist.de.

Analysts do much more
than translate

stakeholders’ wishes
into requirements,
however. Excellent
communicative and
therapeutic skills 

are expected.



2 0 I E E E  S O F T W A R E M a y / J u n e  2 0 0 2 0 7 4 0 - 7 4 5 9 / 0 2 / $ 1 7 . 0 0  ©  2 0 0 2  I E E E

T
his is a troubling column to write. I
hadn’t planned to write on optimiza-
tion, because what I have to say has
already been said numerous times.
Yet, when I give talks, I find there’s
still a surprising number of people

who don’t know, or at least don’t follow, the
advice I’m about to give. So, here goes.
(Many of you have probably seen this ad-

vice before—my thought to you
is to ponder why I need to say
this again.)

First, performance matters. Al-
though relying on Moore’s law to
get us out of slow programs has
its merits, I find it increasingly an-
noying when I get a new version
of a program and must upgrade
my hardware for it to work ac-
ceptably. The question is, “How
do we achieve a fast program?”

For many programmers, performance is
something you pay continuous attention to
as you program. Every time you write a
fragment of code, you consider the perfor-
mance implications and code the program to
maximize performance. This is an obvious
technique—pity it works so badly. 

Performance is not something you can
work on in this way. It involves specific dis-
cipline. Some performance work comes
from architectural decisions, some from a
more tactical optimization activity. But
what both share is the fact that it is diffi-
cult to make decisions about performance
from just looking at the design. Rather, you
have to actually run the code and measure
performance.

Steps for optimization
Optimizing an existing program follows

a specific set of steps. First, you need a pro-
filer—a program that can analyze how
much time your program spends in its var-
ious parts. Software performance has an
80/20 rule: 80 percent of the program’s
time is spent on about 20 percent of the
code. Trying to optimize performance in
the 80 percent of code is futile, so the first
order of business is to find that 20 percent
of code. Trying to deduce where the pro-
gram will spend its time is also futile. I
know plenty of experienced programmers
who always get this wrong. You have to
use a profiler.

To give the profiler something to chew on,
perform some kind of automated run that
reasonably simulates the program under its
usual conditions. An automated test suite is a
good starting point, but make sure you simu-
late the actual conditions. My colleague Dave
Rice has a rule: “Never optimize a multiuser
system with single-user tests.” Experience has
taught us that a multiuser database system
has very different bottlenecks than a single
user system—often focused around transac-
tion interactions. The wrong set of tests can
easily lead you to the wrong 20 percent of
code.

Once you’ve found your bottlenecks,
you have two choices: speed up the slow
things or do the slow things less often. In ei-
ther case, you must change the software.
This is where having a well-designed piece
of software really helps. It’s much easier to
optimize cohesive, loosely coupled mod-
ules. Breaking down a system into many

design

Yet Another Optimization
Article
Martin Fowler

E d i t o r :  M a r t i n  F o w l e r  � T h o u g h t Wo r k s  � f o w l e r @ a c m . o r g



M a y / J u n e  2 0 0 2 I E E E  S O F T W A R E 2 1

DESIGN

small pieces lets you narrow down
the bottlenecks. Having a good auto-
mated test suite makes it easier to
spot bugs that might slip in during
optimization.

It’s worth knowing about various
optimization tricks, many of which
are particular to specific languages
and environments. The most impor-
tant thing to remember is that the
tricks are not guaranteed to work—as
the saying goes, “Measure twice, cut
once.” Unlike a tailor, however, you
measure before and after you’ve ap-
plied the optimization. Only then do
you know if it’s had any effect. It’s re-
vealing how often an optimization has
little—or even a negative—effect. If
you make an optimization and don’t
measure to confirm the performance
increase, all you know for certain is
that you’ve made your code harder to
read.

This double measurement is all the
more important these days. With opti-
mizing compilers and smart virtual
machines, many of the standard opti-
mizing techniques are not just ineffec-
tive but also counterintuitive. Craig
Larman really brought this home
when he told me about some com-
ments he received after a talk at
JavaOne about optimization in Java.
One builder of an optimizing virtual
machine said, in effect, “The com-
ments about thread pools were good,
but you shouldn’t use object pools be-
cause they will slow down our VM.”
Then another VM builder said, “The
comments about object pools were
good, but you shouldn’t use thread
pools because they slow down our
VM.” Not only does this reinforce the
need to measure with every optimiza-
tion change, it also suggests that you
should log every change made for op-
timization (a comment tag in the
source code is a good option) and
retest your optimizations after up-
grading your compiler or VM. The
optimization you did six months ago
could be your bottleneck today.

All of this reinforces the key rule
that first you need to make you pro-
gram clear, well factored, and nicely
modular. Only when you’ve done
that should you optimize. 

Some exceptions
Although most performance issues

are best dealt with in these kinds of
optimizations, at times other forms
of thinking are important—for ex-
ample, during early architectural
stages, when you’re making decisions
that will be costly to reverse later.
Again, the only way to really under-
stand these performance issues is to
use measurements. In this case, you
build exploratory prototypes to per-
form crude simulations of the envi-
ronments with which you’re going to
work and to get a sense of the rela-
tive speeds. It’s tricky, of course, to
get a good idea of what the actual en-
vironment might be, but then it’s
likely that everything you’re working
with will be upgraded before you go
live anyway. Experiments are still
much better than wild guesses.

There are also some cases where
there are broad rules about slow
things. An example I always come
across is the slowness of remote pro-
cedure calls. Because remote calls are
orders of magnitude slower than in-
process calls, it’s important to mini-
mize them, which greatly affects
overall design. However, this doesn’t
trump measuring. I once came across
a situation where people optimize re-
mote methods only to find their bot-
tlenecks were elsewhere. However,
minimizing remote calls has proven
to be a solid rule of thumb.

Some have taken this further, com-
ing up with performance models that
you can use to assess different archi-
tectural designs. Although this can be
handy, it’s difficult to take it too far.
It all depends on how good the per-
formance model is, and people usu-
ally cannot predict the key factors,
even at a broad level. Again, the only
real arbiter is measurement.

In the end, however, performance
is not an absolute. Getting a program
to run faster costs money, and it’s a
business decision whether to invest in
a quicker program. One value of an
explicit optimization phase is that
the cost of getting fast performance is
more explicit, so businesses can trade
it against time to market or more fea-
tures. Much of the reason why I
curse at slow software is because it
makes good business sense for the
builder. 

A s I’ve said, much of this advice—in particular, the advice to write agood clean program first and op-
timize it later—is well worn. (For a
longer description of this approach,
read Chapters 28 and 29 of Steve
McConnell’s Code Complete, Mi-
crosoft Press, 1993.) Good quotes
about the perils of premature opti-
mization have been around for over
20 years. The pity is that some peo-
ple still object when I call the same
query method twice in a routine. For
everyone who finds nothing new in
this column, there exists another
challenge—how to make it so there is
no need to rewrite it in 10 years. 

Martin Fowler is the chief scientist for ThoughtWorks, an
Internet systems delivery and consulting company. Contact him
at fowler@acm.org.

If you make an
optimization and 
don’t measure to 

confirm the performance
increase, all you know

for certain is that 
you’ve made your code

harder to read.



2 2 I E E E  S O F T W A R E M a y / J u n e  2 0 0 2 0 7 4 0 - 7 4 5 9 / 0 2 / $ 1 7 . 0 0  ©  2 0 0 2  I E E E

O
ne thing that makes unit-testing code
so hard is the way the real world
keeps intruding. If all we had to do
was code up tests for methods that
sort arrays or generate Fibonacci se-
ries, life would be easy. But in the real

world we have to test code that uses data-
bases, communications devices, user inter-
faces, and external applications. We might

have to interface to devices that aren’t yet
available or simulate network errors that are
impossible to generate locally. This all con-
spires to stop our unit tests from being neat,
self-contained (and orthogonal) chunks of
code. Instead, if we’re not careful, we find
ourselves writing tests that end up initializ-
ing nearly every system component just to
give the tests enough context to run. Not
only is this time consuming, it also intro-
duces a ridiculous amount of coupling into
the testing process: someone changes an in-
terface or a database table, and suddenly the
setup code for your poor little unit test dies
mysteriously. Even the best-intentioned de-
velopers become discouraged after this hap-
pens a few times. Eventually, testing starts to

drop off, and we all know where that leads.
Fortunately there’s a testing pattern that

can help. Using mock objects, you can test
code in splendid isolation, simulating all
those messy real-world things that would
otherwise make automated testing impossi-
ble. And, as with many other testing prac-
tices, the discipline of using mock objects
can improve your code’s structure.

An example: Testing a servlet
Servlets are chunks of code that a Web

server manages. Requests to certain URLs
are forwarded to a servlet container (or man-
ager) such as Jakarta Tomcat (http://jakarta.
apache.org/tomcat), which in turn invokes
the servlet code. The servlet then builds a re-
sponse that it sends back to the requesting
browser. From the end user’s perspective, it’s
just like accessing any other page.

Figure 1 shows part of the source of a
trivial servlet that converts temperatures
from Fahrenheit to Celsius. Let’s quickly
step through its operation. When the servlet
container receives the request, it automati-
cally invokes the servlet method doGet(),
passing in two parameters, a request and a
response. (These are important for our test-
ing later). The request parameter contains
information about the request; the servlet’s
job is to fill in the response. The servlet’s
body gets the contents of the field “Fahren-
heit” from the request, converts it to Cel-
sius, and writes the results back to the user.
The writing is done via a PrintWriter object,
which a factory method in the response ob-
ject provides. If an error occurs converting
the number (perhaps the user typed “boo!”

software construction

Mock Objects
Dave Thomas and Andy Hunt

E d i t o r s :  A n d y  H u n t  a n d  D a v e  T h o m a s  � T h e  P r a g m a t i c  P r o g r a m m e r s
a n d y @ p r a g m a t i c p r o g r a m m e r. c o m  � d a v e @ p r a g m a t i c p r o g r a m m e r. c o m

Yet sit and see; Minding true things by what their mockeries be. —Shakespeare, Henry V



M a y / J u n e  2 0 0 2 I E E E  S O F T W A R E 2 3

SOFTWARE CONSTRUCTION

into the form’s temperature field), we
catch the exception and report the er-
ror in the response.

Having written this code (or be-
fore writing it, for those in the Ex-
treme Programming tribe), we’ll want
a unit test to verify it. This is where
things start looking difficult. This
snippet of code runs in a fairly com-
plex environment (a Web server and a
servlet container), and it requires a
user sitting at a browser to interact
with it. This is hardly the basis of a
good automated unit test.

But let’s look at our servlet code
again. Its interface is pretty simple: as
we mentioned before, it receives two
parameters, a request and a response.
The request object must be able to
provide a reasonable string when its
getParameter() method is called,
and the response object must support
setContentType() and getWriter().
It’s starting to look as if we might be
able to write some stubs: objects that
pretend to be real request and response
objects but that contain just enough
logic to let us run our code. In principle
this is easy: both HttpServletRequest
and HttpServletResponse are inter-
faces, so all we have to do is whip up a
couple of classes that implement the in-
terfaces and we’re set. Unfortunately,
when we look at the interface, we dis-
cover that we’ll need to implement
dozens of methods just to get the thing
to compile. Fortunately, other folks
have already done the work for us.

Mock objects
Tim Mackinnon, Steve Freeman,

and Philip Craig introduced the con-
cept of mock objects in their paper
“Endo-Testing: Unit Testing with
Mock Objects” (www.cs.ualberta.ca/
~hoover/cmput401/XP-Notes/xp-conf/
Papers/4_4_MacKinnon.pdf), which
they presented at XP2000. Their idea
is a natural extension of the ad hoc
stubbing that testers have been doing
all along. The difference is that they
describe a framework to make writ-
ing mock objects and incorporating
them into unit testing easier.

Their paper lists seven good rea-
sons to use a mock object (para-
phrased slightly here):

� The real object has nondeterminis-
tic behavior.

� The real object is difficult to set up.
� The real object has behavior that

is hard to trigger (for example, a
network error).

� The real object is slow.
� The real object has (or is) a user

interface.
� The test needs to ask the real ob-

ject about how it was used (for ex-
ample, a test might need to check
to see that a callback function was
actually called).

� The real object does not yet exist.

Mackinnon, Freeman, and Craig
also developed the code for a mock ob-
ject framework for Java programmers
(available at www.mockobjects.com).
Let’s use that code to test our servlet.

The good news is that in addition
to the underlying framework code, the
mockobjects package comes with a
number of mocked-up application-level
objects. You’ll find mock output objects
(OutputStream, PrintStream, and
PrintWriter), objects that mock the
java.sql library, and classes for fak-
ing out a servlet environment. In par-
ticular, the package provides mocked-
up versions of HttpServletRequest
and HttpServletResponse, which by
an incredible coincidence are the types
of the parameters of the method we
want to test.

We can use mock objects in two dis-
tinct ways. First, we can use them to
set up an environment in which our
test code runs: we can initialize values
in the objects that the method under
test uses. Figure 2 shows a typical set
of tests using the JUnit testing frame-
work, which is available at www.junit.
org. We use a MockHttpServletRe-
quest object to set up the context in
which to run the test. On line six of the
code, we set the parameter “Fahren-
heit” to the value “boo!” in the re-
quest object. This is equivalent to the
user entering “boo!” in the corre-
sponding form field; our mock object
eliminates the need for human input
when the test runs.

Mock objects can also verify that
actions were taken. On line seven of
Figure 2, we tell the response object
that we expect the method under test
to set the response’s content type to
text/html. Then, on lines 9 and 22,
after the method under test has run,
we tell the response object to verify
that this happened. Here, the mock
object eliminates the need for a hu-
man to check the result visually. This
example shows a pretty trivial verifi-
cation: in reality, mock objects can
verify that fairly complex sequences
of actions have been performed.

Mock objects can also record the
data that was given to them. In our
case, the response object receives the

Figure 1. A trivial servlet that converts temperatures from 
Fahrenheit to Celsius.

1 public void doGet(HttpServletRequest req,
2 HttpServletResponse res)
3 throws ServletException, IOException
4 {
5 String str_f = req.getParameter(“Fahrenheit”);
6
7 res.setContentType(“text/html”);
8 PrintWriter out = res.getWriter();
9

10 try {
11 int temp_f = Integer.parseInt(str_f);
12 double temp_c = (temp_f – 32) * 5.0 / 9.0;
13 out.println(“Fahrenheit: “ + temp_f +

“, Celsius: “ + temp_c);
14 }
15 catch (NumberFormatException e) {
16 out.println(“Invalid temperature: “ + str_f);
17 }
18 }



2 4 I E E E  S O F T W A R E M a y / J u n e  2 0 0 2

SOFTWARE CONSTRUCTION

text that our servlet wants to dis-
play on the browser. We can query
this value (lines 10 and 23) to
check that we’re returning the text
we were expecting.

Mock objects
It’s likely that we’ve all been using
mock objects for years without
knowing it. However, it’s also likely
that we’ve used them only on an ad

hoc basis, coding up stubs when we
needed them. However, we
personally have recently started
benefiting from adopting a more
systematic approach to creating
mock objects. Even things as simple
as consistent naming schemes have
helped make our tests more readable
and the mock objects themselves more
portable from project to project.

There are several mock object frame-
works to choose from. Three for Java
are at www.c2.com/cgi/wiki?MockOb-
ject, and a fine implementation for Ruby
is at www.b13media.com/dev/ruby/
mock.html. If the thought of writing
all the mock object classes you might
need is intimidating, look at Easy-
Mock (www.easymock.org), a con-
venient Java API for creating mock
objects dynamically. All these imple-
mentations are a starting point; you’ll
probably need to add new mock object
implementations to stub out real ob-
jects in your environment.

There are also alternatives to
mock objects in the servlet environ-
ment. In particular, the Jakarta Cac-
tus system (http://jakarta.apache.org/
cactus) is a heavier-weight frame-
work for testing server-side compo-
nents. Compared to the mock-objects
approach, Cactus runs your tests in
the actual target environment and
tends to produce less fine-grained
tests. Depending on your needs, this
might or might not be a good thing.

A funny thing happens when youstart using mock objects. As withother low-level testing practices,
you might find that your code be-
comes not only better tested but also
better designed and easier to under-
stand (Nat Pryce discusses this in the
sidebar). Mock objects won’t solve
all your development problems, but
they are exceptionally sharp tools to
have in your toolbox.

Dave Thomas and Andy Hunt are partners in The
Pragmatic Programmers, LLC. They feel that software consultants
who can’t program shouldn’t be consulting, so they keep current
by developing complex software systems for their clients. They
also offer training in modern development techniques to program-
mers and their management. They are coauthors of The Pragmatic
Programmer and Programming Ruby, both from Addison-Wesley.
Contact them via www.pragmaticprogrammer.com.

Figure 2. Mock objects in action—a typical set of tests using the 
JUnit testing framework.

1 public void test_bad_parameter() throws Exception {

2 TemperatureServlet s = new TemperatureServlet();

3 MockHttpServletRequest request = 

new MockHttpServletRequest();

4 MockHttpServletResponse response = 

new MockHttpServletResponse();

5

6 request.setupAddParameter(“Fahrenheit”, “boo!”);

7 response.setExpectedContentType(“text/html”);

8 s.doGet(request, response);

9 response.verify();

10 assertEquals(“Invalid temperature: boo!\r\n”, 

11 response.getOutputStreamContents());

12 }

13

14 public void test_boil() throws Exception {

15 TemperatureServlet s = new TemperatureServlet();

16 MockHttpServletRequest request = 

new MockHttpServletRequest();

17 MockHttpServletResponse response = 

new MockHttpServletResponse();

18

19 request.setupAddParameter(“Fahrenheit”, “212”);

20 response.setExpectedContentType(“text/html”);

21 s.doGet(request, response);

22 response.verify();

23 assertEquals(“Fahrenheit: 212, Celsius: 100.0\r\n”, 

24 response.getOutputStreamContents());

25 }

Practical Experience with Mock Objects
Nat Pryce

My experience is that using mock objects and a test-first methodology forces
you to think about design differently from traditional object-oriented design tech-
niques. First, you end up with many small, decoupled classes that are used
through composition, rather than inheritance. These classes implement interfaces
more than they inherit state and behavior. Second, you think about object inter-
faces in terms of the services that an object both provides and requires from its
environment, making an object’s requirements explicit. This is different from 
traditional OO design methods that concentrate only on an object’s provided 
services and try to hide an object’s requirements through encapsulation of pri-
vate-member variables. Third, you end up thinking more explicitly in terms of
interobject protocols. Those protocols are often defined using interface definitions
and tested using mock objects before a concrete implementation is produced.

Nat Pryce is the technical director at B13media Ltd. Contact him at nat.pryce@b13media.com; www.b13media.com.



2 6 I E E E  S O F T W A R E M a y / J u n e  2 0 0 2 0 7 4 0 - 7 4 5 9 / 0 2 / $ 1 7 . 0 0  ©  2 0 0 2  I E E E

sustaining the level of competence needed to
win contracts and fulfill undertakings.

Knowledge management is an emerging
discipline that promises to capitalize on or-
ganizations’ intellectual capital. The concept
of taming knowledge and putting it to work is
not new; phrases containing the word knowl-
edge, such as knowledge bases and knowledge
engineering, existed before KM became popu-
larized. The artificial intelligence community
has long dealt with knowledge representation,
storage, and application. Software engineers
have engaged in KM-related activities aimed
at learning, capturing, and reusing experience,
even though they were not using the phrase
“knowledge management.” KM is unique be-
cause it focuses on the individual as an expert
and as the bearer of important knowledge that
he or she can systematically share with an or-
ganization. KM supports not only the know-
how of a company, but also the know-where,

know-who, know-what, know-when, and
know-why.

The KM concept emerged in the mid-
1980s from the need to derive knowledge
from the “deluge of information” and was
mainly used as a “business world” term. In
the 1990s, many industries adopted the term
KM in connection with commercial com-
puter technologies, facilitated by develop-
ment in areas such as the Internet, group sup-
port systems, search engines, portals, data
and knowledge warehouses, and the applica-
tion of statistical analysis and AI techniques. 

KM implementation and use has rapidly
increased since the 1990s: 80 percent of the
largest global corporations now have KM
projects.1 Over 40 percent of Fortune 1000
companies now have a chief knowledge of-
ficer, a senior-level executive who creates an
infrastructure and cultural environment for
knowledge sharing.2

focus
Knowledge Management 
in Software Engineering

Ioana Rus and Mikael Lindvall, Fraunhofer Center for Experimental 
Software Engineering, Maryland 

Knowledge
management is an
expanding and
promising discipline
that has drawn
considerable
attention. Recent
developments in
computer and IT
technologies enable
sharing documented
knowledge
independently of
time and location.
The guest editors
provide an overview
of these
developments to
introduce the issue.

S
oftware organizations’ main assets are not plants, buildings, or
expensive machines. A software organization’s main asset is its in-
tellectual capital, as it is in sectors such as consulting, law, invest-
ment banking, and advertising. The major problem with intellec-

tual capital is that it has legs and walks home every day. At the same rate
experience walks out the door, inexperience walks in the door. Whether or
not many software organizations admit it, they face the challenge of 

guest editors’ introduction



But will KM really help organizations ad-
dress the problems they face while trying to
achieve their business objectives? Or is it
just one of those hyped concepts that rise
quickly, ambitiously claim to cure organiza-
tional headaches, and then fail and fall qui-
etly? KM has drawn widespread attention
and has promising features, but it is too
soon to say whether or not it will stay.

However, we can address other questions:
What kind of problems can KM help solve
for software organizations? What are its
challenges? What are the success factors?
How can KM leverage all the knowledge that
exists in software organizations? This special
issue of IEEE Software addresses these types
of questions.

Motivation for KM in software
engineering

Software development is a quickly
changing, knowledge-intensive business in-
volving many people working in different
phases and activities. The available re-
sources are not increasing along with the in-
creasing needs; therefore, software organi-
zations expect a rise in productivity.
Knowledge in software engineering is di-
verse and its proportions immense and
steadily growing. Organizations have prob-
lems identifying the content, location, and
use of the knowledge. An improved use of
this knowledge is the basic motivation and
driver for KM in software engineering and
deserves deeper analysis.

Business needs
Organizations can apply KM to provide

solutions to pressing business issues.

Decreasing time and cost and increasing
quality. Organizations constantly need to
decrease software projects’ development
time and costs. Avoiding mistakes reduces
rework; repeating successful processes in-
creases productivity and the likelihood of
further success. So, organizations need to
apply process knowledge gained in previ-
ous projects to future projects. Unfortu-
nately, the reality is that development teams
do not benefit from existing experience and
they repeat mistakes even though some in-
dividuals in the organization know how to
avoid them. Project team members acquire
valuable individual experience with each

project—the organization and individuals
could gain much more if they could share
this knowledge.

Making better decisions. In software de-
velopment, every person involved con-
stantly makes technical or managerial deci-
sions. Most of the time, team members
make decisions based on personal knowl-
edge and experience or knowledge gained
using informal contacts. This is feasible in
small organizations, but as organizations
grow and handle a larger volume of infor-
mation, this process becomes inefficient.
Large organizations cannot rely on informal
sharing of employees’ personal knowledge.
Individual knowledge must be shared and
leveraged at project and organization levels.
Organizations need to define processes for
sharing knowledge so that employees
throughout the organization can make cor-
rect decisions.

Knowledge needs
Software organizations have vast amounts

of knowledge in various areas, such as
knowledge that is critical to achieve business
goals. We now look at some of these knowl-
edge areas and organizations’ related needs.

Acquiring knowledge about new technolo-
gies. The emergence of new technologies
makes software development more efficient,
but at the same time, they can be a project
manager’s worst nightmare. It is difficult for
developers to become proficient with a new
technology and managers to understand its
impact and estimate a project’s cost when us-
ing it. When developers or project managers
use a technology that a project’s team mem-
bers are unfamiliar with, engineers often resort
to the “learning by doing” approach, which
can result in serious delays. So, organizations
must quickly acquire knowledge about new
technologies and master them.

Accessing domain knowledge. Software
development requires access to knowledge
not only about its domain and new tech-
nologies but also about the domain for
which software is being developed. An or-
ganization must acquire new domain
knowledge either by training or by hiring
knowledgeable employees and spreading it
throughout the team.

M a y / J u n e  2 0 0 2 I E E E  S O F T W A R E 2 7

Is KM just one
of those hyped
concepts that
rise quickly,
ambitiously

claim to cure
organizational

headaches, 
and then fail

and fall quietly?



Sharing knowledge about local policies and
practices. Every organization has its own poli-
cies, practices, and culture, which are not only
technical but also managerial and administra-
tive. New developers in an organization need
knowledge about the existing software base
and local programming conventions. Unfor-
tunately, such knowledge typically exists as
organizational folklore. Experienced develop-
ers often disseminate it to inexperienced de-
velopers through ad hoc informal meetings;
consequently, not everyone has access to the
knowledge they need. Passing knowledge in-
formally is an important aspect of a knowl-
edge-sharing culture that should be encour-
aged. Nonetheless, formal knowledge
capturing and sharing ensures that all em-
ployees can access it. So, organizations must
formalize knowledge sharing while continu-
ing informal knowledge sharing.

Capturing knowledge and knowing who
knows what. Software organizations depend
heavily on knowledgeable employees be-
cause they are key to the project’s success.
However, accessing these people can be dif-
ficult. Software developers apply just as
much effort and attention determining
whom to contact in an organization as they
do getting the job done.3 These knowledge-
able people are also very mobile. When a
person with critical knowledge suddenly
leaves an organization, it creates severe
knowledge gaps—but probably no one in
the organization is even aware of what
knowledge they lost. Knowing what employ-
ees know is necessary for organizations to
create a strategy for preventing valuable
knowledge from disappearing. Knowing
who has what knowledge is also a require-
ment for efficiently staffing projects, identi-
fying training needs, and matching employ-
ees with training offers.

Collaborating and sharing knowledge. Soft-
ware development is a group activity. Group
members are often geographically scattered
and work in different time zones. Nonetheless,
they must communicate, collaborate, and co-
ordinate. Communication in software engi-
neering is often related to knowledge transfer.
Collaboration is related to mutual sharing of
knowledge. Group members can coordinate
independently of time and space if they can
easily access their work artifacts. So, group

members need a way to collaborate and share
knowledge independently of time and space.

KM background
Before we go further into how KM can

address software engineering needs, let us
introduce some key KM concepts.

Knowledge levels
The three levels of refinement to knowl-

edge items are data, information, and knowl-
edge. Data consists of discrete, objective facts
about events but nothing about its own im-
portance or relevance; it is raw material for
creating information. Information is data
that is organized to make it useful for end
users who perform tasks and make decisions.
Knowledge is broader than data and infor-
mation and requires understanding of infor-
mation. It is not only contained in informa-
tion, but also in the relationships among
information items, their classification, and
metadata (information about information,
such as who has created the information).
Experience is applied knowledge. 

Knowledge characteristics
KM deals not only with explicit knowl-

edge, which is generally easier to handle, but
also tacit knowledge. Explicit knowledge, also
known as codified knowledge, is expressed
knowledge. It corresponds to the information
and skills that employees can easily communi-
cate and document, such as processes, tem-
plates, and data. Tacit knowledge is personal
knowledge that employees gain through expe-
rience; this can be hard to express and is
largely influenced by their beliefs, perspectives,
and values. The knowledge scope refers to
where the knowledge is applicable, to whom it
is accessible, and whose activity it supports.
Along this dimension, knowledge can be at an
individual, group, organization, multiple or-
ganization, or industry-wide level.

The knowledge evolution cycle defines the
phases of organizational knowledge.4 Orga-
nizations should have a knowledge manage-
ment strategy in place for implementing these
phases systematically. The phases are

1. Originate/create knowledge. Members of
an organization develop knowledge
through learning, problem solving, inno-
vation, creativity, and importation from
outside sources. 

Passing
knowledge

informally is 
an important
aspect of a
knowledge-

sharing culture
that should be
encouraged.

2 8 I E E E  S O F T W A R E M a y / J u n e  2 0 0 2



2. Capture/acquire knowledge. Members 
acquire and capture information about
knowledge in explicit forms. 

3. Transform/organize knowledge. Organi-
zations organize, transform, or include
knowledge in written material and knowl-
edge bases. 

4. Deploy/access knowledge. Organizations
distribute knowledge through education,
training programs, automated knowl-
edge-based systems, or expert networks. 

5. Apply knowledge. The organization’s ulti-
mate goal is applying the knowledge—this
is the most important part of the life cycle.
KM aims to make knowledge available
whenever it is needed. 

Knowledge management
An organization’s intellectual capital con-

sists of tangible and intangible assets. Tangible
assets, which correspond to documented, ex-
plicit knowledge, can vary for different 
industries and applications but usually include
manuals; directories; correspondence with
(and information about) clients, vendors, 
and subcontractors; competitor intelligence;
patents; licenses; and knowledge derived from
work processes (such as proposals and project
artifacts). Intangible assets, which correspond
to tacit and undocumented explicit knowl-
edge, consist of skills, experience, and knowl-
edge of an organization’s people. 

Learning and KM
Learning is a fundamental part of KM 

because employees must internalize (learn)
shared knowledge before they can use it to
perform specific tasks. Individuals primarily
learn from each other, by doing, and through
self study. Knowledge also spreads from in-
dividuals to groups, and throughout organi-
zations and industries. KM aims to elevate
individual knowledge to the organizational
level by capturing and sharing individual
knowledge and turning it into knowledge
the organization can access. Individuals
eventually perform tasks to achieve organi-
zational-level goals. Therefore, the iterative
knowledge processing and learning activities
at the individual level are of utmost impor-
tance. As Peter M. Senge says, “Organiza-
tions learn only through individuals who
learn. Individual learning does not guarantee
organizational learning. But without it no
organizational learning occurs.”5

KM’s role in software engineering
In software development, different ap-

proaches have been proposed to reduce
project costs, shorten schedules, and in-
crease quality. These approaches address
factors such as process improvement, intro-
ducing new technologies, and improving
people’s performance (“peopleware”). KM
addresses mainly peopleware. Because soft-
ware development is a human and knowl-
edge-intensive creative activity, KM ac-
knowledges the importance of individuals
having access to the correct information and
knowledge when they need to complete a
task or make a decision.

KM does not ignore the value or need to
address other software development as-
pects, such as process and technology, nor
does it seek to replace them. Instead, it
works toward software process improve-
ment by explicitly and systematically ad-
dressing the management of organizational
knowledge, such as its acquisition, storage,
organization, evolution, and effective ac-
cess. Other software process improvement
approaches, such as the Capability Maturity
Model, might suggest that knowledge
should be managed but do not explicitly
state what knowledge needs to be managed
and how, when, where, or by and for
whom. KM ties together daily production
activities, improvement initiatives, and busi-
ness goals, thereby supporting the establish-
ment of a learning organization.

Organizations can view KM as a risk pre-
vention and mitigation strategy, because it
explicitly addresses risks that are too often
ignored, such as 

� Loss of knowledge due to attrition 
� Lack of knowledge and an overly long

time to acquire it due to steep learning
curves 

� People repeating mistakes and perform-
ing rework because they forgot what
they learned from previous projects 

� Individuals who own key knowledge be-
coming unavailable

Software engineering involves several
knowledge types—technical, managerial,
domain, corporate, product, and project
knowledge. Knowledge can be transferred
through formal training or through learning
by doing. Formal training is often time con-

M a y / J u n e  2 0 0 2 I E E E  S O F T W A R E 2 9

Organizations
can view KM 

as a risk
prevention and

mitigation
strategy,
because it
explicitly

addresses
risks that are

too often
ignored.



suming and expensive, and if done exter-
nally does not cover local knowledge.
Learning by doing can be risky because peo-
ple continue to make mistakes until they get
it right. KM does not replace organized
training, but supports it. Documented
knowledge can provide the basis for internal
training courses based on knowledge pack-
aged as training material. However, KM
mainly supports learning by doing. It pro-
vides knowledge or pointers to people who
have the knowledge, when and where it is
needed.

KM does not come for free; it requires ef-
fort and resources. In KM systems that or-
ganizations have implemented so far (see
the Experience Factory sidebar and this is-
sue’s feature articles), people other than de-
velopers often perform KM activities (such
as a chief knowledge officer and his staff, an
Experience Factory group, or a software

process improvement group). This supports
developers in their daily work instead of
loading them with extra effort.

Software engineering’s core task is devel-
oping software. Documents (such as con-
tracts, project plans, and requirements and
design specifications) are also produced dur-
ing software development. These documents
capture knowledge that emerged from solv-
ing the project’s problems. Team members
can then reuse this knowledge for subsequent
projects, for example, by analyzing accepted
solutions to different problems. If individuals
own knowledge that is not explicitly cap-
tured, the organization can leverage that
knowledge only if it can identify and access
these individuals. 

Organizations wishing to improve a
team’s software engineering capabilities can
conduct the task of ensuring that knowledge
gained during the project is not lost. They

3 0 I E E E  S O F T W A R E M a y / J u n e  2 0 0 2

Victor R. Basili
Carolyn Seaman 

The basis for the Experience Factory
Organization1 concept is that software
development projects can improve their
performance (in terms of cost, quality,
and schedule) by leveraging experience
from previous projects. The
concept also takes into account
the reality that managing this
experience is not trivial and
cannot be left to individual
projects. With deadlines, high
expectations for quality and
productivity, and challenging
technical issues, most develop-
ment projects cannot devote
the necessary resources to
making their experience avail-
able for reuse. 

The EFO solves this prob-
lem by separating these re-
sponsibilities into two distinct
organizations. We call these
organizations the Project Or-
ganization, which uses pack-
aged experience to deliver
software products, and the 

Experience Factory, which supports soft-
ware development by providing tailored
experience. Figure A depicts the EFO,
which assumes separate logical or phys-
ical organizations with different priori-
ties, work processes, and expertise 
requirements.

The Experience Factory analyzes and

synthesizes all experience types, includ-
ing lessons learned, project data, and
technology reports, and provides reposi-
tory services for this experience. The Ex-
perience Factory employs several meth-
ods to package the experience, including
designing measures of various software
process and product characteristics and

then building models of these
characteristics that describe
their behavior in different con-
texts. These models’ data come
from development projects via
people, documents, and auto-
mated support.

When using EFO, not only
must the organization add an-
other suborganization for
learning, packaging, and stor-
ing experience, but it also must
change the way it does its
work. An organization adopt-
ing the EFO approach must
believe that exploiting prior
experience is the best way to
solve problems and ensure that
the development process incor-
porates seeking and using this
experience. The EFO also as-

The Experience Factory Organization

Plan

Do

Project
support

Project
organization

Project
plan

Experience Factory Organization
Experience factory

Needs

Packaged
experience,
consulting

Package
(synthesize)Experience 

base

Raw experience

Packaged
experience

Feedback
Analyze

Figure A. Flow of information through the 
Experience Factory Organization.



can conduct this task during the project and
shortly after they complete it. It addresses
both acquiring knowledge that was not
documented as part of the core activities
and analyzing documents to create new
knowledge. Included in this task are all
forms of lessons learned and postmortem
analyses that identify what went right or
wrong regarding both the software product
and process.

These activities also include project
data analyses, such as comparisons of esti-
mated and actual costs and effort, planned
and actual calendar time, or analysis of
change history to reflect project events.
These tasks  collect and create knowledge
about a particular project; any organiza-
tion can perform them. Although these
activities’ results are useful by themselves,
they can also be the basis for further knowl-
edge creation and learning. They can be

stored in repositories and experience bases.
At a higher level, organizations and indus-

tries must analyze multiple past projects to
improve their software developing abilities.
This requires extensive knowledge based on
many different software development experi-
ences, as well as insights into analysis and
synthesis of new knowledge. Patterns, heuris-
tics, best practices, estimation models, and
industry-wide standards and recommenda-
tions are examples of outcomes from these
knowledge-processing activities.

We group KM activities that support soft-
ware development into three categories: by the
purpose of their outputs (supporting core SE
activities, project improvement, or organiza-
tional improvement), the scope of their inputs
(documents or data from one or multiple proj-
ects), and the effort level required to process
inputs to serve SE needs. We use this classifi-
cation to describe how both existing and new

M a y / J u n e  2 0 0 2 I E E E  S O F T W A R E 3 1

sumes that the activities of the Experi-
ence Factory and those of the Project
Organization are integrated. That is, the
activities by which the Experience Fac-
tory extracts experience and then pro-
vides it to projects are well integrated
into the activities by which the Project
Organization performs its function. Fig-
ure A represents this interaction and ex-
change of experience.

Making experience available and us-
able is crucial but is not the essence of
an EFO. “Experience” in an Experience
Factory is not only the raw information
reported directly from projects. It also
includes the valuable results of the
analysis and synthesis of that local ex-
perience, such as “new” knowledge
generated from experience. But the new
knowledge is based on applying previ-
ous experience on real projects, not on
analysis in a vacuum. 

Thus, an EFO must

� Package experience by analyzing,
synthesizing, and evaluating raw
experience and build models that
represent abstractions of that expe-
rience

� Maintain an experience base or
repository of data, experience,
models, and other forms of knowl-
edge and experience

� Support projects in identifying and
using the appropriate experiences
for the situation 

Victor Basili first presented the EFO
concept in a keynote address at COMP-
SAC in 1989.2 This was before the term
“knowledge management” became pop-
ular, but the EFO addresses many of the
same concerns. This learning-organiza-
tion concept evolved from our experi-
ences in the NASA Software Engineering
Laboratory, a joint effort of the University
of Maryland, Computer Sciences Corpo-
ration, and the NASA Goddard Space
Flight Center. The SEL’s high-level goal
was to improve Goddard’s software
processes and products.

The application of EFO ideas resulted
in a continuous improvement in software
quality and cost reduction during the SEL’s
quarter-century lifespan.3 Measured over
three baseline periods in 1987, 1991,
and 1995 (each baseline was calculated
based on about three years’ worth of

data), demonstrated improvements in-
cluded decreases in development defect
rates of 75 percent between the 1987
and 1991 baselines and 37 percent be-
tween the 1991 and 1995 baselines. We
also observed reduced development costs
between subsequent baselines of 55 per-
cent and 42 percent, respectively. 

References
1. V.R. Basili and G. Caldiera, “Improve Soft-

ware Quality by Reusing Knowledge and Ex-
perience,” Sloan Management Rev., vol. 37,
no. 1, Fall 1995, pp. 55–64.

2. V.R. Basili, “Software Development: A Para-
digm for the Future,” Proc. 13th Int’l Com-
puter Software and Applications Conf.
(COMPSAC 89) IEEE CS Press, Los Alamitos,
Calif., 1989, pp. 471–485. 

3. V.R. Basili et al., “Special Report: SEL’s Soft-
ware Process-Improvement Program,” IEEE
Software, vol. 12, no. 6, Nov./Dec. 1995,
pp. 83–87. 

Victor R. Basili’s biography appears on page 49.

Carolyn Seaman is an assistant professor of Information
Systems at the University of Maryland, Baltimore County and a
research scientist at the Fraunhofer Center for Experimental
Software Engineering, Maryland. Contact her at cseaman@
umbc.edu.



software engineering processes and tools can
fit into a KM strategy. In the remainder of this
section, we also discuss resources external to
the organization that are built around the no-
tion of capturing and sharing knowledge. 

Supporting core SE activities 
Document management, competence

management, and software reuse are knowl-
edge management activities that support
software development.

Document management. A software devel-
opment project involves a variety of docu-
ment-driven processes and activities. The
work frequently focuses on authoring, re-
viewing, editing, and using these documents,
which become the organization’s assets in
capturing explicit knowledge. Therefore, doc-
ument management is a basic activity toward
supporting an organization’s implementation
of a knowledge management system. DM sys-
tems enable employees throughout the organ-
ization to share documented knowledge.
Many commercial tools support DM, such as
Hyperwave, Microsoft Sharepoint, Lotus
Domino, and Xerox DocuShare (see “The
Experience Factory Organization” sidebar),
and include features such as defining process
workflows and finding experts. 

Competence management and expert identi-
fication. Far from all of an organization’s tacit
knowledge can be made explicit, and far from
all explicit knowledge can be documented. So,
an organization must track who knows what
to fully utilize undocumented knowledge. An
elaborate solution to this problem is compe-
tence management, or skills management. As
we noted earlier, a common problem that
knowledge management addresses is expert
identification. Competence management sys-
tems, such as SkillScape and SkillView, include
tools that let experts generate and edit their
own profiles. Other tools, such as Knowledge-
Mail, automatically generate competence
profiles by assuming that peoples’ emails
and documents reflect their expertise. These
tools analyze email repositories and docu-
ments and build keyword-based profiles
that characterize each employee. Software
organizations can use them to identify ex-
perts in various technical areas, such as spe-
cific programming languages, database
technologies, or operating systems.

Software reuse. There are endless stories
about how programmers continuously im-
plement the same solutions in slightly dif-
ferent ways. Software reuse approaches at-
tempt to reduce this rework by establishing
a reuse repository. Programmers submit
software they believe would be useful to
others. The software development process
must change so that instead of developing
all software from scratch, a programmer
first searches the repository for reusable
parts. Only if the programmer found noth-
ing useful would he or she write the soft-
ware from scratch. This same concept can
apply to all software engineering artifacts. 

Supporting product and project memory 
Learning from experience requires a

product and project memory. The environ-
ment in which software engineers conduct
their daily work often supports creating
such a memory. Version control, change
management, documenting design deci-
sions, and requirements traceability are
software engineering practices that help
build such memories as a direct or side ef-
fect of software development.

Version control systems, such as the Source
Code Control System, represent a class of
tools that indirectly create a project memory.
Each version of a document has a record at-
tached with information about who made the
change, as well as when and why they made
it. This “memory” indicates the software’s
evolution. Software engineers use this infor-
mation for advanced analysis of software
products and processes.6 They can also use
the who-changed-it information to identify
experts. Design Rationale is an example of an
approach that explicitly captures software de-
sign decisions to create a product memory
and help avoid repeating mistakes.7 This is
important because during design, engineers
test different technical solutions and make de-
cisions on the basis of these test results. Un-
fortunately, these decisions are rarely cap-
tured, making it difficult for anyone else to
understand the reasons behind the solutions.

Software requirements drive the develop-
ment of software systems, but the connection
between the final system and its requirements
is fuzzy. Traceability is an approach that ex-
plicitly connects the requirements and the fi-
nal software system. It indirectly contributes
to the product memory and helps answer

A common
problem that
knowledge

management
addresses 
is expert

identification. 

3 2 I E E E  S O F T W A R E M a y / J u n e  2 0 0 2



questions such as “What requirements led to
a particular piece of source code?” and
“What code did the engineers develop to sat-
isfy this particular requirement?”

Supporting learning and improvement  
Project managers need to make a series of

decisions at the beginning of and during proj-
ects. Typically, their personal experience and
“gut feeling” guide their decisions. But be-
cause software development is such a com-
plex and diverse process, gut feelings might
not be sufficient, and not all managers have
extensive experience. For these reasons, pre-
dictive models can guide decision making for
future projects based on past projects. This
requires having a metrics program in place,
collecting project data with a well-defined
goal in a metrics repository, and then analyz-
ing and processing the data to generate pre-
dictive models. Managers or process im-
provement personnel analyze, synthesize,
and process input data using different meth-
ods, depending on the model’s purpose and
the input and output type. 

Analytical models have as input data
from a large number of projects (either nu-
merical data or qualitative data converted
into quantitative levels). Using these formu-
las for the data that characterize a new proj-
ect, project managers can estimate cost, ef-
fort, defects, reliability, and other product
and project parameters. Building, using, and
improving these models become a natural
part of the KM strategy.

Another class of predictive models that
captures the development process structure
and the internal-process variable relation-
ships consists of executable behavioral
process models and simulators based on sys-
tem dynamics8 and discrete event modeling.
Engineers and managers can execute these
models, simulate what-if scenarios, and an-
alyze possible outcomes for multiple deci-
sions. These models capture knowledge that
addresses process structure and behavior.
The prediction quality these models offer
depends on the collected data quality.

Project information can also be in a qual-
itative form (such as cases and lessons
learned, success and failure stories, and
problems and corresponding solutions) rep-
resented in formats such as rules, indexed
cases, or semantic networks. Applying in-
duction, generalization, and abstraction on

this knowledge can generate new knowl-
edge (manually, or automatically by apply-
ing AI techniques) applicable to similar fu-
ture problems in similar contexts. This is
how patterns, best-practice guidelines,
handbooks, and standards are derived. The
models transform raw point data into ex-
plicit and more applicable knowledge. 

For example, case based-systems capture
project experiences to accommodate soft-
ware development process diversity while
retaining a level of discipline and standards.
These experiences provide developers with
knowledge of an organization’s previous de-
velopment issues. Deviations from the stan-
dard process are opportunities to improve
the process itself. Deviations also work as
cases in the experience base. As organiza-
tions acquire more experience in the form of
cases, the development process is iterated
and becomes even more refined. 

Implementing KM 
Implementing KM involves many chal-

lenges and obstacles. Three issues are par-
ticularly important:1

� Technology issues. Software technology
supports KM, but it is not always possi-

M a y / J u n e  2 0 0 2 I E E E  S O F T W A R E 3 3

ACM Special Interest Groups: www.acm.org/sigs/guide98.html 
CeBASE: www.cebase.org 
Carnegie Mellon’s Software Process Improvement Network:

www.sei.cmu.edu/collaborating/spins
Expert Exchange: www.experts-exchange.com 
Fraunhofer Center for Experimental Software Engineering, Maryland:

http://fc-md.umd.edu
Hyperwave: www.hyperwave.com 
IBM’s Lotus: www.lotus.com/home.nsf/welcome/km
KnowledegeMail: www.knowledgemail.com
Microsoft Product Support Services:

http://search.support.microsoft.com/kb
Microsoft Sharepoint Technologies: www.microsoft.com/sharepoint
Oracle Support Services: www.oracle.com/support/index.

html?content.html
Perl’s FAQ: www.perl.com/pub/q/faqs
Skillscape: www.skillscape.com
Skillview: www.skillview.com
Software Engineering Body of Knowledge: www.swebok.org
Software Program Managers Network: www.spmn.com
Sun’s Java community: http://developer.java.sun.com/

developer/community 
ViSEK at Fraunhofer Center for Experimental Software Engineering,

Germany: www.iese.fhg.de/Projects/ViSEK
Xerox Docushare: http://docushare.xerox.com

Useful URLs



ble to integrate all the different subsys-
tems and tools to achieve the planned
level of sharing. Security is a require-
ment that the available technology does
not often provide satisfactorily. 

� Organizational issues. It is a mistake for
organizations to focus only on technology
and not on methodology. It is easy to fall
into the technology trap and devote all re-
sources to technology development, with-
out planning for KM implementation. 

� Individual issues. Employees often do
not have time to input or search for
knowledge, do not want to give away
their knowledge, and do not want to
reuse someone else’s knowledge.

An analysis of KM failures reveals that
many organizations who failed did not de-
termine their goals and strategy before im-
plementing KM systems. In fact, 50 to 60
percent of KM deployments failed because
organizations did not have a good KM de-
ployment methodology or process, if any.1

Some organizations ended up managing
documents instead of meaningful knowl-
edge. This is an easy mistake to make, be-
cause many tools advertised as KM tools
address document management rather than
knowledge management. 

Lightweight KM approaches
A problem with KM is that it might take

time before measurable benefits appear. It
generally takes a long time before knowledge
bases contain a critical mass of knowledge.
However, organizations can quickly and eas-
ily implement lightweight KM approaches—
this can pay off quickly while laying the foun-
dation for long-term goals. One such tool is
the Knowledge Dust Collector,9 which sup-
ports peer-to-peer knowledge sharing by cap-
turing knowledge that employees exchange
and use every day. The knowledge “dust”
evolves over time into knowledge “pearls,” a
refined knowledge form. The Knowledge
Dust Collector captures dialogs regarding
technical problems (knowledge dust), ana-
lyzes them, and turned them into frequently
asked questions (knowledge pearls). These
FAQs are then analyzed and turned into best
practices (extended knowledge pearls).

Organizational culture
Although new technology makes sharing

knowledge easier than ever, organizational
cultures might not promote it. Some cultures
even overly encourage individualism and ban
cooperative work. Lack of a “knowledge cul-
ture” has been cited as the number one ob-
stacle to successful KM.4 If organizations
don’t foster a sharing culture, employees
might feel possessive about their knowledge
and won’t be forthcoming in sharing it. Em-
ployees know that the organization values
them because of their knowledge; they might
fear that they will be considered redundant
and disposable as soon as the employer has
captured their knowledge. Employees might
not be willing to share negative experiences
and lessons learned based on failures because
of their negative connotation. So although
KM’s purpose is to avoid similar mistakes,
employees might fear that such information
could be used against them. Another hurdle
is the “not invented here” syndrome—some
believe that software engineers are reluctant
to reuse other people’s solutions. Although
change is hard, such beliefs must be revisited
and replaced by a positive attitude that en-
genders and rewards sharing. 

KM champions
Hewlett-Packard advocates using an

evangelist or champion for any KM initia-
tive—someone who encourages employees
to contribute and use the system and who is
always its proponent.10

Reward systems
Organizations must not only encourage

but also reward employees who share their
knowledge, search for knowledge, and use
others’ knowledge. To encourage sharing and
reusing knowledge, Xerox recommends cre-
ating a “hall of fame” for those people whose
contributions have solved real business prob-
lems. Xerox rewards staff that regularly
share useful information and identifies them
as key contributors to the program. Bruce
Karney, evangelist of a Hewlett-Packard KM
initiative, gave out free Lotus Notes licenses
and free airline miles to prospective users.10

Infosys rewards employee contribution and
use of knowledge with “knowledge currency
units,” which they can convert into cash. The
online expertise provider ExpertExchange re-
wards experts with points for answering
questions and recognizes those with the most
points on the front page of their Web site.

An analysis of
KM failures

reveals 
that many

organizations
who failed did
not determine
their goals and
strategy before
implementing
KM systems.

3 4 I E E E  S O F T W A R E M a y / J u n e  2 0 0 2



Leveraging employees’ expertise 

Most approaches that support experience
reuse and knowledge management assume
that all relevant experience can be collected
and recorded, but in practice, this does not
hold true. A variety of KM solutions ad-
dress different aspects and tasks of this
problem.

For example, Ericsson Software Technol-
ogy implemented a version of the Experi-
ence Factory (see the related sidebar) called
the Experience Engine.11 Instead of relying
on experience stored in experience bases,
the Experience Engine relies on tacit knowl-
edge. Ericsson created two roles to make the
tacit knowledge accessible to a larger group
of employees. The experience communica-
tor is an employee who has in-depth knowl-
edge on one or more topics. The experience
broker connects the experience communica-
tor with the employee who owns the prob-
lem. The communicator should not solve
the problem but instead educate the prob-
lem’s owner on how to solve it. The German
organization sd&m has successfully imple-
mented a similar approach.12 Relying on
tacit rather than explicit knowledge is ap-
pealing because it relaxes the requirement to
extensively document knowledge. However,
although this approach uses the knowledge,
it still does not solve the problem of an or-
ganization being dependent on its employ-
ees and their tacit knowledge.

The benefit of explicit knowledge or ex-
perience is that it can be stored, organized,
and disseminated to a third party without
the originator’s involvement. The drawback
is that organizations spend considerable ef-
fort to produce explicit knowledge. Some
development practices, such as pair pro-
gramming, facilitate knowledge sharing be-
tween peers, while job rotation helps
knowledge spread throughout the project or
organization. Software organizations
should encourage these habits to create a
knowledge-sharing culture. To achieve the
maximum benefit from knowledge sharing,
organizations should encourage employees
to document and store their knowledge in a
KM repository whenever they help another
employee. In doing so, they ensure that the
information they passed on is recorded and
will help other employees, because what is a
problem for one can also be a problem for
many.

The most used form of knowledge shar-
ing probably occurs in communities in
which members can come together and
share knowledge and experience. Commu-
nities are popular because they are relatively
easy to form. Software organizations have
formed numerous useful communities.
Some examples include the Software Pro-
gram Managers Network; the Software Ex-
perience Consortium, for companies seek-
ing to share experience; Sun’s community
for Java programmers, the Software Process
Improvement Network; and special interest
groups of the IEEE and ACM. 

Leveraging organizational expertise
Organizations strive to learn more and

leverage their expertise through input that
comes from outside the organization.

Sharing experience with customers. Organi-
zations learn not only from their own experi-
ences but also from external sources, typically
technology vendors. Several software vendors
provide online knowledge bases, such as Mi-
crosoft’s Knowledge Base, Oracle’s Support
Center, and Perl’s FAQ. Such knowledge bases
are often open to the public and let software
engineers search for knowledge. These knowl-
edge bases resulted from capturing vendor
representatives’ product knowledge and mak-
ing it available to the vendors’ customers.

Industry-wide knowledge sharing and edu-
cation. At the software industry level, com-
mittees or groups of experts identify patterns
(such as software design patterns) and gener-
ate handbooks and standards (such as those
from the IEEE and ISO) that are generally
applicable to software development, to lever-
age the experience and knowledge of all soft-
ware development organizations. This is not
something any individual or organization can
perform, because it takes much effort and re-
quires a considerable amount of SE knowl-
edge and access to project data.

An example of the numerous industry-
wide knowledge initiatives is the Software
Engineering Body of Knowledge (SWE-
BOK). It  defines the knowledge that a prac-
ticing software engineer needs to master on
a daily basis. Another example is ISO15504,
a comprehensive collection of software engi-
neering knowledge that describes processes
related to SE.

M a y / J u n e  2 0 0 2 I E E E  S O F T W A R E 3 5

The benefit 
of explicit

knowledge or
experience is

that it can 
be stored,

organized, and
disseminated to

a third party
without the
originator’s
involvement.



Projects whose goal is to build knowl-
edge bases include the Center for Empiri-
cally Based Software Engineering and
ViSEK (Virtual SE Competence Center).
They accumulate empirical models to pro-
vide validated guidelines for selecting tech-
niques and models, recommend areas for re-
search, and support SE education.

KM challenges
As we noted earlier, implementing KM is

challenging because many resources and
much time and effort are required before
benefits become visible. Project managers
who feel they need to focus on completing
their current project on time, and not on
helping the next project manager succeed,
often consider this a burden. 

Another obstacle is that most SE knowl-
edge is not explicit. Organizations have lit-
tle time to make knowledge explicit. Addi-
tionally, there are few approaches and tools
that turn tacit into explicit knowledge.
Technology’s fast pace often discourages
software engineers from analyzing the
knowledge they gained during the project,
believing that sharing the knowledge in the
future will not be useful.

Opportunities 
Despite the challenges we presented earlier,

there are good reasons to believe that KM for
software engineering can succeed if organiza-
tions appropriately focus and implement it. A
major argument for KM is that software or-
ganizations should already have much of the
appropriate information technology in place
to support KM systems. IT might be intimi-
dating to many people, but not to software en-
gineers. Instead, we expect that engineers will
benefit even more from advanced technology.
Additionally, all artifacts are already in elec-
tronic form and thus can easily be distributed
and shared. In fact, software engineers already
share knowledge to a large degree in some en-
vironments. A good example is Google, whose
user community of software engineers share
knowledge by answering questions and help-
ing solve problems that other software engi-
neers post, without being compensated. This
shows that software engineers are willing to
share their knowledge, even outside their com-
pany, and that capturing gained knowledge is
worth the effort even though technology
changes quickly.

State of the practice
This special issue of IEEE Software in-

vestigates KM’s state of the practice in soft-
ware engineering. The large number of sub-
missions we received (over 40 papers
presenting SE applications of KM) shows
the interest for this topic throughout the
software community. The selected articles
report on the needs, implementations, is-
sues, results, success factors, and lessons
learned from a variety of KM applications. 

These articles cover a diverse ground; they
come from the US, Europe (Germany, Nor-
way, and Finland), and Asia (India and Tai-
wan) and from commercial and government
organizations developing either software for
diverse domains (satellites, cars, electronics,
telecommunications, and the Internet) or in-
tegrated circuits. The authors are practition-
ers, consultants, researchers, and KM offi-
cers. The articles address different levels and
scopes of KM activities, from project-level
knowledge (captured by postmortem analy-
sis or by gathering knowledge on demand),
to organization-wide initiatives. They exam-
ine various techniques through case studies,
from local analysis to traceability, to complex
and highly automated knowledge and experi-
ence repositories.

Jay Liebowitz’s “A Look at NASA God-
dard Space Flight Center’s Knowledge Man-
agement Initiatives” describes the creation of
a NASA knowledge management team, a KM
officer position, and a series of KM initiatives
at NASA Goddard Space Flight Center. They
learned that KM should start small and see
what works in a specific environment and
that knowledge should be collected during
projects, not after their completion. They also
learned to capture individual knowledge
through interviews before people leave the
organization and to embed KM processes in
daily activities.

Andreas Birk, Torgeir Dingsøyr, and Tor
Stålhane’s “Postmortem: Never Leave a
Project without It” addresses the problem
that the individual knowledge that engi-
neers gain during projects is not reused or
shared between teams. Birk and his col-
leagues’ solution was to use systematic post-
mortem analysis for capturing and reusing
experience and improvement suggestions.
Having an open atmosphere for the project
team to discuss issues was a key success fac-
tor. As a result, teams experienced increased

There are good
reasons to

believe that KM
for software
engineering 

can succeed if
organizations
appropriately

focus and
implement it.

3 6 I E E E  S O F T W A R E M a y / J u n e  2 0 0 2



understanding and experience sharing, iden-
tification of potential improvement needs,
and increased job satisfaction. 

Software development and acquisition
competencies are a scarce resource in many
companies, and DaimlerChrysler is no excep-
tion. To cope with this, DaimlerChrysler re-
searchers set up a Software Experience Center
project. SEC explicitly reuses experience from
previous software projects using a customized
Experience Factory approach (see related
sidebar). Kurt Schneider, Jan-Peter von Hun-
nius, and Victor R. Basili’s “Experience in Im-
plementing a Learning Software Organiza-
tion” presents the most important challenges
they faced: learning implies change—and
change is not easy. Learning involves risks
that must be mitigated, and experience reuse
requires packaging, which is not trivial. Suc-
cess factors included understanding that
change takes time and that experience must
be packaged and tailored for the organiza-
tion. Thanks to the SEC project, learning has
become part of developers’ daily routine,
leading to process improvements.

From studying best practices in 30 system
development organizations, Balusubrama-
niam Ramesh observed the need to link
knowledge fragments (which were spread
across the organization) to facilitate success-
ful knowledge transfer and reuse. In “Process
Knowledge Management with Traceability,”
he proposes creating, storing, retrieving,
transferring, and applying knowledge by us-
ing traceability (creating and maintaining re-
lationships between objects and people in
software development). This is possible
where development of facilities that support
KM processes (traceability schemes and
tools) exist. This approach’s benefits are en-
hanced collaboration and communication,
new knowledge creation, knowledge reten-
tion, and increased knowledge availability
and access for software developers.

In Shivram Ramasubramanian and Goku-
lakrishnan Jagadeesan’s article, “Knowledge
Management at Infosys,” Infosys, an Indian
consultancy and software services company,
realized that locating information and knowl-
edge needed for project activities took too
long. Senior management therefore launched
a company-wide initiative for building and
maintaining a KM infrastructure based on
central repositories of documents and expert-
ise maps. They introduced an incentive sys-

tem that converted “knowledge units” into
cash to encourage use of and contributions to
these repositories. The initiative resulted in re-
duced defects, increased productivity, de-
creased cost (mainly by reducing mistakes
and rework), and better teamwork.

The problem of retaining, reusing, and
transferring knowledge occurs not only in SE
but also in other knowledge-intensive do-
mains. Borrowing solutions and experience
from these other domains can benefit SE.
Chin-Ping Wei, Paul Jen-Hwa Hu, and
Hung-Huang Chen’s “Design and Evaluation
of a Knowledge Management System” re-
ports on KM implementation in an inte-
grated-circuit assembly and testing organiza-
tion. The company implemented a system
that supports expert identification and col-
laboration for when engineers need to solve a
specific problem. Once a problem is solved,
its history and solution are stored in a codifi-
cation-based knowledge repository for later
reuse. This system leads to ease of locating
experts and sharing knowledge, increases
productivity, reduces production interrup-
tions caused by lack of knowledge and search
for knowledge, and reduces delayed re-
sponses to customer inquiries or complaints.

Seija Komi-Sirviö, Annukka Mäntyniemi,
and Veikko Seppänen in “Toward a Practical
Solution for Capturing Knowledge for Soft-
ware Projects,” emphasize the need for ad-
dressing local needs, problems, and specific
context for KM initiative implementation.
At an organization that develops software-
intensive electronic products, KM activities
were not as efficient as they expected. They
replaced their current large scale, technol-
ogy-centered KM approach with a project
need-based approach to knowledge collec-
tion and delivery. This proved successful,
leading to project staff satisfaction.

N ot all KM initiatives presented here(maybe none of them) address all as-pects of KM. This is normal because
KM has not matured yet, especially in SE.
When an organization implements such a
program, it has to start small. It needs to iden-
tify which specific problems and priorities it
should address and what does or does not
work for that organization. The results must
show benefits relatively quickly and convince

M a y / J u n e  2 0 0 2 I E E E  S O F T W A R E 3 7

Recent
developments
in IT definitely
enable sharing

documented
knowledge

independent of
time and space.



both the employees to use the system and the
management to support it (by continuous in-
vestment in the necessary resources).

It is too early to say whether knowledge
management in software engineering will
survive, but it is clear that software organiza-
tions struggle to retain valuable knowledge
and they still depend on knowledge pos-
sessed by their employees. From the enthusi-
astic response to this special issue, reflected
both by the large number of high quality ar-
ticles submitted (many of them presenting
implementation of KM in industry) and by
the impressive number of volunteering re-
viewers, we infer that KM is of interest to
many software professionals and researchers
and is being tested by many companies as a
potential solution to these problems. 

Recent developments in IT definitely en-
able sharing documented knowledge inde-
pendent of time and space. We foresee that
in the future there will also be support for
capturing and disseminating knowledge in
various formats, enabling organizations and
individuals to share knowledge on a world-
wide scale. 

Acknowledgments
We thank Tom McGibbon and David Nicholls

from Data Analysis Center for Software, who sup-
ported our work when writing two knowledge man-
agement state-of-the-art reports on which we based
this tutorial.14,15 We thank Seija Komi-Sirvio, Patricia
Costa, Scott Henninger, Raimund Feldman, Forrest
Shull, and Rose Pajerski for reviewing; Kathleen Dan-
gle and Tricia Larsen for useful discussions on KM
and software process improvement; and Jennifer Dix
for proofreading. Many thanks go to the IEEE Soft-
ware editorial board who anticipated the benefit of
this special issue, and to the reviewers of the articles
who kindly offered their help for improving the qual-
ity of the published material.

References
1. G. Lawton, “Knowledge Management: Ready for Prime

Time?” Computer, vol. 34, no. 2, Feb. 2001, pp.
12–14.

2. D.E. O’Leary, “Enterprise Knowledge Management,”
Computer, vol. 31, no. 3, Mar. 1998, pp. 54–61.

3. D.E. Perry, N. Staudenmayer, and L. Votta, “People,
Organizations, and Process Improvement,” IEEE Soft-
ware, vol. 11, no. 4, July/Aug. 1994, pp. 36–45.

4. W. Agresti, “Knowledge Management,” Advances in
Computers, vol. 53, Academic Press, London, 2000, pp.
171–283.

5. P.M. Senge, The Fifth Discipline: The Art and Practice
of the Learning Organization, Currency Doubleday,
New York, 1990, p. 139.

6. S.G. Eick et al., “Does Code Decay? Assessing the Evi-
dence from Change Management Data,” Trans. Soft-
ware Eng., vol. 27, no. 1, Jan. 2000, pp. 1–12.

7. C. Potts and G. Bruns, “Recording the Reasons for De-
sign Decisions,” Proc. 10th Int’l Conf. Software Eng.
(ICSE 88), IEEE CS Press, Los Alamitos, Calif., 1988,
pp. 418–427.

8. T. Abdel-Hamid and S.E. Madnick, Software Project
Dynamics: An Integrated Approach, Prentice-Hall, 
Englewood Cliffs, N.J., 1991. 

9. M. Lindvall et al., “An Analysis of Three Experience
Bases,” Proc. Workshop on Learning Software Organi-
zations. (LSO 01), Springer-Verlag, Hiedelberg, Ger-
many, pp. 106–119.

10. T. Davenport, Knowledge Management Case Study,
Knowledge Management at Hewlett-Packard, 2002,
www.bus.utexas.edu/kman/hpcase.htm. 

11. C. Johansson and P.C.M. Hall, “Talk to Paula and Pe-
ter—They Are Experienced,” Proc. Workshop on
Learning Software Organizations (LSO 99), Springer-
Verlag, Hiedelberg, Germany, 1999, pp. 171–185. 

12. P. Brössler, “Knowledge Management at a Software En-
gineering Company—An Experience Report,” Proc.
Workshop Learning Software Organizations (LSO 99),
Springer-Verlag, Hiedelberg, Germany, 1999, pp.
163–170. 

13. T. Davenport, Knowledge Management Case Study,
Knowledge Management at Microsoft, 2002,
www.bus.utexas.edu/kman/microsoft.htm. 

14. M. Lindvall, Software Tools for Knowledge Manage-
ment, tech. report, DoD Data Analysis Center for Soft-
ware, Rome, N.Y., 2001.

15. I. Rus, M. Lindvall, and S. Sinha, Knowledge Manage-
ment in Software Engineering, tech. report, DoD Data
Analysis Center for Software, Rome, N.Y., 2001. 

3 8 I E E E  S O F T W A R E M a y / J u n e  2 0 0 2

About the Authors

Ioana Rus is a scientist at Fraunhofer Center for Empirical Software Engineering, Mary-
land. Her research interests include experience and knowledge management, software process
modeling and simulation, process improvement, measurement, and empirical studies in soft-
ware development. She has a PhD in computer science and Engineering and is a member of
the IEEE Computer Society and ACM. Contact her at irus@fc-md.umd.edu. 

Mikael Lindvall is a scientist at Fraunhofer Center for Experimental Software Engineer-
ing, Maryland. He specializes in work on experience and knowledge management in software
engineering. He is currently working on ways of building experience bases to attract users to
both contribute and use experience bases. He received his PhD in computer science from
Linköpings University, Sweden. His PhD work was based on a commercial development project
at Ericsson Radio and focused on the evolution of object-oriented systems. Contact him at
mlindvall@fc-md.umd.edu.



4 0 I E E E  S O F T W A R E M a y / J u n e  2 0 0 2 0 7 4 0 - 7 4 5 9 / 0 2 / $ 1 7 . 0 0  ©  2 0 0 2  I E E E

management and knowledge-sharing initia-
tives within organizations. For example,
NASA formed a Knowledge Management
Team, comprised of NASA representatives.

Government agencies are turning to
knowledge management as they face down-
sizing and the retirement of half their work-
force in the next few years.1 The US Navy
has spent US$30 billion to transform itself
into a knowledge-centric organization.1 The
General Services Administration (GSA) sees
knowledge management as the foundation
for electronic government.1 The Federal Avi-
ation Administration has embraced knowl-
edge management and finds that communi-
ties are the primary resource for transfer of
tacit knowledge to explicit knowledge.1 The
Social Security Administration feels that
subject matter experts are beginning to
share best practices readily through its SSA
intranet, PRIDE (Project Resource guIDE).1

Many of these organizations are reaping
benefits from their efforts, including retain-
ing the expertise of personnel, increasing
customer satisfaction, supporting e-govern-
ment initiatives, and increasing innovation.
Knowledge management offers the public
sector and nonprofits ways to improve prac-
tices and processes, enhance employee capa-
bilities, improve customer service, and de-
crease costs.

Knowledge management initiatives
NASA Goddard Space Flight Center

(GSFC) has several knowledge management
initiatives under way on the expert and
knowledge retention side, including

� An innovative best practices and lessons-
learned CD, based on an ongoing satel-
lite mission called MAP, developed in co-
operation with NASA’s Academy for

focus
A Look at NASA Goddard
Space Flight Center’s
Knowledge Management
Initiatives

Jay Liebowitz, NASA Goddard Space Flight Center

Knowledge
management offers
the public sector and
nonprofits ways to
improve practices
and processes,
enhance employee
capabilities, improve
customer service,
and decrease costs.
NASA Goddard Space
Flight Center has
several knowledge
management
initiatives underway.

A
s we move from the information age to the knowledge era,
knowledge is being treated as a key asset. Organizations try to
maintain their competitive edge by sharing knowledge internally
with their employees and externally with their customers and

stakeholders. NASA and others are realizing that to maximize their organi-
zations’ potential, they must fully leverage their knowledge or intellectual
capital.1 To accomplish this task, an emerging trend is to develop knowledge

knowledge management



Program and Project Leadership
� Narrative case studies on GSFC’s suc-

cessful and problematic missions
� Launching a knowledge preservation pi-

lot project to capture stories of GSFC
personnel in the program and project
management and systems engineering
areas via online Web-searchable video

� Enhancing the NASA Lessons Learned
Information System (see http://llis.nasa.
gov) including a “push” capability to
send appropriate lessons-learned to indi-
viduals based on completed user profiles
and designing the next-generation LLIS 

� Conducting knowledge sharing forums
in the project management, engineering,
and scientific areas

� Exploring learning and knowledge shar-
ing proficiencies

� Building an expertise locator (myEx-
perts) via the GSFC intranet portal

� Building codification and Web access to
explicit knowledge via a historical data-
base of documents relating to various
GSFC programs and projects

� Spawning online communities at GSFC
through the Process-Based Mission Assur-
ance (PBMA) Knowledge Management
System (see http://pbma.hq.nasa.gov)

� Capturing knowledge via exit interview
packets and tapping the knowledge of the
GSFC Retirees and Alumni Association

� Establishing “Creative Learning Groups”
at GSFC

� Discussing a Knowledge Recognition
Day for GSFC 

In addition to these initiatives, and to
provide knowledge stewardship roles,
GSFC has formed the Knowledge Manage-
ment Working Group, chaired by the
Knowledge Management Officer and com-
prised of representatives from each direc-
torate, human resources, library, public af-
fairs, and the CIO office. 

One knowledge management project that
has direct implications for GSFC software
development activities involves the Knowl-
edge Preservation Pilot Project. This project
captures video nuggets of key GSFC experts
who “tell” their critical stories and lessons-
learned involving the systems engineering
area (including software development) and
program and project management. These
videos are made available over the Web

(currently, these video nuggets are captured
in the PBMA Web resource). We are still ex-
ploring the best video indexing, ingesting,
and search and retrieval software (for ex-
ample, Virage, Convera, and so on) for the
online Web searchable video component.
We expect to share valuable lessons-learned
to avoid reinventing the wheel and to
heighten the potential for future project
success. Additionally, we are developing
metrics to help determine the value-added
benefits of using knowledge management. 

Measuring the success of knowledge
management initiatives is an important part
of GSFC’s knowledge management imple-
mentation plan. We are considering metrics
from the software engineering and intellec-
tual asset management areas, including in-
novation, productivity, people retention,
knowledge retention, and return on vision
(“results and mission success” in NASA lan-
guage) measures. Quantifying intangible as-
sets is difficult, but hopefully the knowledge
management and software engineering com-
munities can collaborate and address this
critical area.

Lessons learned
According to Rosina Weber and her col-

leagues’ study,2 most lessons-learned pro-
grams are ineffective. This is partly because
of reliance on a “passive” information col-
lection, analysis, and dissemination approach
versus an “active” one. The enhanced ver-
sion of the NASA LLIS that GSFC operates
includes a “push” feature based on user
profiles that disseminates appropriate lessons

M a y / J u n e  2 0 0 2 I E E E  S O F T W A R E 4 1

� T. Davenport and J. Beck, The Attention Economy, HBS Press, Cam-
bridge, Mass., 2001.

� J. Liebowitz (ed.), The Knowledge Management Handbook, CRC Press,
Boca Raton, Fla., 1999.

� J. Liebowitz and T. Beckman, Knowledge Organizations: What Every
Manager Should Know, St. Lucie Press, Boca Raton, Fla., 1998.

� J. Liebowitz, Building Organizational Intelligence: A Knowledge
Management Primer, CRC Press, Boca Raton, Fla., 2000.

� B. Montano et al., “SMARTVision Methodology: A Methodology for
Knowledge Management,” J. Knowledge Management, vol. 5, no. 4,
2001.

� M. Rumizen, The Complete Idiot’s Guide to Knowledge Management,
PWS Publishing, Boston, Mass., 2002.

Selected Bibliography



(matching the user’s profile) to individuals
who can benefit from these lessons. Artifi-
cial intelligence technology, via intelligent
agents, could improve the utility of lessons-
learned programs, for example, by provid-
ing customized search capabilities.

Various lessons-learned can be gleaned
from knowledge management efforts. Try a
lessons-learned pilot before scaling up to an
enterprise-wide solution. You should care-
fully consider people, culture, technology,
communities of practice, costs, and per-
ceived power shift issues. Consider having a
lessons-learned component before a project
is finally finished. Specifically, enter a set of
lessons-learned from a given project into
your organization’s lessons-learned reposi-
tory before the project is officially com-
pleted. Conduct exit interviews to glean a
set of lessons-learned from the individual
before he or she leaves the organization. For
a lessons-learned program about knowledge
management to work well, in general, in-
centives must be strengthened to reuse
knowledge. Tying the contribution and ap-
plication of knowledge to performance and
review criteria (for example, merit pay)
might help encourage a knowledge-sharing
environment. For substantial projects, doc-
ument and encode lessons-learned as stan-
dard practice. Include the name of the indi-
vidual associated with lessons and tips to
give him or her greater recognition, as done
in the Eureka system at Xerox. American
Management Systems publishes Best
Knews, a newsletter listing the most fre-
quently used nuggets of knowledge from
their lessons-learned repository. Systemati-
cally rotating jobs provides vocational stim-
ulation and awareness of the entire system
process, helping in knowledge reuse and un-
derstanding. Lastly, having a senior-level
management official spearhead these initia-
tives is useful for developing a knowledge

management team across the various busi-
ness units as a steering committee.

From a technical viewpoint, we have also
learned some valuable lessons, including

� You need an integrated knowledge man-
agement architecture to cut across the
functional silos.

� Knowledge management processes must
be embedded within the daily activities
of the employee for knowledge manage-
ment to be successful.

� Continue research to advance the state
of the art in knowledge management.

Some organizations still might not be
ready for knowledge management because
of budgetary constraints, cultural misalign-
ments, short-term interests versus longevity
goals, and history (being burned before on
management fads or the hot management
concept of the day). Knowledge manage-
ment might not be for everyone, but at least
awareness can be created. Hopefully, if
these lessons are applied, an organization
will have a greater likelihood to succeed in
knowledge management, resulting in im-
proved organizational morale, increased in-
novation, and better customer and stake-
holder relations. 

T he years ahead look promising fororganizations that make learning apriority. Knowledge management is
putting us on the road to getting there, but
it’s only a start. Hopefully, senior manage-
ment throughout industry and government
will embrace these concepts as the intangi-
bles become tangible.

References
1. M. Eisenhart, “Knowledge Management in the Public

Sector, Washington’s Need to Know,” Knowledge Man-
agement Magazine, vol. 4, no. 1, Jan. 2001.

2. R. Weber et al., “Intelligent Lessons Learned Systems,”
Expert Systems with Applications: An Int’l J., vol. 20,
no. 1, 2001.

For more information on this or any other computing topic, please visit our
Digital Library at http://computer.org/publications/dlib.

4 2 I E E E  S O F T W A R E M a y / J u n e  2 0 0 2

About the Author

Jay Liebowitz is the knowledge management officer for the NASA Goddard Space Flight
Center. His research interests are expert/intelligent systems, knowledge management, and
knowledge audits. He holds a Doctor of Science from George Washington University. Contact
him at NASA Goddard Space Flight Center, Code 300, Greenbelt, MD 20771; liebowit@
pop300.gsfc.nasa.gov.



focus

0 7 4 0 - 7 4 5 9 / 0 2 / $ 1 7 . 0 0  ©  2 0 0 2  I E E E M a y / J u n e  2 0 0 2 I E E E  S O F T W A R E 4 3

improvement suggestions from completed
projects and works even in small- and
medium- size companies that cannot afford
extensive KM investments. However, PMA
has been mainly advocated for situations
such as completion of large projects, learning
from success, or recovering from failure.2–4

When used appropriately, PMA ensures
that team members recognize and remember
what they learned during a project. Individ-
uals share their experiences with the team
and communicate them to other project
groups. Additionally, PMA identifies im-
provement opportunities and provides a
means to initiate sustained change. 

We have applied a lightweight approach
to PMA in several projects5,6 by focusing on
a few vital principles: 

� PMA should be open for participation
from the entire team and other project
stakeholders. 

� Goals can—but need not—provide a fo-
cus for analysis. 

� The PMA process comprises three
phases: preparation, data collection, and
analysis. For each phase, team members

can apply a number of fairly simple
methods, such as the KJ method (after
Japanese ethnologist Jiro Kawakita)7

that collects and structures the data from
a group of people.

Preparation
When we conduct PMA in software com-

panies, two software process improvement
group members work as facilitators together
with two to all project team members. Facili-
tators organize the analysis, steer the discus-
sion, and document the results. They can be
employees in the company where the PMA is
conducted or external, as we are. External fa-
cilitators often have an advantage performing
the PMA because participants regard them as
more neutral and objective. However, they
might not know the company as well as inter-
nal facilitators, so preparation is important.

During the preparation phase, we walk
through the project history to better under-
stand what has happened. We review all
available documents, such as the work
breakdown structure, project plans, review
reports, and project reports. 

We also determine a goal for the PMA.

Postmortem: Never Leave
a Project without It

Andreas Birk, sd&m

Torgeir Dingsøyr, Sintef Telecom and Informatics

Tor Stålhane, Norwegian University of Science and Technology

Although primarily
used for large
projects and
companies,
postmortem analysis
also offers a quick
and simple way to
initiate knowledge
management in
small- or medium-
size software
projects. 

I
n every software project, the team members gain new knowledge and
experience that can benefit future projects and each member’s own
professional development. Unfortunately, much of this knowledge re-
mains unnoticed and is never shared between individuals or teams.

Our experience with project postmortem analysis proves that it is an ex-
cellent method for knowledge management,1 which captures experience and

knowledge management



Goals might be “Identify major project
achievements and further improvement op-
portunities” or “Develop recommendations
for better schedule adherence.” If a PMA
does not have a specific focus to guide our
preparation, we briefly discuss the project
with the project manager and key engineers. 

We find it practical to distinguish between
two PMA types: One is a general PMA that
collects all available experience from an ac-
tivity. The other is a focused PMA for un-
derstanding and improving a project’s spe-
cific activity, such as cost estimation. It helps
to explicitly state goals for both of these
PMA variants during this phase.

Data collection
In the data collection phase, we gather the

relevant project experience. Usually, project
team members and stakeholders have a
group discussion, or experience-gathering
session. We can often conduct data collec-
tion and the subsequent analysis within the
same session. You shouldn’t limit experience
gathering to the project’s negative aspects,
such as things to avoid in the future. Instead,
maintain a balance by identifying a project’s
successful aspects, such as recommended
practices. For example, during a PMA at a
medical software company, the team realized
that the new incremental software integra-
tion process significantly improved process
control and product quality. Integration had
been so smooth that without the PMA, its
important role might have gone unnoticed.

Some techniques that we find useful for
data collection include

� Semistructured interviews. The facilita-
tor prepares a list of questions, such as
“What characterizes the work packages
that you estimated correctly?” and
“Why did we get so many changes to
the work in package X?” 

� Facilitated group discussions. The facil-
itator leads and focuses the discussion
while documenting the main results on a
whiteboard.

� KJ sessions. The participants write down
up to four positive and negative project
experiences on post-it notes. Then they
present their issues and put the notes on
a whiteboard. The participants re-
arrange all notes into groups according
to topic and discuss them.

Once the group identifies the important
topics, we must prioritize them before pro-
ceeding with the analysis. This will ensure that
we address the most significant issues first. 

For example, during a PMA we per-
formed in a satellite software company, fre-
quent and late requirements changes
emerged as an important topic. A software
developer commented that during the proj-
ect, team members found it difficult to iden-
tify when the requirements had changed, so
much so that the code had to be rewritten
completely. In such situations, they made a
few wrong decisions, which reduced the
software’s quality. After this PMA session,
other project members made requirements
changes a high-priority topic for analysis.

Analysis
In this phase, as facilitators, we conduct a

feedback session in which we ask the PMA
participants: “Have we understood what you
told us, and do we have all the relevant facts?”

When we know that we have sufficient
and reliable data, we use Ishikawa dia-
grams6 in a collaborative process to find the
causes for positive and negative experiences.
We draw an arrow on a whiteboard, which
we label with an experience. Then, we add
arrows with causes—which creates a dia-
gram looking like a fishbone. In our exam-
ple from the satellite software company, we
found four causes for changing require-
ments: poor customer requirements specifi-
cation, new requirements emerging during
the project, little contact between the cus-
tomer and software company, and the soft-
ware company’s poor management of re-
quirements documents.

Because PMA participants are a project’s
real experts and we have time limitations,
we perform all analysis in this step.

Results and experience
Facilitators document the PMA results in a

project experience report. The report contains

� A project description, including products
developed, development methods used,
and time and effort needed

� The project’s main problems, with de-
scriptions and Ishikawa diagrams to
show causes

� The project’s main successes, with de-
scriptions and Ishikawa diagrams

Once the group
identifies the

important
topics, we must
prioritize them

before
proceeding with

the analysis.

4 4 I E E E  S O F T W A R E M a y / J u n e  2 0 0 2



� A PMA meeting transcript as an appen-
dix, to let readers see how the team dis-
cussed problems and successes 

In an example from the satellite software
company, facilitators wrote a 15-page re-
port in which they documented the problem
with changing requirements with an
Ishikawa diagram that showed the four
main causes. After facilitators submit a re-
port, the knowledge management or quality
department must follow up.

In our experience, PMA is suitable when a
project reaches a milestone and when the com-
pany is looking for qualitative experience that
will help improve a similar, future project. You
should not apply PMA in situations with un-
finished activities, or when serious underlying
conflicts might remove the focus from im-
provement. If the atmosphere isn’t appropriate
for discussing a project’s problems, we prefer
using approaches other than PMA, such as
those outlined in Project Retrospectives: A
Handbook for Team Reviews.2 When there
have been serious conflicts in the project, this
is more appropriate for managing the risk that
discussions degenerate into a hunt for scape-
goats. Finally, you must have enough time for
following up on PMA results.

In our experience, if teams apply PMA in
the right setting, it is an excellent step into
continuous knowledge management and im-
provement activities. It makes project team
members share and understand one another’s
perspectives, integrates individual and team
learning, and illuminates hidden conflicts. It
documents good practice and problems, and
finally, it increases job satisfaction by giving
people feedback about their work.

Performing a PMA can even improve proj-
ect cost estimation. We applied PMA to three
projects in an Internet software development
company, which all had serious cost over-
runs. The company could not allocate work-
ers with skills specific to the project. This led
to a need for courses—the team’s experts had
to act as tutors for the rest of the team and
were distracted from their roles in the proj-
ect. By performing the PMA, the company
realized the gravity of the qualification issue
and how it led to the project going over
budget. As an improvement action, a training
budget was set up on the company level in-
stead of the project level. The company no
longer charged staff qualification to the pro-

ject’s budget, and now views it as an invest-
ment into quality and competitive advantage.
As a result of this PMA, management real-
ized the strategic importance of staff qualifi-
cation and knowledge management—a truth
that often gets buried in the hectic rush of In-
ternet software business. 

W e received a lot of positive feed-back from PMA participants indifferent companies. Particularly,
they like that PMA offers a simple yet effec-
tive way to uncover both achievements and
improvement opportunities. One developer
at the satellite software company noted, “If
you do a PMA on the project...you have to
think through things,” which is a crucial
part of knowledge management. So, never
leave a project without it!

References
1. C. Collison and G. Parcell, Learning to Fly: Practical

Lessons from One of the World’s Leading Knowledge
Companies, Capstone, New York, 2001. 

2. B. Collier, T. DeMarco, and P. Fearey, “A Defined
Process For Project Post Mortem Review,” IEEE Soft-
ware, vol. 13, no. 4, July/Aug. 1996, pp. 65–72.

3. N.L. Kerth, Project Retrospectives: A Handbook for Team
Reviews, Dorset House Publishing, New York, 2001.

4. A.J. Nolan, “Learning from Success,” IEEE Software,
vol. 16 no. 1, Jan./Feb. 1999, pp. 97–105.

5. T. Stålhane et al., “Post Mortem—An Assessment of
Two Approaches,” Proc. European Software Process
Improvement (EuroSPI 01), ICSN, Bray, Ireland. 

6. T. Dingsøyr, N.B. Moe, and Ø. Nytrø, “Augmenting Ex-
perience Reports with Lightweight Postmortem Re-
views,” 3rd Int’l Conf. Product Focused Software
Process Improvement (Profes 01), Lecture Notes in
Computer Science, vol. 2188, Springer-Verlag, Berlin,
pp. 167–181.

7. D. Straker, A Toolbook for Quality Improvement and
Problem Solving, Prentice Hall International, London,
1995, pp. 89–98 and 117–124.

M a y / J u n e  2 0 0 2 I E E E  S O F T W A R E 4 5

About the Authors

Andreas Birk is a consultant and software engineering professional at sd&m, software
design and management. His special interests include software engineering methods, knowl-
edge management, and software process improvement. He holds a Dr.-Ing. in software engi-
neering and a Dipl-Inform. in computer science and economics from the University of Kaiser-
slautern, Germany. He is a member of the IEEE Computer Society, ACM, and German Computer
Society. Contact him at sd&m, Industriestraße 5, D-70565 Stuttgart, Germany; 
andreas.birk@sdm.de.

Torgeir Dingsøyr is a research scientist at Sintef Telecom and Informatics research foun-
dation in Trondheim, Norway. He wrote his doctoral thesis on “Knowledge Management in
Medium-Sized Software Consulting Companies” at the Department of Computer and Information
Science, Norwegian University of Science and Technology. Contact him at Sintef Telecom and In-
formatics, SP Andersens vei 15, NO-7465 Trondheim, Norway; torgeir.dingsoyr@sintef.no.

Tor Stålhane is a full professor of software engineering at the Norwegian University of
Science and Technology. He has a MSc in electronics, and a PhD in applied statistics from Nor-
wegian University of Science and Technology. He has worked on compiler development and
maintenance and software reliability, and on software process improvement and systems
safety.  Contact him at Department of Computer and Information Science, Norwegian Univer-
sity of Science and Technology, NO-7491 Trondheim, Norway; tor.stalhane@idi.ntnu.no.



4 6 I E E E  S O F T W A R E M a y / J u n e  2 0 0 2 0 7 4 0 - 7 4 5 9 / 0 2 / $ 1 7 . 0 0  ©  2 0 0 2  I E E E

to better use its internal software knowledge
in two ways. First, it wanted to improve soft-
ware development and acquisition processes
to increase software quality and repeatability
of success. Second, it wanted to explicitly
reuse knowledge from previous software
projects to enhance future ones. In particular,
DaimlerChrysler considered reusing experi-
ences as a key to better project performance
and higher software quality. 

We can view such experience exploita-
tion as a variant of knowledge manage-
ment.1 Unlike factual knowledge, we can’t
find experience in textbooks. Experiences
are related to the environment and context
in which they occurred, and when reused in
their original context, they can direct soft-
ware process improvement (SPI). For exam-
ple, we can calibrate the frequency and in-
tensity of design or code inspections to
optimize effort spent and errors detected. 

Here, we report on DaimlerChrysler’s
Software Experience Center project. The
SEC aimed to investigate experience reuse (as
a variant of knowledge management) and ap-
ply insights and collected experiences to SPI. 

Experience-based SPI
The SEC’s operational goal was to provide

business units with the concepts of a learning
organization and a prototype of an experience
base. To do this, researchers acted as experi-
ence and knowledge engineers and coaches for
the business units to assist them in their expe-
rience exploitation activities and transfer in-
sights into SPIs. DaimlerChrysler expected a
learning software organization would better
use the scarce resource of available software
competency.2

The SEC built on the concept of an experi-
ence factory,3 which is an organizational unit
that supports several software projects. As a

focus
Experience in
Implementing a Learning
Software Organization

Kurt Schneider and Jan-Peter von Hunnius, DaimlerChrysler Research Center 

Victor R. Basili, University of Maryland

DaimlerChrysler
created its Software
Experience Center to
investigate
experience reuse
and encourage
experience-based
software process
improvement. Here,
the authors report
on challenges the
company faced when
creating the SEC.

C
ompetence in software development and acquisition has become
essential for the automotive industry. As a manufacturer of pre-
mium-class cars, DaimlerChrysler depends on high-quality soft-
ware for its sophisticated electronic control units. ECUs imple-

ment features such as intelligent brake assistants, electronic stability, and
engine controls. Unfortunately, software development and acquisition com-
petencies are a scarce resource. Consequently, DaimlerChrysler decided 

knowledge management



separate entity, an experience factory receives
plans, status information, and experiences
from all participating projects. Incoming data
is organized in models, such as defect density
models, Pareto charts of defect class baselines,
algorithms, and so forth.4 These models pro-
vide projects with immediate feedback—such
as “your error density is now 10 percent
higher than usual”—and experience-based
advice—“when you inspect more than five
pages of code at a time, your performance
goes down.” NASA first implemented this
concept.5 DaimlerChrysler’s situation and en-
vironment called for some modifications.6 Its
experiences were more qualitative than those
of a classical experience factory because it had
less quantitative data available. Consequently,
it also used process models rather than para-
metric equations. 

DaimlerChrysler’s SEC supported all activ-
ities, from experience elicitation to making
experience available for a software task at
hand. Experience elicitation involved inter-
views, feedback forms, and an optimized ver-
sion of a lightweight elicitation workshop.7

To spread consolidated experiences, we used
an intranet description of the software
process at hand (for example, of an inspection
process). Using HTML, we could describe a
process and link it to such items as training
materials, experiences, checklists, frequently
asked questions, and expert emails. We called
this collection our experience base for any
given topic (topics include software inspec-
tions, software risk management, and re-
quirements engineering). An experience base
is the persistent storage of experiences, organ-
ized around a software process.8 It offers
email contact in different contexts and users
are encouraged to provide feedback. We had
some scripts that could quickly update an ex-
perience base, but beyond this, there was no
automation. Elicitation and reuse procedures
were more critical than tools or automation. 

Three major challenges for a
learning software organization

Establishing an SEC as a learning organ-
ization in the business units participating in
this project was surprisingly difficult. We
discuss two cultural challenges and one
technical challenge that were decisive for
turning a department or business unit into a
learning software organization at Daimler-
Chrysler.

Learning implies broad and deep change
Introducing a learning software organiza-

tion deeply changes how developers and man-
agers should work. Developers must change
their mindsets, skills, and behavior; managers
must change their expectations about what
gets delivered and when; and the organization
must reconsider its training approach and any
processes that interact with the primary
process it is changing. If the effects of change
do not occur across the organization, im-
provements will not happen, and the cost and
effort invested will have been wasted.

Experiences at DaimlerChrysler. We experi-
enced several situations in which manage-
ment commitment was limited in time and
scope. For example, one manager complained
about an insufficient effect of change after
only two months of SEC operation. In an-
other case, a manager constantly refocused
the area for improvement. In both cases, man-
agement blocked emerging change by not un-
derstanding an important implication of
change: it takes time. 

On the other hand, when handled with
care, change can lead to success. For example,
over a three-year period, a quality manage-
ment support group encouraged a software
risk management process by developing pilot
projects, discussing the process in user
groups, submitting it to management for ap-
proval and commitment, offering training in
tailored courses, and using a specific, custom-
made risk management experience base. The
group considered all stakeholders and their
concerns, differences, and needs; it antici-
pated and addressed ripple effects (for exam-
ple, the need for training).9 Risk management
is now accepted in the entire department and
applied to all top strategic software projects
as well as to a growing number of other soft-
ware projects. Risks are now identified earlier
and can be mitigated more effectively.

Recommendations. Improvement causes
change, and effective change is not local. Be-
cause we cannot always control the speed of
ripple effects, patience and a long-term vi-
sion are indispensable. All stakeholders must
understand what is happening, why it is hap-
pening, how it affects their jobs, and why it
takes so long. Things usually differ from
what we first believe, so adaptation and iter-
ation are needed. Spreading best practices in

M a y / J u n e  2 0 0 2 I E E E  S O F T W A R E 4 7

Introducing a
learning
software

organization
deeply changes
how developers
and managers
should work. 



experience bases can support the ripple of
change and its speed. Plan activities that help
stakeholders adjust to the ripple effects, and
advise key people that they must adjust their
own behavior—not just provide funding.

Capitalizing on learning involves risk
There is much to gain by making changes

based on learning—for example, there are
many opportunities in SPI.2 However, im-
provement programs and learning initia-
tives imply that skills must change and ac-
quired expertise might become obsolete,
which could be perceived as personal risks.
Consequently, employees sometimes try to
avoid the change. Ignoring or neglecting
personal risks can turn into a serious risk
for the entire SPI activity. Unfortunately,
risk management is rarely applied to SPI,
and an ignored risk is the worst kind.10

Experiences at DaimlerChrysler. We intro-
duced a use case technique for requirements
engineering in one group, not considering the
risk this introduction posed for several group
members. As electrical engineers, they were
used to model-based development. Conse-
quently, the new technique was neither openly
rejected nor actively adopted. Because we
couldn’t modify the team’s background, we
decided to cancel the method introduction
when we reconsidered it from a risk perspec-
tive. We thus immediately stopped wasting
money and no longer risked the SPI initiative’s
reputation. Instead, we refocused and concen-
trated on a more appropriate improvement ac-
tivity (improving documentation structure).

Recommendations. Apply risk management
to your software improvement initiative and
learning efforts just as you would to a soft-
ware development project. Use experience-
based checklists of common risks in your
environment (such as personnel and budget
risks), thus reusing knowledge and experi-
ence gained in earlier projects. Watch out
for personal risks that an activity might cre-
ate for important stakeholders. Stop an ac-
tivity before you lose too much money and
your reputation. Better yet, refocus activi-
ties long before risks turn into problems,
and you’ll avoid failure completely.

Experience value through packaging
The experience base must provide infor-

mation in a format that is useful for intended
users. At DaimlerChrysler, qualitative expe-
riences are most common. Packaging refers
to the activity of comparing, analyzing, and
combining several pieces of raw experiences
received from various projects (for example,
from developers or project leaders). We or-
ganize this material according to the steps of
the process models. The result is a document
that represents either a consolidated view of
the packaged experiences or modifications
to an existing (process) model. 

Packaging is a technical challenge because
it requires identifying and working with sev-
eral models.4 The key is to make experience-
related material relevant to a real user. This
includes tailoring contents and format to a
concrete anticipated usage situation. Experi-
ence is only valuable when set in context. We
must base iteration, evolution, and learning
on explicit information to form the seed for
the next cycle.11

Experiences at DaimlerChrysler. SEC re-
searchers observed quality circles: Quality
assurance staff gathered in an informal way
to exchange experiences. Unfortunately,
they captured little information, writing
down almost nothing. The few items they
did capture were hardly reusable. To assist
their experience capturing, the SEC team
needed to develop expertise in the subject
under discussion (such as inspections and
review plans, and quality management is-
sues). Over time, the QA staff adopted this
practice and wrote down more information
and introduced an experience base for qual-
ity assurance. Establishing the base was a
lengthy process of describing, reworking,
and representing (on paper and on slides)
inspection processes, support material, and
review plans.8 

Recommendations. Develop packaging tech-
niques and tailor them for your organization.
Domain experts should create packages based
on anticipated user needs. It should be made
as simple as possible to create and use pack-
ages. Be aware of the importance and timeli-
ness of feedback. Iterate often to know what
the users really need. Don’t fall in love with an
idea you might have packaged, and don’t try
to reach the final solution in one iteration. Let
the user see, use, and comment on partial so-
lutions over several iterations. 

Apply risk
management to
your software
improvement
initiative and

learning efforts
just as you
would to a
software

development
project. 

4 8 I E E E  S O F T W A R E M a y / J u n e  2 0 0 2



E ach issue we’ve discussed was moreof a challenge than tool support andautomation. We have learned to bet-
ter cope with these challenges, but they re-
main the most decisive issues in implement-
ing a learning software organization through
explicit experience exploitation. We recom-
mend providing advice on dealing with the
issues, but each organization must find its
own approach.

Once we recognized these challenges and
dealt with them directly, the SEC ran more
smoothly. It has improved many processes,
and learning from experience has become a
more natural part of daily life in the busi-
ness units. 

References 
1. T.G.P. Davenport, Knowledge Management Case Book,

John Wiley & Sons, New York, 2000.
2. R.V. Solingen et al., “No Improvement without Learn-

ing: Prerequisites for Learning the Relations between
Process and Product Quality in Practice,” Product Fo-
cused Software Process Improvement (PROFES 2000),
Springer-Verlag, New York, 2000, pp. 36–47. 

3. V. Basili, G. Caldiera, and D.H. Rombach, “The Expe-
rience Factory,” Encyclopedia of Software Eng., John
Wiley & Sons, New York, 1994, pp. 469–476.

4. V. Basili and F. McGarry, “The Experience Factory:
How to Build and Run One,” Proc. Int’l Conf. Soft-
ware Eng. (ICSE 19), ACM Press, New York, 1997, 
pp. 643–644.

5. V. Basili et al., “The Software Engineering Laboratory:
An Operational Software Experience Factory,” 14th 
Int’l Conf. Software Eng. (ICSE ’92), ACM Press, New
York, 1992, pp. 370–381.

6. F. Houdek and K. Schneider, “Software Experience Cen-
ter: The Evolution of the Experience Factory Concept,”
Int’l NASA-SEL Workshop, Proc. 24th Ann. Software
Eng. Workshop, NASA Goddard Software Eng. Lab
(SEL), Greenbelt, Md., 1999.

7. K. Schneider, “LIDs: A Light-Weight Approach to Expe-
rience Elicitation and Reuse,” Product Focused Soft-
ware Process Improvement (PROFES 2000), Springer-
Verlag, New York, 2000, pp. 407–424.

8. K. Schneider and T. Schwinn, “Maturing Experience
Base Concepts at DaimlerChrysler,” Software Process
Improvement and Practice, vol. 6, 2001, pp. 85–96.

9. K. Schneider, “Experience-Based Training and Learning
as a Basis for Continuous SPI,” Proc. 6th Ann. European
Software Eng. Process Group Conf. (EuropeanSEPG),
2001.

10. E.M. Hall, Managing Risk: Methods for Software Sys-
tems Development, Addison-Wesley, Reading, Mass.,
1997.

11. G. Fischer, “Seeding, Evolutionary Growth and Reseed-
ing: Constructing, Capturing and Evolving Knowledge
in Domain-Oriented Design Environments,” Automated
Software Eng., vol. 5, no. 4, Oct. 1998, pp. 447–464.

For more information on this or any other computing topic, please visit our
Digital Library at http://computer.org/publications/dlib.

M a y / J u n e  2 0 0 2 I E E E  S O F T W A R E 4 9

About the Authors

Kurt Schneider is a researcher and project leader at the DaimlerChrysler Research Cen-
ter, Ulm, Germany. He currently works in software process improvement, software quality, and
lightweight approaches to software development. He has led large research projects—in par-
ticular the SEC project—with several business units and international partner companies. He
studied computer science at the Friedrich-Alexander Universität Erlangen-Nürnberg, Germany,
and received his doctoral degree in software engineering from the Universität Stuttgart, Ger-
many. Contact him at DaimlerChrysler Research Center Ulm, P.O. Box 2360, 89013 Ulm, Ger-
many; kurt.schneider@daimlerchrysler.com.

Jan-Peter von Hunnius is a researcher and PhD student at the DaimlerChrysler Re-
search Center, Ulm, Germany. His research interests include experience-based process improve-
ment, software development processes in general, extreme programming, and the rational uni-
fied process. He received his Diplom-Informatiker in computer science from the Albert-Einstin
Universität Ulm, Germany. Contact him at DaimlerChrysler Research and Technology, Software
Process Engineering (RIC/SP), P.O. Box 2360, 89013 Ulm, Germany; jan.hunnius@
daimlerchrysler.com.

Victor R. Basili is a professor of computer science at the University of Maryland, 
College Park, and the executive director of the Fraunhofer Center, Maryland. He is also one 
of the founders and principals in the Software Engineering Laboratory.  He works on measur-
ing, evaluating, and improving the software development process and product and has con-
sulted for many organizations. He is co-editor-in-chief of the International Journal of 
Empirical Software Engineering, published by Kluwer. He is an IEEE and ACM Fellow.

Get 
access
to individual IEEE Computer 

Society documents online.

More than 67,000 articles 

and conference papers available!

$5US per article for members 

$10US for nonmembers

http://computer.org/publications/dlib



5 0 I E E E  S O F T W A R E M a y / J u n e  2 0 0 2 0 7 4 0 - 7 4 5 9 / 0 2 / $ 1 7 . 0 0  ©  2 0 0 2  I E E E

solutions. A key component in process knowl-
edge is traceability: the ability to follow the life
of an object developed during software engi-
neering from its creation to its use. Creating
and maintaining the relationships that exist
among objects and people in software develop-
ment helps realize traceability. 

This ability to produce a web of relation-
ships is essential in the development of knowl-
edge networks, which are formal and informal
networks of people and objects and the rela-
tionships between them.2 A primary role of
KM systems is to craft such networks to accu-
mulate and use knowledge. Thus, by gluing
the fragmented sources of knowledge, trace-
ability helps create, store, retrieve, transfer,
and apply process knowledge in software de-
velopment organizations.

Why traceability?
Seamlessly connecting the knowledge

fragments spread across a software devel-
opment organization is a major KM chal-
lenge. For example, design rationale
knowledge typically resides in different
repositories or sources such as CASE tools,
intranets, design notebooks, and annota-
tion tools. The absence of any traceability
to link designs to their rationale (which can
contain the alternatives considered and crit-
ical design decisions or assumptions) se-
verely restricts the usefulness of these
knowledge fragments. Even design ration-
ale authors find that interpreting the ra-
tionale they’ve created is difficult if suffi-
cient time has elapsed since its creation.
Therefore, successful knowledge transfer
and reuse requires contextualized knowl-
edge. Traceability helps achieve it by link-
ing related knowledge fragments. 

The challenge of connecting knowledge
fragments is more acute with tacit knowl-

focus
Process Knowledge
Management with
Traceability

Balasubramaniam Ramesh, Georgia State University

Traceability helps
process knowledge
evolve in software
development
organizations.
Drawing from a
large empirical
study, this article
examines
traceability’s role in
facilitating critical
knowledge
management
processes.

K
nowledge management involves the collection, assimilation, and
use of the explicit and tacit knowledge spread throughout an or-
ganization. (We can explicate or codify explicit knowledge; tacit
knowledge cannot be codified or expressed in written form.1) In

software engineering, process knowledge is any explicit or tacit knowledge
about the activities, steps, and procedures involved in creating software

knowledge management



edge. Because individuals usually hold it, the
ability to identify those who have the knowl-
edge needed to solve the problem at hand is
essential. Here, traceability can link objects
produced in systems development with the
stakeholders who played different roles in
their creation, maintenance, and use.

The study
The observations reported in this article

come from a study I co-developed of best
practices in traceability over 30 system de-
velopment organizations in a wide variety of
industries.3 A key objective was understand-
ing how traceability could help manage
process knowledge. 

The participants had experience in sev-
eral areas of system development including
project management, system engineering,
requirements management, system testing,
integration, quality assurance, maintenance,
and implementation. They had an average
of 15.5 years of experience in systems de-
velopment. The study represented a wide
range of projects in terms of outlay (US$15
million to several billion dollars), size (sev-
eral thousand lines to over 1 million lines),
and number of requirements (600 to over
10,000). Both users (for example, system
maintainers) and producers (for example,
system designers) of traceability informa-
tion participated. We used multiple data
collection methods (focus groups, inter-
views, and document review) to define and
confirm the findings.

Traceability’s role in KM processes
Maryam Alavi and Dorothy Leidner

present a framework for analyzing the role
of information systems in KM.4 According
to this framework, we can view organiza-
tions as knowledge systems with four sets
of socially enacted knowledge processes:
construction, storage and retrieval, trans-
fer, and application. In the study, we ex-
amined the best practices in traceability to
identify its role in enabling each of these
KM processes.

Construction 
Construction means adding new compo-

nents or replacing existing tacit and explicit
knowledge components. An important as-
pect of knowledge construction is the trans-
fer of knowledge from one source (individ-

ual, document, or group) to another and
from one state (tacit or explicit) to another.4

Traceability helps generate explicit and
tacit knowledge about software develop-
ment processes and their outputs. In a
telecommunications company participating
in our study, we observed a comprehensive
traceability scheme for maintaining design
rationale for all critical design decisions. It
was created within a CASE tool using doc-
ument templates for various components of
rationale such as assumptions and alterna-
tives, which were linked to relevant design
diagrams. An incentive scheme in which de-
sign rationale users assigned bonus points
to contributors encouraged experts to ex-
plicitly document their tacit knowledge
about the design process. Access to this
knowledge helps less experienced designers
not only learn important design tricks but
also convert the explicit knowledge stored
in documents into new, internalized tacit
knowledge. 

The ability to link various related knowl-
edge sources helps users develop new in-
sights and ideas that were not feasible when
the rationale used to appear only in stand-
alone design notebooks. Here, traceability
facilitates collaboration and communication
among project teams and accelerates knowl-
edge creation.

Knowledge storage and retrieval 
Storage and retrieval are key to effec-

tively managing a knowledge network. An
aerospace company in our study realized
that its designers couldn’t reuse past knowl-
edge from related projects due to the lack of
traceability between software components
and their requirements. So, it built a trace-
ability tool that linked code fragments to
design elements and requirements using
Corba-based object linking. The tool sup-
ports multimedia documents and provides
indexing and querying facilities to trace rel-
evant knowledge fragments stored in the
knowledge base. This lets the organization
readily store and access knowledge from di-
verse sources—something most organiza-
tions find impossible to do.5 

The loss of tacit knowledge at the conclu-
sion of individual projects and the inability to
retain and reuse knowledge at an organiza-
tional level frustrated the management of a
consulting organization in our study. So, it

M a y / J u n e  2 0 0 2 I E E E  S O F T W A R E 5 1

The ability to
link various

related
knowledge

sources helps
users develop
new insights

and ideas that
were not
feasible.



tasked each project team to create an evolving
knowledge map that links knowledge-inten-
sive tasks to relevant individuals and docu-
ments in a corporate directory of knowledge
sources. Many organizations use a variant
such as a corporate yellow pages directory for
locating both tacit and explicit knowledge.

Knowledge transfer
An important aspect of KM is the trans-

fer of knowledge to where it is needed and
can be used.4 An electronics manufacturer
in our study fashioned a requirements man-
agement system to provide traceability be-
tween requirements, video clips of usage
scenarios of its target product, and docu-
mentation on previous designs. The system
also provides links to relevant experts
through a corporate directory of knowl-
edge sources. This system helps knowledge
transfer by immersing designers in the con-
text of the system they’re designing. For ex-
ample, at the beginning of new engage-
ments, a software consulting company in
our study links project documentation to
relevant lessons learned from similar proj-
ects stored in its knowledge base. 

Availability of knowledge is not sufficient
for knowledge transfer. Other factors such
as the receiver’s absorptive capacity and the
perceived value of the knowledge are critical
for successful knowledge transfer.

Knowledge application
Knowledge integration is central to its

application in organizations. In several or-
ganizations, we observed homegrown tools
that use wrappers to integrate legacy data
and systems, message-brokering-based mid-
dleware, and knowledge servers for inte-
grating various knowledge sources. 

An interesting example is the extension

of a collaboration support system that links
design team discussions with design dia-
grams in Rational Rose and project docu-
ments in tools such as Microsoft Office and
Project using ActiveX controls. This helps
developers understand the organizational
knowledge embedded in rules and routines
(for example, coding and architecture stan-
dards set by the organization) and apply
them appropriately. By providing appropri-
ate contextual information, the potential for
misapplying the knowledge also reduces.

T raceability can play a crucial KMrole in software organizations—ifthe facilities to support the KM
processes discussed here are developed.
Lack of well-defined traceability schemes
and the inability of traceability tools to link
knowledge fragments in heterogeneous en-
vironments are common problems that limit
the usefulness of traceability in KM in most
organizations.

References
1. I. Nonaka and H. Takeuchi, The Knowledge-Creating

Company, Oxford Univ. Press, New York, 1995.
2. A. Tiwana and B. Ramesh, “Integrating Knowledge on

the Web,” IEEE Internet Computing, vol. 5, no. 3,
May/June 2001, pp. 32–39.

3. B. Ramesh and M. Jarke, “Toward Reference Models
for Requirements Traceability,” IEEE Trans. Software
Eng., vol. 37, no. 1, Jan. 2001, pp. 58–93.

4. M. Alavi and D.E. Leidner, “Knowledge Management
and Knowledge Management Systems: Conceptual
Foundations and Research Issues,” MIS Quarterly, vol.
25, no. 1, Mar. 2001, pp. 107–136.

5. E.W. Stein and V. Zwass, “Actualizing Organizational
Memory with Information Systems,” Information Sys-
tems Research, vol. 6, no. 2, 1995, pp. 85–117.

For more information on this or any other computing topic, please visit our
Digital Library at http://computer.org/publications/dlib.

Knowledge
integration is
central to its
application in
organizations.

5 2 I E E E  S O F T W A R E M a y / J u n e  2 0 0 2

About the Author

Balasubramaniam Ramesh is an associate professor of computer information systems at
Georgia State University. His research interests include requirements engineering and traceabil-
ity, knowledge management, and collaborative work. Several major funding agencies have
supported his work, including the US Office of Naval Research and the US National Science
Foundation. He received his PhD in information systems from New York University. Contact him
at 35 Broad St., Atlanta, GA 30303; bramesh@gsu.edu.



focusknowledge management

0 7 4 0 - 7 4 5 9 / 0 2 / $ 1 7 . 0 0  ©  2 0 0 2  I E E E M a y / J u n e  2 0 0 2 I E E E  S O F T W A R E 5 3

writing email, and so forth), but this is just to
properly format the knowledge. Hence,
knowledge management assumes enormous
significance. Here, we review KM practices
at Infosys, a software services company head-
quartered in India, discussing how to imple-
ment KM and revealing some of its potential
benefits. 

Organization-wide KM 
Infosys provides consultancy and software

services worldwide to Fortune 500 compa-
nies. It employs approximately 10,200 em-
ployees and executes nearly 1,000 software
projects at any given point in time in diverse
areas such as telecom, manufacturing, insur-
ance, finance, and so forth. Needless to say,
managing knowledge at Infosys is a huge
challenge. 

Until late 2000, Infosys restricted KM to
certain pockets within the company. Then, it
began a KM initiative with a steering commit-
tee that had representation from the Board of
Directors and senior management. Currently,
there are approximately eight full-time people
designated as brand managers who help build
and maintain the KM infrastructure. A central
pool created for this purpose funds the major-
ity of the KM costs. However, for project-level
KM, the software project bears the cost—typ-
ically 2 to 3 percent of the total cost.

Infosys manages organization-wide knowl-
edge using three centrally operated knowl-
edge repositories: the Knowledge Shop (K-
Shop), Process Asset Database, and People
Knowledge Map.

Infosys built the K-Shop architecture on
Microsoft site server technology, and all em-

Knowledge Management 
at Infosys 

Late in 2000, Infosys,
a software services
company, started 
a knowledge
management
initiative to better
enable knowledge
dissemination. The
authors review this
initiative, highlighting
KM’s importance and
benefits.

S
cenarios such as this are common across software companies
worldwide. Knowledge workers alone can’t sufficiently gather
and distribute knowledge. Organizations require an infrastruc-
ture to support such workers and an information flow that en-

ables knowledge dissemination. If you look at typical software scenarios,
there is a greater percentage of knowledge workers than so-called hand
workers. Knowledge workers spend some time doing hand work (typing, 

“I was working on a proposal for a new client. The project was for a new technology, so I had to
search the Internet and library for two weeks to get the details.”

“You could have referred to our research division’s paper on this new technology. The details are
available on our intranet!”

Shivram Ramasubramanian and Gokulakrishnan Jagadeesan, Infosys 



ployees can access it through a Web interface.
The company encourages people to submit
papers related to technology, domain, trends,
culture, project experiences, internal or exter-
nal literature, and so forth. They can submit
the articles in any format that the Web sup-
ports (for example, HTML or Word), and we
designed templates for various content types
to ensure uniformity. In addition, the K-Shop
has an excellent search facility that offers
search through multiple parameters. K-Shop
documents are available to all Infosys em-
ployees and are segregated based on the user’s
selected keywords and content type.

Because only a few employees write their
experiences in the form of a paper, a LAN
system called the Process Asset Database
captures the “as is” project deliverables. This
contains project artifacts such as project
plans, design documents, and test plans.
Users can search the documents based on do-
main, technology, project type, project code,
customer name, and so forth. This helps pro-
vide new projects with information on simi-
lar, previously executed projects and helps set
quantitative goals.

The People Knowledge Map is a direc-
tory of experts in various fields. It is an in-
tranet-based system where employees can
search and locate experts. The directory’s
usability is enormous because it provides
multiple nodes or topics. It serves as the
bridge between two knowledge workers: the
user and the provider.

As mentioned earlier, we need more than
one system or mechanism for KM. Infosys’s
intranet portal Sparsh serves as the window
for all systems and acts as the central tool.
The company’s Quality System Documentation
is a repository of all process-related guide-
lines, checklists, and templates. These serve
to standardize the process followed in a proj-
ect and hence the project’s outputs. Infosys
also has electronic bulletin boards for dis-

cussing technical and domain-related topics.
In addition, there are news groups and
newsletters from various departments that
discuss the latest technology and business
trends. 

KM at the project level  
KM is even more important for a partic-

ular software project than it is for an entire
organization. The benefits of effectively
managing and sharing knowledge in a proj-
ect team include the ability to

� Easily react to customer requests
� Improve productivity through fewer de-

fects and rework
� Improve teamwork

Dynamic KM is essential in a software
project because in many cases the project de-
pends on the availability of the latest knowl-
edge. Many Infosys projects have Web sites
to manage knowledge content. Each project
also maintains a project-specific knowledge
repository and a detailed training plan with
material to tackle project attritions. In addi-
tion, projects also hold weekly knowledge
sharing sessions where team members dis-
cuss and document past learning, so projects
reuse knowledge effectively and can reduce
effort and cost.

KM in projects is everybody’s responsibil-
ity. The project manager defines the KM ac-
tivities in the project plan, which then serves
as a guide. Typically, about 2 to 3 percent of
project effort is spent in KM activities. 

An incentive for knowledge sharing
When a person submits a document to the

K-Shop, experts chosen from the People
Knowledge Map review the document in de-
tail. If found acceptable, the K-Shop publishes
it. The reviewer and author are rewarded with
knowledge currency units. When an employee
reads or uses a document from the K-Shop, he
or she is encouraged to give KCUs for that
document based on the benefits gained from
reading it. Authors can accumulate KCUs for
their documents and redeem them for cash or
other gifts. Thus, KCUs serve twin objectives:
they act as a mechanism both for rewarding
knowledge sharing and rating the quality of
assets in the repository.

For example, suppose an author submits a
document. He or she can get anywhere be-

Dynamic KM is
essential in 
a software

project because
in many cases

the project
depends on 

the availability
of the latest
knowledge.

5 4 I E E E  S O F T W A R E M a y / J u n e  2 0 0 2

Presidents and fellows of Harvard College, Harvard Business Review on Knowledge
Management, Harvard Business School Press, Boston, 1998, pp. 22–23.

F. Horible, Managing Knowledge Workers, John Wiley & Sons, New York, 1999.
J. Honeycutt, Knowledge Management Strategies, Microsoft Press, Redmond, Wash., 2000.
J. Leibowitz, Building Organizational Intelligence: A Knowledge Management Primer, CRC Press,

Boca Raton, Fla., 1999.

Further Reading



tween three and 10 KCUs, depending on the
document type. An external literature would
get three points and an internal white paper
would fetch 10. In cash terms, 10 KCUs trans-
late to around .50 percent of the author’s
salary. Each time another employee uses the
document, the author could earn between 0
and 10 KCUs. Hence, the author’s cumulative
earning will increase each time the document
is used. 

D uring K-Shop’s first year, employeessubmitted over 7,600 documents.KM has helped Infosys increase its
productivity and reduce defect levels. A
rough estimate shows that Infosys reduced its
defect levels by as much as 40 percent. This
significantly reduces the associated rework
and thus the cost of detecting and preventing
defects. Also, effective reuse has increased
productivity by 3 percent. All of this has been
possible due to faster access to accurate in-
formation and reuse of knowledge.

We’re now starting the next phase of KM

revolution, where we’ll leverage technology
to further enhance its usability. We also
need to separate the good content from the
bad by scoring each asset. This will help us
better decide which artifact to use. 

For more information on this or any other computing topic, please visit our
Digital Library at http://computer.org/publications/dlib.

M a y / J u n e  2 0 0 2 I E E E  S O F T W A R E 5 5

About the Authors

Shivram Ramasubramanian is a software quality advisor at Infosys. His research
interests include knowledge management, metrics, and software quality models such as CMM.
He graduated in mechanical engineering from Regional Engineering College, University of Cali-
cut, and has a diploma in business finance from ICFAI, Hyderabad. Contact him at Infosys Tech-
nologies Limited, No. 138, Old Mahabalipuiram Rd., Sholinganallur, Chennai 600 119, India;
shivram_r@infy.com.

Gokulakrishnan Jagadeesan is a software quality advisor at Infosys. His re-
search interests include knowledge management and CMM. He graduated in electrical and
electronics engineering from the University of Madras and has a Masters of Engineering in ad-
vanced manufacturing technology from the University of South Australia. Contact him at In-
fosys Technologies Limited, No. 138, Old Mahabalipuiram Rd., Sholinganallur, Chennai 600
119, India; gokulakrishnan_j@infy.com.

Doing Software Right
• Demonstrate your level of ability in 

relation to your peers

• Measure your professional knowledge
and competence

The CSDP Program differentiates between 
you and others in a field that has every kind of 
credential, but only one that was developed by, 
for, and with software engineering professionals.

Register Today

Visit the CSDP web site at http://computer.org/certification
or contact certification@computer.org

Get CSDP Certified
Announcing IEEE Computer Society's new 

Certified Software Development

Professional Program

"The exam is valuable to me for two reasons:
One, it validates my knowledge in various areas of expert-
ise within the software field, without regard to specific
knowledge of tools or commercial products...
Two, my participation, along with others, in the exam and
in continuing education sends a message that software de-
velopment is a professional pursuit requiring advanced ed-
ucation and/or experience, and all the other requirements
the IEEE Computer Society has established. I also believe
in living by the Software Engineering code of ethics en-
dorsed by the Computer Society. All of this will help to im-
prove the overall quality of the products and services we
provide to our customers..."
— Karen Thurston, Base Two Solutions



5 6 I E E E  S O F T W A R E M a y / J u n e  2 0 0 2 0 7 4 0 - 7 4 5 9 / 0 2 / $ 1 7 . 0 0  ©  2 0 0 2  I E E E

Responding to the challenge
We concentrated on the design and evalu-

ation of a KM system for an integrated cir-
cuit assembly and testing firm (see the related
sidebar), where knowledge intensity, speci-
ficity, and volatility are comparable to those
in software engineering and other knowledge-
intense industries. Generally, such firms offer
services in semiconductor product assembly
and testing and therefore add value to the
overall IC production and manufacturing
process at a late stage. For instance, upon a
wafer’s manufacture, machines automatically
saw it into dies on which to perform particu-
lar functional tests. Subsequently, specialized
facilities assemble and mount the dies that
pass the required tests into packages.

Effective KM is crucial to IC assembly
and testing firms, which operate in a highly

competitive industry characterized by short
lead time, high-yield performance, and strin-
gent quality standards. Typically, employees
possess knowledge pertinent to the organiza-
tion’s core competence, and their departure
would result in substantial knowledge
drainage. Additionally, knowledge retained
at an individual level is difficult to pass on
and reuse throughout the organization. An-
choring in a widely adopted  knowledge pro-
cess,5–7 we responded to the KM challenge
by developing a system that supports organ-
ization-wide knowledge creation, update,
sharing, and reuse. Based on our findings,
we also discuss important implications for
KM in software engineering, where knowl-
edge sharing and reuse are equally critical to
ultimate design quality, organization pro-
ductivity, and service level.8,9

focus
Design and Evaluation
of a Knowledge
Management System 

Chih-Ping Wei, National Sun Yat-Sen University

Paul Jen-Hwa Hu, University of Utah

Hung-Huang Chen, International Semiconductor Technology

This article
discusses important
implications for
knowledge
management in
software
engineering based
on a system created
for an integrated
circuit assembly and
testing firm. This
system supports
organization-wide
knowledge creation,
update, sharing, and
reuse.

T
o compete in the emerging knowledge-centric economy, organi-
zations around the world have undertaken various initiatives to
manage their most important yet volatile asset—knowledge.1,2

In this article, we consider knowledge management (KM) as a
systematic and organizational process for retaining, organizing, sharing,
and updating knowledge critical to individual performance and organiza-
tional competitiveness.3 We can best describe our overall approach as codi-
fication-based knowledge repository building, which has become prevalent
in current knowledge management practices.4

knowledge management



Design and implementation
Basically, our system architecture com-

prises a lessons-learned knowledge repository,
a case repository, an organizational directory,
a communication and distribution layer, and
a set of common knowledge-management
functions. Consistent with existing practice,
we structured the knowledge in the system as
cases rather than embedding it in the product
design or quality review process.

The lessons-learned knowledge reposi-
tory contains validated knowledge, whereas
the case repository stores previously disap-
proved cases or those undergoing valida-
tion. A case in either repository includes a
problem or phenomenon description, analy-
ses, comments, and recommendations for
treatment, as well as the contributors’ con-
tact information. The organizational direc-
tory contains organization structure, indi-
viduals’ profiles and contact information,
and the names of domain experts in various
areas. The system also has a communication
and distribution layer integrated with the
existing Lotus Notes email and groupware
systems. This layer is vital to knowledge cre-
ation and update and offers users conven-
ient access to both repositories.

The system supports a set of common KM
functions—knowledge creation, mainte-
nance, search, and usage analysis, for exam-
ple. We implemented the system using Notes
Script running on Windows NT Server 4.0
(with Service Pack 6.0). We built the reposi-
tories, organizational directory, and access
log using Notes Domino 4.62. Additionally,
we developed the client browser and email
services using Lotus Notes 4.62.

Typical knowledge creation begins when
an individual posts a problem. The relevant
section and department managers then as-
sess the problem’s significance. A case con-
sidered to be insufficiently significant is
stored in the case repository, rather than un-
dergoing  knowledge solicitation and valida-
tion. When approving a case, a department
manager identifies and selects experts to give
analyses, comments, or recommendations.
Notified by email, these individuals collabo-
rate to analyze and solve the problem using
the groupware system. The participating ex-
perts can access all analyses and comments
submitted to the system, which fosters syn-
ergetic behavior. The department manager
monitors the intensity of case discussion—

measured by the number of submitted com-
ments—and, when it levels off, concludes
with the final significance assessment and
knowledge validation. The manager then
stores the validated and approved knowl-
edge in the lessons-learned knowledge repos-
itory, which is indexed by product type,
manufacturing process stage, concerned de-
partment, and so on. Additionally, the KM
system automatically “pushes” the newly
created knowledge via email to the potential
users, the problem poster, and the solicited
experts. Knowledge maintenance follows a
similar process, which is activated by an ap-
proved knowledge update request and com-
pleted with the dissemination of updated
knowledge to potential users.

System status and evaluation
The system became operational in April

2001. Immediately after the system was
available, the firm’s IS department provided
user training throughout the organization.
As of December 2001, employees had cre-
ated a total of 1,204 cases. Production, re-
search and development, process engineer-
ing, equipment engineering, and quality
assurance departments were the most active
in knowledge creation and update, and sys-
tem use. The turnaround time for knowl-
edge creation was approximately five days,
a significant improvement over the previous
manual process.

Our evaluation compared knowledge
sharing and subsequent improvements in
productivity and customer service before and
after the system’s availability. Seventy-eight
employees—including managers, engineers,
and support staff from the previously-men-
tioned knowledge-intensive departments—
participated in the evaluation. A majority of
the respondents held nonmanagerial posi-
tions (85 percent), and many of them had oc-
cupied their current posts for more than one
year (48 percent). We measured each evalua-
tion dimension using multiple question items
based on a five-point Likert scale, with one
being “extremely disagree” or “extremely
frequent” and five being “extremely agree”
or “extremely infrequent.”

Our comparative analysis results sug-
gested that the system’s use significantly fa-
cilitated individual knowledge sharing in
terms of the ease of locating experts for tar-
get knowledge and sharing personal knowl-

M a y / J u n e  2 0 0 2 I E E E  S O F T W A R E 5 7

Results
suggested that
the system’s

use significantly
facilitated
individual
knowledge
sharing.



edge with colleagues (significant at the 95
percent level). The system’s use also en-
hanced productivity. Particularly, the sub-
jects’ responses suggested that using the sys-
tem reduced the frequency of production
interruptions due to a lack of knowledge
support or access, or prolonged trouble
shooting for equipment malfunction (signif-
icant at the 95 percent level). Additionally,
use of the system significantly reduced the
frequency of delayed replies to customer in-
quiries, partly because of a considerable re-
duction in the reanalysis or re-solving of
previously examined problems.

Implications for KM
in software engineering

Difficulties such as poorly defined re-
quirements, frequent staff turnover, and
volatile hardware and software platforms
constantly challenge software engineering
projects.10 These challenges require a case-
based approach to organizational learning
of important lessons from previous experi-
ences or emerging requirements.11 By en-
compassing a fundamental KM process un-
derpinned by effective learning from
previous designs and practices, a software
engineering organization can improve its

project planning, implementation, and con-
trol. As illustrated in the case reported, im-
plementing a validated and accessible orga-
nizational repository that contains knowl-
edge from various projects can support and
enhance such KM processes.

In addition to highlighting a promising
use of KM in IC assembly and testing, this
study also has several implications for KM
in software engineering. Specifically, man-
agement support, integration with existing
technology infrastructure, and an organiza-
tional culture that values knowledge cre-
ation and sharing are critical success factors
for KM system implementation. Integrating
a KM system with existing technology is
particularly important, because it will re-
duce individuals’ learning time and cogni-
tive load and foster system acceptance and
use. A system’s ease of use is relevant and
might greatly depend on its interfaces with
users. Findings from interviews with engi-
neers and support staff suggested the need
for further streamlining of problem-posting
procedures and for concise presentation of
problems, analyses, and recommendations.
When supported by a KM system that insti-
tutionalizes and supports sharing critical,
task-specific knowledge previously retained
by individuals, software engineers and de-
velopers, like their IC assembly-testing
counterparts, should be able to design and
deliver better systems at a faster pace.

S everal research directions are worthcontinued investigation. For instance,we have yet to examine the system’s
cost-effectiveness. This study demonstrates
the system’s positive effects on knowledge
sharing, productivity, and service level but
does not address costs. Also interesting is the
development and evaluation of similar sys-
tems in the software engineering context,
where project processes and stages are usu-
ally dynamic and intertwined, and knowl-
edge validity is profoundly important. 

References
1. D. Leonard-Barton, Wellsprings of Knowledge: Building

and Sustaining the Sources of Innovation, Harvard
Business School Press, Boston, 1995.

2. G.V. Krogh, K. Ichijo, and I. Nonaka, Enabling Knowl-
edge Creation: How to Unlock the Mystery of Tacit
Knowledge and Release the Power of Innovation, Ox-
ford Univ. Press, Oxford, UK, 2000.

5 8 I E E E  S O F T W A R E M a y / J u n e  2 0 0 2

Located in southern Taiwan, the study firm specializes in total solutions to
tape-carrier-package and integrated circuit smart-card assembly and test-
ing. Its sales revenues have grown rapidly, reaching US$7 million in 2000
(a 700 percent increase since its launch in 1998). The firm has a fairly so-
phisticated technology infrastructure, which consists of several integrated in-
formation systems for transaction processing, management control, and de-
cision support.

Managers at all levels value knowledge sharing and collaborative prob-
lem solving. The firm created several centralized repositories early on to
provide convenient access to its structured internal knowledge, such as stan-
dard operating procedures, process guidelines, and organization policies.
The firm emphasized individual continued learning of task-specific knowl-
edge through interdepartmental product reviews or case discussions, for ex-
ample. However, the firm often maintained knowledge generated by these
collaborative efforts in paper-based documents. Searching for or accessing
this knowledge was difficult and time-consuming.

One of the firm’s central challenges had been acquiring, organizing,
sharing, and using important lessons learned from previous design reviews
and customer service. In response, we developed a system that retains and
makes available such core knowledge through collaborative problem solv-
ing. We felt that the study firm, when equipped with the system, would im-
prove product design at early stages and enhance its customer relationship
management. We integrated our system with the existing technology infra-
structure, which was familiar to the target users, including managers, engi-
neers, and support staff.

Study Firm Overview



3. T.H. Davenport, D.W. DeLong, and M.C. Beers, “Suc-
cessful Knowledge Management Projects,” Sloan Man-
agement Rev., vol. 39, no. 2, Winter 1998, pp. 43–57.

4. M.T. Hansen, N. Nohria, and T. Tierney, “What’s Your
Strategy for Managing Knowledge?” Harvard Business
Rev., vol. 77, no. 2, Mar./Apr. 1999, pp. 106–116.

5. S. Staab et al., “Knowledge Process and Ontologies,”
IEEE Intelligent Systems, vol. 16, no. 1, Jan./Feb. 2001,
pp. 26–34.

6. V. Grover and T. H. Davenport, “General Perspectives
on Knowledge Management: Fostering a Research
Agenda,” J. Management Information Systems, vol. 18,
no. 1, Summer 2001, pp. 5–21.

7. G. Fischer and J. Ostwald, “Knowledge Management:
Problems, Promises, Realities, and Challenges,” IEEE
Intelligent Systems, vol. 16, no. 1, Jan./Feb. 2001, pp.
60–72.

8. T.C. Lethbridge, “What Knowledge Is Important to a
Software Professional?” Computer, May 2000, vol. 33,
no. 5, pp. 44–50.

9. D.E. O’Leary, “How Knowledge Reuse Informs Effec-
tive System Design and Implementation,” IEEE Intelli-
gent Systems, vol. 16, no. 1, Jan./Feb. 2001, pp. 44–49.

10. T.K. Abdel-Hamid, “The Slippery Path to Productivity
Improvement,” IEEE Software, vol. 13, no. 4, July/Aug.
1996, pp. 43–52.

11. S. Henninger, “Tools Supporting the Creation and Evo-
lution of Software Development Knowledge,” Proc.
12th Int’l. Conf. Automated Software Eng., IEEE CS
Press, Los Alamitos, Calif., 1997, pp. 46–53.

For more information on this or any other computing topic, please visit our
Digital Library at http://computer.org/publications/dlib.

M a y / J u n e  2 0 0 2 I E E E  S O F T W A R E 5 9

About the Authors
Chih-Ping Wei is an associate professor in the Department of Information Management
at National Sun Yat-Sen University. His research interests include knowledge discovery and
data mining, information retrieval and text mining, knowledge management, multidatabase
management and integration, and data warehouse design. He received his BS in management
science from the National Chiao-Tung University and his MS and PhD in management informa-
tion systems from the University of Arizona. He is a member of the ACM and IEEE. Contact him
at the Department of Information Management, National Sun Yat-Sen University, Kaohsiung,
Taiwan; cwei@mis.nsysu.edu.tw.

Hung-Huang Chen is a senior MIS manager at Interna-
tional Semiconductor Technology. His research interests include knowledge management and
data mining. He received a BS in mechanical engineering from Tamkang University and an
MBA in information management from National Sun Yat-Sen University. Contact him at
rbchen@ist.com.tw.

Paul J. Hu is an assistant professor of information systems for the David Eccles School of
Business at the University of Utah. His research interests include information technology man-
agement and applications in health care, electronic commerce, human-computer interaction,
knowledge management, and information systems project management. He received his PhD
in management information systems from the University of Arizona. Contact him at Account-
ing and Information Systems, David Eccles School of Business, 1645 E. Campus Center Drive,
108 KDGB, Univ. of Utah, Salt Lake City, UT 84112; actph@business.utah.edu.

Choose from 100 courses at the IEEE Computer Society’s Distance Learning Campus. 
Subjects covered include…

* Java * Project management * HTML
* PowerPoint * Visual C++ * Visual Basic
* Cisco * TCP/IP protocols * CompTIA
* Windows Network Security * Unix

With this benefit, offered exclusively to members, you get…
* Access from anywhere at any time * Vendor-certified courseware
* A multimedia environment for optimal learning * A personalized “campus”
* Courses powered by KnowledgeNet®—a leader 

in online training

Sign up and start learning now!
http://computer.org/DistanceLearning

Get thousands of dollars 
worth of online training—
FREE for members New 2002MembershipBenefit



6 0 I E E E  S O F T W A R E M a y / J u n e  2 0 0 2 0 7 4 0 - 7 4 5 9 / 0 2 / $ 1 7 . 0 0  ©  2 0 0 2  I E E E

In the context of software engineering, we
define knowledge management as a set of ac-
tivities, techniques, and tools supporting the
creation and transfer of SE knowledge
throughout the organization. One use of KM
is to support software process improvement
(SPI) activities. This support is important be-
cause both software engineering and quality
management techniques fail if they are not
based on a thorough knowledge of what is
needed and what has been done in a software
development organization.

So, how can the existing knowledge in a
software organization be captured efficiently?
To try to answer this question, we conducted
a case study in an independent business unit
of a global corporation developing software-
intensive electronic products. The company
wanted to improve the capture and reuse of

software development knowledge for a par-
ticular project. It had made several attempts
to improve knowledge reuse, but all these at-
tempts had failed. Why did the earlier at-
tempts not succeed? What would be a work-
ing solution? We set out to study those
questions to learn from the previous difficul-
ties and build on the previous successes.

Analyzing the status of KM-based SPI
Employee interviews and relevant docu-

ments revealed that both the managers and
designers felt that a lot of knowledge was
being wasted. Existing knowledge was diffi-
cult to find, and when found it was not
reusable. The practices implemented earlier
had obviously not been successful.

The underlying goal had been to reduce
software defects by increasing the knowledge

focus
Toward a Practical
Solution for Capturing
Knowledge for Software
Projects

Seija Komi-Sirviö and Annukka Mäntyniemi, VTT Electronics

Veikko Seppänen, University of Oulu

A needs-based
approach to reusing
software
development-
related knowledge
can overcome past
failures at
knowledge reuse. 

R
arely has a professional field evolved as quickly as software devel-
opment. Software organizations are continuously struggling to
keep abreast of new technologies frequently changing customer re-
quirements; and increasingly complex software architectures,

methods, and tools. Recently, many organizations have come to understand
that to succeed in the future, they must manage and use knowledge more ef-
fectively at individual, team, and organizational levels.1,2 Efficient creation,
distribution, and reuse of up-to-date knowledge are critical success factors
that unfortunately remain difficult to achieve in practice.3–5

knowledge management



transfer between different projects. The infor-
mation to be shared was stored in a Lessons to
Learn database. Interviews clearly indicated
that many project managers were not aware of
the database and few used it. An analysis of
the database revealed that a number of entries
were incomplete and only one of the four the-
matic sections was in active use. According to
the database concept owner, this was because
preparing database entries was time-consum-
ing and administering the data was difficult.
Moreover, the data’s accuracy and relevancy
were not obvious, because most of the data
was provided without structure.

Another way to share knowledge between
projects was Data Transfer Days. These meet-
ings focused on analyzing past problems and
success stories. The participants captured and
shared important knowledge during the meet-
ings, even though they had trouble remem-
bering past successes and pitfalls once their
projects had ended. The intention was to an-
alyze, package, and save the results of these
meetings as a reference for new projects. Un-
fortunately, the enthusiasm usually disap-
peared at this point. The meetings were useful
mainly for those who were able to attend.
Nevertheless, free face-to-face conversation
between group members turned out to be a
better way of sharing knowledge than the
database (compare this to Thomas Davenport
and Laurence Prusak’s idea that the human
network is a highly efficient knowledge-shar-
ing mechanism1).

As you can see, neither Data Transfer
Days nor, particularly, the Lessons to Learn
Database were working as initially intended.
An obvious reason for the latter was that the
project management processes did not incor-
porate guidelines for the storing or searching
of knowledge. Efficient use of the database
would have required more disciplined
processes and much more effort at captur-
ing, packaging, searching, maintaining, and
reusing the knowledge. Furthermore, most
project managers were too busy coping with
their everyday problems and were unwilling
to undertake any further duties. Our inter-
views also indicated that software designers
tended to trust anyone nearby, rather than
any specific experts or the shared database.

Researchers who have investigated these
problems elsewhere have found problems
similar to ours and give these reasons for
reuse failures: the knowledge capturing

process is too informal, is not incorporated
into the engineering processes, or is not sup-
ported by the structures of the organization.6

Davenport and Prusak have stated that if you
start with technology-centered solutions (for
example, a database) and ignore behavioral,
cultural, and organizational change, the ex-
pected advantages never materialize.1

Looking for a new solution
The organization set a challenging re-

quirement: new solutions should have mini-
mal impact on the software development or-
ganization and processes and should not
require new technologies. Because the exist-
ing processes should not be touched, simple,
manual, and offline means were preferable,
removing the excessive burden of KM. This
SPI action aimed to create a process that
would help the company acquire experience
from existing sources—such as the com-
pany’s databases and individuals—that it
could apply to ongoing SE projects. One new
idea to define and test was to use the SPI ex-
perts as knowledge-capturing agents instead
of having software developers do it by them-
selves, on the fly. This method viewed proj-
ects as individual customers that required
specific knowledge. Efforts would focus on
the customer’s current knowledge require-
ments, as opposed to a large-scale acquisi-
tion, analysis, packaging, sharing, and up-
dating of knowledge for subsequent projects.

The company’s SPI persons and external
experts established a new approach for cap-
turing knowledge. This new approach con-
sisted of a knowledge-capturing project and
customer projects. The former gathered
knowledge from relevant sources and pack-
aged and provided it to a customer project for
reuse on demand. This solution neither
changed the organizational setting nor re-
quired any new tools. The knowledge would
come from existing sources such as project fi-
nal reports, error databases, discussion fo-
rums, and—most important—people. Later,
this assumption proved true. Figure 1 shows
a simplified capturing process. 

Unlike other approaches,6,7 this one did not
expect ongoing software projects to supply
their experience. The knowledge-capturing
project is similar to the analysis organization
in the Experience Factory framework6 in that
it analyzes knowledge and packages it into
reusable assets. However, it does this for the

M a y / J u n e  2 0 0 2 I E E E  S O F T W A R E 6 1

The underlying
goal had been 

to reduce
software
defects by
increasing 

the knowledge
transfer
between
different
projects. 



customer projects’ immediate needs.
Together with the company, we tested this

approach on a project that urgently needed
interface-related knowledge. This special
knowledge was spread among various docu-
ments, memos, databases, and people. The
Lessons to Learn Database did not provide
this knowledge; Data Transfer Days hadn’t
helped. The customer project’s needs were
structured to indicate what specific knowl-
edge was required, what form of knowledge
was needed, and how the knowledge would
be reused. The needs were also divided into
process- and product-related knowledge.
The former included, for example, software
design and testing tasks, roles, organiza-
tions, skills, methods, and tools. The latter
included descriptions and interfaces of prod-
ucts and product family hierarchies. 

As planned, we followed the knowledge-
capturing process, using semi-structured in-
terviews as the main technique to acquire
knowledge. The process took 300 hours; the
most laborious phase was experience packag-
ing, which took more than one-half of the

time. The delivered interface knowledge
package fully met all its requirements: the cus-
tomer project retrieved needed knowledge of
the existing interfaces. The selected approach
worked well, and the customer project was
served the required knowledge just in time.

A lthough we acknowledge the limita-tions of a single case study, we donot hesitate to call into question
technology-centered solutions as the main
means for managing software development
knowledge. We feel our study is a first step
toward a more comprehensive needs-based
KM approach. We will continue to expand
the use of the just in time KM process and
will work to make it a part of normal proj-
ect initiation procedures. Over time our ef-
forts should help provide structured and
packaged information that will have real
value for software projects.

References 
1. T.H. Davenport and L. Prusak, Working Knowledge:

How Organizations Manage What They Know, Har-
vard College Business School Press, Boston, 1998, 
p. 199.

2. I. Nonaka and H. Takeuchi, The Knowledge-Creating Com-
pany: How Japanese Companies Create the Dynamics of
Innovation, Oxford Univ. Press, New York, 1995, p. 284. 

3. V. Basili et al., “Implementing the Experience Factory
Concepts as a Set of Experience Bases,” Proc. 13th Int’l
Conf. Software Eng. and Knowledge Eng. (SEKE 01),
Knowledge Systems Inst., Skokie, Ill., 2001, pp.
102–109.

4. T. Kucza et al., “Utilizing Knowledge Management 
in Software Process Improvement: The Creation of a
Knowledge Management Process Model,” Proc. 7th 
Int’l Conf. Concurrent Enterprising (ICE 2001), Univ.
of Nottingham, Center for Concurrent Enterprising,
Nottingham, UK, 2001, pp. 241–249.

5. K. Schneider, “Experience Magnets: Attracting Experi-
ences, Not Just Storing Them,” Proc. 3rd Int’l Conf.
Product Focused Software Process Improvement (PRO-
FES 2001), Lecture Notes in Computer Science, no.
2188, Springer-Verlag, Heidelberg, Germany, 2001, pp.
126–140.

6. V. Basili, “The Experience Factory,” Encyclopedia of
Software Eng., vol. 1, John Wiley & Sons, New York,
1994, pp. 469–476.

7. A. Birk and C. Tauz, “Knowledge Management of Soft-
ware Engineering: Lessons Learned,” Proc. 10th Conf.
Software Eng. and Knowledge Eng. (SEKE 98), Knowl-
edge Systems Inst., Skokie, Ill., 1998, pp. 24–31.

For more information on this or any other computing topic, please visit our
Digital Library at http://computer.org/publications/dlib.

Figure 1. The 
knowledge-
capturing process.

6 2 I E E E  S O F T W A R E M a y / J u n e  2 0 0 2

Need

1. Define  scope
and requirements
for knowledge
capturing

2. Acquire
knowledge

3. Package
knowledge

Knowledge
package

About the Authors

Seija Komi-Sirviö is a research scientist at the Fraunhofer Center for Experimental
Software Engineering, Maryland. She is visiting from VTT Electronics, Finland, where she has
carried out research into software process improvement and metrics in applied research projects
from 1994 onwards. As a research group manager, she has been responsible for initiating and
managing both applied research projects and industrial development projects for a broad range
of clients in software engineering. Her current research interests include software process and
product improvement, measurement, and knowledge management. She received her MSc in in-
formation processing science from the University of Oulu, Finland. She is a member of the IEEE
Computer Society. Contact her at ssirvio@fc-md.umd.edu.

Annukka Mäntyniemi is a research scientist at VTT Electronics, Oulu, where she has
worked  since 1998. She received her Master’s degree in Information Processing Science from
University of Oulu, Finland in 2001. Her thesis concerned the reuse of software development
experiences. Her current research interests involve utilizing knowledge management in soft-
ware process improvement and software reuse. Contact at Annukka.Mantyniemi@vtt.fi.

Veikko Seppänen is a software business research professor at the University of Oulu,
Finland with almost 20 year’s experience in software research and development. He finished
his engineering doctoral thesis on software reuse in 1990 and his second dissertation on eco-
nomic sciences in 2000. Seppänen has published about a hundred scientific and practical publi-
cations. He was an Asla Fulbright scholar at UC Irvine in 1986-87 and a JSPS Postdoctoral Fel-
low at Kyoto University in 1991-93. His present research involves software business strategies
and models, including value network based approaches to industrial marketing, acquisition and
use of commercial-off-the-shelf software components and products, and knowledge-driven soft-
ware product and business development methods. Contact him at veikko.seppanen@oulu.fi. 



feature

0 7 4 0 - 7 4 5 9 / 0 2 / $ 1 7 . 0 0  ©  2 0 0 2  I E E E M a y / J u n e  2 0 0 2 I E E E  S O F T W A R E 6 3

In essence, the study found that patients’
rating of overall pain during a painful med-
ical examination was closely tied to two
particular moments: the point during the
procedure when the pain was most intense,
and their experience in the procedure’s final
moments, regardless of pain levels. Given
this, patients’ perception of pain might be
significantly different than their actual ex-
perience of pain, particularly if the final mo-
ments are relatively painless.

How might such findings relate to soft-
ware usability? We investigated this question
through a series of experiments. Here, we de-
scribe our studies, their results, and the im-
plications they hold for software engineering.

Usability and pain 
The software industry generally views us-

ability as a major system design and devel-
opment issue, and usability researchers have
drawn explicitly on a considerable body of
previous psychological research. Specifi-
cally, the shift toward user-centered design
has increased researchers’ use of approaches

such as user modeling and feedback. How-
ever, even evaluation approaches that don’t
involve direct feedback from users—such as
cognitive walkthroughs2 and heuristic eval-
uation3—focus on the software’s usability,
rather than on its functionality. One of Rolf
Molich and Jakob Nielsen’s evaluation
heuristics,3 for example, is that text should
be written in simple and natural language,
which is important to users but irrelevant to
functional issues. Usability literature also
strongly emphasizes making software pleas-
ant for users, removing features that might
confuse or irritate them.

Despite this emphasis on enhancing us-
ability and reducing irritation, usability lit-
erature has paid surprisingly little attention
to pain research. There is considerable liter-
ature on this topic that highlights how peo-
ple’s perceptions of unpleasant experiences
differ markedly from traditional expecta-
tions. (Such differences extend even to the
definition of “pain,” which the literature
hotly debates.) Motivated by the fact that
poorly designed software can create in-

Pleasure and Pain:
Perceptual Bias and Its
Implications for Software
Engineering

Sheila Guilford, Gordon Rugg, and Niall Scott, University College Northampton

Pain research
indicates a bias in
human perception
that has implications
for software
development. The
authors discuss how
they investigated
this bias and share
their findings.

W
hat does the journal Pain have to do with software design, de-
bugging, and evaluation? At first glance, not a lot. However, a pa-
per published in that journal reported a seemingly unlikely effect
related to patients’ perception of unpleasant medical procedures.1 

usability



tensely annoying and unpleasant user expe-
riences, Niall Scott wrote a postgraduate
thesis examining how we might apply pain
research findings to usability issues.4

Scott’s focus was on the “peak and end
effect,”1 an idea originating in judgment
and decision-making literature.5 Donald
Redelmeier and Daniel Kahneman discov-
ered the effect during a study in which they
first asked patients undergoing a painful
medical examination to rate their pain at
regular intervals during the procedure.
Then, following the procedure, they asked
patients to assign their pain an overall rat-
ing retrospectively. 

Some people might expect these retro-
spective ratings to be close to the procedural
ratings’ arithmetic mean; a more psycholog-
ically sophisticated prediction would favor
a primacy and recency effect, in which pa-
tients’ first and last ratings would best pre-
dict the global retrospective rating. How-
ever, neither proved true in the medical
study. The retrospective ratings—and, cor-
respondingly, patients’ memory of the over-
all pain levels—were best predicted by a
combination of the worst pain patients ex-
perienced during the procedure (regardless
of when it occurred) and the pain they ex-
perienced in the procedure’s final minutes.
This finding is consistent with those from
human memory literature (see, for example,
studies by Alan Baddeley6).

Redelmeier and Kahneman’s study had
significant ethical and practical implications
for medicine, which Kahneman and collab-
orators investigated in subsequent research.
One implication that they confirmed, for
example, was that if at the end of painful
examinations, doctors added some med-
ically trivial, low-pain examinations, pa-
tients would remember the overall session

as being less painful, even though the ses-
sion’s total quantity of pain increased.

Experiments with
the peak–end effect

Using Scott’s thesis as a foundation, we
conducted subsequent experiments to test
the applicability of the peak–end effect to
software engineering. Given the domain dif-
ferences, it was impossible to simply repli-
cate Kahneman and Redelmeier’s exact ex-
periments, and our findings are indicative
rather than conclusive. 

Scott’s original experiment
In his original work, Scott attempted to

replicate the Kahneman and Redelmeier ex-
periment as closely as possible. He also ana-
lyzed qualitative differences between the pain
domain and that of usability evaluation. 

Scott’s subjects were 10 students using a
software package as part of their course. He
studied the subjects while they used the soft-
ware, and asked them at regular intervals to
rate their feelings about the software. Users
recorded their ratings on paper, using a 20-
point Likert-style scale ranging from 0
(“content”) to 20 (“unhappy”). At the ses-
sion’s end, Scott asked users to rate their
overall feelings on a scale of –10 (“pleas-
ant”) to 10 (“unpleasant”). He used a minus
number scale for global ratings to reduce the
risk of subjects simply copying their re-
sponse from the last time-sliced rating. 

Table 1 shows the results; for clarity, we
converted the global ratings to a 0–20 scale.
In this table and those that follow, lower
ratings indicate higher pleasure, and

� Peak indicates users’ highest rating at
any point during the task 

� Final indicates users’ last rating during
the task

� Mean indicates users’ average rating
during the task

� Global indicates users’ overall rating
immediately after task completion

� Peak–end indicates the average of users’
peak and final ratings

� Predictor indicates the best predictor of
the global rating (peak–end or mean) 

A clear finding that emerged from sub-
jects’ comments during the experiment was
that having a researcher interrupt them at

6 4 I E E E  S O F T W A R E M a y / J u n e  2 0 0 2

Table 1
Results from first experiment

Subject Peak Final Peak–end Global Mean Predictor

1 7 1.50 4.25 10.00 2.25 Peak–end
2 11.5 8.50 10.00 3.00 8.30 Mean
3 11 9.00 10.00 11.00 8.70 Peak–end
4 12 10.50 11.25 14.50 8.90 Peak–end
5 13 11.00 12.00 14.50 10.30 Peak–end
6 11.5 10.00 10.75 11.00 10.40 Peak–end
7 10.5 5.00 8.00 3.00 6.80 Mean
8 20 15.50 17.75 14.50 14.60 Mean
9 13.5 7.00 10.25 14.00 8.90 Peak–end

10 15 6.50 10.75 12.00 11.15 Mean
Mean 12.5 8.45 10.75 10.50 9.03 Peak–end



regular intervals to gather data was intru-
sive. This was not surprising. Redelmeier
and Kahneman’s subjects were not active
participants in the task; having a researcher
present might well have provided a wel-
comed distraction from an unpleasant and
prolonged medical procedure. Scott’s sub-
jects, on the other hand, were being inter-
rupted while trying to achieve various goals.

Nonetheless, the data does show a
peak–end effect, although as a tendency
rather than an invariable rule. For six out of
the 10 subjects, the peak–end value was a
better predictor of the global value than the
mean rating. This is an interesting outcome
because it differs from expectations of both
naive models and those that are more psy-
chologically sophisticated, such as primacy
and recency. Although recency is involved
here, primacy is not. Anecdotally, upon
hearing the findings, several colleagues
claimed with hindsight that there was noth-
ing surprising about them. However, no one
predicted this effect with foresight (another
instance, presumably, of hindsight having
20/20 vision).

Follow-up experiments 
Our obvious next step would be to see

whether we could collect data less intru-
sively. We considered an apparently simple
method, using a software tool to gather
data while subjects used the software. Al-
though the approach has obvious advan-
tages, and subsequent researchers have used
it and found evidence of the peak–end ef-
fect,7 the less obvious disadvantages (dis-
cussed in “Outstanding issues” below) led
us to investigate other approaches. 

We also had questions about the cases in
which the peak–end value was not the best
predictor. The peak–end effect is based on a
human cognition model that strongly pre-
dicts biases in human information process-
ing. Using this model, we expected the effect
to be a strong one, perhaps even the norm.
Why, then, were there several subjects who
did not behave as the model predicted?

To investigate this, we decided to repli-
cate Scott’s study on a larger scale with two
different user groups. Our goal was to see
whether the manual approach could reliably
detect an effect despite its intrusiveness. 

For comparability across studies, we
used the same experimental design. We

again collected data at regular intervals but
with more sensitive data collection using
Likert-style 150-mm visual scales, anchored
at one end with “pleasant” (0) and at the
other with “unpleasant” (150). We used the
same scale for the global value. Our subjects
were students from two different courses,
using a software package as part of their
course. Experiment 2 had 14 subjects, and
Experiment 3 had 11. We gathered data
from each group separately and in the same
manner as in Experiment 1.

Our initial analysis uncovered something
interesting: When we calculated the mean of
the highest session rating and the final rating,
we found the peak–end effect only in a mi-
nority of cases—five of the 25 total subjects in
the two experiments. The implication was
that users’ most unpleasant experience with
the software (their highest rating) was not a
particularly strong predictor of their retro-
spective feelings about it. We then decided to
look at pleasure rather than pain as a predic-
tor. We found that calculating the mean of the
lowest rating (the most pleasant experience)
and the final rating produced a peak–end ef-
fect in about half the cases across the two
groups (11 out of the 25 subjects). Tables 2
and 3 show the results from Experiments 2
and 3, respectively. In both cases, we calcu-
lated the peak–end rating using subjects’ low-
est value (most pleasant experience).

Our findings suggest that in the second
and third experiments, good experiences out-
weighed bad ones as predictors of global ret-
rospective ratings. However, when we reana-
lyzed Scott’s data from Experiment 1 using
this approach, the prediction rate of the

M a y / J u n e  2 0 0 2 I E E E  S O F T W A R E 6 5

Table 2
Results from second experiment

Subject Peak Final Peak–end Global Mean Predictor

1 130 150 140.0 138 143.4 Peak–end
2 83 150 116.5 150 127.9 Mean
3 18 6 12.0 44 82.5 Peak–end
4 30 40 35.0 59 60.0 Mean
5 7 0 3.5 30 49.3 Mean
6 0 0 0.0 7 8.1 Mean
7 29 37 33.0 52 36.4 Peak–end
8 47 11 29.0 30 59.5 Peak–end
9 25 103 64.0 44 61.0 Mean

10 28 100 64.0 59 110.0 Peak–end
11 27 37 32.0 44 40.9 Mean
12 15 70 42.5 51 60.6 Peak–end
13 59 139 99.0 110 115.6 Mean
14 63 108 85.5 51 100.0 Mean

Mean 68 62.07 75.37



peak–end effect dropped from six out of 10
subjects to three out of 10. Quite what this
shows is an interesting question, and one that
requires further investigation. Possible expla-
nations would be that the decline in peak–end
effect was a result of our change from a nu-
meric to a visual scale or our change from
Scott’s original “negative to positive” scale to
our “zero to 150” scale for global ratings. 

Outstanding issues
The peak–end effect appears in a significant

proportion of users’ software evaluations. De-
pending on how we define it, the effect was ev-
ident for 20 to 50 percent of the subjects in
our study. However, the details of the effect’s
influence on user perceptions are still far from
clear, and there are several outstanding issues
that require further investigation. 

Domain differences
Differences between the medical and

software domains are complex and have far-
reaching implications for research method-
ology. Such implications must be resolved
before researchers can carry out large-scale
work in this area. 

A key difference between our domain
and that of Redelmeier and Kahneman’s
medical experiments was that subjects in the
latter were unlikely to experience any sig-
nificant pleasure during the procedures.
Software users, however, can experience
pleasure, and the ratings clearly show that
our subjects did. Researchers continue to
debate the respective natures of pleasure
and pain: Are they, for example, opposite
ends of a single scale, or two separate
things? The complexity of such issues makes
it difficult to directly extrapolate from the
medical to the software domain without re-

search to test the extrapolations. 
Another important domain difference is

that the medical study involved a single con-
tinuous process that had a clear beginning
and end. Participants can thus easily reflect
on the overall experience. Software use, in
contrast, usually involves several discontinu-
ous activities, each of which has its own be-
ginning and end. For example, users might
pull down a menu, choose an item, work
from another window, then close that win-
dow and return to the previous window.
Some of these activities are very brief but can
be annoying—for example, when a word
processing package decides you need help
and pops up an intrusive help feature. Par-
ticipants might experience both painful and
pleasant activities within the experiment,
which they might then find difficult to glob-
alize. In our experiment, such events were
limited because it was based on time sam-
pling, rather than being event-based. Further
experimentation is needed using both a
larger sample size and, ideally, a smart tool
that can log details of the activity while
recording the user’s satisfaction rating.

Data gathering 
Although automated data collection is less

intrusive, it is not necessarily the best choice
for measuring user satisfaction with soft-
ware. In addition to missing smaller annoy-
ances (such as with the pop-up help exam-
ple), automated tools cannot necessarily tie a
user rating to a specific event. Knowing that
a user’s peak value was indicated, say, 10
minutes into the session does software devel-
opers little good unless they know which task
the user was engaged in at the time. There are
thus significant advantages to using a human
observer, despite the fact that their intrusive-
ness seems to mask the peak–end effect. 

At the most obvious level, human ob-
servers ask users to rate their feelings when
they are clearly experiencing pleasure or
pain. The drawback, of course, is that users
wrangling with recalcitrant software are un-
likely to welcome the request to share their
feelings at such a moment. Fortunately, the
literature on judgment and decision-making
offers a neat (and again, counterintuitive) so-
lution to the problem. Evidence strongly sug-
gests that outside observers can assess a sub-
ject’s emotion with surprising accuracy.1,8

Given this, we envision future experiments

6 6 I E E E  S O F T W A R E M a y / J u n e  2 0 0 2

Table 3
Results from third experiment

Subject Peak Final Peak–end Global Mean Predictor

1 34 122 78.0 33 78.25 Peak–end
2 25 20 22.5 59 59.00 Mean
3 15 53 34.0 44 47.75 Mean
4 40 34 37.0 37 70.38 Peak–end
5 22 7 14.5 30 41.88 Mean
6 34 33 33.5 29 39.00 Peak–end
7 27 55 41.0 37 82.50 Peak–end
8 11 7 9.0 29 31.63 Mean
9 31 97 64.0 51 83.63 Peak–end

10 15 15 15.0 44 63.63 Mean
11 38 46 42.0 66 63.00 Mean

Mean 44 41.73 60.06



in this area based on an evaluator’s observa-
tion, which is standard practice in system de-
velopment. During software use, the evalua-
tor would estimate the user’s feelings for each
screen or module, then note them on a stan-
dard Likert-style scale, with the aim of identi-
fying peak–end values. This would not entail
much extra effort, and our usual procedures
could easily accommodate the change.

Limitations of Likert-style scales
Although Likert-style scales have been in

widespread use for decades, they have well-
recognized problems that have important
methodological implications for anyone in-
vestigating the peak–end effect or conduct-
ing general usability research. 

One major problem is the potential for ex-
perimenter bias. Researchers must label each
end of the scale, and these labels have signifi-
cant implications for subjects’ likely answers.

In pleasure and pain research, for example,
the researcher must choose whether the scale
should begin at zero (“no pleasure at all”) or
with a minus value (“extreme pain”).

As our own findings suggest, the wording
and values used for Likert-style scales can
affect the results. This is something we must
consider seriously in any future work on the
peak–end effect.

Implicit attitudes
Although we asked experiment partici-

pants to rate their feelings on a scale, re-
peated psychological research has found that
people can hold implicit attitudes that differ
significantly from those they’re consciously
aware of. This has considerable implications
for usability evaluation research (as well as
for market research, where the problem is
more widely known). The psychology litera-
ture describes various established methods

M a y / J u n e  2 0 0 2 I E E E  S O F T W A R E 6 7

Although the judgment and decision-making literature has significant implications for software engineering, the implications
have received comparatively little attention. The field’s seminal text is the 1982 book, Judgement under Uncertainty: Heuristics
and Biases.1 The text contains classic articles that show numerous shortcomings in human judgement and decision-making. A
particularly interesting feature of these shortcomings is that a high proportion of them affect experts just as much as novices, sug-
gesting that underlying features of human cognition are involved, rather than simple ignorance.

Two decades of subsequent research have confirmed the robustness of these findings in a range of real-world contexts with
real-world implications, including the pain research that is the basis for our article.2–5 A full description of judgment and deci-
sion-making shortcomings and their implications would require an article of its own, but there are two key themes running
through this research that are particularly relevant to software engineering:

� The vast majority of people, including experts, have significant conceptual problems in dealing with probability estimation. 
� People are very bad in general at predicting their own future actions.

Although the robustness of these findings is well established, there is debate within the judgement and decision-making com-
munity about the extent to which they reflect real shortcomings in human cognition. Gerd Gigerenzer, for example, argues that
although humans are bad at dealing with problems involving probability estimation, they are good at handling the same prob-
lems if they are rephrased as estimates of frequency.6 Similarly, other researchers argue that many of the alleged shortcomings
are actually sensible heuristics for dealing with real-life situations where there are no algorithmic best solutions. An excellent overview
of the topic is provided in the book Subjective Probability, which includes contributions from leading researchers in this area.7

Despite these debates, there is consensus within the judgement and decision-making field that significant and predictable short-
comings in human cognition do occur. This has meaningful implications for many areas of software engineering, including risk esti-
mation, and forecasting and evaluating user satisfaction. We are working on several related projects, both individually and with our
colleagues Carole Morrell and Mandy Morrell at University College Northampton; our findings will be reported in later articles.

References
1. D. Kahneman, P. Slovic, and A. Tversky, eds., Judgement under Uncertainty: Heuristics and Biases, Cambridge Univ. Press, Cambridge, UK, 1982.
2. E. Eich et al., “Memory for Pain: Relation Between Past and Present Pain Intensity,” Pain, vol. 23, 1985, pp. 375–389.
3. E. Ohnhaus and R. Adlev, “Methodological Problems in the Measurement of Pain: A Comparison between the Verbal Rating Scale and the Visual Analogue

Scale,” Pain, vol. 1, 1975, pp. 379–384.
4. D. Price et al., “The Validation of Visual Analogue Scales as Ratio Scale Measures for Chronic and Experimental Pain,” Pain, vol. 17, 1983, pp. 45–56.
5. S. Rachman and K. Eyrl, “Predicting and Remembering Recurrent Pain,” Behaviour Research and Therapy, vol. 27, no. 6, 1989, pp. 621–635.
6. G. Gigerenzer, “Why the Distinction between Single Event Probabilities and Frequencies Is Important for Psychology (and Vice Versa),” Subjective Probability,

G. Wright and P. Ayton, eds., John Wiley & Sons, Chichester, UK, 1994, pp. 129–161
7. G. Wright and P. Ayton, eds., Subjective Probability, John Wiley & Sons, Chichester, UK, 1994.

Judgment and Decision-Making Literature



for indirectly measuring implicit attitudes.
Applying these methods to usability evalua-
tion is an area for future research.

Research implications
Although preliminary, our results are sig-

nificant and have implications for usability
design and evaluation, as well as for areas
such as prioritizing debugging efforts. We
view all of the following implications as
drivers for further research rather than as
truths on tablets of stone.

Ethical implications
The ethical implications of our results

could be considerable. An obvious concern
is safety-critical software, where the
peak–end effect might result in dangerously
flawed software passing an acceptance test.
Given the well-documented cases in which
software flaws have led to deaths, such as in
the Therac-25 system,9 the risk is nontrivial.
Adherence to standard development and
testing methodologies should prevent this
problem. Testing methodologies are de-
signed to produce a systematic testing
process unskewed by subjective effects of
the sort we describe. However, such
methodologies are not uniformly applied;
developers of the Therac-25 software, for
example, did not follow a standard method-
ology, and the software apparently under-
went no standard acceptance testing. 

In non-safety-critical areas, the risk of
peak–end effect misuse is probably higher
because of commercial pressure. The ethical
issues here are also more complex than those
involved in safety-critical systems. A devel-
oper might claim, for example, that it’s un-
ethical to leave software users with anything
but the most pleasant memory possible. Al-
though this argument has some initial plau-
sibility, we consider it flawed. Software de-
velopers need not choose between producing
good software and abusing knowledge of the
peak–end effect, nor is the peak–end effect
an excuse for deliberately producing bad
software. An unscrupulous developer might,
for example, deliberately supply flawed soft-
ware using the peak-end effect rather than
fix an expensive and obscure bug.

Our work suggests two overlapping ways
to subjectively evaluate software quality:
contemporaneous and retrospective. In our
opinion, software developers have an ethical

responsibility to produce software that is
user friendly on both counts. Most design
guidelines implicitly concentrate on contem-
poraneous evaluation and provide developers
more than adequate guidance. Regarding ret-
rospective evaluation, developers also have a
responsibility to produce user-friendly soft-
ware, but the dividing line between user
friendliness and consumer manipulation is
not always clear. In this area, we need in-
formed debate among relevant professional
bodies, along with best practice guidelines.

To mitigate the potential ethical abuse of
the peak–end effect, we have two recom-
mendations. For safety-critical applica-
tions, we strongly advise the use of best
practice methods for acceptance testing and
considerable caution in using testing crite-
rion that involves subjective opinions about
the software as a whole. For non-safety-
critical applications, we recommend adher-
ence to standard best practice in software
design and evaluation.

Other implications
Our results have implications for at least

three other areas as well.

� Debugging and usability design. Our re-
sults have implications for debugging
strategy and for usability design that
can be tested fairly easily. The peak–end
effect implies that a single experience of
a particularly unpleasant bug will leave
a stronger impression on a user than
several experiences of a bug that is not
quite so unpleasant. Similarly, it implies
that a single experience of a particularly
pleasant design feature will leave a
stronger impression than several experi-
ences of a feature that is less pleasant. 

� Measurement. The same principle likely
applies to other aspects of customer
contact, including measures of customer
care and satisfaction. We are currently
investigating customer satisfaction
measures in an industrial case study. 

� Methodological implications. As we dis-
cussed earlier, our investigation high-
lighted broader methodological issues—
including implicit attitudes and the
problems with Likert-style scales—that
are particularly relevant to researchers in
this area. They are also highly relevant to
usability and market research in general.

For safety-
critical

applications,
we strongly

advise the use
of best-practice

methods for
acceptance

testing.

6 8 I E E E  S O F T W A R E M a y / J u n e  2 0 0 2



More work is needed in all of the above
areas to further understand the implications
of such effects within the software engineer-
ing domain.

A s the “Judgment and Decision-Mak-ing Literature” sidebar describes,that field’s research contains many
robust findings with implications for usabil-
ity evaluation; the peak–end effect is just
one of these. The psychology literature has
given much attention to such issues, but the
direction and nature of the effects are not
yet well understood. Nonetheless, usability
researchers are well advised to consider
these issues and their implications for stan-
dard usability practice.

Regarding our own future work, one pri-
ority is investigating data collection on an
event-driven rather than a time-driven basis.
Another priority involves clarifying the
methodological issues involved in investi-
gating the peak–end effect in the software
domain. We also want to investigate the ex-
tent to which expressed attitudes, as meas-
ured in the experiments described here, cor-
relate with actual behavior. For example,
does a strong peak–end effect during a free
product trial mean that users are more likely
to buy the product? 

If the peak–end effect emerges as a strong
influence on user behavior, then ethical and
professional issues must be addressed. We
hope this article will prove a useful starting
point. In any case, users everywhere can
take comfort in the fact that, although pain
research might tell us much about software
design, it seems we can learn even more
from research into pleasure.

References
1. D. Redelmeier and D. Kahneman, “Patients’ Memories

of Painful Medical Treatments: Real-Time and Retro-
spective Evaluations of Two Minimally Invasive Proce-
dures,” Pain, vol. 66, 1996, pp. 3–8.

2. P. Polson et al., “Cognitive Walkthroughs: A Method
for Theory-Based Evaluation of User Interfaces,” Int’l J.
Man-Machine Studies, vol. 36, no. 5, 1992, pp.
741–773.

3. R. Molich and J. Nielsen, “Improving Human-Com-
puter Dialogue: What Designers Know about Tradi-
tional Interface Design,” Comm. ACM, vol. 33, no. 3,
Mar. 1990, pp. 338–342.

4. N. Scott, An Investigation into Neophyte Software Users
and Their Real-Time and Retrospective Evaluations, mas-
ters’ thesis, School of Information Systems, Univ. College
Northampton, UK, Jan. 1999; for a copy, contact Sheila
Guilford (sheila.guilford@Northampton.ac.uk).

5. D. Redelmeier, P. Rozin, and D. Kahneman, “Under-
standing Patients’ Decisions,” J. Am. Medical Assoc.,
vol. 270, no. 1, 1993, pp. 72–76.

6. A. Baddeley, Human Memory: Theory and Practice,
Lawrence Erlbaum Assoc., Hillsdale, N.J., 1990.

7. A. Badii and A. Murphy,  “PopEval_MB: A New Us-
ability Evaluation Toolkit to Serve a System for IS/IT
Cultural Match Management,” Proc. 6th European
Conf. on Information Systems Evaluation, D. Remenyi,
ed., Brunel Univ., Uxbridge, UK, pp. 352–368. 

8. B. Fredrickson and D. Kahneman, “Duration Neglect in
Retrospective Evaluations of Affective Episodes,” J.
Personality and Social Psychology, vol. 65, no. 1, 1993,
pp. 45–55.

9. N. Leveson and C.S. Turner, “An Investigation of the
Therac-25 Accidents,” Computer, vol. 25, no. 7, July
1992, pp. 18–41.

For more information on this or any other computing topic, please visit our
Digital Library at http://computer.org/publications/dlib.

M a y / J u n e  2 0 0 2 I E E E  S O F T W A R E 6 9

About the Authors

Sheila Guilford is a senior lecturer in information systems at University College
Northampton. Her research interests are in human factors and human–computer interaction.
She has an MS in man–computer systems from De Montfort University. Contact her at Univ.
College Northampton, Park Campus, Northampton NN2 7AL, England;
sheila.guilford@Northampton.ac.uk.

Niall Scott is a systems executive at Broadsystem in Manchester. His research interests in-
clude the application of psychology to real-world problems. He has a bachelor’s degree in psy-
chology from the University of Newcastle Upon Tyne and a master’s degree in computing from
University College Northampton. Contact him c/o Sheila Guilford at Univ. College Northamp-
ton, Park Campus, Northampton NN2 7AL, England; sheila.guilford@Northampton.ac.uk.

Gordon Rugg is a senior lecturer in computer science at Keele University, and was previ-
ously a reader in technology acceptance in the School of Information Systems, University Col-
lege Northampton. He previously worked as a senior lecturer in the School of Computing Sci-
ence, Middlesex University and is editor of Expert Systems: the International Journal of
Knowledge Engineering and Neural Networks. He has a PhD in psychology from the University
of Reading. Contact him at the Dept. of Computer Science, Keele University, Staffordshire ST5
5BG, England; g.rugg@cs.keele.ac.uk.



7 0 I E E E  S O F T W A R E M a y / J u n e  2 0 0 2 0 7 4 0 - 7 4 5 9 / 0 2 / $ 1 7 . 0 0  ©  2 0 0 2  I E E E

during the reengineering process. We pro-
pose tracing “all around”—from existing
code to the legacy system’s design to its re-
quirements during reverse engineering, and
from the modified requirements to the new
design and the new code during develop-
ment of the successor system. In effect, this
means integrating traceability during re-
verse engineering with (more standard)
traceability during forward engineering.

Our approach uses the Requirements Engi-
neering Through Hypertext (RETH) method
and tool to install traces during both reverse
and forward engineering.2,3 Our experience in
a real-world reengineering project suggests
that all-around tracing can yield immediate
benefits that appear to be specific to reengi-
neering. In this article, we describe all-around
tracing and present a case study to demon-
strate how our approach can be useful in
reengineering efforts. 

Traceability in legacy 
software reengineering

Installing traces takes time and can be
costly (unless installation can be auto-
mated), and real-world projects are severely

time- and resource-constrained. So, for a
practitioner, an important question is
whether and how traceability can immedi-
ately benefit a reengineering effort. To date,
no studies have shown a positive trade-off
between the cost and short-term benefits of
traceability. We have found empirical evi-
dence of short-term benefits and thus argue
in favor of incorporating traceability
throughout both legacy software reverse en-
gineering and forward engineering. 

A high-level view
To illustrate our high-level view of trace-

ability in reengineering, we use a Unified
Modeling Language (UML, see www.omg.
org/uml) class diagram enhanced with ar-
rows, as shown in Figure 1. In all-around
tracing for reengineering, we trace from the
old system implementation to its design and
requirements during reverse engineering,
and from the modified requirements to the
new design and system implementation dur-
ing forward engineering.

Once we have explicitly established trace-
ability from the old to the new implementa-
tion via design and requirements specifica-

feature
Tracing All Around 
in Reengineering

Gerald Ebner, Significant Software

Hermann Kaindl, Siemens AG Österreich, PSE

Traceability in
forward engineering
has well-known
long-term benefits.
Tracing “all around”
in reengineering,
from code through
specifications to
code, can also yield
immediate benefits,
such as simplified
requirements
change and early
error diagnosis.

L
egacy software must often be reengineered, a process that involves
both reverse1 and forward engineering. Typically, no require-
ments, design, or design rationale documentation for legacy soft-
ware exist. This lack of documentation means an unavailability of

traces as well, making reengineering difficult and expensive.
One way to improve reengineering efforts is to establish traceability—the

ability to follow requirements information through the software life cycle—

traceability



tions, these implementations are indirectly
connected. The arrow with the broken line
in Figure 1 illustrates this relationship. From
a theoretical perspective, the relationship is
included in the transitive closure of explicitly
represented traceability relations. In prac-
tice, this means that we do not need to rep-
resent an explicit relation between the im-
plementations because we can derive it.

In our view, we need only one require-
ments definition if the old and the new soft-
ware have more or less the same require-
ments. Although reengineering projects in
practice typically deal with at least minor
changes or additions to the requirements, a
single specification defining both old and
new requirements can be sufficient. 

Metamodel for all-around tracing
Several traceability metamodels exist in

the literature; however, they are not suffi-
ciently specific for immediate application in
practice. (See the sidebar, “Related Work on
Traceability.”) The detailed UML class dia-
gram in Figure 2 illustrates our metamodel
for representing requirements, design, and
implementation, as well as the connections
between them. This is the model of how the
models look during reverse and forward en-
gineering in our approach.  

We use RETH for requirements engineer-
ing and traceability. RETH is both a method
and a tool that supports the method. The
top section of Figure 2 (the yellow-green
boxes, or metaclasses) is the RETH meta-
model for requirements engineering.2,3 This
part is aggregated in Figure 2 by require-
ments definition. The other classes ex-
tend this metamodel to include design
(green boxes) and implementation artifacts
(dark green boxes). Our extended meta-
model also includes a simple representation
of design rationale (design decision,
which is part of design). Trace associa-
tions represent traces, which we install be-
tween requirements definition and design
and between design and implementation. 

For the latter traces in particular, trace
granularity is of practical importance. Ac-
cording to our experience, traces should not
be too coarse—for example, to a source
code module as a whole. But if a trace is too
fine-grained—for example, to each state-
ment in the source code—installing it is too
expensive. For balance, we put traceability

tags at the procedure level. This requires
reasonable effort and is still useful.

We include a correspondence associa-
tion between domain object and design
object to support additional traces. In our
approach, a domain object is an abstraction
of an entity in the problem domain. While we
can clearly distinguish between domain ob-
jects and (software) design objects,4 a corre-
spondence between them can certainly exist.

Our representation of design rationale is a
practical compromise. Each design deci-
sion describes only the design chosen and
the alternatives considered. It is un-
likely that we could have maintained more
elaborate structures within the project time
constraints. It was important, however, that
the design decisions be traceable.

During reengineering, we develop two
models based on this metamodel: one during
reverse and one during forward engineering.
More precisely, a common requirements def-
inition links the models of the old and the
new designs and implementations for all-
around tracing (as Figure 1 illustrates). 

This approach is sufficient as long as the
requirements are completely stable. We
should anticipate, however, that require-
ments will change between the old and new
systems. Figure 3 shows that the require-

M a y / J u n e  2 0 0 2 I E E E  S O F T W A R E 7 1

Requirements definition

RETH hypertext link

Traceability tag

New system designOld system design

Explicitly installed traces during reverse and forward engineering
Indirect traceability

New system implementationOld system implementation

Figure 1. Tracing all around. With all-around tracing, we can
trace from the old system implementation to its design and re-
quirements, and then from the modified requirements to the
new design and implementation. 



ments definition can include common re-
quirements as well as requirements specific to
one system. Note that not all of these must
exist in practice. Often the majority are com-
mon requirements, and only a few are new
system requirements. When one or more new
requirements replace one or more old re-
quirements, we associate them with respect
to traceability (see Figure 3).

Implementing traceability 
In our case study, the requirements engi-

neer and chief designer (the first author of this
article) used the RETH tool not only for re-
quirements capture, but also for software de-
sign (both old and new) and traceability im-
plementation. Although the RETH method as
previously published does not cover software

design, the approach and its supporting tool
are flexible and extensible, simplifying the ex-
tensions to the metamodel in Figure 2.2

Hyperlinks in the RETH tool let us install
traces by linking various artifacts in the re-
quirements specification to artifacts in the
two software designs (illustrated in Figure
1). The RETH tool’s semiautomatic support
for link generation makes installing these hy-
perlinks inexpensive.5 The mechanism for
generating glossary links can also semiauto-
matically generate traceability links based on
textual references. An immediate advantage
of such links is that users can easily navigate
them to follow traceability paths. This navi-
gation is possible in both directions because
RETH hyperlinks are bidirectional.

For technical reasons, the source code of

7 2 I E E E  S O F T W A R E M a y / J u n e  2 0 0 2

Several articles in a special section (edited by M. Jarke) of
the Communications of the ACM present high-level proposals
for traceability.1,2 Still, we found little in the literature about the
trade-off between the cost and utility of traceability, and in par-
ticular about its short-term benefits. In current practice, how-
ever, it is very important to show developers that rewards can
be immediately gained from installing traces. Finally, we found
no mention of the specific utility of tracing during reengineering
or even of all-around tracing. 

Traceability tools
Traceability among requirements, primarily from higher- to

lower-level requirements, has long been considered and is well
supported by tools. The Automated Requirements Traceability
System (ARTS), for example, is a software database manage-
ment system for requirements information management.3 Sev-
eral tools for requirements management and traceability based
on database systems are commercially available—for example,
Requirements Traceability and Management (RTM, www.
chipware.com), Dynamic Object-Oriented Requirements 
System (DOORS, www.telelogic.com), and RequisitePro
(www.rational.com).

Of course, such tools could have managed traces in the
Trade Bridge system as well as the RETH tool did. One of the
main reasons developers chose RETH instead of one of these
COTS tools was its automatic link-generation facility. Reflecting
more recent research, Traceability of Object-Oriented Require-
ments (TOOR) treats requirements and their relations as objects
similarly to our approach.4

Metamodels for traceability
Balasubramaniam Ramesh and colleagues present a re-

quirements traceability metamodel that also covers design and
implementation.5 It is more comprehensive than ours because it
includes, for example, stakeholders. It is rather high-level, how-
ever. For example, it simply states “design satisfies require-

ments,” instead of relating concrete design and requirements
artifacts. Our metamodel defines in greater and more concrete
detail specialized metaclasses and their relationships. In this
sense, we think that our metamodel is easier to apply. 

In their case study, Ramesh and colleagues report that in-
stalling traces according to their metamodel is very expensive,5

but they do not report short-term benefits or the specific utility
of traceability in reengineering.

Design decision support
A great deal of literature on design rationale exists,5,6 most

of it proposing quite elaborate argumentation structures. Bala-
subramaniam Ramesh and Vasant Dhar, for example, present
a conceptual model for recording discussions and decisions
about requirements in a network of issues, positions, and ar-
guments connected by special links.6 We could easily repre-
sent such structures using hypertext in the RETH tool. They
would have been too expensive to maintain under the Trade
Bridge project conditions, however. As a practical compro-
mise, we included a simple representation for design rationale
and integrated it with the traces.

References
1. G. De Michelis et al., “A Three-Faceted View of Information Systems,”

Comm. ACM, vol. 41, no. 12, Dec. 1998, pp. 64–70.
2. R. Dömges and K. Pohl, “Adapting Traceability Environments to Project-

Specific Needs,” Comm. ACM, vol. 41, no. 12, Dec. 1998, pp. 54–62.
3. R.F. Flynn and M. Dorfman, “The Automated Requirements Traceability Sys-

tem (ARTS): An Experience of Eight Years,” in R.H. Thayer and M. Dorf-
man, eds., System and Software Requirements Eng., IEEE CS Press, Los
Alamitos, Calif., 1990, pp. 423–438.

4. F.A.C. Pinheiro and J.A. Goguen, “An Object-Oriented Tool for Tracing Re-
quirements,” IEEE Software, vol. 13, no. 2, Mar. 1996, pp. 52–64.

5. B. Ramesh et al., “Requirements Traceability: Theory and Practice,” Annals
Software Eng., vol. 3, Sept. 1997, pp. 397–415.

6. B. Ramesh and V. Dhar, “Supporting Systems Development by Capturing
Deliberations During Requirements Engineering,” IEEE Trans. Software Eng.,
vol. 18, no. 6, June 1992, pp. 498–510.

Related Work on Traceability 



the old and new software had to remain
outside this tool, which made it infeasible to
install RETH hyperlinks to or from source
code. To compensate, we use traceability

tags between design artifacts and source
code (see Figure 1).

A traceability tag in our case study is sim-
ply a hexadecimal number surrounded by

M a y / J u n e  2 0 0 2 I E E E  S O F T W A R E 7 3

+considers

+fulfills

<<stereotype>>
Functional requirement

+constrains

<<stereotype>>
Scenario

+constrains

+makes possible

<<stereotype>>
Source code file

<<stereotype>>
Database script

+is achieved by

+achieves

Description

<<stereotype>>
Domain object

Statement
Further explanation
Source
Priority
Reason(s)

<<stereotype>>
Requirement

+conflicts with

+is similar to

<<stereotype>>
Design goal

Statement

Correspondence

Trace

Trace

<<stereotype>>
Design scenario

<<stereotype>>
Design object

DescriptionStatement
Alternatives considered

+justifies

+considers

Trace

Trace

<<stereotype>>
Design decision

<<stereotype>>
Design

Design chosen
Alternatives considered

+affects

+justifies  

Trace

Trace

<<stereotype>>
Requirements definition

Trace

<<stereotype>>
Implementation

Statement

<<stereotype>>
Goal

<<stereotype>>
Quality requirement

Figure 2. Metamodel
for representing 
requirements, 
design, and 
implementation 
and the connections 
between them. 
We install 
traces between 
requirements and 
design artifacts and
design and 
implementation 
artifacts. A 
ccoorrrreessppoonnddeennccee

association supports
additional traces 
between ddoommaaiinn 
oobbjjeeccttss and ddeessiiggnn
oobbjjeeccttss.

<<stereotype>>
Old and new system requirement

<<stereotype>>
Requirements definition

<<stereotype>>
New system requirement only

+replaces<<stereotype>>
Old system requirement only

Figure 3. Require-
ments change. In 
our metamodel, 
the requirements 
definition allows 
for common 
requirements as well
as requirements 
specific to the old 
or new system.



the special character ° (for example, °34F5°).
Inserting the same tag at all the places in the
various documents to be linked together can
implement binary as well as higher-order re-
lations, but a single tag by itself does not 
indicate traceability direction. From a prac-
tical viewpoint, this does not pose real prob-
lems because the metamodel clearly shows
the direction.

We implemented a simple tool to gener-
ate and then find unique tag names. We can
easily find tags in virtually any kind of doc-
ument as long as the corresponding MS
Windows tool provides full-text search.
Further support for renaming, deleting, and
checking tags might be desirable, but even
the simple tool support available in our
project was useful.

A real-world case study
Our case study involved the reengineer-

ing of Trade Bridge, a real-world legacy
software system that continuously checks
the consistency of received stock data and
distributes that data to other software sys-
tems. The old Trade Bridge system was un-
reliable, difficult to maintain, and based on
old technology. Because users wanted addi-
tional functionality, a new manager decided
to reengineer the system. The project, from
kick-off to full operation, lasted approxi-
mately one year.

During the project, developers installed
and used traceability in real time. Here, we
describe the task, the project’s basic reengi-
neering approach, and some experiences,
focusing exclusively on traceability and its
benefits for developers of the new software.

Figure 4 illustrates on an abstract level
how Trade Bridge interfaces with other soft-
ware systems. The front-office system deliv-
ers trade, position, and other data to Trade
Bridge, which stores the data in a repository
(implemented as a relational database). After
checking the data for consistency, Trade
Bridge sends the checked data to the mid-
office system, the risk-management system,
the ticket printer, and the back-office system.

Basic reengineering approach
Immediately after the reengineering proj-

ect began, the requirements engineer ac-
quired information about the old software
requirements from its developers. Fortu-
nately, this was possible, and it saved some
requirements and design recovery effort.
Later, when the developers of the old soft-
ware were no longer available, the reengi-
neering team had to recover everything (in-
cluding requirements). Our approach does
not prescribe a process that requires either
the original system developers or documen-
tation. Rather, it defines a metamodel for
representing useful information. 

The requirements engineer/chief designer
first represented the old software require-
ments in the RETH tool. In his design role,
he tried to figure out how these require-
ments related to the implementation. He
represented the resulting design information
and the design rationale—which he partly
acquired from the developers of the old sys-
tem and partly hypothesized—in the RETH
tool (compliant with the metamodel in Fig-
ure 2). During both reverse and forward en-
gineering, he immediately installed traces as
hypertext links and traceability tags. There
were immediate benefits to having these
traces available.

Handling requirements change
In the Trade Bridge project, traces facili-

tated the handling of requirements change
with respect to related design decisions. One
requirement, for example, concerned the ac-
cessibility of configuration parameters, which
influences their storage method in Trade

7 4 I E E E  S O F T W A R E M a y / J u n e  2 0 0 2

1: Query

2: Front-office data

Mid-office
system

Ticket
printer

Back-office
system

Control flow

Data flow

Front-office
system

Trade
Bridge

Risk-management
system

Figure 4. Trade Bridge collaborations with other software.
Trade Bridge receives data from the front-office system. It
checks this data for consistency, then sends it to the mid-
office, risk-management, and back-office systems and the
ticket printer.



Bridge. The chief designer reverse-engineered
the design decision, “storing configura-
tion parameters,” from the old Trade Bridge
software code, with the design chosen,
“configuration parameters are generally
stored in ASCII files, or, more precisely, in MS
Windows INI format.”

Fortunately, the old system developers
were still available for an interview, which
revealed the functional requirement (see
Table 1) that justifies this design. We
represented this requirement, the related de-
sign decision, and the link between them in
the RETH tool compliant with our meta-
model. No other requirements or design as-
pects appeared to be related to this func-
tional requirement.

Later, during forward engineering, users
of the new software required different func-
tionality. The requirements engineer formu-
lated the new functional requirement, which
Table 1 also shows. By this time, the devel-
opers of the old Trade Bridge software had
left the company and the project. Nonethe-

less, the RETH tool traceability link meant
that the old design decision was just a hy-
perlink away from the old requirement. 

In this example, the old and the new re-
quirements conflict. The new requirement
therefore replaces the original, which the
corresponding association in Figure 3 repre-
sents, thereby facilitating requirements
change traceability. A new quality require-
ment related to the configuration parameters
(see Table 1) also influenced the new design.

The requirements change made it impossi-
ble to keep the old design, which presumably
only the replaced requirement justified. No
other design aspect was related to the re-
placed requirement. Therefore, the new de-
sign decision (see Table 2) only considered the
new requirements (see Table 1), and it was
linked to them to facilitate traceability. As il-
lustrated in Table 2, the design chosen was
clearly preferable to the alternative design.

When requirements change, developers
must determine the consequences of this
change on the design. In reengineering,

M a y / J u n e  2 0 0 2 I E E E  S O F T W A R E 7 5

Table 1 
Conflicting requirements of the old and new software systems

Requirement Old system New system

Functional Easy access to configuration parameters: Restricted access to configuration parameters:
“Trade Bridge shall allow for easy change “Trade Bridge configuration parameters shall be accessible for change to
of its configuration parameters.” system administrators only.”

Quality Hierarchical structure of configuration parameters: 
“To give system administrators an overview of the configuration parameters, 
the parameters shall be represented and displayed hierarchically.” 

Table 2
Arguments for and against two designs for the new software

Design decision: Storing configuration parameters (new)

Design chosen: Alternative considered: 
Store configuration parameters in the MS Windows registry Store configuration parameters in a relational database
Pros Cons Pros Cons

For MS Windows NT (as used Providing context-sensitive online Context-sensitive online help Implementation of the database and access to
for Trade Bridge), only users help is difficult. could be easily provided. it would require extra effort.
with administration rights can 
change the registry.
Programs can easily Implementation of a hierarchical representation
access the registry. would be particularly expensive.
System administrators are The database would need extra administration
already familiar with the registry. during use.
The registry is already available.



knowing which design decisions are related
to replaced requirements is important. Devel-
opers should reconsider these decisions when
designing the new system. Knowing the ra-
tionale behind old decisions can usually help
them make better new decisions. In this way,
traces installed during reverse engineering
can assist developers during the forward-en-
gineering process. This is one example of the
short-term benefits of traceability.

In particular, this example shows how
the following features of our traceability ap-
proach can be useful.

� We establish traces during reverse engi-
neering (Figure 1).

� Requirements justify design decisions
(Figure 2).

� New requirements replace old ones
(Figure 3).

Diagnosing reimplementation errors
In the Trade Bridge project, traces also fa-

cilitated diagnosis of reimplementation er-
rors. After implementation of the new Trade
Bridge software, tests revealed a perform-
ance problem. The front-office system could
not deliver the amount of data the new soft-
ware requested—a problem that hadn’t oc-
curred with the old Trade Bridge software.

During redesign and reimplementation,
developers had used different names for ar-
tifacts and had distributed functionality dif-

ferently. Finding the corresponding parts in
the old and the new implementations could
therefore be tedious and time consuming.

The installed traces, however, made it easy
to find the corresponding parts in the old and
new implementations immediately using our
tool support for finding traceability tags, and
thus helped determine the problem. The old
software avoided the data-delivery problem
by requesting only five data records at a time
from the collaborating front-office system.
This fact was well hidden in the old proce-
dure’s code, as Figure 5 shows.

The critical condition is and (nLines < 5).
This is clearly below tag granularity, where
°41° is the procedure traceability tag. (In
several cases not reported in this article, the
granularity of installed traceability tags was
sufficient.) Having the traces in place let us
correctly diagnose the problem within one
hour; with another method, such as search-
ing artifact names, it might have taken days.
Thus, traces can also help solve problems re-
sulting from incomplete reimplementation.

In particular, this example shows how us-
ing our approach for all-around tracing in
reengineering can be useful: establishing
traces during reverse and forward engineer-
ing provides (derived) indirect traceability
between the old and new implementations
(see Figure 1).

Lessons learned 
Our experience with the Trade Bridge

reengineering effort made clear several ben-
efits of implementing traceability in both re-
verse and forward engineering.

� Traces to the reverse-engineered require-
ments and design information can facil-
itate new software development within
the same reengineering effort, if they are
installed immediately. 

� Although elaborate design rationale
might be too costly to include, simple
information on design decisions is feasi-
ble to maintain and trace in practice,
even under heavy time constraints, mak-
ing traceability even more useful.

� A trade-off in trace granularity exists. It
relates to the more general trade-off be-
tween the cost of installing and main-
taining traces and the benefit of having
them available. Common wisdom sug-
gests that traceability should pay off in

7 6 I E E E  S O F T W A R E M a y / J u n e  2 0 0 2

Figure 5. Code from the old procedure. The old software 
requested only five data records at a time from the 
collaborating front-office system.

function MakeInstrumentQuery(): string;

// determine which instrument data to request from the 

// front-office system and build the “where” part of 

// the query string °41°

begin

...

SqlExecute(‘select distinct ID into strID from 

Instrument_Get where tries < 10 order by tries’);

while SqlFetch() and (nLines < 5) do begin

strQuery = strQuery + strID + ‘, ‘;

nLines = nLines + 1;

end;

if (nLines > 0) then

...

end;



the long run, which would include main-
tenance. 

Unfortunately, whether a software devel-
opment or reengineering project provides
traces to facilitate later maintenance does
not usually determine its success or failure
in practice. Budget and time constraints are
major issues in any project. Project budgets
often cover only the cost of delivering a run-
ning system; costs arising later due to miss-
ing traces are outside their scope. From a
higher perspective, this is both shortsighted
and regrettable. 

In addition, many software systems are
developed but never deployed, so develop-
ers focus on delivering running systems on
time. Preparing for an uncertain future is
less important: other people will be respon-
sible for any maintenance costs.

We therefore argue for a distinction be-
tween immediate and long-term benefits.
Although we cannot provide quantitative
data to show that traceability paid off
within the Trade Bridge reengineering ef-
fort, it did provide benefits over the course
of the project. 

In fact, other cases in the Trade Bridge
project also support traceability’s usefulness
in the short term. These cases were particu-
larly related to

� Additional requirements
� Software design changes 
� Improved code completeness
� Iterative and incremental reverse engi-

neering

I n all cases, developers were immedi-ately rewarded for their trace installa-tion effort, which further motivated
them to do this “extra” work. This situa-
tion is different from most development ef-
forts with separate maintenance. Whenever
a single team is responsible for all aspects of
a reengineering project, the team can
quickly harvest the fruit of “seeding” traces.
Such short-term benefits can motivate them

to provide traces that might also result in
further long-term benefits later, possibly for
other people than the developers.

Acknowledgments
We thank the project members involved in the

Trade Bridge reengineering effort for their coopera-
tion. Mario Hailing and Vahan Harput provided use-
ful comments on an earlier draft of the article. Mark
Sinclair made grammatical corrections. Finally, we ac-
knowledge the very useful comments of the anony-
mous reviewers, who identified several weaknesses in
an earlier version of this article.

References
1. E.J. Chikofsky and J.H. Cross, “Reverse Engineering

and Design Recovery: A Taxonomy,” IEEE Software,
vol. 7, no. 1, Jan./Feb. 1990, pp. 13–17.

2. H. Kaindl, “A Practical Approach to Combining Re-
quirements Definition and Object-Oriented Analysis,”
Annals Software Eng., vol. 3, Sept. 1997, pp. 319–343.

3. H. Kaindl, “A Design Process Based on a Model Com-
bining Scenarios with Goals and Functions,” IEEE
Trans. Systems, Man, and Cybernetics (SMC), Part A,
vol. 30, no. 5, Sept. 2000, pp. 537–551.

4. H. Kaindl, “Difficulties in the Transition from OO
Analysis to Design,” IEEE Software, vol. 16, no. 5,
Sept./Oct. 1999, pp. 94–102.

5. H. Kaindl, S. Kramer, and P.S.N. Diallo, “Semiauto-
matic Generation of Glossary Links: A Practical Solu-
tion,” Proc. 10th ACM Conf. Hypertext and Hyperme-
dia (Hypertext 99), ACM Press, New York, 1999, pp.
3–12.

For more information on this or any other computing topic, please visit our
digital library at http://computer.org/publications/dlib.

M a y / J u n e  2 0 0 2 I E E E  S O F T W A R E 7 7

About the Authors

Gerald Ebner is managing partner of Significant Software, an Austrian consulting and
software development company that focuses on e-business and MIS projects, as well as scien-
tific software. His research interests include requirements engineering with a focus on trace-
ability and the documentation of design rationale. He received the Dipl.-Ing. in electrical engi-
neering from the Technical University of Vienna. He is a member of the ACM. Contact him at
Significant Software, Zeltgasse 14, A–1080 Vienna, Austria; ebner@significantsoftware.com.

Hermann Kaindl is a senior consultant at Siemens AG Österreich, PSE. His research in-
terests include software engineering, with a focus on requirements engineering; human-com-
puter interaction as it relates to scenario-based design and hypertext; and artificial intelligence,
including heuristic search and knowledge-based systems. He received the Dipl.-Ing. in computer
science and his doctoral degree in technical science, both from the Technical University of Vi-
enna, where he has lectured since 1984 and served as an adjunct professor since 1989. He is a
senior member of the IEEE and a member of the ACM, and is on the executive board of the
Austrian Society for Artificial Intelligence. Contact him at Siemens AG Österreich, PSE,
Geusaugasse 17, A–1030 Vienna, Austria; hermann.kaindl@siemens.com.



7 8 I E E E  S O F T W A R E M a y / J u n e  2 0 0 2 0 7 4 0 - 7 4 5 9 / 0 2 / $ 1 7 . 0 0  ©  2 0 0 2  I E E E

“architectural mismatch.”2 However, the
absence of a clear business model also con-
tributes to the lack of profitability (as Butler
Lampson mentioned in his presentation at
the 21st International Conference on Soft-
ware Engineering). This article discusses
how software patents can support the soft-
ware component business model and sup-
press multiple standards. These standards
are due to marketing strategies in which
each vendor promotes standardization as
long as the standard is its own.

We advocate neither the COTS approach
nor software patents per se. Others have ad-
vanced or critiqued both positions (see the
“Further Reading” sidebar). Rather, we fo-
cus on the interaction between patents and
components. We argue that software
patents, now a permanent facet of the busi-
ness landscape though not yet universally
employed, can support the commercial

adoption of component-based software.
This salutary effect of software patents can-
not be demonstrated within one organiza-
tion or for one product. It is a systemic
change that will take place only as the entire
patent system develops further and is ap-
plied regularly to software components.

Why patent software?
The three major mechanisms that US law

offers to protect intellectual property are
trade secrets, copyrights, and patents. (In
this article, we consider primarily US law.
With increasing globalization, however, the
laws of other countries and the interna-
tional agreements that attempt to reconcile
them are becoming increasingly relevant.) 

A trade secret is any information a busi-
ness uses that is not generally known. The
computer industry has traditionally relied
on trade secret protection of the source

feature
How Software Patents Can
Support COTS Component
Business

Michael S. Guntersdorfer, Duke University

David G. Kay, University of California, Irvine

Commercial-off-the-shelf software components are an important technology for software

Commercial off-
the-shelf software
components 
are an important
technology 
for software
development. This
article discusses
how software
patents can
strengthen COTS
software’s weak
business model 
and lead to
improvements in 
existing technology.

S
oftware components are on the rise. They form the basis of many
well-designed modern software systems, provide a suitable level
of granularity for software reuse, and are even a sales unit them-
selves, traded off-the-shelf like complete software systems.1

However, compared to its potential, component vending is still unprofitable.
The software community appears to blame this on missing, insufficient, 
or incompatible interaction standards, leading to the oft-cited problem of

software components



code for mass-market software by selling
that software only in object code form (pos-
sibly encrypted to hamper reverse engineer-
ing by decompilation or disassembly). Trade
secret law prohibits people with access to
the secret from divulging that secret without
authorization. However, it does not protect
against the independent discovery or cre-
ation of the same information, or even
against reverse engineering the information
from publicly available sources. Trade se-
crets are a method of intellectual property
protection that is hard to suppress. Deliver-
ing only object code hinders innovation by
hiding innovative concepts. This is the an-
tithesis of reuse.

Copyrights protect original works of au-
thorship against copying—works such as
prose, drama, music, art, and software,
whether published or unpublished. A copy-
right does not protect against independent
creation of the same information. It does
not protect the underlying ideas in a work;
it only protects the particular way those
ideas are expressed. This protection goes be-
yond the literal; copyright also protects de-
rivative works, such as those translated to
other programming languages or platforms.

Similarity clearly indicates copying in cre-
ative works such as fiction and music, where
most of the work is determined only by the
author’s creativity and is subject to few prac-
tical constraints. But for works with a func-
tional aspect, such as software, that func-
tionality is part of the work’s underlying
idea and is not subject to copyright protec-
tion. Software with similar functionality to a
copyrighted work, but created independ-
ently, does not infringe. Reliance on copy-
right protection alone does nothing to cur-
tail independent development of functional
equivalents. Moreover, the copyright term—
nearly a century—is completely foreign to
the time scale of software development.

Trade secrets and copyrights protect pri-
marily the creator’s lead time, forcing subse-
quent implementers to do their own work
rather than piggyback on the creator’s effort.
Stemming the proliferation of similar com-
ponents requires stronger, deeper protection.

Patents protect inventions, not just spe-
cific embodiments or implementations. A US
patent gives an inventor up to 20 years from
the filing date (subject to certain adjust-
ments) to exclude others from making, us-

ing, offering for sale, or selling the invention.
As a practical matter, this gives the patent
holder control over how others employ the
invention. Rather than simply barring all
others, the patent holder will typically grant
licenses on payment of a royalty. These li-
censes are contracts that can restrict modifi-
cation, require adherence to standards, or
otherwise further the inventor’s aims.

To receive this protection, an invention
must be novel and not merely an obvious ex-
tension of the “prior art” (inventions that
are already patented, published, in use, or
known). The invention must also involve the
appropriate subject matter: “any new and
useful process, machine, [article of] manu-
facture, or composition of matter, or any
new and useful improvement thereof.”3

Patent law does not apply to laws of nature,
scientific phenomena, or mathematical for-
mulas,4,5 because awarding exclusivity to
such concepts (which arguably have always
existed) would grant control over the physi-
cal world that individuals should not rea-
sonably exercise.

Patents can protect truly novel software
ideas from unfair exploitation, while releas-
ing the knowledge itself to the community.
They work against secrecy and support sci-
entific progress, which, in the case of soft-
ware, includes software reuse. Historically,
patents have worked with new technologies
in this way, and we would not expect soft-
ware to be an exception.

How software became patentable
Early US Supreme Court decisions ruled

that software consists essentially of mathe-
matical algorithms and was thus not
patentable.6,7

Between 1972 and 1981, the computer
industry developed rapidly. Without the US

M a y / J u n e  2 0 0 2 I E E E  S O F T W A R E 7 9

Information on commercial-off-the-shelf components is available at
wwwsel.iit.nrc.ca/projects/cots/icse2000wkshp. For the pros and cons of
software patents, we recommend these publications:

� L. Graham, “Debunking Software Patent Myths,” IEEE Software, vol.
17, no. 4, July/Aug. 2000, pp. 122–123. 

� P. Heckel, “Debunking the Software Patent Myths,” Comm. ACM, vol.
35, no. 6, June 1992, pp. 121–140.

� League for Programming Freedom, “Against Software Patents,” Comm.
ACM, vol. 35, no. 1, Jan. 1992, p. 17.

� T. O’Reilly, “The Internet Patent Land Grab,” Comm. ACM, vol. 43, no.
6, June 2000, pp. 29–31.

Further Reading



Patent Office acting during this period as a
repository for the prior art of software—ap-
plications, coding techniques, algorithms,
and data structures—ascertaining the nov-
elty of software patent applications was dif-
ficult. Results in the quality of issued
patents have been mixed, causing uncer-
tainty in the software industry. (Because the
20-year mark since 1981 has passed and the
repository of prior art is becoming more de-
veloped, we would expect these problems to
diminish.)

In 1981, the US Supreme Court ruled that
the Patent Office could not deny a patent
solely because it contained a computer pro-
gram.8 After that, the US Patent Office de-
veloped guidelines and began issuing patents
that were explicitly software-related.

Another milestone was cleared in 1998,
when the Court of Appeals for the Federal
Circuit held that business methods were
patentable on the same basis as other meth-
ods, and that mathematical algorithms can
be patented unless they represent only ab-
stract ideas.9 This overruled a district court
case equating software with mathematical
algorithms, essentially putting an end to
both the business method and the mathe-
matical algorithm exceptions.10

The 1990s also brought changes on in-
ternational grounds: Effective 1 January
1995, the Uruguay Round of negotiations
of the World Trade Organization revised the
General Agreement on Tariffs and Trade.
The WTO amended the Agreement on
Trade-Related Aspects of Intellectual Prop-
erty Rights (the TRIPS Agreement), which
regulates standards, enforcement, and dis-
pute settlement of intellectual property
rights, including patents, to make patents

available for any inventions, whether products
or processes, in all fields of technology, provided
that they are new, involve an inventive step, and
are capable of industrial application.11

The amended agreement also required that

patents…be available and patent rights enjoy-
able without discrimination as to the place of
invention, the field of technology, and whether
products are imported or locally produced.11

At the time of publication, the WTO had
144 member countries, all bound to its agree-
ments. (For a list of members, see www.wto.
org/english/thewto_e/whatis_e/tif_e/org6_e.

htm.) The TRIPS Agreement is therefore
likely to significantly affect intellectual prop-
erty protection worldwide. Similarities in the
wording of the GATT and US patent law are
apparent. So, it is reasonable to believe that
software patents eventually will be an ac-
cepted common practice internationally.

Why patent components?
Two main issues hinder component-

based software reuse. One is technical: Soft-
ware component interaction is implemented
by multiple standards. The other is eco-
nomic: It is difficult to identify and quantify
the exact economic benefit of developing
reusable software components. 

Multiple standards
The problem with multiple standards is

that a component must support several (or
all) of them or it will enjoy only a fraction
of the market. Unfortunately, no recovery is
in sight. The Object Management Group
promotes the continuously changing and
growing Corba specification.12 Sun Micro-
systems offers Java-based technologies, such
as Enterprise Java Beans (EJB) and Java 2
Enterprise Edition (J2EE).13 And Microsoft
Corporation has produced its own suite of
technologies, from OLE (Object Linking
and Embedding) via ActiveX14 to Compo-
nent Object Model and DCOM (Distrib-
uted COM)15 to its latest offering, SOAP
(Simple Object Access Protocol).16

Many of these technologies or “stan-
dards” use similar underlying methods. Java
RMI (remote method invocation) and Mi-
crosoft’s COM use remote procedure calls
(RPCs). COM also serves as the foundation
for ActiveX and OLE.17 Java Beans uses the
more decoupled and asynchronous ap-
proach of event-based interaction. Corba,
traditionally trying to subsume all technolo-
gies in one, stays ambivalent and offers both
RPC- and event-based interaction.

Some of these methods could have been
patentable along the lines of Sun Microsys-
tems’ patent for “methods for routing events
among publishers and subscribers on a com-
puter network,” which claims a “method …
for publishing and receiving events to and
from a network” (US Patent No. 6,021,443).
If these methods had been patented, the
patent holders could have used their influ-
ence to reduce the proliferation of different

In 1981, the US
Supreme Court
ruled that the
Patent Office

could not deny 
a patent solely

because it
contained 

a computer
program.

8 0 I E E E  S O F T W A R E M a y / J u n e  2 0 0 2



standards that use the same underlying meth-
ods. Merely imitating the patented technolo-
gies would have been infringement.

If there were an original inventor of the
concept of component-based interaction,
that inventor might even have patented the
general concept before particular interac-
tion technologies were introduced. The
patent holder could have licensed this
broader, conceptual invention to developers
of particular interaction technologies (who
would still be able to obtain patents for
their specific inventions). The patent holder
could then have influenced which technolo-
gies the developers introduced on the mar-
ket and in which field they introduced those
technologies.

Patenting successful methods might work
in two distinct, positive ways. First, com-
petitors might settle for licensing the
patented technology, concentrating on its
application rather than its reinvention in a
minimally different form. This would lead
to fewer competing standards (addressing
the technical issue). At the same time, the
patented technology would be used more
frequently, leading to more user testing,
more feedback, and, as a result, improved
implementation of the technology. Second,
competitors might decide to focus on devel-
oping truly innovative solutions patentable
in their own right, rather than merely incre-
mental tweaks. Both approaches could lead
to better scientific progress than the current
approach of imitating existing technology.

Economic benefits
Component development is not obvi-

ously profitable for two reasons.
First, a component’s development costs

increase or its target market decreases with
every additional, newly established interac-
tion technology. Fewer, better-differentiated
technologies, which patents would encour-
age (as we noted earlier), could ameliorate
this situation.

Second, factors in the software develop-
ment process make reuse unattractive. Pro-
ject management often chooses cheap reim-
plementation of existing functionality over
buying expensive licenses that might also reg-
ulate the use of the purchased software com-
ponent. Coders gratefully comply, because
they prefer designing from scratch to work-
ing their way through other programmers’

code. They also know that their importance
to management as individual coders increases
as the code’s idiosyncrasy increases. Depen-
dence on individual coders decreases, how-
ever, if the system’s code is an assembly of
standardized and reused components. 

An idiosyncratic software system’s infe-
rior quality, which causes higher mainte-
nance costs for the system later, is frequently
overlooked or ignored in initial develop-
ment. From the original developers’ view,
these costs affect the maintenance and prod-
uct support personnel rather than them-
selves. Software patents would prohibit an
unlicensed third party from reengineering a
protected software component’s functional-
ity. Moreover, license terms could require li-
censees to use the purchased component
without modifying its code. 

The idea to use software patents as a tool
to facilitate software reuse is not entirely
new.18 The argument essentially is that copy-
right-protected code (predominant in the
1980s) penalizes reuse but not reimplemen-
tation. Patents, however, prohibit unlicensed
reimplementation while supporting licensed
code reuse. If software patents support soft-
ware reuse in general, it is only rational to
believe that software patents will also sup-
port component-based reuse and, ultimately,
a viable software component business.

Arguments against patents
Software patents have been controversial

in the software community. Software devel-
opers often complain that honoring patents
would limit their creative freedom. Al-
though patents certainly impose constraints
on development, they also introduce a new
freedom. Today, the constraints of time-to-
market software development strategies se-
verely curtail the development process. The
results are lower-quality products. Patent
protection buys inventive programmers lead
time that lets them produce higher-quality
products using better but more time-con-
suming software engineering methods.

Because software was not patentable
early in its history, software patent oppo-
nents argue that technology has evolved dif-
ferently and that software, therefore, does
not fit the patent model. This argument will
certainly lose weight as time passes. One
hundred years from now, it won’t matter
that few software inventions were patented

M a y / J u n e  2 0 0 2 I E E E  S O F T W A R E 8 1

Software
developers

often complain
that honoring
patents would

limit their
creative
freedom.



in the 1970s. Even though wheels and axles
existed long before patents in any form, a
modern automobile contains hundreds of
patented inventions reflecting more recent
innovations. A particular technology such
as software need not “fit” the patent sys-
tem, because patents are designed to fit the
very concept of invention. 

Others object to the cost of patent in-
fringement actions. Lawsuits are indis-
putably expensive, but few infringement
questions proceed to the courtroom, much
less to a full-blown trial. In most cases, a
comparison of the patent and the allegedly
infringing work will produce a clear result
one way or the other. Rational business peo-
ple acting on the advice of counsel will not
pursue a hopeless case. Moreover, the jus-
tice system, by means of mandatory settle-
ment conferences and injunctions, summary
judgments, or other immediate relief, helps
curtail the irrational. The legal system,
whatever its costs and flaws, exists to re-
solve certain business disputes no matter
what form they take. Patent infringement
actions are not unique.

Some suggest eliminating intellectual
property protection for software entirely.
We could rely on open-source principles,
which would limit revenues to those gained
from maintenance and support. This model
has its attractions, but it has not been
proved viable for all segments of the soft-
ware market.

Software evolution
Software evolves quickly. By the time one

version of a product is shipped, developers
are already working on the next, less buggy,
better-performing, more feature-laden ver-
sion. The process of patent application and
issuance is slow in comparison. It currently
takes two to three years for a software
patent to issue; by that time, the software
invention might already be outdated. How
could this problem be addressed?

A patent already protects inventors in a
limited way immediately after it is filed, al-
though potential infringers cannot be prose-
cuted before the patent is granted. As soon as
an inventor files a patent application, it keeps
others from seeking a patent for the same in-
vention. For improvements, software inven-
tors simply file new patents, building on the
old ones. A typical example is Microsoft’s

“method of encoding arbitrary shapes” (US
Patent No. 5,764,814), which they later im-
proved by “separating the encoding of shape
and texture” (US Patent No. 5,946,419). An
inventor can secure legal protection at low
cost early in the development cycle with a
provisional patent application. Provisional
applications, first authorized in June 1995,
have few formal requirements for describing
the invention, and inventors can file them
quickly and inexpensively.

Software engineers can also expect legis-
lation to address their needs. The recently
established Patent Term Guarantee Act of
1999, for example, extends patent terms in
proportion to delays by the patent office—
an attempt to make the office process patent
applications quickly. Other proposals in-
clude lowering the burden of proof required
to invalidate patents in court and replacing
the US first-to-invent rule with the first-to-
file rule more prevalent overseas (defining
the inventor as whoever first files a patent
application for the invention). These steps
might seem small, but “they reflect an effort
to adjust the patent laws to keep pace with
the technology they protect.”19

I n the final analysis, any radical changein the current applicability of patentlaw to software is unlikely. Software
professionals should consider how to
achieve the most effective results within the
existing legal framework. (“Code reuse”
might be valued in the software community,
but in the legal community it is a core prin-
ciple.) By working with the legal profession
to supply the appropriate technical guid-
ance, software professionals can shape the
outcomes that will most benefit the entire
software industry.

Acknowledgments
We thank David Rosenblum of the Department of

Information and Computer Science at the University
of California, Irvine, for fruitful discussions regarding
the issues covered in this article. We also thank Jeffrey
Voas of Cigital for his input and support and for initi-
ating the contacts that led to this publication.

Software
professionals

should consider
how to achieve

the most
effective

results within
the existing

legal
framework.

8 2 I E E E  S O F T W A R E M a y / J u n e  2 0 0 2



References
1. V. Trass and J. van Hillegersberg, “The Software Com-

ponent Market on the Internet: Current Status and
Conditions for Growth,” Software Eng. Notes, vol. 25,
no. 1, Jan. 2000, pp. 114–117.

2. D. Garlan, R. Allen, and J. Ockerbloom, “Architectural
Mismatch or Why It’s Hard to Build Systems Out of Ex-
isting Parts,” Proc. 17th Int’l Conf. Software Eng., IEEE
CS Press, Los Alamitos, Calif., 1995, pp. 179–185.

3. Inventions Patentable, US Code, vol. 35, sec. 101, 1994.
4. Mackay Radio & Telegraph Co. v. Radio Corp. of

America, 306 US 86, 1939.
5. Funk Bros. Seed Co. v. Kalo Co., 333 US 127, 1948.
6. Gottschalk v. Benson, 409 US 63, 1972.
7. Parker v. Flook, 437 US 584, 1978.
8. Diamond v. Diehr, 450 US 175, 1981.
9. State Street Bank & Trust Co. v. Signature Financial

Group, 149 F. 3d 1368, Fed. Cir., 1998.
10. M. Chu and P. Wied, “Avoiding Patent Potholes,” (pre-

sented at 20th Ann. Computer Law Inst.), Univ. of
Southern California, Los Angeles, 1999.

11. General Agreement on Tariffs and Trade: Agreement on
Trade Related Aspects of Intellectual Property Rights,
Uruguay Round negotiations (1986–1994), World
Trade Organization, Geneva, 1 Jan. 1995, Article 27.1,
www.wto.org/english/docs_e/legal_e/27-trips.pdf.

12. T. Mowbray and R. Zahavi, The Essential CORBA:
Systems Integration Using Distributed Objects, John
Wiley & Sons, New York, 1995.

13. V. Matena and B. Stearns, Applying Enterprise Java
Beans, Addison-Wesley, Reading, Mass., 2000.

14. D. Chappell, Understanding ActiveX and OLE, Mi-
crosoft Press, Redmond, Wash., 1996.

15. S. Willliams and C. Kindel, The Component Object

Model: A Technical Overview, white paper, Microsoft
Corp., Redmond, Wash., 1994.

16. D. Chappell, “Simple Object Access Protocol (SOAP),”
white paper, Microsoft Corp., Redmond, Wash., 1999.

17. K. Sullivan, J. Socha, and M. Marchukov, “Using For-
mal Methods to Reason about Architectural Stan-
dards,” Proc. 19th Int’l Conf. Software Eng., IEEE CS
Press, Los Alamitos, Calif., 1997, pp. 503–513.

18. M. Lemley and D. O’Brien, “Encouraging Software
Reuse,” Stanford Law Rev., vol. 49, Jan. 1997, pp.
255–304.

19. L. Graham, “Recent Patent Reform Legislation,” IEEE
Software, vol. 17, no. 3, May/June 2000, pp. 101–103.

For more information on this or any other computing topic, please visit our
Digital Library at http://computer.org/publications/dlib.

M a y / J u n e  2 0 0 2 I E E E  S O F T W A R E 8 3

About the Authors

Michael S. Guntersdorfer is a Juris Doctor student at the Duke University School
of Law, where he serves as a staff writer on the Duke Law and Technology Review. He is al-
ways on the quest for synergetic effects between business, law, and software development and
is particularly interested in software processes, object-oriented programming, and component-
based reuse as well as business models and intellectual property law. He holds a Diplom-Infor-
matiker Universität from the Technical University of Munich and an MS in information and
computer science from the University of California, Irvine. Contact him at michael.guntersdor-
fer@law.duke.edu; www.duke.edu/~msg5.

David G. Kay is a lecturer in the Department of Information and Computer Science at the
University of California, Irvine. He teaches courses in computer law, programming languages,
introductory computer science, and the social impact of computing. He holds a BA in linguistics
and an MS in computer science from UCLA and a Juris Doctor from Loyola Law School. He is a
member of the ACM, American Bar Association, and Computer Law Association. Contact him at
the Department of Information and Computer Science, University of California, Irvine, CA
92697-3425; kay@uci.edu.

SET
INDUSTRY

STANDARDS

computer.org/standards/

HELP SHAPE FUTURE TECHNOLOGIES • JOIN A COMPUTER SOCIETY STANDARDS WORKING GROUP AT 

Computer Society members work together to define standards like 
IEEE 1003, 1394, 802, 1284, and many more.

Posix

FireWire
token rings

gigabit Ethernet

wireless
networks

enhanced parallel ports


