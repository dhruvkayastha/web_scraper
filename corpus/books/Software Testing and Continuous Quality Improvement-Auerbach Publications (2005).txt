***file:///C|/Documents and Settings/Administrator/桌面/tool/Cfg.txt***




































file:///C|/Documents and Settings/Administrator/桌面/tool/Cfg.txt


TEAM LinG



Software Testing and
Continuous Quality

Improvement

TEAM LinG



 

The Complete Project Management Office 
Handbook

 

Gerard M. Hill
0-8493-2173-5

 

Complex IT Project Management: 
16 Steps to Success

 

Peter Schulte
0-8493-1932-3

 

Creating Components: Object Oriented, 
Concurrent, and Distributed Computing 
in Java

 

Charles W. Kann
0-8493-1499-2

 

Dynamic Software Development: 
Manging Projects in Flux

 

Timothy Wells
0-8493-129-2

 

The Hands-On Project Office: 
Guaranteeing ROI and On-Time Delivery

 

Richard M. Kesner
0-8493-1991-9

 

Interpreting the CMMI®: A Process 
Improvement Approach 

 

Margaret Kulpa and Kent Johnson
0-8493-1654-5

 

Introduction to Software Engineering

 

Ronald Leach
0-8493-1445-3

 

ISO 9001:2000 for Software and Systems 
Providers: An Engineering Approach

 

Robert Bamford and William John Deibler II
0-8493-2063-1

 

The Laws of Software Process: 
A New Model for the Production 
and Management of Software 

 

Phillip G. Armour
0-8493-1489-5

 

Real Process Improvement Using 
the CMMI®

 

Michael West
0-8493-2109-3

 

Six Sigma Software Development

 

Christine Tanytor
0-8493-1193-4

 

Software Architecture Design Patterns 
in Java

 

Partha Kuchana
0-8493-2142-5

 

Software Configuration Management

 

Jessica Keyes
0-8493-1976-5

 

Software Engineering for Image 
Processing

 

Phillip A. Laplante
0-8493-1376-7

 

Software Engineering Handbook

 

Jessica Keyes
0-8493-1479-8

 

Software Engineering Measurement

 

John C. Munson
0-8493-1503-4

 

Software Engineering Processes: 
Principles and Applications

 

Yinxu Wang, Graham King, and Saba Zamir
0-8493-2366-5

 

Software Metrics: A Guide to Planning, 
Analysis, and Application

 

C.R. Pandian
0-8493-1661-8

 

Software Testing: A Craftsman’s 
Approach, 2e

 

Paul C. Jorgensen
0-8493-0809-7

 

Software Testing and Continuous Quality 
Improvement, Second Edition 

 

William E. Lewis
0-8493-2524-2

 

IS Management Handbook, 8th Edition

 

Carol V. Brown and Heikki Topi, Editors
0-8493-1595-9

 

Lightweight Enterprise Architectures

 

Fenix Theuerkorn 
0-9493-2114-X

 

AUERBACH PUBLICATIONS

 

www.auerbach-publications.com
To Order Call: 1-800-272-7737 • Fax: 1-800-374-3401

E-mail: orders@crcpress.com

 

Other CRC/Auerbach Publications in Software 
Development, Software Engineering, 

and Project Management

 

Series_A_master  Page 1  Friday, January 23, 2004  8:49 AM

TEAM LinG



AUERBACH PUBLICATIONS

A CRC Press Company

Boca Raton   London   New York   Washington, D.C.

Second Edition

William E. Lewis
Gunasekaran Veerapillai, Technical Contributor

Software Testing and
Continuous Quality

Improvement

TEAM LinG



 

This book contains information obtained from authentic and highly regarded sources. Reprinted material
is quoted with permission, and sources are indicated. A wide variety of references are listed. Reasonable
efforts have been made to publish reliable data and information, but the author and the publisher cannot
assume responsibility for the validity of all materials or for the consequences of their use.

Neither this book nor any part may be reproduced or transmitted in any form or by any means, electronic
or mechanical, including photocopying, microfilming, and recording, or by any information storage or
retrieval system, without prior permission in writing from the publisher.

The consent of CRC Press LLC does not extend to copying for general distribution, for promotion, for
creating new works, or for resale. Specific permission must be obtained in writing from CRC Press LLC
for such copying.

Direct all inquiries to CRC Press LLC, 2000 N.W. Corporate Blvd., Boca Raton, Florida 33431. 

 

Trademark Notice: 

 

Product or corporate names may be trademarks or registered trademarks, and are
used only for identification and explanation, without intent to infringe.

 

Visit the Auerbach Web site at www.auerbach-publications.com

 

© 2005 by CRC Press LLC 
Auerbach is an imprint of CRC Press LLC

No claim to original U.S. Government works
International Standard Book Number 0-8493-2524-2

Library of Congress Card Number 2004052492
Printed in the United States of America  1  2  3  4  5  6  7  8  9  0

 

Library of Congress Cataloging-in-Publication Data

 

Lewis, William E.
Software testing and continuous quality improvement / William E. Lewis ; Gunasekaran

    Veerapillai, technical contributor.--2nd ed.
p. cm.

Includes bibliographical references and index.
ISBN 0-8493-2524-2 (alk. paper)
  1. Computer software--Testing. 2. Computer software--Quality control.  I. Veerapillai,

    Gunasekaran. II. Title.

   QA76.76.T48L495 2004
   005.1

 

′

 

4--dc22
2004052492

TEAM LinG



 

v

 

About the Authors

 

William E. Lewis

 

 holds a B.A. in Mathematics and an M.S. in Operations
Research and has 38 years experience in the computer industry. Currently
he is the founder, president, and CEO of Smartware Technologies, Inc., a
quality assurance consulting firm that specializes in software testing. He is
the inventor of Test Smart

 

TM

 

, a patented software testing tool that creates
optimized test cases/data based upon the requirements (see www.smart-
waretechnologies.com for more information about the author).

He is a certified quality analyst (CQA) and certified software test engi-
neer (CSTE) sponsored by the Quality Assurance Institute (QAI) of
Orlando, Florida. Over the years, he has presented several papers to con-
ferences. In 2004 he presented a paper to QAI’s 

 

Annual International Infor-
mation Technology Quality Conference

 

, entitled “Cracking the Requirements/
Test Barrier.” He also speaks at meetings of the American Society for Qual-
ity and the Association of Information Technology Practitioners. 

Mr. Lewis was a quality assurance manager for CitiGroup where he man-
aged the testing group, documented all the software testing, quality assur-
ance processes and procedures, actively participated in the CitiGroup
CMM effort, and designed numerous WinRunner automation scripts.

Mr. Lewis was a senior technology engineer for Technology Builders,
Inc. of Atlanta, Georgia, where he trained and consulted in the require-
ments-based testing area, focusing on leading-edge testing methods and
tools.

Mr. Lewis was an assistant director with Ernst & Young, LLP, located in
Las Colinas, Texas. He joined E & Y in 1994, authoring the company’s soft-
ware configuration management, software testing, and application evolu-
tionary handbooks, and helping to develop the navigator/fusion methodol-
ogy application improvement route maps. He was the quality assurance
manager for several application development projects and has extensive
experience in test planning, test design, execution, evaluation, reporting,
and automated testing. He was also the director of the ISO initiative, which
resulted in ISO9000 international certification for Ernst & Young.

TEAM LinG



 

vi

 

Software Testing and Continuous Quality Improvement

 

Lewis also worked for the Saudi Arabian Oil Company (Aramco) in Jed-
dah, Saudi Arabia, on an overseas contract assignment as a quality assur-
ance consultant. His duties included full integration and system testing,
and he served on the automated tool selection committee and made rec-
ommendations to management. He also created software testing standards
and procedures.

In 1998 Lewis retired from IBM after 28 years. His jobs included 12 years
as a curriculum/course developer and instructor, and numerous years as a
system programmer/analyst and performance analyst. An overseas assign-
ment included service in Seoul, Korea, where he was the software engineer-
ing curriculum manager for the Korean Advanced Institute of Science and
Technology (KAIST), which is considered the MIT of higher education in
Korea. Another assignment was in Toronto, Canada, at IBM Canada’s head-
quarters, where he was responsible for upgrading the corporate education
program. In addition, he has traveled throughout the United States, Rome,
Amsterdam, Southampton, Hong Kong, and Sydney, teaching software devel-
opment and quality assurance classes with a specialty in software testing.

He has also taught at the university level for five years as an adjunct pro-
fessor. While so engaged he published a five-book series on computer prob-
lem solving. 

For further information about the training and consulting services pro-
vided by Smartware Technologies, Inc., contact:

 

Smartware Technologies, Inc.
2713 Millington Drive
Plano, Texas 75093
(972) 985-7546 

 

Gunasekaran Veerapillai

 

, a certified software quality analyst (CSQA), is
also a project management professional (PMP) from the PMI USA. After his
15 years of retail banking experience with Canara Bank, India he was man-
ager of the EDP section for 4 years at their IT department in Bangalore. He
was in charge of many critical, internal software development, testing, and
maintenance projects. He worked as project manager for testing projects
with Thinksoft Global Services, a company that specializes in testing in the
BFSI sector.

Currently Guna is working as project manager in the Testing Center of
Excellence of HCL Technologies (www.hcltechnologies.com), a level-5
CMM Company that has partnered itself with major test automation tool
vendors such as Mercury Interactive and IBM Rational. Guna has success-
fully turned out various testing projects for international bankers such as
Citibank, Morgan Stanley, and Discover Financial. He also contributes arti-
cles to software testing Web sites such as Sticky Minds.

TEAM LinG



 

vii

 

Contents

 

SECTION I SOFTWARE QUALITY IN PERSPECTIVE  . . . . . . . . . . . . . . . 1

1 Quality Assurance Framework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5

 

What Is Quality? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
Prevention versus Detection  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
Verification versus Validation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
Software Quality Assurance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
Components of Quality Assurance  . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
Software Testing. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
Quality Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
Software Configuration Management  . . . . . . . . . . . . . . . . . . . . . . . . . 12
Elements of Software Configuration Management. . . . . . . . . . . . . . . 12
Component Identification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
Version Control. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
Configuration Building. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
Change Control. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
Software Quality Assurance Plan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
Steps to Develop and Implement a Software Quality 
Assurance Plan. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16

Step 1. Document the Plan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
Step 2. Obtain Management Acceptance . . . . . . . . . . . . . . . . . . . . 18
Step 3. Obtain Development Acceptance . . . . . . . . . . . . . . . . . . . . 18
Step 4. Plan for Implementation of the SQA Plan  . . . . . . . . . . . . . 19
Step 5. Execute the SQA Plan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19

Quality Standards. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
ISO9000 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
Capability Maturity Model (CMM)  . . . . . . . . . . . . . . . . . . . . . . . . . 20

Level 1 — Initial. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
Level 2 — Repeatable. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
Level 3 — Defined . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
Level 4 — Managed. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
Level 5 — Optimized  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23

PCMM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
CMMI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
Malcom Baldrige National Quality Award  . . . . . . . . . . . . . . . . . . . 24

Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27

TEAM LinG



 

viii

 

Software Testing and Continuous Quality Improvement

 

2 Overview of Testing Techniques  . . . . . . . . . . . . . . . . . . . . . . . . . . 29

 

Black-Box Testing (Functional). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
White-Box Testing (Structural). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
Gray-Box Testing (Functional and Structural) . . . . . . . . . . . . . . . . . . 30

Manual versus Automated Testing . . . . . . . . . . . . . . . . . . . . . . . . . 31
Static versus Dynamic Testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31

Taxonomy of Software Testing Techniques  . . . . . . . . . . . . . . . . . . . . 32

 

3 Quality through Continuous Improvement Process . . . . . . . . . . . 41

 

Contribution of Edward Deming. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
Role of Statistical Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42

Cause-and-Effect Diagram. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
Flow Chart  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
Pareto Chart . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
Run Chart . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
Histogram . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
Scatter Diagram . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
Control Chart . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43

Deming’s 14 Quality Principles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
Point 1: Create Constancy of Purpose. . . . . . . . . . . . . . . . . . . . . . . 43
Point 2: Adopt the New Philosophy  . . . . . . . . . . . . . . . . . . . . . . . . 44
Point 3: Cease Dependence on Mass Inspection . . . . . . . . . . . . . . 44
Point 4: End the Practice of Awarding Business on Price 
Tag Alone  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
Point 5: Improve Constantly and Forever the System 
of Production and Service  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
Point 6: Institute Training and Retraining  . . . . . . . . . . . . . . . . . . . 45
Point 7: Institute Leadership  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
Point 8: Drive Out Fear . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
Point 9: Break Down Barriers between Staff Areas . . . . . . . . . . . . 46
Point 10: Eliminate Slogans, Exhortations, and Targets 
for the Workforce. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
Point 11: Eliminate Numerical Goals . . . . . . . . . . . . . . . . . . . . . . . . 47
Point 12: Remove Barriers to Pride of Workmanship . . . . . . . . . . 47
Point 13: Institute a Vigorous Program of Education and 
Retraining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
Point 14: Take Action to Accomplish the Transformation . . . . . . 48

Continuous Improvement through the Plan, Do, Check, 
Act Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
Going around the PDCA Circle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50

 

SECTION II LIFE CYCLE TESTING REVIEW  . . . . . . . . . . . . . . . . . . . . . 51

4 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53

 

Waterfall Development Methodology  . . . . . . . . . . . . . . . . . . . . . . . . . 53
Continuous Improvement “Phased” Approach  . . . . . . . . . . . . . . . . . 54

TEAM LinG



 

ix

 

Contents

 

Psychology of Life Cycle Testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54
Software Testing as a Continuous Improvement Process. . . . . . . . . 55
The Testing Bible: Software Test Plan . . . . . . . . . . . . . . . . . . . . . . . . . 58
Major Steps to Develop a Test Plan . . . . . . . . . . . . . . . . . . . . . . . . . . . 60

1. Define the Test Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60
2. Develop the Test Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60
3. Define the Test Environment . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60
4. Develop the Test Specifications  . . . . . . . . . . . . . . . . . . . . . . . . . 61
5. Schedule the Test  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
6. Review and Approve the Test Plan . . . . . . . . . . . . . . . . . . . . . . . 61

Components of a Test Plan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
Technical Reviews as a Continuous Improvement Process . . . . . . . 61
Motivation for Technical Reviews . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
Types of Reviews . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66
Structured Walkthroughs  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66
Inspections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66
Participant Roles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
Steps for an Effective Review . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70

1. Plan for the Review Process. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
2. Schedule the Review . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
3. Develop the Review Agenda. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
4. Create a Review Report  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71

 

5 Verifying the Requirements Phase . . . . . . . . . . . . . . . . . . . . . . . . . 73

 

Testing the Requirements with Technical Reviews . . . . . . . . . . . . . . 74
Inspections and Walkthroughs. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74
Checklists . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74
Methodology Checklist . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75
Requirements Traceability Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76
Building the System/Acceptance Test Plan  . . . . . . . . . . . . . . . . . . . . 76

 

6 Verifying the Logical Design Phase  . . . . . . . . . . . . . . . . . . . . . . . . 79

 

Data Model, Process Model, and the Linkage. . . . . . . . . . . . . . . . . . . 79
Testing the Logical Design with Technical Reviews  . . . . . . . . . . . . . 80
Refining the System/Acceptance Test Plan  . . . . . . . . . . . . . . . . . . . . 81

 

7 Verifying the Physical Design Phase  . . . . . . . . . . . . . . . . . . . . . . . 83

 

Testing the Physical Design with Technical Reviews . . . . . . . . . . . . 83
Creating Integration Test Cases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85
Methodology for Integration Testing. . . . . . . . . . . . . . . . . . . . . . . . . . 85

Step 1: Identify Unit Interfaces. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85
Step 2: Reconcile Interfaces for Completeness . . . . . . . . . . . . . . . 85
Step 3: Create Integration Test Conditions  . . . . . . . . . . . . . . . . . . 86
Step 4: Evaluate the Completeness of Integration Test 
Conditions  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86

TEAM LinG



 

x

 

Software Testing and Continuous Quality Improvement

 

8 Verifying the Program Unit Design Phase . . . . . . . . . . . . . . . . . . . 87

 

Testing the Program Unit Design with Technical Reviews . . . . . . . . 87
Sequence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87
Selection  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87
Iteration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87
Creating Unit Test Cases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88

 

9 Verifying the Coding Phase. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91

 

Testing Coding with Technical Reviews  . . . . . . . . . . . . . . . . . . . . . . . 91
Executing the Test Plan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91
Unit Testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92
Integration Testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93
System Testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93
Acceptance Testing  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94
Defect Recording  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95

SECTION III SOFTWARE TESTING METHODOLOGY. . . . . . . . . . . . . . 97

10 Development Methodology Overview . . . . . . . . . . . . . . . . . . . . . . 99
Limitations of Life Cycle Development . . . . . . . . . . . . . . . . . . . . . . . . 99
The Client/Server Challenge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
Psychology of Client/Server Spiral Testing. . . . . . . . . . . . . . . . . . . . 101

The New School of Thought . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
Tester/Developer Perceptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102
Project Goal: Integrate QA and Development  . . . . . . . . . . . . . . . 103
Iterative/Spiral Development Methodology . . . . . . . . . . . . . . . . . 104

Role of JADs  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106
Role of Prototyping  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
Methodology for Developing Prototypes  . . . . . . . . . . . . . . . . . . . . . 108

1. Develop the Prototype  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108
2. Demonstrate Prototypes to Management. . . . . . . . . . . . . . . . . 110
3. Demonstrate Prototype to Users. . . . . . . . . . . . . . . . . . . . . . . . 110
4. Revise and Finalize Specifications. . . . . . . . . . . . . . . . . . . . . . . 111
5. Develop the Production System  . . . . . . . . . . . . . . . . . . . . . . . . 111

Continuous Improvement “Spiral” Testing Approach . . . . . . . . . . . 112

11 Information Gathering (Plan) . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
Step 1: Prepare for the Interview . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117

Task 1: Identify the Participants  . . . . . . . . . . . . . . . . . . . . . . . . . . 117
Task 2: Define the Agenda. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118

Step 2: Conduct the Interview  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118
Task 1: Understand the Project . . . . . . . . . . . . . . . . . . . . . . . . . . . 118
Task 2: Understand the Project Objectives  . . . . . . . . . . . . . . . . . 121
Task 3: Understand the Project Status . . . . . . . . . . . . . . . . . . . . . 121
Task 4: Understand the Project Plans . . . . . . . . . . . . . . . . . . . . . . 122
Task 5: Understand the Project Development Methodology . . . 122
Task 6: Identify the High-Level Business Requirements . . . . . . . 123

TEAM LinG



xi

Contents

Task 7: Perform Risk Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124
Computer Risk Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124
Method 1 — Judgment and Instinct . . . . . . . . . . . . . . . . . . . . . 125
Method 2 — Dollar Estimation . . . . . . . . . . . . . . . . . . . . . . . . . 125
Method 3 — Identifying and Weighting Risk Attributes. . . . . 125

Step 3: Summarize the Findings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126
Task 1: Summarize the Interview. . . . . . . . . . . . . . . . . . . . . . . . . . 126
Task 2: Confirm the Interview Findings  . . . . . . . . . . . . . . . . . . . . 127

12 Test Planning (Plan)  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129
Step 1: Build a Test Plan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130

Task 1: Prepare an Introduction  . . . . . . . . . . . . . . . . . . . . . . . . . . 130
Task 2: Define the High-Level Functional Requirements 
(in Scope) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
Task 3: Identify Manual/Automated Test Types  . . . . . . . . . . . . . 132
Task 4: Identify the Test Exit Criteria . . . . . . . . . . . . . . . . . . . . . . 133
Task 5: Establish Regression Test Strategy  . . . . . . . . . . . . . . . . . 134
Task 6: Define the Test Deliverables . . . . . . . . . . . . . . . . . . . . . . . 136
Task 7: Organize the Test Team . . . . . . . . . . . . . . . . . . . . . . . . . . . 137
Task 8: Establish a Test Environment . . . . . . . . . . . . . . . . . . . . . . 138
Task 9: Define the Dependencies. . . . . . . . . . . . . . . . . . . . . . . . . . 139
Task 10: Create a Test Schedule  . . . . . . . . . . . . . . . . . . . . . . . . . . 139
Task 11: Select the Test Tools  . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142
Task 12: Establish Defect Recording/Tracking Procedures . . . . 143
Task 13: Establish Change Request Procedures . . . . . . . . . . . . . 145
Task 14: Establish Version Control Procedures. . . . . . . . . . . . . . 147
Task 15: Define Configuration Build Procedures . . . . . . . . . . . . . 147
Task 16: Define Project Issue Resolution Procedures. . . . . . . . . 148
Task 17: Establish Reporting Procedures. . . . . . . . . . . . . . . . . . . 148
Task 18: Define Approval Procedures . . . . . . . . . . . . . . . . . . . . . . 149

Step 2: Define the Metric Objectives . . . . . . . . . . . . . . . . . . . . . . . . . 149
Task 1: Define the Metrics  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150
Task 2: Define the Metric Points  . . . . . . . . . . . . . . . . . . . . . . . . . . 151

Step 3: Review/Approve the Plan. . . . . . . . . . . . . . . . . . . . . . . . . . . . 154
Task 1: Schedule/Conduct the Review . . . . . . . . . . . . . . . . . . . . . 154
Task 2: Obtain Approvals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154

13 Test Case Design (Do)  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157
Step 1: Design Function Tests. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157

Task 1: Refine the Functional Test Requirements . . . . . . . . . . . . 157
Task 2: Build a Function/Test Matrix  . . . . . . . . . . . . . . . . . . . . . . 159

Step 2: Design GUI Tests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163
Ten Guidelines for Good GUI Design. . . . . . . . . . . . . . . . . . . . . . . 164
Task 1: Identify the Application GUI Components  . . . . . . . . . . . 165
Task 2: Define the GUI Tests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165

TEAM LinG



xii

Software Testing and Continuous Quality Improvement

Step 3: Define the System/Acceptance Tests  . . . . . . . . . . . . . . . . . . 167
Task 1: Identify Potential System Tests. . . . . . . . . . . . . . . . . . . . . 167
Task 2: Design System Fragment Tests . . . . . . . . . . . . . . . . . . . . . 168
Task 3: Identify Potential Acceptance Tests. . . . . . . . . . . . . . . . . 169

Step 4: Review/Approve Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169
Task 1: Schedule/Prepare for Review . . . . . . . . . . . . . . . . . . . . . . 169
Task 2: Obtain Approvals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169

14 Test Development (Do)  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173
Step 1: Develop Test Scripts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173

Task 1: Script the Manual/Automated GUI/Function Tests  . . . . 173
Task 2: Script the Manual/Automated System Fragment 
Tests  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173

Step 2: Review/Approve Test Development . . . . . . . . . . . . . . . . . . . 174
Task 1: Schedule/Prepare for Review . . . . . . . . . . . . . . . . . . . . . . 174
Task 2: Obtain Approvals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174

15 Test Coverage through Traceability . . . . . . . . . . . . . . . . . . . . . . . 177
Use Cases and Traceability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 180

16 Test Execution/Evaluation (Do/Check). . . . . . . . . . . . . . . . . . . . . 181
Step 1: Setup and Testing  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181

Task 1: Regression Test the Manual/Automated Spiral Fixes. . . 181
Task 2: Execute the Manual/Automated New Spiral Tests . . . . . 182
Task 3: Document the Spiral Test Defects  . . . . . . . . . . . . . . . . . . 183

Step 2: Evaluation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183
Task 1: Analyze the Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183

Step 3: Publish Interim Report . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 184
Task 1: Refine the Test Schedule . . . . . . . . . . . . . . . . . . . . . . . . . . 184
Task 2: Identify Requirement Changes . . . . . . . . . . . . . . . . . . . . . 185

17 Prepare for the Next Spiral (Act)  . . . . . . . . . . . . . . . . . . . . . . . . . 187
Step 1: Refine the Tests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187

Task 1: Update the Function/GUI Tests. . . . . . . . . . . . . . . . . . . . . 187
Task 2: Update the System Fragment Tests . . . . . . . . . . . . . . . . . 188
Task 3: Update the Acceptance Tests . . . . . . . . . . . . . . . . . . . . . . 189

Step 2: Reassess the Team, Procedures, and Test Environment. . . . 189
Task 1: Evaluate the Test Team . . . . . . . . . . . . . . . . . . . . . . . . . . . 189
Task 2: Review the Test Control Procedures . . . . . . . . . . . . . . . . 189
Task 3: Update the Test Environment . . . . . . . . . . . . . . . . . . . . . . 190

Step 3: Publish Interim Test Report . . . . . . . . . . . . . . . . . . . . . . . . . . 191
Task 1: Publish the Metric Graphics . . . . . . . . . . . . . . . . . . . . . . . 191

Test Case Execution Status . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191
Defect Gap Analysis  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191
Defect Severity Status. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191
Test Burnout Tracking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192

TEAM LinG



xiii

Contents

18 Conduct the System Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 195
Step 1: Complete System Test Plan . . . . . . . . . . . . . . . . . . . . . . . . . . 195

Task 1: Finalize the System Test Types. . . . . . . . . . . . . . . . . . . . . 195
Task 2: Finalize System Test Schedule . . . . . . . . . . . . . . . . . . . . . 197
Task 3: Organize the System Test Team . . . . . . . . . . . . . . . . . . . . 197
Task 4: Establish the System Test Environment . . . . . . . . . . . . . 197
Task 5: Install the System Test Tools  . . . . . . . . . . . . . . . . . . . . . . 200

Step 2: Complete System Test Cases . . . . . . . . . . . . . . . . . . . . . . . . . 200
Task 1: Design/Script the Performance Tests  . . . . . . . . . . . . . . . 200

Monitoring Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201
Probe Approach  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 202
Test Drivers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 202

Task 2: Design/Script the Security Tests  . . . . . . . . . . . . . . . . . . . 203
A Security Design Strategy  . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203

Task 3: Design/Script the Volume Tests . . . . . . . . . . . . . . . . . . . . 204
Task 4: Design/Script the Stress Tests  . . . . . . . . . . . . . . . . . . . . . 205
Task 5: Design/Script the Compatibility Tests. . . . . . . . . . . . . . . 206
Task 6: Design/Script the Conversion Tests. . . . . . . . . . . . . . . . . 206
Task 7: Design/Script the Usability Tests . . . . . . . . . . . . . . . . . . . 207
Task 8: Design/Script the Documentation Tests . . . . . . . . . . . . . 208
Task 9: Design/Script the Backup Tests . . . . . . . . . . . . . . . . . . . . 208
Task 10: Design/Script the Recovery Tests  . . . . . . . . . . . . . . . . . 209
Task 11: Design/Script the Installation Tests . . . . . . . . . . . . . . . . 209
Task 12: Design/Script Other System Test Types . . . . . . . . . . . . 210

Step 3: Review/Approve System Tests  . . . . . . . . . . . . . . . . . . . . . . . 211
Task 1: Schedule/Conduct the Review . . . . . . . . . . . . . . . . . . . . . 211
Task 2: Obtain Approvals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 212

Step 4: Execute the System Tests. . . . . . . . . . . . . . . . . . . . . . . . . . . . 212
Task 1: Regression Test the System Fixes  . . . . . . . . . . . . . . . . . . 212
Task 2: Execute the New System Tests . . . . . . . . . . . . . . . . . . . . . 213
Task 3: Document the System Defects . . . . . . . . . . . . . . . . . . . . . 213

19 Conduct Acceptance Testing. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215
Step 1: Complete Acceptance Test Planning  . . . . . . . . . . . . . . . . . . 215

Task 1: Finalize the Acceptance Test Types. . . . . . . . . . . . . . . . . 215
Task 2: Finalize the Acceptance Test Schedule . . . . . . . . . . . . . . 215
Task 3: Organize the Acceptance Test Team . . . . . . . . . . . . . . . . 215
Task 4: Establish the Acceptance Test Environment  . . . . . . . . . 217
Task 5: Install Acceptance Test Tools . . . . . . . . . . . . . . . . . . . . . . 218

Step 2: Complete Acceptance Test Cases . . . . . . . . . . . . . . . . . . . . . 218
Task 1: Subset the System-Level Test Cases  . . . . . . . . . . . . . . . . 218
Task 2: Design/Script Additional Acceptance Tests . . . . . . . . . . 219

Step 3: Review/Approve Acceptance Test Plan . . . . . . . . . . . . . . . . 219
Task 1: Schedule/Conduct the Review . . . . . . . . . . . . . . . . . . . . . 219
Task 2: Obtain Approvals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 220

TEAM LinG



xiv

Software Testing and Continuous Quality Improvement

Step 4: Execute the Acceptance Tests . . . . . . . . . . . . . . . . . . . . . . . . 220
Task 1: Regression Test the Acceptance Fixes. . . . . . . . . . . . . . . 220
Task 2: Execute the New Acceptance Tests . . . . . . . . . . . . . . . . . 220
Task 3: Document the Acceptance Defects  . . . . . . . . . . . . . . . . . 221

20 Summarize/Report Spiral Test Results . . . . . . . . . . . . . . . . . . . . . 223
Step 1: Perform Data Reduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223

Task 1: Ensure All Tests Were Executed/Resolved  . . . . . . . . . . . 223
Task 2: Consolidate Test Defects by Test Number  . . . . . . . . . . . 223
Task 3: Post Remaining Defects to a Matrix . . . . . . . . . . . . . . . . . 223

Step 2: Prepare Final Test Report . . . . . . . . . . . . . . . . . . . . . . . . . . . . 224
Task 1: Prepare the Project Overview. . . . . . . . . . . . . . . . . . . . . . 225
Task 2: Summarize the Test Activities. . . . . . . . . . . . . . . . . . . . . . 225
Task 3: Analyze/Create Metric Graphics. . . . . . . . . . . . . . . . . . . . 225

Defects by Function  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 225
Defects by Tester  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227
Defect Gap Analysis  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227
Defect Severity Status. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227
Test Burnout Tracking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227
Root Cause Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227
Defects by How Found . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 230
Defects by Who Found . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 230
Functions Tested and Not  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 230
System Testing Defect Types. . . . . . . . . . . . . . . . . . . . . . . . . . . 230
Acceptance Testing Defect Types . . . . . . . . . . . . . . . . . . . . . . . 232

Task 4: Develop Findings/Recommendations  . . . . . . . . . . . . . . . 232
Step 3: Review/Approve the Final Test Report . . . . . . . . . . . . . . . . . 233

Task 1: Schedule/Conduct the Review  . . . . . . . . . . . . . . . . . . . . . 233
Task 2: Obtain Approvals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 236
Task 3: Publish the Final Test Report . . . . . . . . . . . . . . . . . . . . . . 236

SECTION IV TEST PROJECT MANAGEMENT . . . . . . . . . . . . . . . . . . . 237

21 Overview of General Project Management . . . . . . . . . . . . . . . . . 239
Define the Objectives. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 239
Define the Scope of the Project  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 240
Identify the Key Activities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 240
Estimate Correctly . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 240
Design  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241
Manage People . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241

Leadership . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 242
Communication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 242
Solving Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 243
Continuous Monitoring . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 243
Manage Changes  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 243

TEAM LinG



xv

Contents

22 Test Project Management . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245
Understand the Requirements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 246
Test Planning  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 246
Test Execution  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247
Identify and Improve Processes. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247
Essential Characteristics of a Test Project Manager . . . . . . . . . . . . 248

Requirement Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 248
Gap Analysis. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 248
Lateral Thinking in Developing Test Cases  . . . . . . . . . . . . . . . . . 248
Avoid Duplication and Repetition . . . . . . . . . . . . . . . . . . . . . . . . . 249
Test Data Generation  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 249
Validate the Test Environment. . . . . . . . . . . . . . . . . . . . . . . . . . . . 249
Test to Destroy  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 249
Analyze the Test Results  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 249
Do Not Hesitate to Accept Help from Others. . . . . . . . . . . . . . . . 250
Convey Issues as They Arise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 250
Improve Communication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 250
Always Keep Updating Your Business Knowledge . . . . . . . . . . . 250
Learn the New Testing Technologies and Tools . . . . . . . . . . . . . 250
Deliver Quality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 251
Improve the Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 251
Create a Knowledge Base . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 251
Repeat the Success . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 251

23 Test Estimation  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 253
Finish-to-Start: (FS) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 255
Start-to-Start: (SS) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 255
Finish-to-Finish: (FF) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 255
Start-to-Finish (SF). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 255

Critical Activities for Test Estimation . . . . . . . . . . . . . . . . . . . . . . . . 255
Test Scope Document . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 256
Test Strategy  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 256
Test Condition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257
Test Case  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257
Test Script  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257
Execution/Run Plan. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257

Factors Affecting Test Estimation  . . . . . . . . . . . . . . . . . . . . . . . . . . . 257
Test Planning Estimation  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 258
Test Execution and Controlling Effort . . . . . . . . . . . . . . . . . . . . . . . . 259
Test Result Analysis. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 259
Effort Estimation — Model Project  . . . . . . . . . . . . . . . . . . . . . . . . . . 259

24 Defect Monitoring and Management Process  . . . . . . . . . . . . . . . 263
Defect Reporting  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 264
Defect Meetings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 265

TEAM LinG



xvi

Software Testing and Continuous Quality Improvement

Defect Classifications. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 265
Defect Priority. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 266
Defect Category  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 266

Defect Metrics  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 267

25 Integrating Testing into Development Methodology. . . . . . . . . . 269
Step 1. Organize the Test Team  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 270
Step 2. Identify Test Steps and Tasks to Integrate . . . . . . . . . . . . . . 270
Step 3. Customize Test Steps and Tasks . . . . . . . . . . . . . . . . . . . . . . 271
Step 4. Select Integration Points. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 271
Step 5. Modify the Development Methodology  . . . . . . . . . . . . . . . . 272
Step 6. Incorporate Defect Recording . . . . . . . . . . . . . . . . . . . . . . . . 272
Step 7. Train in Use of the Test Methodology. . . . . . . . . . . . . . . . . . 272

26 On-Site/Offshore Model  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 275
Step 1: Analysis. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 275
Step 2: Determine the Economic Tradeoffs. . . . . . . . . . . . . . . . . . . . 276
Step 3: Determine the Selection Criteria . . . . . . . . . . . . . . . . . . . . . . 276
Project Management and Monitoring  . . . . . . . . . . . . . . . . . . . . . . . . 276
Outsourcing Methodology  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 277

On-Site Activities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 278
Offshore Activities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 278

Implementing the On-Site/Offshore Model . . . . . . . . . . . . . . . . . . . . 279
Knowledge Transfer  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 279
Detailed Design  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 280
Milestone-Based Transfer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 280
Steady State  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 280
Application Management . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 280
Relationship Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 281

Standards . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282
Benefits of On-Site/Offshore Methodology . . . . . . . . . . . . . . . . . . . . 283

On-Site/Offshore Model Challenges. . . . . . . . . . . . . . . . . . . . . . . . 285
Out of Sight  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285
Establish Transparency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285
Security Considerations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285
Project Monitoring  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285
Management Overhead  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285
Cultural Differences  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285
Software Licensing  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285

The Future of Onshore/Offshore  . . . . . . . . . . . . . . . . . . . . . . . . . . . . 286

SECTION V MODERN SOFTWARE TESTING TOOLS  . . . . . . . . . . . . . 287

27 A Brief History of Software Testing . . . . . . . . . . . . . . . . . . . . . . . 289
Evolution of Automated Testing Tools  . . . . . . . . . . . . . . . . . . . . . . . 293

Static Capture/Replay Tools (without Scripting Language). . . . 294
Static Capture/Replay Tools (with Scripting Language). . . . . . . 294

TEAM LinG



xvii

Contents

Variable Capture/Replay Tools  . . . . . . . . . . . . . . . . . . . . . . . . . . . 295
Functional Decomposition Approach  . . . . . . . . . . . . . . . . . . . 295
Test Plan Driven (“Keyword”) Approach. . . . . . . . . . . . . . . . . 296

Historical Software Testing and Development Parallels . . . . . . . . . 298
Extreme Programming  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 299

28 Software Testing Trends . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 301
Automated Capture/Replay Testing Tools . . . . . . . . . . . . . . . . . . . . 301
Test Case Builder Tools. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 302
Advanced Leading-Edge Automated Testing Tools . . . . . . . . . . . . . 302
Advanced Leading-Edge Test Case Builder Tools  . . . . . . . . . . . . . . 304
Necessary and Sufficient Conditions. . . . . . . . . . . . . . . . . . . . . . . . . 304
Test Data/Test Case Generation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 305

Sampling from Production . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 305
Starting from Scratch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 306
Seeding the Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 306
Generating Data Based upon the Database . . . . . . . . . . . . . . . . . 307
Generating Test Data/Test Cases Based upon the 
Requirements. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 308

29 Taxonomy of Testing Tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 311
Testing Tool Selection Checklist  . . . . . . . . . . . . . . . . . . . . . . . . . . . . 311
Vendor Tool Descriptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 312
When You Should Consider Test Automation  . . . . . . . . . . . . . . . . . 312
When You Should NOT Consider Test Automation . . . . . . . . . . . . . 320

30 Methodology to Evaluate Automated Testing Tools  . . . . . . . . . . 323
Step 1: Define Your Test Requirements . . . . . . . . . . . . . . . . . . . . . . . 323
Step 2: Set Tool Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 323
Step 3a: Conduct Selection Activities for Informal 
Procurement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 324

Task 1: Develop the Acquisition Plan . . . . . . . . . . . . . . . . . . . . . . 324
Task 2: Define Selection Criteria  . . . . . . . . . . . . . . . . . . . . . . . . . . 324
Task 3: Identify Candidate Tools . . . . . . . . . . . . . . . . . . . . . . . . . . 324
Task 4: Conduct the Candidate Review  . . . . . . . . . . . . . . . . . . . . 325
Task 5: Score the Candidates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 325
Task 6: Select the Tool  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 325

Step 3b: Conduct Selection Activities for Formal 
Procurement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 326

Task 1: Develop the Acquisition Plan . . . . . . . . . . . . . . . . . . . . . . 326
Task 2: Create the Technical Requirements Document . . . . . . . 326
Task 3: Review Requirements  . . . . . . . . . . . . . . . . . . . . . . . . . . . . 326
Task 4: Generate the Request for Proposal  . . . . . . . . . . . . . . . . . 326
Task 5: Solicit Proposals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 326
Task 6: Perform the Technical Evaluation . . . . . . . . . . . . . . . . . . 327
Task 7: Select a Tool Source . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 327

TEAM LinG



xviii

Software Testing and Continuous Quality Improvement

Step 4: Procure the Testing Tool  . . . . . . . . . . . . . . . . . . . . . . . . . . . . 327
Step 5: Create the Evaluation Plan . . . . . . . . . . . . . . . . . . . . . . . . . . . 327
Step 6: Create the Tool Manager’s Plan . . . . . . . . . . . . . . . . . . . . . . . 328
Step 7: Create the Training Plan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 328
Step 8: Receive the Tool  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 328
Step 9: Perform the Acceptance Test. . . . . . . . . . . . . . . . . . . . . . . . . 329
Step 10: Conduct Orientation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 329
Step 11: Implement Modifications . . . . . . . . . . . . . . . . . . . . . . . . . . . 329
Step 12: Train Tool Users. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 329
Step 13: Use the Tool in the Operating Environment. . . . . . . . . . . . 330
Step 14: Write the Evaluation Report. . . . . . . . . . . . . . . . . . . . . . . . . 330
Step 15: Determine Whether Goals Have Been Met . . . . . . . . . . . . . 330

APPENDICES. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 331

A Spiral Testing Methodology  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 333

B Software Quality Assurance Plan . . . . . . . . . . . . . . . . . . . . . . . . . 343

C Requirements Specification  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 345

D Change Request Form . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 347

E Test Templates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 349
E1: Unit Test Plan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 349
E2: System/Acceptance Test Plan  . . . . . . . . . . . . . . . . . . . . . . . . . . . 349
E3: Requirements Traceability Matrix . . . . . . . . . . . . . . . . . . . . . . . . 351
E4: Test Plan (Client/Server and Internet Spiral Testing) . . . . . . . . 353
E5: Function/Test Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 355
E6: GUI Component Test Matrix 
(Client/Server and Internet Spiral Testing). . . . . . . . . . . . . . . . . . . . 355
E7: GUI-Based Functional Test Matrix 
(Client/Server and Internet Spiral Testing). . . . . . . . . . . . . . . . . . . . 356
E8: Test Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 357
E9: Test Case Log . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 357
E10: Test Log Summary Report  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 359
E11: System Summary Report. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 361
E12: Defect Report . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 362
E13: Test Schedule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 364
E14: Retest Matrix  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 366
E15: Spiral Testing Summary Report 
(Client/Server and Internet Spiral Testing). . . . . . . . . . . . . . . . . . . . 368
E16: Minutes of the Meeting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 368
E17: Test Approvals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 370
E18: Test Execution Plan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 371
E19: Test Project Milestones . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 372
E20: PDCA Test Schedule. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 373
E21: Test Strategy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 374

TEAM LinG



xix

Contents

E22: Clarification Request . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 377
E23: Screen Data Mapping. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 378
E24: Test Condition versus Test Case . . . . . . . . . . . . . . . . . . . . . . . . 379
E25: Project Status Report  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 380
E26: Test Defect Details Report  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 381
E27: Defect Report . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 383
E28: Test Execution Tracking Manager . . . . . . . . . . . . . . . . . . . . . . . 383
E29: Final Test Summary Report  . . . . . . . . . . . . . . . . . . . . . . . . . . . . 384

F Checklists . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 387
F1: Requirements Phase Defect Checklist. . . . . . . . . . . . . . . . . . . . . 388
F2: Logical Design Phase Defect Checklist . . . . . . . . . . . . . . . . . . . . 389
F3: Physical Design Phase Defect Checklist . . . . . . . . . . . . . . . . . . . 390
F4: Program Unit Design Phase Defect Checklist . . . . . . . . . . . . . . . 393
F5: Coding Phase Defect Checklist. . . . . . . . . . . . . . . . . . . . . . . . . . . 394
F6: Field Testing Checklist  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 396
F7: Record Testing Checklist  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 398
F8: File Test Checklist  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 400
F9: Error Testing Checklist . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 401
F10: Use Test Checklist . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 403
F11: Search Test Checklist  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 404
F12: Match/Merge Checklist . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 405
F13: Stress Test Checklist . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 407
F14: Attributes Testing Checklist . . . . . . . . . . . . . . . . . . . . . . . . . . . . 409
F15: States Testing Checklist  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 411
F16: Procedures Testing Checklist . . . . . . . . . . . . . . . . . . . . . . . . . . . 412
F17: Control Testing Checklist . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 413
F18: Control Flow Testing Checklist. . . . . . . . . . . . . . . . . . . . . . . . . . 418
F19: Testing Tool Selection Checklist  . . . . . . . . . . . . . . . . . . . . . . . . 419
F20: Project Information Gathering Checklist  . . . . . . . . . . . . . . . . . 421
F21: Impact Analysis Checklist . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 423
F22: Environment Readiness Checklist . . . . . . . . . . . . . . . . . . . . . . . 425
F23: Project Completion Checklist. . . . . . . . . . . . . . . . . . . . . . . . . . . 427
F24: Unit Testing Checklist . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 429
F25: Ambiguity Review Checklist . . . . . . . . . . . . . . . . . . . . . . . . . . . . 433
F26: Architecture Review Checklist . . . . . . . . . . . . . . . . . . . . . . . . . . 435
F27: Data Design Review Checklist  . . . . . . . . . . . . . . . . . . . . . . . . . . 436
F28: Functional Specification Review Checklist . . . . . . . . . . . . . . . . 437
F29: Prototype Review Checklist . . . . . . . . . . . . . . . . . . . . . . . . . . . . 442
F30: Requirements Review Checklist. . . . . . . . . . . . . . . . . . . . . . . . . 443
F31: Technical Design Review Checklist . . . . . . . . . . . . . . . . . . . . . . 447
F32: Test Case Preparation Review Checklist. . . . . . . . . . . . . . . . . . 449

G Software Testing Techniques  . . . . . . . . . . . . . . . . . . . . . . . . . . . . 451
G1: Basis Path Testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 451

PROGRAM: FIELD-COUNT. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 451

TEAM LinG



xx

Software Testing and Continuous Quality Improvement

G2: Black-Box Testing. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 452
Extra Program Logic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 453

G3: Bottom-Up Testing. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 453
G4: Boundary Value Testing  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 453

Numeric Input Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 454
Field Ranges. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 454

Numeric Output Data  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 454
Output Range of Values  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 454

Nonnumeric Input Data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 454
Tables or Arrays  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 454
Number of Items . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 454

Nonnumeric Output Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 454
Tables or Arrays  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 454
Number of Outputs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 454

GUI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 454
G5: Branch Coverage Testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 455

PROGRAM: FIELD-COUNT. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 455
G6: Branch/Condition Coverage Testing . . . . . . . . . . . . . . . . . . . . . . 455

PROGRAM: FIELD-COUNT. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 456
G7: Cause-Effect Graphing. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 456

Cause-Effect Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 457
Specification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 458

Causes. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 458
Effects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 458

G8: Condition Coverage. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 460
PROGRAM: FIELD-COUNT. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 460

G9: CRUD Testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 461
G10: Database Testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 461

Integrity Testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 461
Entity Integrity. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 462
Primary Key Integrity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 462
Column Key Integrity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 462
Domain Integrity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 463
User-Defined Integrity. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 463
Referential Integrity  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 463

Data Modeling Essentials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 464
What Is a Model? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 465
Why Do We Create Models? . . . . . . . . . . . . . . . . . . . . . . . . . . . . 465

Tables — A Definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 466
Table Names  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 467
Columns . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 467
Rows . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 467
Order. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 467

Entities — A Definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 467

TEAM LinG



xxi

Contents

Identification — Primary Key  . . . . . . . . . . . . . . . . . . . . . . . . . . . . 468
Compound Primary Keys. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 468
Null Values . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 468
Identifying Entities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 469
Entity Classes  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 469

Relationships — A Definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 470
Relationship Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 470
One-to-One. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 470
One-to-Many  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 472
Many-to-Many . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 473
Multiple Relationships . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 475
Entities versus Relationships . . . . . . . . . . . . . . . . . . . . . . . . . . 475

Attributes — A Definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 476
Domain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 477
Domain Names. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 478

Attributes versus Relationships . . . . . . . . . . . . . . . . . . . . . . . . 478
Normalization — What Is It? . . . . . . . . . . . . . . . . . . . . . . . . . . . 479
Problems of Unnormalized Entities . . . . . . . . . . . . . . . . . . . . . 479

Steps in Normalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 480
First Normal Form (1NF) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 480
Second Normal Form (2NF) . . . . . . . . . . . . . . . . . . . . . . . . . . . . 482
Third Normal Form (3NF)  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 484
Model Refinement. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 485
Entity Subtypes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 486
A Definition  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 486
Referential Integrity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 486

Dependency Constraints  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 488
Constraint Rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 488
Recursion. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 489
Using the Model in Database Design . . . . . . . . . . . . . . . . . . . . 491
Relational Design  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 491

G11: Decision Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 492
PROGRAM: FIELD-COUNT. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 492

G12: Desk Checking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 493
G13: Equivalence Partitioning  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 493

Numeric Input Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 494
Field Ranges. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 494

Numeric Output Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 494
Output Range of Values . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 494

Nonnumeric Input Data  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 494
Tables or Arrays . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 494
Number of Items . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 494

Nonnumeric Output Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 494
Tables or Arrays . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 494
Number of Outputs. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 494

TEAM LinG



xxii

Software Testing and Continuous Quality Improvement

G14: Exception Testing  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 494
G15: Free Form Testing  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 494
G16: Gray-Box Testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 495
G17: Histograms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 496
G18: Inspections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 496
G19: JADs. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 497
G20: Orthogonal Array Testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 498
G21: Pareto Analysis  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 499
G22: Positive and Negative Testing  . . . . . . . . . . . . . . . . . . . . . . . . . . 501
G23: Prior Defect History Testing. . . . . . . . . . . . . . . . . . . . . . . . . . . . 502
G24: Prototyping. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 502

Cyclic Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 502
Fourth-Generation Languages and Prototyping. . . . . . . . . . . . . . 503
Iterative Development Accounting . . . . . . . . . . . . . . . . . . . . . . . . 504
Evolutionary and Throwaway . . . . . . . . . . . . . . . . . . . . . . . . . . . . 504
Application Prototyping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 505
Prototype Systems Development  . . . . . . . . . . . . . . . . . . . . . . . . . 505
Data-Driven Prototyping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 505
Replacement of the Traditional Life Cycle . . . . . . . . . . . . . . . . . . 506
Early-Stage Prototyping  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 506
User Software Engineering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 507

G25: Random Testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 507
G26: Range Testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 507
G27: Regression Testing. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 509
G28: Risk-Based Testing. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 509
G29: Run Charts  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 510
G30: Sandwich Testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 510
G31: Statement Coverage Testing. . . . . . . . . . . . . . . . . . . . . . . . . . . . 511

PROGRAM: FIELD-COUNT. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 511
G32: State Transition Testing  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 511

PROGRAM: FIELD-COUNT. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 512
G33: Statistical Profile Testing  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 512
G34: Structured Walkthroughs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 512
G35: Syntax Testing  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 514
G36: Table Testing  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 514
G37: Thread Testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 515
G38: Top-Down Testing  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 515
G39: White-Box Testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 516

Bibliography. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 517

Glossary  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 523

Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 529

TEAM LinG



xxiii

Acknowledgments

I would like to express my sincere gratitude to Carol, my wife, who has
demonstrated Herculean patience and love in the preparation of this sec-
ond edition, and my mother and father, Joyce and Bill Lewis, whom I will
never forget.

I thank John Wyzalek, Senior Acquisitions Editor at Auerbach Publica-
tions, for recognizing the importance of developing a second edition of this
book, and Gunasekaran Veerapillai who was a technical contributor and
editor. He has demonstrated an in-depth knowledge of software testing
concepts and methodology.

Finally, I would like to thank the numerous software testing vendors who
provided descriptions of their tools in the section, “Modern Software Test-
ing Tools.”

TEAM LinG



TEAM LinG



xxv

Introduction

Numerous textbooks address software testing in a structured development
environment. By “structured” is meant a well-defined development cycle in
which discretely defined steps provide measurable outputs at each step. It
is assumed that software testing activities are based on clearly defined
requirements and software development standards, and that those stan-
dards are used to develop and implement a plan for testing. Unfortunately,
this is often not the case. Typically, testing is performed against changing,
or even wrong, requirements.

This text aims to provide a quality framework for the software testing
process in the traditional structured as well as unstructured environ-
ments. The goal is to provide a continuous quality improvement approach
to promote effective testing methods and provide tips, techniques, and
alternatives from which the user can choose.

The basis of the continuous quality framework stems from Edward Dem-
ing’s quality principles. Deming was the pioneer in quality improvement,
which helped turn Japanese manufacturing around. Deming’s principles
are applied to software testing in the traditional “waterfall” and rapid appli-
cation “spiral” development (RAD) environments. The waterfall approach
is one in which predefined sequential steps are followed with clearly
defined requirements. In the spiral approach, these rigid sequential steps
may, to varying degrees, be lacking or different.

Section I, Software Quality in Perspective, reviews modern quality assur-
ance principles and best practices. It provides the reader with a detailed
overview of basic software testing techniques, and introduces Deming’s con-
cept of quality through a continuous improvement process. The Plan, Do,
Check, Act (PDCA) quality wheel is applied to the software testing process.

The Plan step of the continuous improvement process starts with a def-
inition of the test objectives, or what is to be accomplished as a result of
testing. The elements of a test strategy and test plan are described. A test
strategy is a concise statement of how to meet the goals of testing and pre-
cedes test plan development. The outline of a good test plan is provided,
including an introduction, the overall plan, testing requirements, test pro-
cedures, and test plan details.

TEAM LinG



xxvi

Software Testing and Continuous Quality Improvement

The Do step addresses how to design or execute the tests included in
the test plan. A cookbook approach describes how to perform component,
integration, and system acceptance testing in a spiral environment.

The Check step emphasizes the importance of metrics and test report-
ing. A test team must formally record the results of tests and relate them to
the test plan and system objectives. A sample test report format is pro-
vided, along with several graphic techniques.

The Act step of the continuous improvement process provides guide-
lines for updating test cases and test scripts. In preparation for the next
spiral, suggestions for improving the people, process, and technology
dimensions are provided.

Section II, Life Cycle Testing Review, reviews the waterfall development
methodology and describes how continuous quality improvement can be
applied to the phased approach through technical reviews and software
testing. The requirements, logical design, physical design, program unit
design, and coding phases are reviewed. The roles of technical reviews and
software testing are applied to each. Finally, the psychology of software
testing is discussed.

Section III, Software Testing Methodology, contrasts the waterfall devel-
opment methodology with the rapid application spiral environment from a
technical and psychological point of view. A spiral testing approach is sug-
gested when the requirements are rapidly changing. A spiral methodology
is provided, broken down into parts, steps, and tasks, applying Deming’s
continuous quality improvement process in the context of the PDCA qual-
ity wheel. 

Section IV, Test Project Management, discusses the fundamental chal-
lenges of maintaining and improving existing systems. Software changes
are described and contrasted. Strategies for managing the maintenance
effort are presented, along with the psychology of the software mainte-
nance activity. A maintenance testing methodology is then broken down
into parts, steps, and tasks, applying Deming’s continuous quality improve-
ment process in the context of the PDCA quality wheel. 

Section V, Modern Software Testing Tools, provides a brief historical
perspective from the 1950s to date, and a preview of future testing tools.
Next, an overview of tools and guidelines for when to consider a testing
tool and when not to is described. It also provides a checklist for selecting
testing tools, consisting of a series of questions and responses. Examples
are given of some of the most popular products. Finally, a detailed method-
ology for evaluating testing tools is provided, ranging from the initial test
goals through training and implementation.

TEAM LinG



Section I
Software 
Quality in 

Perspective

TEAM LinG



2

SOFTWARE QUALITY IN PERSPECTIVE

The new century has started with a major setback to the information tech-
nology world, which was booming in the last decade of the twentieth cen-
tury. When the Year 2000 issues evaporated there was a major breakdown
in the growth of the IT industry. Hundreds of new companies vanished with
closures and mergers and a state of consolidation began to occur in the
industry. Major restructuring started happening in the multinational com-
panies across the globe. The last three years were a period of consolida-
tion and the one good thing that emerged is the importance of software
quality.

• The adventure of Web business has necessitated the importance of
sound, robust, reliable software else the hackers would vanish with
millions of dollars. So all businesses have started giving special
importance to their software quality assurance departments that
were hitherto a neglected sector.

• The Sarbanes−Oxley Act, the new legislation passed in reaction to
Enron, WorldCom, and similar fiascos, has greatly improved the
importance of QA and testing in the United States and the corporate
heads have started giving equal importance to their QA teams, which
were another previously neglected sector.

Software quality is something everyone wants. Managers know that they
want high quality, software developers know they want to produce a quality
product, and users insist that software work consistently and be reliable.

The American Software Quality (ASQ) Institute and Quality Assurance
Institute (QAI) have defined various quality assurance practices that are
being adopted by various organizations. Many organizations form software
quality assurance groups to improve and evaluate their software applica-
tions. However, there is no commonly accepted practice for quality assur-
ance. Thus the quality assurance groups in various organizations may per-
form different roles and may execute their planning using different
procedures. In some organizations, software testing is a responsibility of
that group. In others, software testing is the responsibility of the develop-
ment group or an independent organization.

Many software quality groups develop software quality assurance plans,
which are similar to test plans. However, a software quality assurance plan
may include a variety of activities beyond those included in a test plan.
Although the quality assurance plan encompasses the entire quality gam-
bit, the test plan is a part of the quality assurance plan that is one of the
quality control tools.

The objectives of this section are to:

• Define quality and its cost.
• Differentiate quality prevention from quality detection.
• Differentiate verification from validation.

TEAM LinG



3

• Outline the components of quality assurance.
• Outline common testing techniques.
• Describe how the continuous improvement process can be instru-

mental in achieving quality.

SOFTWARE QUALITY IN PERSPECTIVE

TEAM LinG



TEAM LinG



5

Part 1

Quality Assurance 
Framework

What Is Quality?

In Webster’s Dictionary, quality is defined as “the essential character of
something, an inherent or distinguishing character, degree or grade of
excellence.” If you look at the computer literature, you will see that there
are two generally accepted meanings of quality. The first is that quality
means “meeting requirements.” With this definition, to have a quality prod-
uct, the requirements must be measurable, and the product’s require-
ments will either be met or not met. With this meaning, quality is a binary
state; that is, it is a quality product or it is not. The requirements may be
very complete or they may be simple, but as long as they are measurable,
it can be determined whether quality has or has not been met. This is the
producer’s view of quality as meeting the producer’s requirements or spec-
ifications. Meeting the specifications becomes an end in itself.

Another definition of quality, the customer’s, is the one we use. With this
definition, the customer defines quality as to whether the product or ser-
vice does what the customer needs. Another way of wording it is “fit for
use.” There should also be a description of the purpose of the product, typ-
ically documented in a customer’s “requirements specification” (see
Appendix C, Requirements Specification, for more details). The require-
ments are the most important document, and the quality system revolves
around it. In addition, quality attributes are described in the customer’s
requirements specification. Examples include usability, the relative ease
with which a user communicates with the application; portability, the
capability of the system to be executed across a diverse range of hardware
architectures; and reusability, the ability to transfer software components
constructed in one software system into another.

Everyone is committed to quality; however, the following show some of
the confusing ideas shared by many individuals that inhibit achieving a
quality commitment:

• Quality requires a commitment, particularly from top management.
Close cooperation of management and staff is required in order to
make it happen.

TEAM LinG



6

SOFTWARE QUALITY IN PERSPECTIVE

• Many individuals believe that defect-free products and services are
impossible, and accept certain levels of defects as normal and
acceptable.

• Quality is frequently associated with cost, meaning that high quality
equals high cost. This is a confusion between quality of design and
quality of conformance.

• Quality demands requirement specifications in enough detail that
the products produced can be quantitatively measured against those
specifications. Many organizations are not capable or willing to
expend the effort to produce specifications at the level of detail
required.

• Technical personnel often believe that standards stifle their creativ-
ity, and thus do not abide by standards compliance. However, for
quality to happen, well-defined standards and procedures must be
followed.

Prevention versus Detection

Quality cannot be achieved by assessing an already completed product.
The aim, therefore, is to prevent quality defects or deficiencies in the first
place, and to make the products assessable by quality assurance mea-
sures. Some quality assurance measures include: structuring the develop-
ment process with a software development standard and supporting the
development process with methods, techniques, and tools. The undetec-
ted bugs in the software that caused millions of losses to business have
necessitated the growth of independent testing, which is performed by a
company other than the developers of the system.

In addition to product assessments, process assessments are essential
to a quality management program. Examples include documentation of
coding standards, prescription and use of standards, methods, and tools,
procedures for data backup, test methodology, change management,
defect documentation, and reconciliation.

Quality management decreases production costs because the sooner a
defect is located and corrected, the less costly it will be in the long run.
With the advent of automated testing tools, although the initial investment
can be substantial, the long-term result will be higher-quality products and
reduced maintenance costs.

The total cost of effective quality management is the sum of four compo-
nent costs: prevention, inspection, internal failure, and external failure.
Prevention costs consist of actions taken to prevent defects from occur-
ring in the first place. Inspection costs consist of measuring, evaluating,
and auditing products or services for conformance to standards and spec-
ifications. Internal failure costs are those incurred in fixing defective prod-
ucts before they are delivered. External failure costs consist of the costs of

TEAM LinG



7

Quality Assurance Framework

defects discovered after the product has been released. The latter can be
devastating because they may damage the organization’s reputation or
result in the loss of future sales.

The greatest payback is with prevention. Increasing the emphasis on
prevention costs reduces the number of defects that go to the customer
undetected, improves product quality, and reduces the cost of production
and maintenance.

Verification versus Validation

Verification is proving that a product meets the requirements specified
during previous activities carried out correctly throughout the develop-
ment life cycle, and validation checks that the system meets the cus-
tomer’s requirements at the end of the life cycle. It is a proof that the prod-
uct meets the expectations of the users, and it ensures that the executable
system performs as specified. The creation of the test product is much
more closely related to validation than to verification. Traditionally, soft-
ware testing has been considered a validation process, that is, a life cycle
phase. After programming is completed, the system is validated or tested
to determine its functional and operational performance.

When verification is incorporated into testing, testing occurs through-
out the development life cycle. For best results, it is good practice to com-
bine verification with validation in the testing process. Verification
includes systematic procedures of review, analysis, and testing, employed
throughout the software development life cycle, beginning with the soft-
ware requirements phase and continuing through the coding phase. Verifi-
cation ensures the quality of software production and maintenance. In
addition, verification imposes such an organized, systematic development
practice that the resulting program can be easily understood and evalu-
ated by an independent party.

Verification emerged about 20 years ago as a result of the aerospace
industry’s need for extremely reliable software in systems in which an
error in a program could cause mission failure and result in enormous time
and financial setbacks, or even life-threatening situations. The concept of
verification includes two fundamental criteria: the software must ade-
quately and correctly perform all intended functions, and the software
must not perform any function that either by itself or in combination with
other functions can degrade the performance of the entire system. The
overall goal of verification is to ensure that each software product devel-
oped throughout the software life cycle meets the customer’s needs and
objectives as specified in the software requirements document.

Verification also establishes tractability between the various sections of
the software documentation and the associated parts of the requirements

TEAM LinG



8

SOFTWARE QUALITY IN PERSPECTIVE

specification. A comprehensive verification effort ensures that all software
performance and quality requirements in the specification are adequately
tested and that the test results can be repeated after changes are installed.
Verification is a “continuous improvement process” and has no definite ter-
mination. It should be used throughout the system life cycle to maintain
configuration and operational integrity.

Verification ensures that the software functions as intended and has the
required attributes (e.g., portability), and increases the chances that the
software will contain few errors (i.e., an acceptable number in the final
product). It provides a method for closely monitoring the software devel-
opment project and provides management with a detailed status of the
project at any point in time. When verification procedures are used, man-
agement can be assured that the developers follow a formal, sequential,
traceable software development process, with a minimum set of activities
to enhance the quality of the system.

One criticism of verification is that it increases software development
costs considerably. When the cost of software throughout the total life
cycle from inception to the final abandonment of the system is considered,
however, verification actually reduces the overall cost of the software.
With an effective verification program, there is typically a four-to-one
reduction in defects in the installed system. Because error corrections can
cost 20 to 100 times more during operations and maintenance than during
design, overall savings far outweigh the initial extra expense.

Software Quality Assurance

A formal definition of software quality assurance is that it is the systematic
activities providing evidence of the fitness for use of the total software
product. Software quality assurance is achieved through the use of estab-
lished guidelines for quality control to ensure the integrity and prolonged
life of software. The relationships between quality assurance, quality con-
trol, the auditing function, and software testing are often confused.

Quality assurance is the set of support activities needed to provide ade-
quate confidence that processes are established and continuously
improved in order to produce products that meet specifications and are fit
for use. Quality control is the process by which product quality is com-
pared with applicable standards and the action taken when nonconfor-
mance is detected. Auditing is the inspection/assessment activity that ver-
ifies compliance with plans, policies, and procedures.

Software quality assurance is a planned effort to ensure that a software
product fulfills these criteria and has additional attributes specific to the
project, for example, portability, efficiency, reusability, and flexibility. It is
the collection of activities and functions used to monitor and control a

TEAM LinG



9

Quality Assurance Framework

software project so that specific objectives are achieved with the desired
level of confidence. It is not the sole responsibility of the software quality
assurance group but is determined by the consensus of the project man-
ager, project leader, project personnel, and users.

Quality assurance is the function responsible for managing quality. The
word “assurance” means that if the processes are followed, management
can be assured of product quality. Quality assurance is a catalytic function
that should encourage quality attitudes and discipline on the part of man-
agement and workers. Successful quality assurance managers know how to
make people quality conscious and to make them recognize the benefits of
quality to themselves and to the organization.

The objectives of software quality are typically achieved by following a
software quality assurance plan that states the methods the project will
employ to ensure the documents or products produced and reviewed at
each milestone are of high quality. Such an explicit approach ensures that
all steps have been taken to achieve software quality and provides manage-
ment with documentation of those actions. The plan states the criteria by
which quality activities can be monitored rather than setting impossible
goals, such as no software defects or 100 percent reliable software.

Software quality assurance is a strategy for risk management. It exists
because software quality is typically costly and should be incorporated
into the formal risk management of a project. Some examples of poor soft-
ware quality include:

• Delivered software frequently fails.
• Consequences of system failure are unacceptable, from financial to

life-threatening scenarios.
• Systems are often not available for their intended purpose.
• System enhancements are often very costly.
• Costs of detecting and removing defects are excessive.

Although most quality risks are related to defects, this only tells part of
the story. A defect is a failure to comply with a requirement. If the require-
ments are inadequate or even incorrect, the risks of defects are more per-
vasive. The result is too many built-in defects and products that are not
verifiable. Some risk management strategies and techniques include soft-
ware testing, technical reviews, peer reviews, and compliance verification.

Components of Quality Assurance

Most software quality assurance activities can be categorized into soft-
ware testing, that is, verification and validation, software configuration
management, and quality control. But the success of a software quality
assurance program also depends on a coherent collection of standards,
practices, conventions, and specifications, as shown in Exhibit 1.1.

TEAM LinG



10

SOFTWARE QUALITY IN PERSPECTIVE

Software Testing

Software testing is a popular risk management strategy. It is used to verify
that functional requirements were met. The limitation of this approach,
however, is that by the time testing occurs, it is too late to build quality into
the product. Tests are only as good as the test cases, but they can be
inspected to ensure that all the requirements are tested across all possible
combinations of inputs and system states. However, not all defects are dis-
covered during testing. Software testing includes the activities outlined in
this text, including verification and validation activities. In many organiza-
tions, these activities, or their supervision, are included within the charter
for the software quality assurance function. The extent to which personnel
independent of design and coding should participate in software quality
assurance activities is a matter of institutional, organizational, and project
policy.

The major purpose of verification and validation activities is to ensure
that software design, code, and documentation meet all the requirements
imposed on them. Examples of requirements include user requirements;
specifications derived from and designed to meet user requirements; code
review and inspection criteria; test requirements at the modular, sub-
system, and integrated software levels; and acceptance testing of the code
after it has been fully integrated with hardware.

During software design and implementation, verification helps deter-
mine whether the products of one phase of the software development life
cycle fulfill the requirements established during the previous phase. The
verification effort takes less time and is less complex when conducted
throughout the development process.

Exhibit 1.1. Quality Assurance Components

TEAM LinG



11

Quality Assurance Framework

Quality Control

Quality control is defined as the processes and methods used to monitor
work and observe whether requirements are met. It focuses on reviews and
removal of defects before shipment of products. Quality control should be
the responsibility of the organizational unit producing the product. It is
possible to have the same group that builds the product perform the qual-
ity control function, or to establish a quality control group or department
within the organizational unit that develops the product.

Quality control consists of well-defined checks on a product that are
specified in the product quality assurance plan. For software products,
quality control typically includes specification reviews, inspections of
code and documents, and checks for user deliverables. Usually, document
and product inspections are conducted at each life cycle milestone to dem-
onstrate that the items produced are within the criteria specified by the
software quality assurance plan. These criteria are normally provided in
the requirements specifications, conceptual and detailed design docu-
ments, and test plans. The documents given to users are the requirement
specifications, design documentation, results from the user acceptance
test, the software code, user guide, and the operations and maintenance
guide. Additional documents are specified in the software quality assur-
ance plan.

Quality control can be provided by various sources. For small projects,
the project personnel’s peer group or the department’s software quality
coordinator can inspect the documents. On large projects, a configuration
control board may be responsible for quality control. The board may
include the users or a user representative, a member of the software qual-
ity assurance department, and the project leader.

Inspections are traditional functions of quality control, that is, indepen-
dent examinations to assess compliance with some stated criteria. Peers
and subject matter experts review specifications and engineering work
products to identify defects and suggest improvements. They are used to
examine the software project for adherence to the written project rules at
a project’s milestones and at other times during the project’s life cycle as
deemed necessary by the project leader or the software quality assurance
personnel. An inspection may be a detailed checklist for assessing compli-
ance or a brief checklist to determine the existence of such deliverables as
documentation. A report stating the purpose of the inspection and the defi-
ciencies found goes to the project supervisor, project leader, and project
personnel for action.

Responsibility for inspections is stated in the software quality assur-
ance plan. For small projects, the project leader or the department’s qual-
ity coordinator can perform the inspections. For large projects, a member

TEAM LinG



12

SOFTWARE QUALITY IN PERSPECTIVE

of the software quality assurance group may lead an inspection performed
by an audit team, which is similar to the configuration control board men-
tioned previously. Following the inspection, project personnel are assigned
to correct the problems on a specific schedule.

Quality control is designed to detect and correct defects, whereas qual-
ity assurance is oriented toward preventing them. Detection implies flaws
in the processes that are supposed to produce defect-free products and
services. Quality assurance is a managerial function that prevents prob-
lems by heading them off, and by advising restraint and redirection.

Software Configuration Management

Software configuration management is concerned with labeling, tracking,
and controlling changes in the software elements of a system. It controls
the evolution of a software system by managing versions of its software
components and their relationships.

The purpose of software configuration management is to identify all the
interrelated components of software and to control their evolution
throughout the various life cycle phases. Software configuration manage-
ment is a discipline that can be applied to activities including software
development, document control, problem tracking, change control, and
maintenance. It can provide a high cost savings in software reusability
because each software component and its relationship to other software
components have been defined.

Software configuration management consists of activities that ensure
that design and code are defined and cannot be changed without a review
of the effect of the change itself and its documentation. The purpose of
configuration management is to control code and its associated documen-
tation so that final code and its description are consistent and represent
those items that were actually reviewed and tested. Thus, spurious, last-
minute software changes are eliminated.

For concurrent software development projects, software configuration
management can have considerable benefits. It can organize the software
under development and minimize the probability of inadvertent changes.
Software configuration management has a stabilizing effect on all software
when there is a great deal of change activity or a considerable risk of select-
ing the wrong software components.

Elements of Software Configuration Management

Software configuration management identifies a system configuration in
order to systematically control changes, maintain integrity, and enforce
tractability of the configuration throughout its life cycle. Components to be
controlled include planning, analysis, and design documents, source code,

TEAM LinG



13

Quality Assurance Framework

executable code, utilities, job control language (JCL), test plans, test
scripts, test cases, and development reports. The software configuration
process typically consists of four elements: software component identifica-
tion, software version control, configuration building, and software change
control, as shown in Exhibit 1.2.

Component Identification

A basic software configuration management activity is the identification of
the software components that make up a deliverable at each point of its
development. Software configuration management provides guidelines to
identify and name software baselines, software components, and software
configurations.

Software components go through a series of changes. In order to man-
age the development process, one must establish methods and name stan-
dards for uniquely identifying each revision. A simple way to name compo-
nent revisions is to use a series of discrete digits. The first integer could
refer to a software component’s external release number. The second inte-
ger could represent the internal software development release number.
The transition from version number 2.9 to 3.1 would indicate that a new
external release 3 has occurred. The software component version number
is automatically incremented when the component is checked into the soft-
ware library. Further levels of qualifiers could also be used as necessary,
such as the date of a new version.

A software configuration is a collection of software elements that com-
prise a major business function. An example of a configuration is the set of
program modules for an order system. Identifying a configuration is quite
similar to identifying individual software components. Configurations can
have a sequence of versions. Each configuration must be named in a way
that distinguishes it from others. Each configuration version must be dif-
ferentiated from other versions. The identification of a configuration must
also include its approval status and a description of how the configuration
was built.

Exhibit 1.2. Software Configuration Management

TEAM LinG



14

SOFTWARE QUALITY IN PERSPECTIVE

A simple technique for identifying a configuration is to store all its soft-
ware components in a single library or repository. The listing of all the
components can also be documented.

Version Control

As an application evolves over time, many different versions of its software
components are created, and there needs to be an organized process to
manage changes in the software components and their relationships. In
addition, there is usually the requirement to support parallel component
development and maintenance.

Software is frequently changed as it evolves through a succession of
temporary states called versions. A software configuration management
facility for controlling versions is a software configuration management
repository or library. Version control provides the tractability or history of
each software change, including who did what, why, and when.

Within the software life cycle, software components evolve, and at a cer-
tain point each reaches a relatively stable state. But as defects are cor-
rected and enhancement features are implemented, the changes result in
new versions of the components. Maintaining control of these software
component versions is called versioning.

A component is identified and labeled to differentiate it from all other
software versions of the component. When a software component is mod-
ified, both the old and new versions should be separately identifiable.
Therefore, each version, except for the initial one, has a predecessor. The
succession of component versions is the component’s history and tracta-
bility. Different versions also act as backups so that one can return to pre-
vious versions of the software.

Configuration Building

To build a software configuration one needs to identify the correct compo-
nent versions and execute the component build procedures. This is often
called configuration building.

A software configuration consists of a set of derived software compo-
nents. An example is executable object programs derived from source pro-
grams. Derived software components are correctly associated with each
source component to obtain an accurate derivation. The configuration
build model defines how to control the way derived software components
are put together.

The inputs and outputs required for a configuration build model include
the primary inputs such as the source components, the version selection
procedures, and the system model, which describes how the software

TEAM LinG



15

Quality Assurance Framework

components are related. The outputs are the target configuration and
respectively derived software components.

Software configuration management environments use different
approaches for selecting versions. The simplest approach to version selec-
tion is to maintain a list of component versions. Other approaches entail
selecting the most recently tested component versions, or those modified
on a particular date.

Change Control

Change control is the process by which a modification to a software com-
ponent is proposed, evaluated, approved or rejected, scheduled, and
tracked. Its basic foundation is a change control process, a component sta-
tus reporting process, and an auditing process.

Software change control is a decision process used in controlling the
changes made to software. Some proposed changes are accepted and
implemented during this process. Others are rejected or postponed, and
are not implemented. Change control also provides for impact analysis to
determine the dependencies.

Modification of a configuration has at least four elements: a change
request, an impact analysis of the change, a set of modifications and addi-
tions of new components, and a method for reliably installing the modifica-
tions as a new baseline (see Appendix D, Change Request Form, for more
details).

A change often involves modifications to multiple software components.
Therefore, a storage system that provides for multiple versions of a single
file is usually not sufficient. A technique is required to identify the set of
modifications as a single change. This is often called delta storage.

Every software component has a development life cycle. A life cycle con-
sists of states and allowable transitions between those states. When a soft-
ware component is changed, it should always be reviewed and frozen from
further modifications until a new version is created. The reviewing author-
ity must approve or reject the modified software component. A software
library holds all software components as soon as they are frozen and also
acts as a repository for approved components.

A derived component is linked to its source and has the same status as
its source. In addition, a configuration cannot have a more complete status
than any of its components, because it is meaningless to review a configu-
ration when some of the associated components are not frozen.

All components controlled by software configuration management are
stored in a software configuration library, including work products such as
business data and process models, architecture groups, design units,

TEAM LinG



16

SOFTWARE QUALITY IN PERSPECTIVE

tested application software, reusable software, and special test software.
When a software component is to be modified, it is checked out of the
repository into a private workspace. It evolves through many states, which
are temporarily beyond the scope of configuration management control.

When a change is completed, the component is checked into the library
and becomes a new software component version. The previous component
version is also retained.

Software Quality Assurance Plan

The software quality assurance (SQA) plan is an outline of quality mea-
sures to ensure quality levels within a software development effort. The
plan is used as a baseline to compare the actual levels of quality during
development with the planned levels of quality. If the levels of quality are
not within the planned quality levels, management will respond appropri-
ately as documented within the plan.

The plan provides the framework and guidelines for development of
understandable and maintainable code. These ingredients help ensure the
quality sought in a software project. An SQA plan also provides the proce-
dures for ensuring that quality software will be produced or maintained in-
house or under contract. These procedures affect planning, designing,
writing, testing, documenting, storing, and maintaining computer software.
It should be organized in this way because the plan ensures the quality of
the software rather than describing specific procedures for developing and
maintaining the software.

Steps to Develop and Implement a Software Quality Assurance Plan

Step 1. Document the Plan

The software quality assurance plan should include the sections below
(see Appendix B, Software Quality Assurance Plan, which contains a tem-
plate for the plan):

• Purpose Section — This section delineates the specific purpose and
scope of the particular SQA plan. It should list the name(s) of the
software items covered by the SQA plan and the intended use of the
software. It states the portion of the software life cycle covered by
the SQA plan for each software item specified.

• Reference Document Section — This section provides a complete list
of documents referenced elsewhere in the text of the SQA plan.

• Management Section — This section describes the project’s organi-
zational structure, tasks, and responsibilities.

• Documentation Section — This section identifies the documentation
governing the development, verification and validation, use, and
maintenance of the software. It also states how the documents are

TEAM LinG



17

Quality Assurance Framework

to be checked for adequacy. This includes the criteria and the iden-
tification of the review or audit by which the adequacy of each
document will be confirmed.

• Standards, Practices, Conventions, and Metrics Section — This section
identifies the standards, practices, conventions, and metrics to be
applied, and also states how compliance with these items is to be
monitored and assured.

• Reviews and Inspections Section — This section defines the technical
and managerial reviews, walkthroughs, and inspections to be con-
ducted. It also states how the reviews, walkthroughs, and inspec-
tions are to be accomplished including follow-up activities and
approvals.

• Software Configuration Management Section — This section is
addressed in detail in the project’s software configuration manage-
ment plan.

• Problem Reporting and Corrective Action Section — This section is
addressed in detail in the project’s software configuration manage-
ment plan.

• Tools, Techniques, and Methodologies Section — This section identi-
fies the special software tools, techniques, and methodologies that
support SQA, states their purposes, and describes their use.

• Code Control Section — This section defines the methods and facil-
ities used to maintain, store, secure, and document the controlled
versions of the identified software during all phases of development.
This may be implemented in conjunction with a computer program
library and/or may be provided as a part of the software configura-
tion management plan.

• Media Control Section — This section states the methods and facili-
ties to be used to identify the media for each computer product and
the documentation required to store the media, including the copy
and restore process, and protects the computer program physical
media from unauthorized access or inadvertent damage or degrada-
tion during all phases of development. This may be provided by the
software configuration management plan.

• Supplier Control Section — This section states the provisions for
assuring that software provided by suppliers meets established
requirements. In addition, it should state the methods that will be
used to assure that the software supplier receives adequate and
complete requirements. For previously developed software, this sec-
tion states the methods to be used to assure the suitability of the
product for use with the software items covered by the SQA plan.
For software to be developed, the supplier will be required to pre-
pare and implement an SQA plan in accordance with this standard.
This section will also state the methods to be employed to assure
that the developers comply with the requirements of this standard.

TEAM LinG



18

SOFTWARE QUALITY IN PERSPECTIVE

• Records Collection, Maintenance, and Retention Section — This sec-
tion identifies the SQA documentation to be retained. It states the
methods and facilities to assemble, safeguard, and maintain this
documentation, and will designate the retention period. The imple-
mentation of the SQA plan involves the necessary approvals for the
plan as well as development of a plan for execution. The subsequent
evaluation of the SQA plan will be performed as a result of its execution.

• Testing Methodology — This section defines the testing approach,
techniques, and automated tools that will be used.

Step 2. Obtain Management Acceptance

Management participation is necessary for the successful implementation
of an SQA plan. Management is responsible both for ensuring the quality of
a software project and for providing the resources needed for software
development.

The level of management commitment required for implementing an
SQA plan depends on the scope of the project. If a project spans organiza-
tional boundaries, approval should be obtained from all affected areas.
Once approval has been obtained, the SQA plan is placed under configura-
tion control.

In the management approval process, management relinquishes tight
control over software quality to the SQA plan administrator in exchange
for improved software quality. Software quality is often left to software
developers. Quality is desirable, but management may express concern as
to the cost of a formal SQA plan. Staff should be aware that management
views the program as a means of ensuring software quality, and not as an
end in itself.

To address management concerns, software life cycle costs should be
formally estimated for projects implemented both with and without a for-
mal SQA plan. In general, implementing a formal SQA plan makes economic
and management sense.

Step 3. Obtain Development Acceptance

Because the software development and maintenance personnel are the pri-
mary users of an SQA plan, their approval and cooperation in implement-
ing the plan are essential. The software project team members must adhere
to the project SQA plan; everyone must accept it and follow it.

No SQA plan is successfully implemented without the involvement of
the software team members and their managers in the development of the
plan. Because project teams generally have only a few members, all team
members should actively participate in writing the SQA plan. When
projects become much larger (i.e., encompassing entire divisions or

TEAM LinG



19

Quality Assurance Framework

departments), representatives of project subgroups should provide input.
Constant feedback from representatives to team members helps gain
acceptance of the plan.

Step 4. Plan for Implementation of the SQA Plan

The process of planning, formulating, and drafting an SQA plan requires
staff and word-processing resources. The individual responsible for imple-
menting an SQA plan must have access to these resources. In addition, the
commitment of resources requires management approval and, conse-
quently, management support. To facilitate resource allocation, manage-
ment should be made aware of any project risks that may impede the
implementation process (e.g., limited availability of staff or equipment). A
schedule for drafting, reviewing, and approving the SQA plan should be
developed.

Step 5. Execute the SQA Plan

The actual process of executing an SQA plan by the software development
and maintenance team involves determining necessary audit points for
monitoring it. The auditing function must be scheduled during the imple-
mentation phase of the software product so that improper monitoring of
the software project will not hurt the SQA plan. Audit points should occur
either periodically during development or at specific project milestones
(e.g., at major reviews or when part of the project is delivered).

Quality Standards

The following section describes the leading quality standards for IT.

ISO9000

ISO9000 is a quality series and comprises a set of five documents devel-
oped in 1987 by the International Standards Organization (ISO). ISO9000
standards and certification are usually associated with non-IS manufactur-
ing processes. However, application development organizations can bene-
fit from these standards and position themselves for certification, if neces-
sary. All the ISO9000 standards are guidelines and interpretive because of
their lack of stringency and rules. ISO certification is becoming more and
more important throughout Europe and the United States for the manufac-
ture of hardware. Software suppliers will increasingly be required to have
certification. ISO9000 is a definitive set of quality standards, but it repre-
sents quality standards as part of a total quality management (TQM) pro-
gram. It consists of ISO9001, ISO9002, or ISO9003, and it provides the guide-
lines for selecting and implementing a quality assurance standard.

ISO9001 is a very comprehensive standard and defines all the quality ele-
ments required to demonstrate the supplier’s ability to design and deliver

TEAM LinG



20

SOFTWARE QUALITY IN PERSPECTIVE

a quality product. ISO9002 covers quality considerations for the supplier
to control the design and development activities. ISO9003 demonstrates
the supplier’s ability to detect and control product nonconformity during
inspection and testing. ISO9004 describes the quality standards associated
with ISO9001, ISO9002, and ISO9003 and provides a comprehensive quality
checklist.

Exhibit 1.3 shows the ISO9000 and companion international standards.

Capability Maturity Model (CMM)

The Software Engineering Institute−Capability Maturity Model (SEI−CMM)
is a model for judging the maturity of the software processes of an organi-
zation and for identifying the key practices that are required to increase
the maturity of these processes. As organizations enhance their software
process capabilities, they progress through the various levels of maturity.
The achievement of each level of maturity signifies a different component
in the software process, resulting in an overall increase in the process
capability of the organization. The Capability Maturity Model for Software
describes the principles and practices underlying software process matu-
rity and is intended to help software organizations improve the maturity of
their software processes in terms of an evolutionary path from ad hoc cha-
otic processes to mature, disciplined software processes.

The CMM is organized into five maturity levels (see Exhibit 1.4):

1. Initial. The software process is characterized as ad hoc, and occa-
sionally even chaotic. Few processes are defined, and success
depends on individual effort and heroics.

2. Repeatable. Basic project management processes are established to
track cost, schedule, and functionality. The necessary process dis-
cipline is in place to repeat earlier successes on projects with similar
applications.

3. Defined. The software process for both management and engineering
activities is documented, standardized, and integrated into a stan-
dard software process for the organization. All projects use an

Exhibit 1.3.  Companion ISO Standards

International U.S. Europe U.K.

ISO9000 ANSI/ASQA EN29000 BS5750 (Part 0.1)

ISO9001 ANSI/ASQC EN29001 BS5750 (Part 1)

ISO9002 ANSI/ASQC EN29002 BS5750 (Part 2)

ISO9003 ANSI/ASQC EN29003 BS5750 (Part 3)

ISO9004 ANSI/ASQC EN29004 BS5750 (Part 4)

TEAM LinG



21

Quality Assurance Framework

approved, tailored version of the organization’s standard software
process for developing and maintaining software.

4. Managed. Detailed measures of the software process and product
quality are collected. Both the software process and products are
quantitatively understood and controlled.

5. Optimizing. Continuous process improvement is enabled by quanti-
tative feedback from the process and from piloting innovative ideas
and technologies

Level 1 — Initial. The organization typically does not provide a stable
environment for developing and maintaining software. This period is cha-
otic without any procedure and process established for software develop-
ment and testing. When an organization lacks sound management prac-
tices, ineffective planning and reaction-driven commitment systems
undermine the benefits of good software engineering practices.

In this phase, projects typically abandon planned procedures and revert
to coding and testing. Success depends entirely on having an exceptional
manager and effective software team. The project performance depends
upon capable and forceful project managers. But when they leave the
project, their stabilizing influence leaves with them. Even a strong engi-
neering process cannot overcome the instability created by the absence of
sound management practices.

Level 2 — Repeatable. During this phase measures and metrics will be
reviewed to include percent compliance to various processes, percent of
allocated requirements delivered, number of changes to requirements,
number of changes to project plans, variance between estimated and actual
size of deliverables, and variance between actual PQA audits performed and

Exhibit 1.4. Maturity Levels

Initial

Repeatable

Defined

Managed

Optimizing

TEAM LinG



22

SOFTWARE QUALITY IN PERSPECTIVE

planned and number of change requests processed over a period of time.
The following are the key process activities during level 2:

• Software configuration management
• Software quality assurance
• Software subcontract management
• Software project tracking and oversight
• Software project planning
• Requirements management

Level 3 — Defined. During this phase measures and metrics will be
reviewed to include percentage of total project time spent on test activi-
ties, testing efficiency, inspection rate for deliverables, inspection effi-
ciency, variance between actual attendance and planned attendance for
training programs, and variance between actual and planned management
effort. Level 3 compliance means an organization’s processes for manage-
ment and engineering activities have been formally defined, documented,
and integrated into a standard process that is understood and followed by
the organization’s staff when developing and maintaining software. Once
an organization has reached this level, it has a foundation for continuing
progress. New processes and tools can be added with minimal disruption
and new staff members can be easily trained to adapt to the organization’s
practices. The following are the key process areas for Level 3:

• Peer reviews
• Intergroup coordination
• Software product engineering
• Integrated software management
• Training program
• Organization process definition
• Organization process focus

The software process capability of Level 3 organizations can be summa-
rized as standard and consistent because both software engineering and
management activities are stable and repeatable. Within established prod-
uct lines, cost, schedule, and functionality are under control, and software
quality is tracked. This process capability is based on a common organiza-
tionwide understanding of the activities, roles, and responsibilities in a
defined software process.

Level 4 — Managed. This phase denotes that the processes are well
defined and professionally managed. The quality standards are on an
upswing. With sound quality processes in place, the organization is better
equipped to meet customer expectations of high-quality/high-performance
software at reasonable cost and committed deliveries. Delivering consis-
tency in software work products and consistency throughout the software
development life cycle including plans, process, requirements, design,

TEAM LinG



23

Quality Assurance Framework

code, and testing helps create satisfied customers. Projects achieve con-
trol over their products and processes by narrowing the variation in their
process performance to fall within acceptable quantitative boundaries.
Meaningful variations in process performance can be distinguished from
random variations (noise), particularly within established product lines.
The risks involved in moving up the learning curve of a new application
domain are known and carefully managed:

• Software quality management 
• Quantitative process management

The software process capability of Level 4 organizations can be summa-
rized as predictable because the process is measured and operates within
measurable limits. The level of process capability allows an organization to
predict trends in process and product quality within the quantitative
bounds of these limits. When these limits are exceeded, action is taken to
correct the situation. Software products are of predictably high quality.

Level 5 — Optimized. A continuous emphasis on process improvement
and defect reduction avoids process stagnancy or degeneration and
ensures continual improvement, translating into improved productivity,
reduced defect leakage, and greater timeliness. Tracing requirements
across each development phase improves the completeness of software,
reduces rework, and simplifies maintenance. Verification and validation
activities are planned and executed to reduce defect leakage. Customers
have access to the project plan, receive regular status reports, and their
feedback is sought and used for process tuning. The KPA at Level 5 are:

• Process change management
• Technology change management
• Defect prevention

Software project teams in Level 5 organizations analyze defects to deter-
mine their causes. Software processes are evaluated to prevent known
types of defects from recurring, and lessons learned are disseminated to
other projects. The software process capability of Level 5 organizations
can be characterized as continuously improving because Level 5 organiza-
tions are continuously striving to improve the range of their process capa-
bility, thereby improving the process performance of their projects.
Improvement occurs both by incremental advancements in the existing
process and by innovations using new technologies and methods.

PCMM

The People Capability Maturity Model (People CMM) is a framework that
helps organizations successfully address their critical people issues.
Based on the best current practices in fields such as human resources,
knowledge management, and organizational development, the People

TEAM LinG



24

SOFTWARE QUALITY IN PERSPECTIVE

CMM guides organizations in improving their processes for managing and
developing their workforces. The People CMM helps organizations charac-
terize the maturity of their workforce practices, establish a program of
continuous workforce development, set priorities for improvement
actions, integrate workforce development with process improvement, and
establish a culture of excellence. Since its release in 1995, thousands of
copies of the People CMM have been distributed, and it is used worldwide
by organizations small and large.

The People CMM consists of five maturity levels that establish successive
foundations for continuously improving individual competencies, developing
effective teams, motivating improved performance, and shaping the work-
force the organization needs to accomplish its future business plans. Each
maturity level is a well-defined evolutionary plateau that institutionalizes new
capabilities for developing the organization’s workforce. By following the
maturity framework, an organization can avoid introducing workforce prac-
tices that its employees are unprepared to implement effectively.

CMMI

The CMMI Product Suite provides the latest best practices for product and
service development and maintenance.1 The CMMI models as the best pro-
cess improvement models available for product and service development
and maintenance. These models extend the best practices of the Capability
Maturity Model for Software (SW-CMM®), the Systems Engineering Capa-
bility Model (SECM), and the Integrated Product Development Capability
Maturity Model (IPD-CMM).

Organizations reported that CMMI is adequate for guiding their process
improvement activities and that CMMI training courses and appraisal
methods are suitable for their needs, although there are specific opportu-
nities for improvement. The cost of CMMI is an issue that affected adoption
decisions for some but not for others. Finally, return-on-investment infor-
mation is usually helpful to organizations when making the business case
to adopt CMMI.

Malcom Baldrige National Quality Award

As the National Institute of Standards and Technology (NIST) says,

In the early and mid-1980s, many industry and government leaders saw
that a renewed emphasis on quality was no longer an option for Amer-
ican companies but a necessity for doing business in an ever expand-
ing, and more demanding, competitive world market. But many
American businesses either did not believe quality mattered for them
or did not know where to begin.2

TEAM LinG



25

Quality Assurance Framework

Public Law 100-107, signed into law on August 20, 1987, created the Mal-
colm Baldrige National Quality Award. The Award Program led to the cre-
ation of a new public−private partnership. Principal support for the pro-
gram comes from the Foundation for the Malcolm Baldrige National Quality
Award, established in 1988. The Award is named for Malcolm Baldrige, who
served as Secretary of Commerce from 1981 until his tragic death in a
rodeo accident in 1987.

The Baldrige Award is given by the President of the United States to
businesses — manufacturing and service, small and large — and to
education and health care organizations that apply and are judged to
be outstanding in seven areas: leadership, strategic planning, customer
and market focus, information and analysis, human resource focus,
process management, and business results. . . . While the Baldrige
Award and the Baldrige recipients are the very visible centerpiece of
the U.S. quality movement, a broader national quality program has
evolved around the award and its criteria. A report, Building on Bald-
rige: American Quality for the 21st Century, by the private Council on
Competitiveness, said, ‘More than any other program, the Baldrige
Quality Award is responsible for making quality a national priority and
disseminating best practices across the United States.’

Each year, more than 300 experts from industry, educational institu-
tions, governments at all levels, and non-profit organizations volunteer
many hours reviewing applications for the award, conducting site vis-
its, and providing each applicant with an extensive feedback report
citing strengths and opportunities to improve. In addition, board mem-
bers have given thousands of presentations on quality management,
performance improvement, and the Baldrige Award.2

The Baldrige performance excellence criteria are a framework (see
Exhibit 1.5) that any organization can use to improve overall performance.
Seven categories make up the award criteria:

1. Leadership — Examines how senior executives guide the organiza-
tion and how the organization addresses its responsibilities to the
public and practices good citizenship. Evaluations are based upon
the appropriateness, effectiveness, and extent of the leader’s and
the company’s involvement in relation to the size and type of business.

2. Measurement, analysis, and knowledge management — Examines the
management, effective use, analysis, and improvement of data and
information to support key organization processes and the organi-
zation’s performance management system. The scope, management,
and analysis of data depend upon the type of business, its resources,
and the geographical distribution.

3. Strategic planning — Examines how the organization sets strategic
directions and how it determines key action plans. Evaluations are
based upon the thoroughness and effectiveness of the processes.

TEAM LinG



26

SOFTWARE QUALITY IN PERSPECTIVE

4. Human resource focus — Examines how the organization enables its
workforce to develop its full potential and how the workforce is
aligned with the organization’s objectives. Evaluation depends upon
the human resource approach of the company.

5. Process management — Examines aspects of how key produc-
tion/delivery and support processes are designed, managed, and
improved. The types of products and services, customer and gov-
ernment requirements, regulatory requirements, and number of
business locations are the factors influencing this.

6. Business results — Examines the organization’s performance and
improvement in its key business areas: customer satisfaction, finan-
cial and marketplace performance, human resources, supplier and
partner performance, operational performance, and governance and
social responsibility. The category also examines how the organiza-
tion performs relative to competitors.

7. Customer and market focus — Examines how the organization deter-
mines requirements and expectations of customers and markets;
builds relationships with customers; and acquires, satisfies, and
retains customers.

The system for scoring examination items is based upon these evalua-
tion dimensions:

1. Approach: Approach indicates the method that the company uses
to achieve the purposes. The degree to which the approach is pre-
vention-based, the appropriateness of the tools, techniques, and
methods, the effectiveness of their use, whether the approach is
systematic, integrated, and consistently applied, effective self-eval-
uation and feedback, quantitative information gathered, and the

Exhibit 1.5. Baldrige Performance Framework

Leadership

Information Analysis
Planning

Human Resource
Quality Assurance

Results

Customer Satisfaction

TEAM LinG



27

Quality Assurance Framework

uniqueness and innovativeness of the approach are the factors to
decide on the correct approach.

2. Deployment: This concerns the areas where the approach is
deployed. It evaluates whether the approach is implemented in all
the products and services and all internal processes, activities, facil-
ities, and employees.

3. Results: This refers to the outcome of the approach. The quality
levels demonstrated, rate of quality improvement, breadth, signifi-
cance, and comparison of the quality improvement and the extent
to which quality improvement is demonstrated are the key factors
to this.

As compared to other programs like ISO, Japan’s Deming award and
America’s Baldrige Award:

• Focus more on results and service
• Rely upon the involvement of many different professional and trade

groups
• Provide special credits for innovative approaches to quality
• Include a strong customer and human resource focus
• Stress the importance of sharing information

Notes

1. http://www.sei.cmu.edu/cmmi/adoption/cmmi-start.html.
2. http://www.nist.gov/public_affairs/factsheet/baldfaqs.htm.

TEAM LinG



TEAM LinG



29

Part  2

Overview of Testing 
Techniques

As software testing, as a separate process, witnessed vertical growth and
received the attention of project stakeholders and business sponsors in
the last decade, various new techniques have been continuously intro-
duced in the arena of software testing. Apart from the traditional testing
techniques, various new techniques necessitated by the complicated busi-
ness and development logic were realized to make software testing more
meaningful and purposeful. This part discusses some of the popular test-
ing techniques that have been adopted by the testing community.

Black-Box Testing (Functional)

Black-box or functional testing is one in which test conditions are devel-
oped based on the program or system’s functionality; that is, the tester
requires information about the input data and observed output, but does
not know how the program or system works. Just as one does not have to
know how a car works internally to drive it, it is not necessary to know the
internal structure of a program to execute it. The tester focuses on testing
the program’s functionality against the specification. With black-box test-
ing, the tester views the program as a black-box and is completely uncon-
cerned with the internal structure of the program or system. Some exam-
ples in this category include: decision tables, equivalence partitioning,
range testing, boundary value testing, database integrity testing, cause-
effect graphing, orthogonal array testing, array and table testing, excep-
tion testing, limit testing, and random testing.

A major advantage of black-box testing is that the tests are geared to
what the program or system is supposed to do, and it is natural and under-
stood by everyone. This should be verified with techniques such as struc-
tured walkthroughs, inspections, and joint application designs (JADs). A
limitation is that exhaustive input testing is not achievable, because this
requires that every possible input condition or combination be tested. In
addition, because there is no knowledge of the internal structure or logic,
there could be errors or deliberate mischief on the part of a programmer

TEAM LinG



30

SOFTWARE QUALITY IN PERSPECTIVE

that may not be detectable with black-box testing. For example, suppose a
payroll programmer wants to insert some job security into a payroll appli-
cation he is developing. By inserting the following extra code into the appli-
cation, if the employee were to be terminated, that is, his employee ID no
longer existed in the system, justice would sooner or later prevail.

if my employee ID exists
deposit regular pay check into my bank account
else
deposit an enormous amount of money into my bank account
erase any possible financial audit trails
erase this code

White-Box Testing (Structural)

In white-box or structural testing test conditions are designed by examin-
ing paths of logic. The tester examines the internal structure of the pro-
gram or system. Test data is driven by examining the logic of the program
or system, without concern for the program or system requirements. The
tester knows the internal program structure and logic, just as a car
mechanic knows the inner workings of an automobile. Specific examples in
this category include basis path analysis, statement coverage, branch cov-
erage, condition coverage, and branch/condition coverage.

An advantage of white-box testing is that it is thorough and focuses on
the produced code. Because there is knowledge of the internal structure or
logic, errors or deliberate mischief on the part of a programmer have a
higher probability of being detected.

One disadvantage of white-box testing is that it does not verify that the
specifications are correct; that is, it focuses only on the internal logic and
does not verify the logic to the specification. Another disadvantage is that
there is no way to detect missing paths and data-sensitive errors. For exam-
ple, if the statement in a program should be coded “if |a–b| < 10” but is
coded “if (a–b) < 1,” this would not be detectable without specification
details. A final disadvantage is that white-box testing cannot execute all
possible logic paths through a program because this would entail an astro-
nomically large number of tests.

Gray-Box Testing (Functional and Structural)

Black-box testing focuses on the program’s functionality against the speci-
fication. White-box testing focuses on the paths of logic. Gray-box testing is
a combination of black- and white-box testing. The tester studies the
requirements specifications and communicates with the developer to
understand the internal structure of the system. The motivation is to clear
up ambiguous specifications and “read between the lines” to design

TEAM LinG



31

Overview of Testing Techniques

implied tests. One example of the use of gray-box testing is when it appears
to the tester that a certain functionality seems to be reused throughout an
application. If the tester communicates with the developer and under-
stands the internal design and architecture, many tests will be eliminated,
because it may be possible to test the functionality only once. Another
example is when the syntax of a command consists of seven possible
parameters that can be entered in any order, as follows:

Command parm1, parm2, parm3, parm4, parm5, parm6, parm7

In theory, a tester would have to create 7! or 5040 tests. The problem is
compounded further if some of the parameters are optional. If the tester
uses gray-box testing, by talking with the developer and understanding the
parser algorithm, if each parameter is independent, only seven tests may
be required.

Manual versus Automated Testing

The basis of the manual testing categorization is that it is not typically car-
ried out by people and it is not implemented on the computer. Examples
include structured walkthroughs, inspections, JADs, and desk checking.

The basis of the automated testing categorization is that it is imple-
mented on the computer. Examples include boundary value testing,
branch coverage testing, prototyping, and syntax testing. Syntax testing is
performed by a language compiler, and the compiler is a program that exe-
cutes on a computer

Static versus Dynamic Testing

Static testing approaches are time independent and are classified in this
way because they do not necessarily involve either manual or automated
execution of the product being tested. Examples include syntax checking,
structured walkthroughs, and inspections. An inspection of a program
occurs against a source code listing in which each code line is read line by
line and discussed. An example of static testing using the computer is a
static flow analysis tool, which investigates another program for errors
without executing the program. It analyzes the other program’s control
and data flow to discover problems such as references to a variable that
has not been initialized and unreachable code.

Dynamic testing techniques are time dependent and involve executing a
specific sequence of instructions on paper or by the computer. Examples
include structured walkthroughs, in which the program logic is simulated
by walking through the code and verbally describing it. Boundary testing
is a dynamic testing technique that requires the execution of test cases on
the computer with a specific focus on the boundary values associated with
the inputs or outputs of the program.

TEAM LinG



32

SOFTWARE QUALITY IN PERSPECTIVE

Taxonomy of Software Testing Techniques

A testing technique is a set of interrelated procedures that, together,
produce a test deliverable. There are many possible classification schemes
for software testing and Exhibit 2.1 describes one way. The exhibit reviews
formal popular testing techniques and also classifies each per the above
discussion as manual, automated, static, dynamic, functional (black-box),
or structural (white-box).

Exhibit 2.2 describes each of the software testing methods.

TEAM LinG



33

Overview of Testing Techniques

E
xh

ib
it

 2
.1

.
Te

st
in

g 
Te

ch
n

iq
u

e 
C

at
eg

or
ie

s

Te
ch

n
iq

u
e

M
an

u
al

A
u

to
m

at
ed

St
at

ic
D

yn
am

ic
Fu

n
ct

io
n

al
St

ru
ct

u
ra

l

A
cc

ep
ta

nc
e 

te
st

in
g

x
x

x
x

A
d

 h
oc

 t
es

ti
ng

x
x

A
lp

h
a 

te
st

in
g

x
x

x

B
as

is
 p

at
h

 t
es

ti
ng

x
x

x

B
et

a 
te

st
in

g
x

x
x

B
la

ck
-b

ox
 t

es
ti

ng
x

x
x

B
ot

to
m

-u
p

 t
es

ti
ng

x
x

x

B
ou

nd
ar

y 
va

lu
e 

te
st

in
g

x
x

x

B
ra

nc
h

 c
ov

er
ag

e 
te

st
in

g
x

x
x

B
ra

nc
h

/c
on

d
it

io
n 

co
ve

ra
ge

x
x

x

C
au

se
-e

ff
ec

t 
gr

ap
h

in
g

x
x

x

C
om

p
ar

is
on

 t
es

ti
ng

x
x

x
x

x

C
om

p
at

ib
ili

ty
 t

es
ti

ng
x

x
x

C
on

d
it

io
n 

co
ve

ra
ge

 t
es

ti
ng

x
x

x

C
R

U
D

 t
es

ti
ng

x
x

x

D
at

ab
as

e 
te

st
in

g
x

x
x

D
ec

is
io

n 
ta

b
le

s
x

x
x

D
es

k 
ch

ec
ki

ng
x

x
x

En
d

-t
o-

en
d

 t
es

ti
ng

x
x

x

TEAM LinG



34

SOFTWARE QUALITY IN PERSPECTIVE
E

xh
ib

it
 2

.1
.

Te
st

in
g 

Te
ch

n
iq

u
e 

C
at

eg
or

ie
s 

(C
on

ti
n

u
ed

)

Te
ch

n
iq

u
e

M
an

u
al

A
u

to
m

at
ed

St
at

ic
D

yn
am

ic
Fu

n
ct

io
n

al
St

ru
ct

u
ra

l

Eq
ui

va
le

nc
e 

p
ar

ti
ti

on
in

g
x

x

Ex
ce

p
ti

on
 t

es
ti

ng
x

x
x

Ex
p

lo
ra

to
ry

 t
es

ti
ng

x
x

x

Fr
ee

 fo
rm

 t
es

ti
ng

x
x

x

G
ra

y-
b

ox
 t

es
ti

ng
x

x
x

x

H
is

to
gr

am
s

x
x

In
cr

em
en

ta
l i

nt
eg

ra
ti

on
 t

es
ti

ng
x

x
x

x

In
sp

ec
ti

on
s

x
x

x
x

In
te

gr
at

io
n 

te
st

in
g

x
x

x
x

JA
D

s
x

x
x

Lo
ad

 t
es

ti
ng

x
x

x
x

M
ut

at
io

n 
te

st
in

g
x

x
x

x

O
rt

h
og

on
al

 a
rr

ay
 t

es
ti

ng
x

x
x

P
ar

et
o 

an
al

ys
is

x
x

P
er

fo
rm

an
ce

 t
es

ti
ng

x
x

x
x

x

P
os

it
iv

e 
an

d
 n

eg
at

iv
e 

te
st

in
g

x
x

x

P
ri

or
 d

ef
ec

t 
h

is
to

ry
 t

es
ti

ng
x

x
x

P
ro

to
ty

p
in

g
x

x
x

R
an

d
om

 t
es

ti
ng

x
x

x

TEAM LinG



35

Overview of Testing Techniques
R

an
ge

 t
es

ti
ng

x
x

x

R
ec

ov
er

y 
te

st
in

g
x

x
x

x

R
eg

re
ss

io
n 

te
st

in
g

x
x

R
is

k-
b

as
ed

 t
es

ti
ng

x
x

x

R
un

 c
h

ar
ts

x
x

x

Sa
nd

w
ic

h
 t

es
ti

ng
x

x
x

Sa
ni

ty
 t

es
ti

ng
x

x
x

x

Se
cu

ri
ty

 t
es

ti
ng

x
x

x

St
at

e 
tr

an
si

ti
on

 t
es

ti
ng

x
x

x

St
at

em
en

t 
co

ve
ra

ge
 t

es
ti

ng
x

x
x

St
at

is
ti

ca
l p

ro
fil

e 
te

st
in

g
x

x
x

St
re

ss
 t

es
ti

ng
x

x
x

St
ru

ct
ur

ed
 w

al
kt

h
ro

ug
h

s
x

x
x

x

Sy
nt

ax
 t

es
ti

ng
x

x
x

x

Sy
st

em
 t

es
ti

ng
x

x
x

x

Ta
b

le
 t

es
ti

ng
x

x
x

T
h

re
ad

 t
es

ti
ng

x
x

x

To
p

-d
ow

n 
te

st
in

g
x

x
x

x

U
ni

t 
te

st
in

g
x

x
x

x

U
sa

b
ili

ty
 t

es
ti

ng
x

x
x

x

U
se

r 
ac

ce
p

ta
nc

e 
te

st
in

g
x

x
x

x

W
h

it
e-

b
ox

 t
es

ti
ng

x
x

x

TEAM LinG



36

SOFTWARE QUALITY IN PERSPECTIVE

Exhibit 2.2. Testing Technique Descriptions 

Technique Brief Description

Acceptance testing Final testing based on the end-user/customer specifications, 
or based on use by end-users/customers over a defined 
period of time

Ad hoc testing Similar to exploratory testing, but often taken to mean that 
the testers have significant understanding of the software 
before testing it

Alpha testing Testing of an application when development is nearing 
completion; minor design changes may still be made as a 
result of such testing. Typically done by end-users or 
others, not by programmers or testers

Basis path testing Identifying tests based on flow and paths of a program or 
system

Beta testing Testing when development and testing are essentially 
completed and final bugs and problems need to be found 
before final release. Typically done by end-users or others, 
not by programmers or testers

Black-box testing Testing cases generated based on the system’s functionality

Bottom-up testing Integrating modules or programs starting from the bottom

Boundary value 
testing

Testing cases generated from boundary values of equivalence 
classes

Branch coverage 
testing

Verifying that each branch has true and false outcomes at 
least once

Branch/condition 
coverage testing

Verifying that each condition in a decision takes on all 
possible outcomes at least once

Cause–effect 
graphing

Mapping multiple simultaneous inputs that may affect others 
to identify their conditions to test

Comparison testing Comparing software weaknesses and strengths to competing 
products

Compatibility 
testing

Testing how well software performs in a particular 
hardware/software/operating system/network environment

Condition coverage 
testing

Verifying that each condition in a decision takes on all 
possible outcomes at least once

CRUD testing Building a CRUD matrix and testing all object creations, reads, 
updates, and deletions

Database testing Checking the integrity of database field values

Decision tables Table showing the decision criteria and the respective 
actions

Desk checking Developer reviews code for accuracy

TEAM LinG



37

Overview of Testing Techniques

End-to-end testing Similar to system testing; the “macro” end of the test scale; 
involves testing of a complete application environment in a 
situation that mimics real-world use, such as interacting with 
a database, using network communications, or interacting 
with other hardware, applications, or systems if appropriate 

Equivalence 
partitioning

Each input condition is partitioned into two or more groups. 
Test cases are generated from representative valid and 
invalid classes 

Exception testing Identifying error messages and exception handling processes 
and conditions that trigger them

Exploratory testing Often taken to mean a creative, informal software test that is 
not based on formal test plans or test cases; testers may be 
learning the software as they test it

Free form testing Ad hoc or brainstorming using intuition to define test cases

Gray-box testing A combination of black-box and white-box testing to take 
advantage of both

Histograms A graphical representation of measured values organized 
according to the frequency of occurrence used to pinpoint 
hot spots

Incremental 
integration testing

Continuous testing of an application as new functionality is 
added; requires that various aspects of an application’s 
functionality be independent enough to work separately 
before all parts of the program are completed, or that test 
drivers be developed as needed; done by programmers or 
by testers

Inspections Formal peer review that uses checklists, entry criteria, and 
exit criteria

Integration testing Testing of combined parts of an application to determine if 
they function together correctly. The “parts” can be code 
modules, individual applications, or client and server 
applications on a network. This type of testing is especially 
relevant to client/server and distributed systems. 

JADs Technique that brings users and developers together to 
jointly design systems in facilitated sessions

Load testing Testing an application under heavy loads, such as testing of a 
Web site under a range of loads to determine at what point 
the system’s response time degrades or fails

Mutation testing A method for determining if a set of test data or test cases is 
useful, by deliberately introducing various code changes 
(“bugs”) and retesting with the original test data/cases to 
determine if the “bugs” are detected. Proper 
implementation requires large computational resources.

Exhibit 2.2. Testing Technique Descriptions (Continued)

Technique Brief Description

TEAM LinG



38

SOFTWARE QUALITY IN PERSPECTIVE

Orthogonal array 
testing

Mathematical technique to determine which variations of 
parameters need to be tested

Pareto analysis Analyze defect patterns to identify causes and sources

Performance testing Term often used interchangeably with “stress” and “load” 
testing. Ideally “performance” testing (and any other “type” 
of testing) is defined in requirements documentation or QA 
or Test Plans

Positive and 
negative testing

Testing the positive and negative values for all inputs

Prior defect history 
testing

Test cases are created or rerun for every defect found in prior 
tests of the system

Prototyping General approach to gather data from users by building and 
demonstrating to them some part of a potential application

Random testing Technique involving random selection from a specific set of 
input values where any value is as likely as any other

Range testing For each input identifies the range over which the system 
behavior should be the same

Recovery testing Testing how well a system recovers from crashes, hardware 
failures, or other catastrophic problems

Regression testing Testing a system in light of changes made during a 
development spiral, debugging, maintenance, or the 
development of a new release

Risk-based testing Measures the degree of business risk in a system to improve 
testing

Run charts A graphical representation of how a quality characteristic 
varies with time

Sandwich testing Integrating modules or programs from the top and bottom 
simultaneously

Sanity testing Typically an initial testing effort to determine if a new 
software version is performing well enough to accept it for 
a major testing effort. For example, if the new software is 
crashing systems every five minutes, bogging down systems 
to a crawl, or destroying databases, the software may not be 
in a “sane” enough condition to warrant further testing in its 
current state

Security testing Testing how well the system protects against unauthorized 
internal or external access, willful damage, etc.; may require 
sophisticated testing techniques

Exhibit 2.2. Testing Technique Descriptions (Continued)

Technique Brief Description

TEAM LinG



39

Overview of Testing Techniques

State transition 
testing

Technique in which the states of a system are first identified 
and then test cases are written to test the triggers to cause 
a transition from one condition to another state

Statement coverage 
testing

Every statement in a program is executed at least once

Statistical profile 
testing

Statistical techniques are used to develop a usage profile of 
the system that helps define transaction paths, conditions, 
functions, and data tables

Stress testing Term often used interchangeably with “load” and 
“performance” testing. Also used to describe such tests as 
system functional testing while under unusually heavy 
loads, heavy repetition of certain actions or inputs, input of 
large numerical values, or large complex queries to a 
database system

Structured 
walkthroughs

A technique for conducting a meeting at which project 
participants examine a work product for errors

Syntax testing Data-driven technique to test combinations of input syntax

System testing Black-box type testing that is based on overall requirements 
specifications; covers all combined parts of a system

Table testing Testing access, security, and data integrity of table entries

Thread testing Combining individual units into threads of functionality 
which together accomplish a function or set of functions

Top-down testing Integrating modules or programs starting from the top

Unit testing The most “micro” scale of testing; to test particular functions 
or code modules. Typically done by the programmer and 
not by testers, as it requires detailed knowledge of the 
internal program design and code. Not always easily done 
unless the application has a well-designed architecture with 
tight code; may require developing test driver modules or 
test harnesses

Usability testing Testing for “user-friendliness.” Clearly this is subjective, and 
will depend on the targeted end-user or customer. User 
interviews, surveys, video recording of user sessions, and 
other techniques can be used. Programmers and testers are 
usually not appropriate as usability testers

User acceptance 
testing

Determining if software is satisfactory to an end-user or 
customer

White-box testing Test cases are defined by examining the logic paths of a 
system

Exhibit 2.2. Testing Technique Descriptions (Continued)

Technique Brief Description

TEAM LinG



TEAM LinG



41

Part 3
Quality through 
Continuous 
Improvement Process

Contribution of Edward Deming

Although Henry Ford and Fredrick Winslow Taylor made enormous contri-
butions to factory production, Dr. Edward Deming has gone beyond them.
He has influenced every facet of work in every industry, including govern-
ment, schools, and hospitals. Deming has had a profound effect on how
people think, how they see themselves, and how they relate to their cus-
tomers, to one another, and to society.

In 1928 he earned his Ph.D. in physics and in the next four years pub-
lished papers about the effect of electrons on the structure of materials. He
started his career at the frontiers of physics. In 1934 he began to move
away from physics and physical chemistry and published his first paper in
the field of statistics. In 1937 he wrote a paper on the statistical theory of
errors.

By law the federal government is required to take a population census
every ten years, and in 1940 Deming became involved with the Census
Bureau of the Department of Commerce. The proper tool for this task was
statistics, and so we find in his list of publications a series of 26 papers
dealing almost solely with problems of sampling. One paper published in
1944, during World War II, introduced Shewhart’s methods of quality con-
trol to engineers. He took the lead in getting this subject into the wartime
training of engineers, giving the first course himself at Stanford University.
From around 1945 onward, people did not think of Deming as a physicist
but as a statistician. It is not surprising, therefore, that when General Mac-
Arthur needed to make a population survey in Japan in 1948, he called
upon Deming. In 1953 — three years after he started to work with Japanese
managers — he started his crusade to bring quality management princi-
ples to American managers. In 1953 he published Management’s Responsi-
bility for the Use of Statistical Techniques in Industry, thus marking the start

TEAM LinG



42

SOFTWARE QUALITY IN PERSPECTIVE

of a theme he would pursue for the next 40 years. He had begun to see the
transformation in Japan.

Role of Statistical Methods

Deming’s quality method includes the use of statistical methods that he
believed were essential to minimize confusion when there is variation in a
process. Statistics also help us to understand the processes themselves,
gain control, and improve them. This is brought home by the quote, “In
God we trust. All others must use data.” Particular attention is paid to
locating a problem’s major causes which, when removed, improve quality
significantly. Deming points out that many statistical techniques are not
difficult and require a strong background in mathematics. Education, how-
ever, is a very powerful tool and is required on all levels of an organization
to make it work.

The following is an outline of some of the statistical methods that are
further described and applied to software testing. More details are pro-
vided in Section III.

Cause-and-Effect Diagram

Often called the “fishbone” diagram, this method can be used in brain-
storming sessions to locate factors that may influence a situation. This is a
tool used to identify possible causes of a problem by representing the rela-
tionship between some effect and its possible cause.

Flow Chart

This is a graphical method of documenting a process. It is a diagram that
shows the sequential steps of a process or of a workflow that go into creat-
ing a product or service. The justification of flow charts is that in order to
improve a process, one needs first to understand it.

Pareto Chart

This is a commonly used graphical technique in which events to be ana-
lyzed are named. The incidents are counted by name; the events are
ranked by frequency in a bar chart in ascending sequence. Pareto analysis
applies the 80/20 rules. An example of this is when 20 percent of an organi-
zation’s customer’s account for 80 percent of the revenue, for example,
focuses on the 20 percent.

Run Chart

A run chart is a graphical technique that graphs data points in chronolog-
ical order to illustrate trends of a characteristic being measured in order to
assign a potential cause rather than random variation.

TEAM LinG



43

Quality through Continuous Improvement Process

Histogram

A histogram is a graphical description of measured values organized
according to the frequency or relative frequency of occurrence. It also pro-
vides the average and variation.

Scatter Diagram

A scatter diagram is a graph designed to show where there is a relationship
between two variables or changing factors.

Control Chart

A control chart is a statistical method for distinguishing between special
and common variations exhibited by processes. It is a run chart with sta-
tistically determined upper and lower limits drawn on either side of the
process averages.

Deming’s 14 Quality Principles

Deming outlined 14 quality principles, which must be used concurrently in
order to achieve quality. Although these principles were applied to indus-
try, influencing government, schools, and hospitals, many are also applica-
ble to achieving software quality from an information technology perspec-
tive. The following is a brief discussion of each point, followed by a
description of how a quality assurance organization might apply each.

Point 1: Create Constancy of Purpose

Most companies tend to dwell on their immediate problems without ade-
quate attention to the future. According to Deming, “It is easy to stay
bound up in the tangled knots of the problems of today, becoming ever
more and more efficient in the future, but no company without a plan for
the future will stay in business.” A constancy of purpose requires innova-
tion (e.g., long-term planning for it), investment in research and education,
and continuous improvement of products and service.

To apply this point, an information technology quality assurance organi-
zation can:

• Develop a quality assurance plan that provides a long-range quality
direction.

• Require software testers to develop and maintain comprehensive
test plans for each project.

• Encourage quality analysts and testers to come up with new and
innovative ideas to maximize quality.

• Strive to continuously improve quality processes.

TEAM LinG



44

SOFTWARE QUALITY IN PERSPECTIVE

Point 2: Adopt the New Philosophy

Quality must become the new religion. According to Deming, “The cost of
living depends inversely on the goods and services that a given amount of
money will buy, for example, reliable service reduces costs. Delays and
mistakes raise costs.” Consumers of goods and services end up paying for
delays and mistakes, which reduces their standard of living. Tolerance of
acceptability levels and defects in systems is the roadblock between qual-
ity and productivity.

To apply this point, an information technology quality assurance organi-
zation can:

• Educate the information technology organization on the need and
value of quality.

• Promote the quality assurance department to the same level as any
other department.

• Defuse the notion that quality assurance is negative and the “watch
dogs.”

• Develop a risk management plan and not accept any anomalies
outside the range of acceptable risk tolerance.

Point 3: Cease Dependence on Mass Inspection

The old way of thinking is to inspect bad quality out. A better approach is
that inspection should be used to see how we are doing, and not be left to
the final product, when it is difficult to determine where in the process a
defect took place. Quality should be built in without the dependence on
mass inspections.

To apply this point, an information technology quality assurance organi-
zation can:

• Promote and interject technical reviews, walkthroughs, and inspec-
tions as nondefensive techniques for achieving quality throughout
the entire development cycle.

• Instill the need for the whole organization to be quality conscious
and treat it as a tangible, measurable work product deliverable.

• Require statistical evidence of information technology quality.

Point 4: End the Practice of Awarding Business on Price Tag Alone

“Two or more suppliers for the same item will multiply the evils that are
necessarily inherent and bad enough with any one supplier.” A buyer will
serve her company best by developing a long-term relationship of loyalty
and trust with a single vendor. Rather than using standards manuals by
which vendors must qualify for business, a better approach is active
involvement by the supplier’s management with Deming’s 14 points.

TEAM LinG



45

Quality through Continuous Improvement Process

To apply this point, an information technology quality assurance organi-
zation can:

• Require software quality and test vendors to provide statistical evi-
dence of their quality.

• Pick the best vendor for each quality assurance tool, testing tool,
or service, and develop a working relationship consistent with the
quality plan.

Point 5: Improve Constantly and Forever the System of Production 
and Service

Improvement is not a one-time effort: management is obligated to improve
quality continuously. “Putting out fires is not improvement. Finding a point
out of control, finding the special cause and removing it, is only putting the
process back to where it was in the first place. The obligation for improve-
ment is a ceaseless process.”

To apply this point, an information technology quality assurance organi-
zation can:

• Constantly improve quality assurance and testing processes.
• Not rely on judgment.
• Use statistical techniques such as root cause and effect analysis to

uncover the sources of problems and test analysis.

Point 6: Institute Training and Retraining

Often there is little or no training and workers do not know when they have
done their jobs correctly. It is very difficult to erase improper training.
Deming stresses that training should not end as long as performance is not
in statistical control and there is something to be gained.

To apply this point, an information technology quality assurance organi-
zation can:

• Institute modern training aids and practices.
• Encourage quality staff to constantly increase their knowledge of

quality and testing techniques by attending seminars and classes.
• Reward staff for creating new seminars and special interest groups.
• Use statistical techniques to determine when training is needed and

completed.

Point 7: Institute Leadership

“There is no excuse to offer for putting people on a job that they know not
how to do. Most so-called ‘goofing off’ — somebody seems to be lazy,
doesn’t seem to care — that person is almost always in the wrong job, or

TEAM LinG



46

SOFTWARE QUALITY IN PERSPECTIVE

has very poor management.” It is the responsibility of management to dis-
cover the inhibitors that prevent workers from taking pride in their jobs.
From an information technology point of view, development often views
the job of quality to be the QA department’s responsibility. QA should be
very aggressive as quality leaders and point out that quality is everyone’s
responsibility.

To apply this point, an information technology quality assurance organi-
zation can:

• Take the time to train a developer on how to unit test code effectively
if he or she has an excessive number of defects discovered by QA
testing.

• Improve supervision, which is the responsibility of management.
• Allow the project leader to have more time to help people on the job.
• Use statistical methods to indicate where there are faults.

Point 8: Drive Out Fear

There is often no incentive for problem solving. Suggesting new ideas is too
risky. People are afraid of losing their raises, promotions, or jobs. “Fear
takes a horrible toll. Fear is all around, robbing people of their pride, hurt-
ing them, robbing them of a chance to contribute to the company. It is
unbelievable what happens when you unloose fear.” A common problem is
the fear of inspections.

To apply this point, an information technology quality assurance organi-
zation can:

• Promote the idea that quality is goodness and should be rewarded,
and promote any new ideas to improve quality.

• Prior to a structured walkthrough, inspection, or JAD session, make
sure everyone understands the ground rules and promote an “ego-
less” environment.

• Periodically schedule a “Quality Day” in which quality improvement
ideas are openly shared.

Point 9: Break Down Barriers between Staff Areas

There are numerous problems when departments have different goals and
do not work as a team to solve problems, set policies, or define new direc-
tions. “People can work superbly in their respective departments, but if
their goals are in conflict, they can ruin the company. It is better to have
teamwork, working for the company.”

To apply this point, an information technology quality assurance organi-
zation can:

TEAM LinG



47

Quality through Continuous Improvement Process

• Promote the need for the quality assurance and other departments
(particularly development) to work closely together; QA should be
viewed as the “good guys” trying to make the software products the
best in the world.

• Point out that a defect discovered before production is one that
won’t be discovered by the users.

Point 10: Eliminate Slogans, Exhortations, and Targets 
for the Workforce

“Slogans never helped anybody do a good job. They generate frustration
and resentment.” Slogans such as “Zero Defects” or “Do It Right the First
Time” are fine on the surface. The problem is that they are viewed as sig-
nals that management does not understand employees’ problems, or care.
There is a common practice of setting goals without describing how they
are going to be accomplished.

To apply this point, an information technology quality assurance organi-
zation can:

• Encourage management to avoid the use of slogans.
• Rather than generate slogans, develop and document quality stan-

dards, procedures, and processes that the rest of the organization
can use to help maximize quality.

Point 11: Eliminate Numerical Goals

“Quotas or other work standards, such as measured day work or rates,
impede quality perhaps more than any other single working condition. As
work standards are generally used, they guarantee inefficiency and high
costs.” A proper work standard would define what is and is not acceptable
in terms of quality.

To apply this point, an information technology quality assurance organi-
zation can:

• Look not just at the numbers but look carefully at the quality stan-
dards.

• Avoid formally publicizing defect rates by individual or department.
• Work with the development organization to define quality standards

and procedures to improve quality. 
• When there are specific quality issues, have the department manager

address them informally.

Point 12: Remove Barriers to Pride of Workmanship

People are regarded as a commodity, to be used as needed. If not needed,
they are returned to the market. Managers cope with many problems but

TEAM LinG



48

SOFTWARE QUALITY IN PERSPECTIVE

tend to shy away from people problems. They often form “Quality Control
Circles,” but this is often a way for a manager to pretend to be doing some-
thing about a problem. Management seldom invests employees with any
authority, nor does it act upon their recommendations.

To apply this point, an information technology quality assurance organi-
zation can:

• Instill an image that quality is their deliverable and is a very valuable
commodity.

• Delegate responsibility to the staff to seek out quality and do what-
ever it takes to accomplish it.

Point 13: Institute a Vigorous Program of Education and Retraining

People must acquire new knowledge and skills. Education and retraining is
an investment in people, which is required for long-term planning. Educa-
tion and training must fit people into new jobs and responsibilities.

To apply this point, an information technology quality assurance organi-
zation can:

• Encourage quality staff to constantly increase their knowledge of
quality and testing techniques by attending seminars and classes.

• Reward staff for creating new seminars and special interest groups.
• Retrain individuals in new quality skills.

Point 14: Take Action to Accomplish the Transformation

Top management needs to push these 13 points. Every employee, includ-
ing managers, should acquire a precise idea of how to improve quality con-
tinually, but the initiative must come from top management. The following
discusses a process that can be used to apply Deming’s Point 14. It is also
the process that is constantly reinforced in this text to improve software-
testing processes.

Continuous Improvement through the Plan, Do, Check, Act Process

The term control has various meanings, including supervising, governing,
regulating, or restraining. The control in quality control means defining the
objective of the job, developing and carrying out a plan to meet that objec-
tive, and checking to determine if the anticipated results are achieved. If
the anticipated results are not achieved, modifications are made in the
work procedure to fulfill the plan.

One way to describe the above is with the “Deming Cycle” (or PDCA cir-
cle; see Exhibit 3.1), named after Deming in Japan because he introduced it
there, although it was originated by Shewhart. It was the basis of the turn-
around of the Japanese manufacturing industry, in addition to other

TEAM LinG



49

Quality through Continuous Improvement Process

Deming management principles. The word management describes many
different functions, encompassing policy management, human resources
management, and safety control, as well as component control and man-
agement of materials, equipment, and daily schedules. In this text, the
Deming model is applied to software quality.

In the Plan quadrant of the circle, one defines her objectives and deter-
mines the conditions and methods required to achieve them. It is crucial to
clearly describe the goals and policies needed to achieve the objectives at
this stage. A specific objective should be documented numerically, if pos-
sible. The procedures and conditions for the means and methods to
achieve the objectives are described.

In the Do quadrant of the circle, the conditions are created and the nec-
essary training to execute the plan is performed. It is paramount that
everyone thoroughly understands the objectives and the plan. Workers
need to be taught the procedures and skills they need to fulfill the plan and
thoroughly understand the job. The work is then performed according to
these procedures.

In the Check quadrant of the circle, one must check to determine
whether work is progressing according to the plan and whether the
expected results are obtained. The performance of the set procedures
must be checked against changes in conditions, or abnormalities that may
appear. As often as possible, the results of the work should be compared
with the objectives. If a check detects an abnormality — that is, if the
actual value differs from the target value — then a search for the cause of
the abnormality must be initiated to prevent its recurrence. Sometimes it
is necessary to retrain workers and revise procedures. It is important to
make sure these changes are reflected and more fully developed in the next
plan.

In the Action quadrant of the circle, if the checkup reveals that the work
is not being performed according to plan or results are not what was antic-
ipated, measures must be devised for appropriate action.

Exhibit 3.1. The Deming Quality Circle

TEAM LinG



50

SOFTWARE QUALITY IN PERSPECTIVE

Going around the PDCA Circle

The above procedures not only ensure that the quality of the products
meets expectations, but they also ensure that the anticipated price and
delivery date are fulfilled. Sometimes our preoccupation with current con-
cerns makes us unable to achieve optimal results. By going around the
PDCA circle, we can improve our working methods and obtain the desired
results. Repeated use of PDCA makes it possible to improve the quality of
the work, the work methods, and the results. Sometimes this concept is
depicted as an ascending spiral as illustrated in Exhibit 3.2.

Exhibit 3.2. The Ascending Spiral

TEAM LinG



Section II
Life Cycle 

Testing Review

TEAM LinG



52

LIFE CYCLE TESTING REVIEW

The life cycle development methodology consists of distinct phases from
requirements to coding. Life cycle testing means that testing occurs in par-
allel with the development life cycle and is a continuous process. Deming’s
continuous improvement process is applied to software testing using the
quality circle, principles, and statistical techniques.

The psychology of life cycle testing encourages testing to be performed
outside the development organization. The motivation for this is that there
are clearly defined requirements, and it is more efficient for a third party to
verify these requirements.

The test plan is the bible of software testing and is a document prescrib-
ing the test objectives, scope, strategy approach, and test details. There
are specific guidelines for building a good test plan.

The two major quality assurance verification approaches for each life
cycle phase are technical reviews and software testing. Technical reviews
are more preventive; that is, they aim to remove defects as soon as possi-
ble. Software testing verifies the actual code that has been produced.

The objectives of this section are to:

• Discuss how life cycle testing is a parallel activity.
• Describe how Deming’s process improvement is applied.
• Discuss the psychology of life cycle development and testing.
• Discuss the components of a good test.
• List and describe how technical review and testing are verification

techniques.

TEAM LinG



53

Part 4

Overview

The following provides an overview of the “waterfall” life cycle develop-
ment methodology and the associated testing activities. Deming’s continu-
ous quality improvement is applied with technical review and testing tech-
niques.

Waterfall Development Methodology

The life cycle development or waterfall approach breaks the development
cycle down into discrete phases, each with a rigid sequential beginning
and end (see Exhibit 4.1). Each phase is fully completed before the next is
started. Once a phase is completed, in theory during development, one
never goes back to change it.

In Exhibit 4.1 you can see that the first phase in the waterfall is user
requirements. In this phase, the users are interviewed, their requirements
are analyzed, and a document is produced detailing what the users’
requirements are. Any reengineering or process redesign is incorporated
into this phase.

In the next phase, entity relation diagrams, process decomposition dia-
grams, and data flow diagrams are created to allow the system to be broken
into manageable components from a data and functional point of view. The
outputs from the logical design phase are used to develop the physical
design of the system. During the physical and program unit design phases,
various structured design techniques, such as database schemas, Yourdon
structure charts, and Warnier/Orr diagrams, are used to produce a design
specification that will be used in the next phase.

In the program unit design phase, programmers develop the system
according to the physical design produced in the previous phase. Once
complete, the system enters the coding phase, where it will be written in a
programming language, unit or component tested, integration tested, sys-
tem tested, and finally user tested, often called acceptance testing.

Now the application is delivered to the users for the operation and main-
tenance phase, not shown in Exhibit 4.1. Defects introduced during the life
cycle phases are detected, corrected, and new enhancements are incorpo-
rated into the application.

TEAM LinG



54

LIFE CYCLE TESTING REVIEW

Continuous Improvement “Phased” Approach

Deming’s continuous improvement process, which was discussed in the
previous section, is effectively applied to the waterfall development cycle
using the Deming quality cycle, or PDCA; that is, Plan, Do, Check, and Act.
It is applied from two points of view: software testing, and quality control
or technical reviews.

As defined in Section I, Software Quality in Perspective, the three major
components of quality assurance include software testing, quality control,
and software configuration management. The purpose of software testing
is to verify and validate the activities to ensure that the software design,
code, and documentation meet all the requirements imposed on them.
Software testing focuses on test planning, test design, test development,
and test execution. Quality control is the process and methods used to
monitor work and observe whether requirements are met. It focuses on
structured walkthroughs and inspections to remove defects introduced
during the software development life cycle.

Psychology of Life Cycle Testing

In the waterfall development life cycle there is typically a concerted effort
to keep the testing and development departments separate. This testing
organization is typically separate from the development organization with
a different reporting structure. The basis of this is that because require-
ments and design documents have been created at specific phases in the
development life cycle, a separate quality assurance organization should be
able to translate these documents into test plans, test cases, and test specifi-
cations. Underlying assumptions include the belief that (1) programmers

Exhibit 4.1. Waterfall Development Methodology

TEAM LinG



55

Overview

should not test their own programs and (2) programming organizations
should not test their own programs.

It is thought that software testing is a destructive process and that it
would be very difficult for a programmer to suddenly change his perspec-
tive from developing a software product to trying to find defects, or break-
ing the software. It was believed that programmers cannot effectively test
their own programs because they cannot bring themselves to attempt to
expose errors.

Part of this argument was that there will be errors due to the program-
mer’s misunderstanding of the requirements of the programs. Thus, a pro-
grammer testing his own code would have the same bias and would not be
as effective testing it as someone else.

It is not impossible for a programmer to test her own programs, but test-
ing is more effective when performed by someone who does not have a
stake in it, as a programmer does. Because the development deliverables
have been documented, why not let another individual verify them?

It is thought that a programming organization is measured by its ability
to produce a program or system on time and economically. It is difficult for
the programming organization to be objective, just as it is for an individual
programmer. If a concerted effort were made to find as many defects as
possible, it was believed that the project would probably be late and not
cost effective. Less quality is the result.

From a practical point of view, an independent organization should be
responsible for the quality of the software products. Such organizations as
product test or quality assurance were created to serve as independent
parties.

Software Testing as a Continuous Improvement Process

Software life cycle testing means that testing occurs in parallel with the
development cycle and is a continuous process (see Exhibit 4.2). The soft-
ware testing process should start early in the application life cycle, not just
in the traditional validation testing phase after the coding phase has been
completed. Testing should be integrated into application development. In
order to do so, there needs to be a commitment on the part of the develop-
ment organization and close communication with the quality assurance
function.

A test plan is started during the requirements phase. It is an organiza-
tion of testing work. It is a document describing the approach to be taken
for the intended testing activities and includes the items to be tested, the
types of tests to be performed, test schedules, human resources, reporting
procedures, evaluation criteria, and so on.

TEAM LinG



56

LIFE CYCLE TESTING REVIEW

During logical, physical, and program unit design, the test plan is refined
with more details. Test cases are also created. A test case is a specific set
of test data and test scripts. A test script guides the tester through a test
and ensures consistency among separate executions of the test. A test also
includes the expected results to verify whether the test met the objective
correctly. During the coding phase, test scripts and test data are generated.
During application testing, the test scripts are executed and the results are
analyzed.

Exhibit 4.2 shows a correspondence between application development
and the testing activities. The application development cycle proceeds
from user requirements and design until the code is completed. During test
design and development, the acceptance test criteria were established in a
test plan. As more details are refined, the system, integration, and unit test-
ing requirements are established. There may be a separate test plan for
each test type, or one plan may be used.

During test execution, the process is reversed and starts with unit test-
ing. Integration tests are performed that combine individual, unit-tested
pieces of code. Once this is completed, the system is tested from a total
system point of view. This is known as system testing. System testing is a
multifaceted test to evaluate the functionality, performance, and usability
of the system. The final test is the acceptance test, which is a user-run test
that verifies the ability of the system to meet the original user objectives and
requirements. In some cases the system test serves as the acceptance test.

If you will recall, the PDCA approach (i.e., Plan, Do, Check, and Act) is a
control mechanism used to control, supervise, govern, regulate, or restrain
a system. The approach first defines the objectives of a process, develops

Exhibit 4.2. Development Phases versus Testing Types

User
Requirements

Verifies

Verifies

Verifies

Verifies

Coding

Acceptance
Testing

Logical
Design

System
Testing

Physical
Design

Integration
Testing

Program
Unit Design

Unit
Testing

TEAM LinG



57

Overview

and carries out the plan to meet those objectives, and checks to determine
if the anticipated results are achieved. If they are not achieved, the plan is
modified to fulfill the objectives. The PDCA quality cycle can be applied to
software testing.

The Plan step of the continuous improvement process, when applied to
software testing, starts with a definition of the test objectives; for example,
what is to be accomplished as a result of testing. Testing criteria do more
than simply ensure that the software performs according to specifications.
Objectives ensure that all responsible individuals contribute to the defini-
tion of the test criteria to maximize quality.

A major deliverable of this step is a software test plan. A test plan is the
basis for accomplishing testing. The test plan should be considered an
ongoing document. As the system changes, so does the plan. The test plan
also becomes part of the system maintenance documentation after the
application is delivered to the user. The outline of a good test plan includes
an introduction, the overall plan, and testing requirements. As more detail
is available, the business functions, test logs, problem and summary
reports, test software, hardware, data, personnel requirements, test sched-
ule, test entry criteria, and exit criteria are added.

The Do step of the continuous improvement process when applied to
software testing describes how to design and execute the tests included in
the test plan. The test design includes test cases, test procedures and
scripts, expected results, function/test case matrix, test logs, and so on.
The more definitive a test plan is, the easier the test design will be. If the
system changes between development of the test plan and when the tests
are to be executed, the test plan should be updated accordingly; that is,
whenever the system changes, the test plan should change.

The test team is responsible for the execution of the tests and must
ensure that the test is executed according to the plan. Elements of the Do
step include selecting test tools, defining the resource requirements, defin-
ing the test setup conditions and environment, test requirements, and the
actual testing of the application.

The Check step of the continuous improvement process when applied
to software testing includes the evaluation of how the testing process is
progressing. Again, the credo for statisticians, “In God we trust. All others
must use data,” is crucial to the Deming method. It is important to base
decisions as much as possible on accurate and timely data. Testing metrics
such as the number and types of defects, the workload effort, and the
schedule status are key.

It is also important to create test reports. Testing began with setting
objectives, identifying functions, selecting tests to validate the test func-
tions, creating test conditions, and executing the tests. To construct test

TEAM LinG



58

LIFE CYCLE TESTING REVIEW

reports, the test team must formally record the results and relate them to
the test plan and system objectives. In this sense, the test report reverses
all the previous testing tasks.

Summary and interim test reports should be written at the end of testing
and at key testing checkpoints. The process used for report writing is the
same whether it is an interim or a summary report, and, like other tasks in
testing, report writing is also subject to quality control; that is, it should be
reviewed. A test report should at least include a record of defects discov-
ered, data reduction techniques, root cause analysis, the development of
findings, and recommendations to management to improve the testing pro-
cess.

The Act step of the continuous improvement process when applied to
software testing includes devising measures for appropriate actions relat-
ing to work that was not performed according to the plan or results that
were not anticipated in the plan. This analysis is fed back to the plan. Exam-
ples include updating the test suites, test cases, and test scripts, and
reevaluating the people, process, and technology dimensions of testing.

The Testing Bible: Software Test Plan

A test plan is a document describing the approach to be taken for intended
testing activities and serves as a service-level agreement between the qual-
ity assurance testing function and other interested parties, such as devel-
opment. A test plan should be developed early in the development cycle
and help improve the interactions of the analysis, design, and coding activi-
ties. A test plan defines the test objectives, scope, strategy and approach, test
procedures, test environment, test completion criteria, test cases, items to be
tested, the tests to be performed, the test schedules, personnel require-
ments, reporting procedures, assumptions, risks, and contingency planning.

While developing a test plan, one should be sure that it is simple, com-
plete, current, and accessible by the appropriate individuals for feedback
and approval. A good test plan flows logically and minimizes redundant
testing, demonstrates full functional coverage, provides workable proce-
dures for monitoring, tracking, and reporting test status, contains a clear
definition of the roles and responsibilities of the parties involved, has tar-
get delivery dates, and clearly documents the test results.

There are two ways of building a test plan. The first approach is a master
test plan that provides an overview of each detailed test plan, that is, a test
plan of a test plan. A detailed test plan verifies a particular phase in the
waterfall development life cycle. Test plan examples include unit, integra-
tion, system, and acceptance. Other detailed test plans include application
enhancements, regression testing, and package installation. Unit test plans
are code oriented and very detailed but short because of their limited

TEAM LinG



59

Overview

scope. System or acceptance test plans focus on the functional test or
black-box view of the entire system, not just a software unit. See Appendix
E1, Unit Test Plan, and Appendix E2, System/Acceptance Test Plan, for
more details.

The second approach is one test plan. This approach includes all the
test types in one test plan, often called the acceptance/system test plan,
but covers unit, integration, system, and acceptance testing and all the
planning considerations to complete the tests.

A major component of a test plan, often in the Test Procedure section, is
a test case, as shown in Exhibit 4.3. (Also see Appendix E8, Test Case.) A
test case defines the step-by-step process whereby a test is executed. It
includes the objectives and conditions of the test, the steps needed to set
up the test, the data inputs, the expected results, and the actual results.

Exhibit 4.3. Test Case Form

Date: _____________________ Tested by: ____________________________

System: _____________________ Environment: ____________________________

Objective: _____________________ Test ID  _______________   Req. ID ___________

Function: _____________________ Screen: ____________________________

Version: _____________________ Test Type: ____________________________

                             (Unit, Integ., System, Accept.)

Condition to test:

_________________________________________________________________________

_________________________________________________________________________

Data/steps to perform

_________________________________________________________________________

_________________________________________________________________________

Expected results:

_________________________________________________________________________

_________________________________________________________________________

_________________________________________________________________________

Actual results: Passed ____Failed ____

_________________________________________________________________________

_________________________________________________________________________

_________________________________________________________________________

TEAM LinG



60

LIFE CYCLE TESTING REVIEW

Other information such as the software, environment, version, test ID,
screen, and test type are also provided.

Major Steps to Develop a Test Plan

A test plan is the basis for accomplishing testing and should be considered
a living document; that is, as the application changes, the test plan should
change.

A good test plan encourages the attitude of “quality before design and
coding.” It is able to demonstrate that it contains full functional coverage,
and the test cases trace back to the functions being tested. It also contains
workable mechanisms for monitoring and tracking discovered defects and
report status. Appendix E2 is a System/Acceptance Test Plan template that
combines unit, integration, and system test plans into one. It is also used
in this section to describe how a test plan is built during the waterfall life
cycle development methodology.

The following are the major steps that need to be completed to build a
good test plan.

1. Define the Test Objectives

The first step for planning any test is to establish what is to be accom-
plished as a result of the testing. This step ensures that all responsible indi-
viduals contribute to the definition of the test criteria that will be used. The
developer of a test plan determines what is going to be accomplished with
the test, the specific tests to be performed, the test expectations, the crit-
ical success factors of the test, constraints, scope of the tests to be per-
formed, the expected end products of the test, a final system summary
report (see Appendix E11, System Summary Report), and the final signa-
tures and approvals. The test objectives are reviewed and approval for the
objectives is obtained.

2. Develop the Test Approach

The test plan developer outlines the overall approach or how each test will
be performed. This includes the testing techniques that will be used, test
entry criteria, test exit criteria, procedures to coordinate testing activities
with development, the test management approach, such as defect report-
ing and tracking, test progress tracking, status reporting, test resources
and skills, risks, and a definition of the test basis (functional requirement
specifications, etc.).

3. Define the Test Environment

The test plan developer examines the physical test facilities, defines the
hardware, software, and networks, determines which automated test tools
and support tools are required, defines the help desk support required,

TEAM LinG



61

Overview

builds special software required for the test effort, and develops a plan to
support the above.

4. Develop the Test Specifications

The developer of the test plan forms the test team to write the test specifi-
cations, develops test specification format standards, divides up the work
tasks and work breakdown, assigns team members to tasks, and identifies
features to be tested. The test team documents the test specifications for
each feature and cross-references them to the functional specifications. It
also identifies the interdependencies and work flow of the test specifica-
tions and reviews the test specifications.

5. Schedule the Test

The test plan developer develops a test schedule based on the resource
availability and development schedule, compares the schedule with dead-
lines, balances resources and workload demands, defines major check-
points, and develops contingency plans.

6. Review and Approve the Test Plan

The test plan developer or manager schedules a review meeting with the
major players, reviews the plan in detail to ensure it is complete and work-
able, and obtains approval to proceed.

Components of a Test Plan

A system or acceptance test plan is based on the requirement specifica-
tions and is required in a very structured development and test environ-
ment. System testing evaluates the functionality and performance of the
whole application and consists of a variety of tests including performance,
usability, stress, documentation, security, volume, recovery, and so on.
Acceptance testing is a user-run test that demonstrates the application’s
ability to meet the original business objectives and system requirements
and usually consists of a subset of system tests.

Exhibit 4.4 cross-references the sections of Appendix E2, System/Accep-
tance Test Plan, against the waterfall life cycle development phases. “Start”
in the intersection indicates the recommended start time, or first-cut of a
test activity. “Refine” indicates a refinement of the test activity started in a
previous life cycle phase. “Complete” indicates the life cycle phase in
which the test activity is completed.

Technical Reviews as a Continuous Improvement Process

Quality control is a key preventive component of quality assurance. Defect
removal via technical reviews during the development life cycle is an exam-
ple of a quality control technique. The purpose of technical reviews is to

TEAM LinG



62

LIFE CYCLE TESTING REVIEW
E

xh
ib

it
 4

.4
.

 S
ys

te
m

/A
cc

ep
ta

n
ce

 T
es

t 
P

la
n

 v
er

su
s 

P
h

as
e

Te
st

 S
ec

ti
on

R
eq

u
ir

em
en

ts
 

P
h

as
e

Lo
gi

ca
l 

D
es

ig
n

 
P

h
as

e

P
h

ys
ic

al
 

D
es

ig
n

 
P

h
as

e

P
ro

gr
am

 
U

n
it

 D
es

ig
n

 
P

h
as

e
C

od
in

g 
P

h
as

e

1.
In

tr
od

uc
ti

on

a.
Sy

st
em

 d
es

cr
ip

ti
on

St
ar

t
R

efi
ne

R
efi

ne
C

om
p

le
te

b
.

O
b

je
ct

iv
e

St
ar

t
R

efi
ne

R
efi

ne
C

om
p

le
te

c.
A

ss
um

p
ti

on
s

St
ar

t
R

efi
ne

R
efi

ne
C

om
p

le
te

d
.

R
is

ks
St

ar
t

R
efi

ne
R

efi
ne

C
om

p
le

te

e.
C

on
ti

ng
en

ci
es

St
ar

t
R

efi
ne

R
efi

ne
C

om
p

le
te

f.
C

on
st

ra
in

ts
St

ar
t

R
efi

ne
R

efi
ne

C
om

p
le

te

g.
A

p
p

ro
va

l s
ig

na
tu

re
s

St
ar

t
R

efi
ne

R
efi

ne
C

om
p

le
te

2.
Te

st
 a

p
p

ro
ac

h
 a

nd
 s

tr
at

eg
y

a.
Sc

op
e 

of
 t

es
ti

ng
St

ar
t

R
efi

ne
R

efi
ne

C
om

p
le

te

b
.

Te
st

 a
p

p
ro

ac
h

St
ar

t
R

efi
ne

R
efi

ne
C

om
p

le
te

c.
T

yp
es

 o
f t

es
ts

St
ar

t
R

efi
ne

R
efi

ne
C

om
p

le
te

d
.

Lo
gi

st
ic

s
St

ar
t

R
efi

ne
R

efi
ne

C
om

p
le

te

e.
R

eg
re

ss
io

n 
p

ol
ic

y
St

ar
t

R
efi

ne
R

efi
ne

C
om

p
le

te

f.
Te

st
 fa

ci
lit

y
St

ar
t

R
efi

ne
C

om
p

le
te

g.
Te

st
 p

ro
ce

d
ur

es
St

ar
t

R
efi

ne
C

om
p

le
te

h
.

Te
st

 o
rg

an
iz

at
io

n
St

ar
t

R
efi

ne
C

om
p

le
te

i.
Te

st
 li

b
ra

ri
es

St
ar

t
R

efi
ne

C
om

p
le

te

j.
Te

st
 t

oo
ls

St
ar

t
R

efi
ne

C
om

p
le

te

TEAM LinG



63

Overview
k.

Ve
rs

io
n 

co
nt

ro
l

St
ar

t
R

efi
ne

C
om

p
le

te

l.
C

on
fig

ur
at

io
n 

b
ui

ld
in

g
St

ar
t

R
efi

ne
C

om
p

le
te

m
.C

h
an

ge
 c

on
tr

ol
St

ar
t

R
efi

ne
C

om
p

le
te

3.
Te

st
 e

xe
cu

ti
on

 s
et

up

a.
Sy

st
em

 t
es

t 
p

ro
ce

ss
St

ar
t

R
efi

ne
C

om
p

le
te

b
.

Fa
ci

lit
y

St
ar

t
R

efi
ne

C
om

p
le

te

c.
R

es
ou

rc
es

St
ar

t
R

efi
ne

C
om

p
le

te

d
.

To
ol

 p
la

n
St

ar
t

R
efi

ne
C

om
p

le
te

e.
Te

st
 o

rg
an

iz
at

io
n

St
ar

t
R

efi
ne

C
om

p
le

te

4.
Te

st
 s

p
ec

ifi
ca

ti
on

s

a.
Fu

nc
ti

on
al

 d
ec

om
p

os
it

io
n

St
ar

t
R

efi
ne

R
efi

ne
C

om
p

le
te

b
.

Fu
nc

ti
on

s 
no

t 
to

 b
e 

te
st

ed
St

ar
t

R
efi

ne
R

efi
ne

C
om

p
le

te

c.
U

ni
t 

te
st

 c
as

es
St

ar
t

C
om

p
le

te

d
.

In
te

gr
at

io
n 

te
st

 c
as

es
St

ar
t

C
om

p
le

te

e.
Sy

st
em

 t
es

t 
ca

se
s

St
ar

t
R

efi
ne

C
om

p
le

te

f.
A

cc
ep

ta
nc

e 
te

st
 c

as
es

St
ar

t
R

efi
ne

R
efi

ne
C

om
p

le
te

5.
Te

st
 p

ro
ce

d
ur

es

a.
Te

st
 c

as
e,

 s
cr

ip
t,

 d
at

a 
d

ev
el

op
m

en
t

St
ar

t
R

efi
ne

R
efi

ne
R

efi
ne

C
om

p
le

te

b
.

Te
st

 e
xe

cu
ti

on
St

ar
t

R
efi

ne
R

efi
ne

R
efi

ne
C

om
p

le
te

c.
C

or
re

ct
io

n
St

ar
t

R
efi

ne
R

efi
ne

R
efi

ne
C

om
p

le
te

d
.

Ve
rs

io
n 

co
nt

ro
l

St
ar

t
R

efi
ne

R
efi

ne
R

efi
ne

C
om

p
le

te

e.
M

ai
nt

ai
ni

ng
 t

es
t 

lib
ra

ri
es

St
ar

t
R

efi
ne

R
efi

ne
R

efi
ne

C
om

p
le

te

f.
A

ut
om

at
ed

 t
es

t 
to

ol
 u

sa
ge

St
ar

t
R

efi
ne

R
efi

ne
R

efi
ne

C
om

p
le

te

TEAM LinG



64

LIFE CYCLE TESTING REVIEW
E

xh
ib

it
 4

.4
.

 S
ys

te
m

/A
cc

ep
ta

n
ce

 T
es

t 
P

la
n

 v
er

su
s 

P
h

as
e 

(C
on

ti
n

u
ed

)

Te
st

 S
ec

ti
on

R
eq

u
ir

em
en

ts
 

P
h

as
e

Lo
gi

ca
l 

D
es

ig
n

 
P

h
as

e

P
h

ys
ic

al
 

D
es

ig
n

 
P

h
as

e

P
ro

gr
am

 
U

n
it

 D
es

ig
n

 
P

h
as

e
C

od
in

g 
P

h
as

e

g.
P

ro
je

ct
 m

an
ag

em
en

t
St

ar
t

R
efi

ne
R

efi
ne

R
efi

ne
C

om
p

le
te

h
.

M
on

it
or

in
g 

an
d

 s
ta

tu
s 

re
p

or
ti

ng
St

ar
t

R
efi

ne
R

efi
ne

R
efi

ne
C

om
p

le
te

6.
Te

st
 t

oo
ls

a.
To

ol
s 

to
 u

se
St

ar
t

R
efi

ne
C

om
p

le
te

b
.

In
st

al
la

ti
on

 a
nd

 s
et

up
St

ar
t

R
efi

ne
C

om
p

le
te

c.
Su

p
p

or
t 

an
d

 h
el

p
St

ar
t

R
efi

ne
C

om
p

le
te

7.
P

er
so

nn
el

 r
es

ou
rc

es

a.
R

eq
ui

re
d

 s
ki

lls
St

ar
t

R
efi

ne
R

efi
ne

C
om

p
le

te

b
.

R
ol

es
 a

nd
 r

es
p

on
si

b
ili

ti
es

St
ar

t
R

efi
ne

R
efi

ne
C

om
p

le
te

c.
N

um
b

er
s 

an
d

 t
im

e 
re

q
ui

re
d

St
ar

t
R

efi
ne

R
efi

ne
C

om
p

le
te

d
.

T
ra

in
in

g 
ne

ed
s

St
ar

t
R

efi
ne

R
efi

ne
C

om
p

le
te

8.
Te

st
 s

ch
ed

ul
e

a.
D

ev
el

op
m

en
t 

of
 t

es
t 

p
la

n
St

ar
t

R
efi

ne
R

efi
ne

C
om

p
le

te

b
.

D
es

ig
n 

of
 t

es
t 

ca
se

s
St

ar
t

R
efi

ne
R

efi
ne

C
om

p
le

te

c.
D

ev
el

op
m

en
t 

of
 t

es
t 

ca
se

s
St

ar
t

R
efi

ne
R

efi
ne

C
om

p
le

te

d
.

Ex
ec

ut
io

n 
of

 t
es

t 
ca

se
s

St
ar

t
R

efi
ne

R
efi

ne
C

om
p

le
te

e.
R

ep
or

ti
ng

 o
f p

ro
b

le
m

s
St

ar
t

R
efi

ne
R

efi
ne

C
om

p
le

te

f.
D

ev
el

op
in

g 
te

st
 s

um
m

ar
y 

re
p

or
t

St
ar

t
R

efi
ne

R
efi

ne
C

om
p

le
te

g.
D

oc
um

en
ti

ng
 t

es
t 

su
m

m
ar

y 
re

p
or

t
St

ar
t

R
efi

ne
R

efi
ne

C
om

p
le

te

TEAM LinG



65

Overview

increase the efficiency of the development life cycle and provide a method
to measure the quality of the products. Technical reviews reduce the
amount of rework, testing, and “quality escapes,” that is, undetected
defects. They are the missing links to removing defects and can also be
viewed as a testing technique, even though we have categorized testing as
a separate quality assurance component.

Originally developed by Michael Fagan of IBM in the 1970s, inspections
have several aliases. They are often referred to interchangeably as “peer
reviews,” “inspections,” or “structured walkthroughs.” Inspections are per-
formed at each phase of the development life cycle from user requirements
through coding. In the latter, code walkthroughs are performed in which
the developer walks through the code for the reviewer.

Research demonstrates that technical reviews can be a lot more produc-
tive than automated testing techniques in which the application is exe-
cuted and tested. A technical review is a form of testing, or manual testing,
not involving program execution on the computer. Structured walk-
throughs and inspections are a more efficient means of removing defects
than software testing alone. They also remove defects earlier in the life
cycle, thereby reducing defect-removal costs significantly. They represent
a highly efficient, low-cost technique of defect removal and can potentially
result in a reduction of defect-removal costs of greater than two thirds
when compared to dynamic software testing. A side benefit of inspections
includes the ability to periodically analyze the defects recorded and
remove the root causes early in the software development life cycle.

The purpose of the following section is to provide a framework for
implementing software reviews. Discussed is the rationale for reviews, the
roles of the participants, planning steps for effective reviews, scheduling,
allocation, agenda definition, and review reports.

Motivation for Technical Reviews

The motivation for a review is that it is impossible to test all software.
Clearly, exhaustive testing of code is impractical. Technology also does not
exist for testing a specification or high-level design. The idea of testing a
software test plan is also bewildering. Testing also does not address qual-
ity issues or adherence to standards, which are possible with review pro-
cesses.

There is a variety of software technical reviews available for a project,
depending on the type of software product and the standards that affect
the review processes. The types of reviews depend on the deliverables to
be produced. For example, for a Department of Defense contract, there are
certain stringent standards for reviews that must be followed. These
requirements may not be required for in-house application development.

TEAM LinG



66

LIFE CYCLE TESTING REVIEW

A review increases the quality of the software product, reduces rework
and ambiguous efforts, reduces testing and defines test parameters, and is
a repeatable and predictable process. It is an effective method for finding
defects and discrepancies; it increases the reliability of the delivered product,
has a positive impact on the schedule, and reduces development costs.

Early detection of errors reduces rework at later development stages, clar-
ifies requirements and design, and identifies interfaces. It reduces the number
of failures during testing, reduces the number of retests, identifies require-
ments testability, and helps identify missing or ambiguous requirements.

Types of Reviews

There are formal and informal reviews. Informal reviews occur spontane-
ously among peers, and the reviewers do not necessarily have any respon-
sibility and do not have to produce a review report. Formal reviews are
carefully planned meetings in which reviewers are held responsible for their
participation and a review report is generated that contains action items.

The spectrum of review ranges from very informal peer reviews to
extremely formal and structured inspections. The complexity of a review is
usually correlated to the complexity of the project. As the complexity of a
project increases, the need for more formal reviews increases.

Structured Walkthroughs

A structured walkthrough is a presentation review in which a review par-
ticipant, usually the developer of the software being reviewed, narrates a
description of the software, and the remainder of the group provides feed-
back throughout the presentation. Testing deliverables such as test plans,
test cases, and test scripts can also be reviewed using the walkthrough
technique. These are referred to as presentation reviews because the bulk
of the feedback usually occurs only for the material actually presented.

Advanced preparation of the reviewers is not necessarily required. One
potential disadvantage of a structured walkthrough is that, because of its
informal structure, it may lead to disorganized and uncontrolled reviews.
Walkthroughs may also be stressful if the developer is conducting the walk-
through.

Inspections

The inspection technique is a formally defined process for verification of
the software product throughout its development. All software deliver-
ables are examined at defined phases to assess the current status and qual-
ity effectiveness from the requirements to coding phase. One of the major
decisions within an inspection is whether a software deliverable is eligible
to proceed to the next development phase.

TEAM LinG



67

Overview

Software quality is achieved in a product during the early stages when
the cost to remedy defects is 10 to 100 times less than it would be during
testing or maintenance. It is, therefore, advantageous to find and correct
defects as near to their point of origin as possible. Exit criteria are the stan-
dard against which inspections measure completion of the product at the
end of a phase.

The advantages of inspections are that they are very systematic, con-
trolled, and less stressful. The inspection process promotes the concept of
egoless programming. If managed properly, it is a forum in which develop-
ers need not become emotionally protective of the work produced. An
inspection requires an agenda to guide the review preparation and the
meeting itself. Inspections have rigorous entry and exit requirements for
the project work deliverables.

A major difference between structured walkthroughs and inspections is
that inspections collect information to improve the development and
review processes themselves. In this sense, an inspection is more of a qual-
ity assurance technique than walkthroughs.

Phased inspections apply the PDCA (Plan, Do, Check, and Act) quality
model. Each development phase has entrance requirements, for example,
how to qualify to enter an inspection and exit criteria, and how to know
when to exit the inspection. In between the entry and exit are the project
deliverables that are inspected. In Exhibit 4.5, the steps of a phased inspec-
tion and the corresponding PDCA steps are shown.

Exhibit 4.5.  PDCA Process and Inspections

Inspection Step Description Plan Do Check Act

1. Planning Identify participants, get 
materials together, 
schedule the overview

�

2. Overview Educate for the 
inspections

�

3. Preparation Individual preparation 
for the inspections

�

4. Inspection Actual inspection to 
identify defects

� �

5. Rework Rework to correct any 
defects

�

6. Follow-up Follow up to ensure all 
defects are corrected

�

TEAM LinG



68

LIFE CYCLE TESTING REVIEW

The Plan step of the continuous improvement process consists of
inspection planning and preparing an education overview. The strategy of
an inspection is to design and implement a review process that is timely,
efficient, and effective. Specific products are designated, as are acceptable
criteria, and meaningful metrics are defined to measure and maximize the
efficiency of the process. Inspection materials must meet inspection entry
criteria. The right participants are identified and scheduled. In addition, a
suitable meeting place and time is arranged. The group of participants is
educated on what is to be inspected and their roles.

The Do step includes individual preparation for the inspections and the
inspection itself. Participants learn the material, prepare for their assigned
roles, and the inspection proceeds. Each review is assigned one or more
specific aspects of the product to be reviewed in terms of technical accu-
racy, standards and conventions, quality assurance, and readability.

The Check step includes the identification and documentation of the
defects uncovered. Defects are discovered during the inspection, but solu-
tion hunting and the discussion of design alternatives are discouraged.
Inspections are a review process, not a solution session.

The Act step includes the rework and follow-up required to correct any
defects. The author reworks all discovered defects. The team ensures that
all the potential corrective actions are effective and no secondary defects
are inadvertently introduced.

By going around the PDCA cycle for each development phase using
inspections we verify and improve each phase deliverable at its origin and
stop it dead in its tracks when defects are discovered (see Exhibit 4.6). The

Exhibit 4.6. Phased Inspections as an Ascending Spiral

PDCA

PDCA

PDCA

PDCA

PDCA

User
Requirements

Logical
Design

Physical
Design

Program
Unit Design

Unit
Testing

Integration
Testing

System
Testing

Acceptance
Testing

Verifies

Verifies

Verifies

Verifies

Coding

TEAM LinG



69

Overview

next phase cannot start until the discovered defects are corrected. The
reason is that it is advantageous to find and correct defects as near to their
point of origin as possible. Repeated application of the PDCA results in an
ascending spiral to facilitate quality improvement at each phase. The end
product is dramatically improved, and the bewildering task of the software
testing process will be minimized; for example, a lot of the defects will have
been identified and corrected by the time the testing team receives the
code.

Participant Roles

Roles will depend on the specific review methodology being followed, that
is, structured walkthroughs or inspections. These roles are functional,
which implies that it is possible in some reviews for a participant to exe-
cute more than one role. The role of the review participants after the
review is especially important because many errors identified during a
review may not be fixed correctly by the developer. This raises the issue of
who should follow up on a review and whether another review is necessary.

The review leader is responsible for the review. This role requires sched-
uling the review, conducting an orderly review meeting, and preparing the
review report. The review leader may also be responsible for ensuring that
action items are properly handled after the review process. Review leaders
must possess both technical and interpersonal management characteris-
tics. The interpersonal management qualities include leadership ability,
mediator skills, and organizational talents. The review leader must keep
the review group focused at all times and prevent the meeting from becom-
ing a problem-solving session. Material presented for review should not
require the review leader to spend more than two hours for preparation.

The recorder role in the review process guarantees that all information
necessary for an accurate review report is preserved. The recorder must
digest complicated discussions and capture their essence in action items.
The role of the recorder is clearly a technical function and one that cannot
be performed by a nontechnical individual.

The reviewer role is to objectively analyze the software and be account-
able for the review. An important guideline is that the reviewer must keep
in mind that the software is being reviewed and not the producer of the
software. This cannot be overstated. Also, the number of reviewers should
be limited to six. If too many reviewers are involved, productivity will
decrease.

In a technical review, the producer may actually lead the meeting in an
organized discussion of the software. A degree of preparation and planning
is needed in a technical review to present material at the proper level and
pace. The attitude of the producer is also important, and it is essential that

TEAM LinG



70

LIFE CYCLE TESTING REVIEW

he does not take a defensive approach. This can be facilitated by the group
leader’s emphasizing that the purpose of the inspection is to uncover
defects and produce the best product possible.

Steps for an Effective Review

1. Plan for the Review Process

Planning can be described at both the organizational level and the specific
review level. Considerations at the organizational level include the number
and types of reviews that are to be performed for the project. Project
resources must be allocated for accomplishing these reviews.

At the specific review level, planning considerations include selecting
participants and defining their respective roles, scheduling the review, and
developing a review agenda. There are many issues involved in selecting
the review participants. It is a complex task normally performed by man-
agement, with technical input. When selecting review participants, care
must be exercised to ensure that each aspect of the software under review
can be addressed by at least some subset of the review team.

In order to minimize the stress and possible conflicts in the review pro-
cesses, it is important to discuss the role that a reviewer plays in the orga-
nization and the objectives of the review. Focus on the review objectives
will lessen personality conflicts.

2. Schedule the Review

A review should ideally take place soon after a producer has completed the
software but before additional effort is expended on work dependent on
the software. The review leader must state the agenda based on a well
thought-out schedule. If all the inspection items have not been completed,
another inspection should be scheduled.

The problem of allocating sufficient time to a review stems from the dif-
ficulty in estimating the time needed to perform the review. The approach
that must be taken is the same as that for estimating the time to be allo-
cated for any meeting; that is, an agenda must be formulated and time esti-
mated for each agenda item. An effective technique is to estimate the time
for each inspection item on a time line.

Another scheduling problem is the duration of the review when the
review is too long. This requires that review processes be focused in terms
of their objectives. Review participants must understand these review
objectives and their implications in terms of actual review time, as well as
preparation time, before committing to the review. The deliverable to be
reviewed should meet a certain set of entry requirements before the review
is scheduled. Exit requirements must also be defined.

TEAM LinG



71

Overview

3. Develop the Review Agenda

A review agenda must be developed by the review leader and the producer
prior to the review. Although review agendas are specific to any particular
product and the objective of its review, generic agendas should be pro-
duced for related types of products. These agendas may take the form of
checklists (see Appendix F, Checklists, for more details).

4. Create a Review Report

The output of a review is a report. The format of the report is not impor-
tant. The contents should address the management perspective, user per-
spective, developer perspective, and quality assurance perspective.

From a management perspective, the review report serves as a sum-
mary of the review that highlights what was reviewed, who did the review-
ing, and their assessment. Management needs an estimate of when all
action items will be resolved to successfully track the project.

The user may be interested in analyzing review reports for some of the
same reasons as the manager. The user may also want to examine the qual-
ity of intermediate work products in an effort to monitor the development
organization’s progress.

From a developer’s perspective, the critical information is contained in
the action items. These may correspond to actual errors, possible prob-
lems, inconsistencies, or other considerations that the developer must
address.

The quality assurance perspective of the review report is twofold: qual-
ity assurance must ensure that all action items in the review report are
addressed, and it should also be concerned with analyzing the data on the
review forms and classifying defects to improve the software development
and review process. For example, a high number of specification errors
might suggest a lack of rigor or time in the requirements specifications
phase of the project. Another example is a high number of defects
reported, suggesting that the software has not been adequately unit tested.

TEAM LinG



TEAM LinG



73

Part 5

Verifying the 
Requirements Phase

The testing process should begin early in the application development life
cycle, not just at the traditional testing phase at the end of coding. Testing
should be integrated with the application development phases.

During the requirements phase of the software development life cycle,
the business requirements are defined on a high level and are the basis of
the subsequent phases and the final implementation. Testing in its broad-
est sense commences during the requirements phase (see Exhibit 5.1),
which increases the probability of a quality system based on the user’s
expectations. The result is the requirements are verified as correct and
complete. Unfortunately, more often than not, poor requirements are pro-
duced at the expense of the application. Poor requirements ripple down
the waterfall and result in a product that does not meet the user’s expecta-
tions. Some examples of poor requirements include:

• Partial set of functions defined
• Performance not considered
• Ambiguous requirements
• Security not defined
• Interfaces not documented
• Erroneous and redundant requirements
• Requirements too restrictive
• Contradictory requirements

The functionality is the most important part of the specification and
should include a hierarchic decomposition of the functions. The reason for
this is that it provides a description that is described in levels to enable all
the reviewers to read as much detail as needed. Specifically, this will make
the task of translating the specification to test requirements much easier.

Another important element of the requirements specification is the data
description (see Appendix C, Requirements Specification, for more
details). It should contain details such as whether the database is rela-
tional or hierarchical. If it is hierarchical, a good representation is with a
data model or entity relationship diagram in terms of entities, attributes,
and relationships.

TEAM LinG



74

LIFE CYCLE TESTING REVIEW

Another section in the requirements should be a description of the inter-
faces between the system and external entities that interface with the sys-
tem, such as users, external software, or external hardware. The human
interaction should include a description of how the users will interact with
the system. This would include the form of the interface and the technical
capabilities of the users.

During the requirements phase, the testing organization needs to per-
form two functions simultaneously. It needs to build the system/accep-
tance test plan and also verify the requirements. The requirements verifi-
cation entails ensuring the correctness and completeness of the
documentation prepared by the development team.

Testing the Requirements with Technical Reviews

The requirements definition is the result of creative thinking and is the
basis for design. The requirements phase is verified with static techniques,
that is, nonexecution of the application because it does not yet exist. These
techniques check adherence to specification conventions, completeness,
and language syntax, and include the following (see Appendix G, Software
Testing Techniques, for more details).

Inspections and Walkthroughs

These are formal techniques to evaluate the documentation form, interface
requirements, and solution constraints as described in the previous section.

Checklists

These are oriented toward quality control and include questions to ensure
the completeness of the requirements.

Exhibit 5.1. Requirements Phase and Acceptance Testing

TEAM LinG



75

Verifying the Requirements Phase

Methodology Checklist

This provides the methodology steps and tasks to ensure that the method-
ology is followed.

If the review is totally successful with no outstanding issues or defects
discovered, the requirements specification is frozen, and any further
refinements are monitored rigorously. If the review is not totally successful
and there are minor issues during the review, the author corrects them and
it is reviewed by the moderator and signed off. On the other hand, if major
issues and defects are discovered during the requirements review process,
the defects are corrected and a new review occurs with the same review
members at a later time.

Each defect uncovered during the requirements phase review should be
documented. Requirement defect trouble reports are designed to assist in
the proper recording of these defects. It includes the defect category and
defect type. The description of each defect is recorded under the missing,
wrong, or extra columns. At the conclusion of the requirements review, the
defects are summarized and totaled. Exhibit 5.2 shows a partial require-
ments phase defect recording form (see Appendix F1, Requirements Phase
Defect Checklist, for more details).

Exhibit 5.2.  Requirements Phase Defect Recording

Defect Category Missing Wrong Extra Total

1. Operating rules (or information) are 
inadequate or partially missing

2. Performance criteria (or information) are 
inadequate or partially missing

3. Environment information is inadequate or 
partially missing

4. System mission information is inadequate 
or partially missing

5. Requirements are incompatible

6. Requirements are incomplete

7. Requirements are missing

8. Requirements are incorrect

9. The accuracy specified does not conform 
to the actual need

10. The data environment is inadequately 
described

TEAM LinG



76

LIFE CYCLE TESTING REVIEW

Requirements Traceability Matrix

A requirements traceability matrix is a document that traces user require-
ments from analysis through implementation. It can be used as a complete-
ness check to verify that all requirements are present or that there are no
unnecessary/extra features, and as a maintenance guide for new person-
nel. At each step in the development cycle, the requirements, code, and
associated test cases are recorded to ensure that the user requirement is
addressed in the final system. Both the user and developer have the ability
to easily cross-reference the requirements to the design specifications,
programming, and test cases. See Appendix E3, Requirements Traceability
Matrix, for more details.

Building the System/Acceptance Test Plan

Acceptance testing verifies that a system satisfies the user’s acceptance
criteria. The acceptance test plan is based on the requirement specifica-
tions and is required in a formal test environment. This test uses black-box
techniques to test the system against its specifications and is generally
tested by the end user. During acceptance testing, it is important for the
project team to coordinate the testing process and update the acceptance
criteria, as needed. Acceptance testing is often combined with the system-
level test plan, which is the case in this discussion.

The requirements phase is the first development phase that is com-
pleted before proceeding to the logical design, physical design, program
unit design, and coding phases. During the requirements phase, it is not
expected that all sections in the test plan will be completed, for not enough
information is available.

In the Introduction section of the test plan (see Appendix E2, Sys-
tem/Acceptance Test Plan), the documentation of “first-cut” test activities
starts. Included are the system description, the overall system description,
acceptance test objectives, assumptions, risks, contingencies, and con-
straints. At this point some thought about the appropriate authorities for
the approval signatures begins.

The key parts in the Test Approach and Strategy section include: (1) the
scope of testing, (2) test approach, (3) types of tests, (4) logistics, and (5)
the regression policy. The scope of testing defines the magnitude of the
testing effort; for example, whether to test the whole system or part. The
testing approach documents the basis of the test design approach, for
example, black-box, white-box, gray-box testing, incremental integration,
and so on. The types of tests identify the test types, such as unit, integra-
tion, system, or acceptance, that will be performed within the testing
scope. Details of the types of system-level tests may not be available at this
point because of the lack of details but will be available during the next

TEAM LinG



77

Verifying the Requirements Phase

phase. Logistics documents the working relationship between the develop-
ment and testing organizations and other interested parties. It defines such
issues as how and when the testing group will receive the software, and
how defects will be recorded, corrected, and verified. The regression pol-
icy determines whether previously tested system functions perform prop-
erly after changes are introduced.

A major difficulty in testing the requirements document is that testers
have to determine whether the problem definition has been translated
properly to the requirements document. This requires envisioning the final
product and coming up with what should be tested to determine that the
requirement solves the problem.

A useful technique to help analyze, review, and document the initial cut
at the functional decomposition of the system in the Test Specifications
section is the requirement/test matrix (see Exhibit 5.3). This matrix defines
the scope of the testing for the project and ensures that tests are specified for
each requirement as documented in the requirements specification. It also
helps identify the functions to be tested as well as those not to be tested.

Exhibit 5.3. Requirements/Test Matrix

Test CaseTest

Requirement
Functional

1
2
3
4

Performance
1
2
3
4

1
2
3
4

Security

1 2 3 4 5 6 7 8 9 Comment

U – Users reviewed
Q – QA reviewed
T – Ready for testing

Q T

Q Q

Q Q Q

Q

T

T

U

U

TEAM LinG



78

LIFE CYCLE TESTING REVIEW

Some benefits of the requirements/test matrix are that it:

1. Correlates the tests and scripts with the requirements
2. Facilitates status of reviews
3. Acts as a traceability mechanism throughout the development cycle,

including test design and execution

The requirement/test matrix in Exhibit 5.3 documents each requirement
and correlates it with the test cases and scripts to verify it. The require-
ments listed on the left side of the matrix can also aid in defining the types
of system tests in the Test Approach and Strategy section.

It is unusual to come up with a unique test case for each requirement
and, therefore, it takes several test cases to test a requirement thoroughly.
This enables reusability of some test cases to other requirements. Once
the requirement/test matrix has been built, it can be reviewed, and test
case design and script building can commence.

The status column is used to track the status of each test case as it
relates to a requirement. For example, “Q” in the status column can indi-
cate that the requirement has been reviewed by QA, “U” can indicate that
the users had reviewed the requirement, and “T” can indicate that the test
case specification has been reviewed and is ready.

In the Test Specifications section of the test plan, information about the
acceptance tests is available and can be documented. These tests must be
passed in order for the user to accept the system. A procedure is a series
of related actions carried out using an operational mode, that is, one which
tells how to accomplish something. The following information can be doc-
umented in the Test Procedures section: test case, script, data develop-
ment, test execution, correction, version control, maintaining test librar-
ies, automated test tool usage, project management, monitoring, and
status reporting.

It is not too early to start thinking about the testing personnel resources
that will be needed. This includes the required testing skills, roles and
responsibilities, the numbers and time required, and the personnel train-
ing needs.

TEAM LinG



79

Part 6

Verifying the Logical 
Design Phase

The business requirements are defined during the requirements phase.
The logical design phase refines the business requirements in preparation
for system specification that can be used during physical design and cod-
ing. The logical design phase further refines the business requirements
that were defined in the requirement phase from a functional and informa-
tion model point of view.

Data Model, Process Model, and the Linkage

The logical design phase establishes a detailed system framework for
building the application. Three major deliverables from this phase are the
data model, also known as an entity relationship diagram, a process model,
and the linkage between the two.

A data model is a representation of the information needed or data
object types required by the application. It establishes the associations
between people, places, and things of importance to the application and is
used later in physical database design, which is part of the physical design
phase. A data model is a graphical technique used to define the entities and
the relationships. An entity is something about which we want to store
data. It is a uniquely identifiable person, place, thing, or event of interest to
the user, about which the application is to maintain and report data. Exam-
ples of entities are customers, orders, offices, and purchase orders.

Each entity is a table divided horizontally into rows and columns. Each
row is a specific occurrence of each entity, much like records in a file. Each
column is an attribute that helps describe the entity. Examples of
attributes include size, date, value, and address. Each entity in a data
model does not exist by itself and is linked to other entities by relation-
ships. A relationship is an association between two or more entities of
interest to the user, about which the application is to maintain and report
data. There are three types of relationships: a one-to-one relationship links
a single occurrence of an entity to zero or one occurrence of another entity;
a one-to-many relationship links one occurrence of an entity to zero or
more occurrences of an entity; and a many-to-many relationship links

TEAM LinG



80

LIFE CYCLE TESTING REVIEW

many occurrences of an entity to many occurrences of an entity. The type
of relationship defines the cardinality of the entity relationships. See
Appendix G10, Database Testing, for more details about data modeling.

A process is a business activity and the associated inputs and outputs.
Examples of processes are: accept order, update inventory, ship orders,
and schedule class. A process model is a graphical representation and
should describe what the process does but not refer to why, how, or when
the process is carried out. These are physical attributes of a process that
are defined in the physical design phase.

A process model is a decomposition of the business. Process decompo-
sition is the breakdown of the activities into successively more detail. It
starts at the top until elementary processes, the smallest unit of activity
that has meaning to the user, are defined.

A process decomposition diagram is used to illustrate processes in a
hierarchical structure showing successive levels of detail. The diagram is
built iteratively as processes and nonelementary processes are decom-
posed. The root of a process is the starting point of the decomposition. A
parent is the process at a higher level than lower levels. A child is the lower
level that is joined to a higher level, or parent. A data flow diagram is often
used to verify the process decomposition. It shows all the processes, data
store accesses, and the incoming and outgoing data flows. It also shows the
flows of data to and from entities external to the processes.

An association diagram, often called a CRUD matrix or process/data
matrix, links data and process models (see Exhibit 6.1). It helps ensure that
the data and processes are discovered and assessed. It identifies and
resolves matrix omissions and conflicts and helps refine the data and pro-
cess models, as necessary. It maps processes against entities showing
which processes create, read, update, or delete the instances in an entity.

This is often called “entity life cycle analysis” and analyzes the birth and
death of an entity and is performed by process against the entity. The ana-
lyst first verifies that there is an associated process to create instances in
the entity. If there is an entity that has no associated process that creates
it, a process is missing and must be defined. It is then verified that there are
associated processes to update, read, or delete instances in an entity. If
there is an entity that is never updated, read, or deleted, perhaps the entity
may be eliminated. See Appendix G9, CRUD Testing, for more details of how
this can be applied to software testing.

Testing the Logical Design with Technical Reviews

The logical design phase is verified with static techniques, that is, non-
execution of the application. As utilized in the requirements phase, these
techniques check the adherence to specification conventions and

TEAM LinG



81

Verifying the Logical Design Phase

completeness of the models. The same static testing techniques used to
verify the requirements are used in the logical design phase. The work
products to be reviewed include the data model, the process model, and
CRUD matrix.

Each defect discovered during the logical design review should be doc-
umented. A defect trouble report is designed to assist in the proper record-
ing of these defects. It includes the defect category and defect type. The
description of each defect is recorded under the missing, wrong, or extra
columns. At the conclusion of the logical design review, the defects are
summarized and totaled. Exhibit 6.2 shows a sample logical design phase
defect recording form (see Appendix F2, Logical Design Phase Defect
Checklist, for more details).

Refining the System/Acceptance Test Plan

System testing is a multifaceted test that evaluates the functionality, per-
formance, and fit of the whole application. It demonstrates whether the
system satisfies the original objectives. During the requirements phase,
enough detail was not available to define these types of tests. The logical

Exhibit 6.1. CRUD Matrix

TEAM LinG



82

LIFE CYCLE TESTING REVIEW

design provides a great deal more information with data and process mod-
els. The scope of testing and types of tests in the Test Approach and Strat-
egy section (see Appendix E2, System/Acceptance Test Plan) can now be
refined to include details concerning the types of system-level tests to be
performed. Examples of system-level tests to measure the fitness of use
include functional, performance, security, usability, and compatibility. The
testing approach, logistics, and regression policy are refined in this sec-
tion. The rest of the items in this section, such as the test facility, test pro-
cedures, test organization, test libraries, and test tools, are started. Prelim-
inary planning for the software configuration management elements, such
as version and change control and configuration building, can begin. This
includes acquiring a software configuration management tool if it does not
already exist in the organization.

The Test Execution Setup section deals with those considerations for
preparing for testing and includes the system test process, test facility,
required testing resources, the testing tool plan, and test organization.

In the Test Specifications section more functional details are available from
the data and process models and added in the requirements/test matrix. At
this point, system-level test case design is started. However, it is too early to
complete detailed test development, for example, test procedures, scripts,
and the test case input/output data values associated with each test case.
Acceptance test cases should be completed during this phase.

In the Test Procedures section, the items started in the previous phase
are refined. Test items in the Test Tools and Test Schedule sections are
started.

Exhibit 6.2. Logical Design Phase Defect Recording

Defect Category Missing Wrong Extra Total

1. The data has not been adequately defined

2. Entity definition is incomplete

3. Entity cardinality is incorrect

4. Entity attribute is incomplete

5. Normalization is violated

6. Incorrect primary key

7. Incorrect foreign key

8. Incorrect compound key

9. Incorrect entity subtype

10. The process has not been adequately 
defined

TEAM LinG



83

Part 7

Verifying the Physical 
Design Phase

The logical design phase translates the business requirements into system
specifications that can be used by programmers during physical design
and coding. The physical design phase determines how the requirements
can be automated. During this phase a high-level design is created in which
the basic procedural components and their interrelationships and major
data representations are defined.

The physical design phase develops the architecture, or structural
aspects, of the system. Logical design testing is functional; however, phys-
ical design testing is structural. This phase verifies that the design is struc-
turally sound and accomplishes the intent of the documented require-
ments. It assumes that the requirements and logical design are correct and
concentrates on the integrity of the design itself.

Testing the Physical Design with Technical Reviews

The logical design phase is verified with static techniques, that is, nonexe-
cution of the application. As with the requirements and logical design
phases, the static techniques check the adherence to specification conven-
tions and completeness, with a focus on the architectural design. The basis
for physical design verification is design representation schemes used to
specify the design. Example design representation schemes include struc-
ture charts, Warnier−Orr diagrams, Jackson diagrams, data navigation dia-
grams, and relational database diagrams, which have been mapped from
the logical design phase.

Design representation schemes provide mechanisms for specifying algo-
rithms and their inputs and outputs to software modules. Various inconsis-
tencies are possible in specifying the control flow of data objects through
the modules. For example, a module may need a particular data item that
another module creates but is not provided correctly. Static analysis can
be applied to detect these types of control flow errors.

Other errors made during the physical design can also be detected.
Design specifications are created by iteratively supplying detail. Although

TEAM LinG



84

LIFE CYCLE TESTING REVIEW

a hierarchical specification structure is an excellent vehicle for expressing
the design, it does not allow for inconsistencies between levels of detail.
For example, coupling measures the degree of independence between
modules. When there is little interaction between two modules, the mod-
ules are described as loosely coupled. When there is a great deal of inter-
action, they are tightly coupled. Loose coupling is considered a good
design practice.

Examples of coupling include content, common, control, stamp, and
data coupling. Content coupling occurs when one module refers to or
changes the internals of another module. Data coupling occurs when two
modules communicate via a variable or array (table) that is passed directly
as a parameter between the two modules. Static analysis techniques can
determine the presence or absence of coupling.

Static analysis of the design representations detects static errors and
semantic errors. Semantic errors involve information or data decomposi-
tion, functional decomposition, and control flow. Each defect uncovered
during the physical design review should be documented, categorized,
recorded, presented to the design team for correction, and referenced to
the specific document in which the defect was noted. Exhibit 7.1 shows a
sample physical design phase defect recording form (see Appendix F3,
Physical Design Phase Defect Checklist, for more details).

Exhibit 7.1.  Physical Design Phase Defect Recording

Defect Category Missing Wrong Extra Total

1. Logic or sequencing is erroneous

2. Processing is inaccurate

3. Routine does not input or output required 
parameters

4. Routine does not accept all data within 
the allowable range

5. Limit and validity checks are made on 
input data

6. Recovery procedures are not 
implemented or are not adequate

7. Required processing is missing or 
inadequate

8. Values are erroneous or ambiguous

9. Data storage is erroneous or inadequate

10. Variables are missing

TEAM LinG



85

Verifying the Physical Design Phase

Creating Integration Test Cases

Integration testing is designed to test the structure and the architecture of
the software and determine whether all software components interface
properly. It does not verify that the system is functionally correct, only that
it performs as designed.

Integration testing is the process of identifying errors introduced by
combining individual program unit tested modules. It should not begin
until all units are known to perform according to the unit specifications.
Integration testing can start with testing several logical units or can incor-
porate all units in a single integration test.

Because the primary concern in integration testing is that the units
interface properly, the objective of this test is to ensure that they integrate,
that parameters are passed, and the file processing is correct. Integration
testing techniques include top-down, bottom-up, sandwich testing, and
thread testing (see Appendix G, Software Testing Techniques, for more
details).

Methodology for Integration Testing

The following describes a methodology for creating integration test cases.

Step 1: Identify Unit Interfaces

The developer of each program unit identifies and documents the unit’s
interfaces for the following unit operations:

• External inquiry (responding to queries from terminals for informa-
tion)

• External input (managing transaction data entered for processing)
• External filing (obtaining, updating, or creating transactions on com-

puter files)
• Internal filing (passing or receiving information from other logical

processing units)
• External display (sending messages to terminals) 
• External output (providing the results of processing to some output

device or unit)

Step 2: Reconcile Interfaces for Completeness

The information needed for the integration test template is collected for all
program units in the software being tested. Whenever one unit interfaces
with another, those interfaces are reconciled. For example, if program unit
A transmits data to program unit B, program unit B should indicate that it
has received that input from program unit A. Interfaces not reconciled are
examined before integration tests are executed.

TEAM LinG



86

LIFE CYCLE TESTING REVIEW

Step 3: Create Integration Test Conditions

One or more test conditions are prepared for integrating each program
unit. After the condition is created, the number of the test condition is doc-
umented in the test template.

Step 4: Evaluate the Completeness of Integration Test Conditions

The following list of questions will help guide evaluation of the complete-
ness of integration test conditions recorded on the integration testing tem-
plate. This list can also help determine whether test conditions created for
the integration process are complete.

• Is an integration test developed for each of the following external
inquiries?
– Record test
– File test
– Search test
– Match/merge test
– Attributes test
– Stress test
– Control test

• Are all interfaces between modules validated so that the output of
one is recorded as input to another?

• If file test transactions are developed, do the modules interface with
all those indicated files?

• Is the processing of each unit validated before integration testing?
• Do all unit developers agree that integration test conditions are

adequate to test each unit’s interfaces?
• Are all software units included in integration testing?
• Are all files used by the software being tested included in integration

testing?
• Are all business transactions associated with the software being

tested included in integration testing?
• Are all terminal functions incorporated in the software being tested

included in integration testing?

The documentation of integration tests is started in the Test Specifica-
tions section (see Appendix E2, System/Acceptance Test Plan). Also in this
section, the functional decomposition continues to be refined, but the sys-
tem-level test cases should be completed during this phase.

Test items in the Introduction section are completed during this phase.
Items in the Test Approach and Strategy, Test Execution Setup, Test Proce-
dures, Test Tool, Personnel Requirements, and Test Schedule continue to
be refined.

TEAM LinG



87

Part 8
Verifying the Program 
Unit Design Phase
The design phase develops the physical architecture, or structural
aspects, of the system. The program unit design phase is refined to enable
detailed design. The program unit design is the detailed design in which
specific algorithmic and data structure choices are made. It is the specify-
ing of the detailed flow of control that will make it easily translatable to pro-
gram code with a programming language.

Testing the Program Unit Design with Technical Reviews

A good detailed program unit design is one that can easily be translated to
many programming languages. It uses structured techniques such as while,
for, repeat, if, and case constructs. These are examples of the constructs
used in structured programming. The objective of structured programming
is to produce programs with high quality at low cost. A structured program
is one in which only three basic control constructs are used.

Sequence

Statements are executed one after another in the same order as they
appear in the source listing. An example of a sequence is an assignment
statement.

Selection

A condition is tested and, depending on whether the test is true or false,
one or more alternative execution paths are executed. An example of a
selection is an if-then-else. With this structure the condition is tested and,
if the condition is true, one set of instructions is executed. If the condition
is false, another set of instructions is executed. Both sets join at a common
point.

Iteration

Iteration is used to execute a set of instructions a number of times with a
loop. Examples of iteration are dountil and dowhile. A dountil loop exe-
cutes a set of instructions and then tests the loop termination condition. If

TEAM LinG



88

LIFE CYCLE TESTING REVIEW

it is true, the loop terminates and continues to the next construct. If it is
false, the set of instructions is executed again until reaching the termina-
tion logic. A dowhile loop tests the termination condition. If it is true, con-
trol passes to the next construct. If it is false, a set of instructions is exe-
cuted until control is unconditionally passed back to the condition logic.

Static analysis of the detailed design detects semantic errors involving
information and logic control flow. Each defect uncovered during the pro-
gram unit design review should be documented, categorized, recorded,
presented to the design team for correction, and referenced to the specific
document in which the defect was noted. Exhibit 8.1 shows a sample pro-
gram unit design phase defect recording form (see Appendix F4, Program
Unit Design Phase Defect Checklist, for more details).

Creating Unit Test Cases

Unit testing is the process of executing a functional subset of the software
system to determine whether it performs its assigned function. It is ori-
ented toward the checking of a function or a module. White-box test cases
are created and documented to validate the unit logic and black-box test
cases to test the unit against the specifications (see Appendix E8, Test
Case, for a sample test case form). Unit testing, along with the version con-
trol necessary during correction and retesting, is typically performed by
the developer. During unit test case development it is important to know
which portions of the code have been subjected to test cases and which
have not. By knowing this coverage, the developer can discover lines of
code that are never executed or program functions that do not perform

Exhibit 8.1. Program Unit Design Phase Defect Recording

Defect Category Missing Wrong Extra Total

1. Is the if-then-else construct used 
incorrectly?

2. Is the dowhile construct used incorrectly?

3. Is the dountil construct used incorrectly?

4. Is the case construct used incorrectly?

5. Are there infinite loops?

6. Is it a proper program?

7. Are there goto statements?

8. Is the program readable?

9. Is the program efficient?

10. Does the case construct contain all the 
conditions?

TEAM LinG



89

Verifying the Program Unit Design Phase

according to the specifications. When coverage is inadequate, implement-
ing the system is risky because defects may be present in the untested por-
tions of the code (see Appendix G, Software Testing Techniques, for more
unit test case development techniques). Unit test case specifications are
started and documented in the Test Specification section (see Appendix
E2, System/Acceptance Test Plan), but all other items in this section
should have been completed.

All items in the Introduction, Test Approach and Strategy, Test Execu-
tion Setup, Test Tools, and Personnel Resources should have been com-
pleted prior to this phase. Items in the Test Procedures section, however,
continue to be refined. The functional decomposition, integration, system,
and acceptance test cases should be completed during this section.
Refinement continues for all items in the Test Procedures and Test Sched-
ule sections.

TEAM LinG



TEAM LinG



91

Part 9

Verifying the 
Coding Phase

The program unit design is the detailed design in which specific algorith-
mic and data structure choices are made. Specifying the detailed flow of
control will make it easily translatable to program code with a program-
ming language. The coding phase is the translation of the detailed design to
executable code using a programming language.

Testing Coding with Technical Reviews

The coding phase produces executable source modules. The basis of good
programming is programming standards that have been defined. Some
good standards should include commenting, unsafe programming con-
structs, program layout, defensive programming, and so on. Commenting
refers to how a program should be commented and to what level or degree.
Unsafe programming constructions are practices that can make the pro-
gram hard to maintain. An example is goto statements. Program layout
refers to how a standard program should be laid out on a page, indentation
of control constructs, and initialization. A defensive programming practice
describes the mandatory element of the programming defensive strategy. An
example is error condition handling and control to a common error routine.

Static analysis techniques, such as structured walkthroughs and inspec-
tions, are used to ensure the proper form of the program code and docu-
mentation. This is accomplished by checking adherence to coding and
documentation conventions and type checking.

Each defect uncovered during the coding phase review should be docu-
mented, categorized, recorded, presented to the design team for correc-
tion, and referenced to the specific document in which the defect was
noted. Exhibit 9.1 shows a sample coding phase defect recording form (see
Appendix F5, Coding Phase Defect Checklist, for more details).

Executing the Test Plan

By the end of this phase, all the items in each section of the test plan should
have been completed. The actual testing of software is accomplished through
the test data in the test plan developed during the requirements, logical

TEAM LinG



92

LIFE CYCLE TESTING REVIEW

design, physical design, and program unit design phases. Because results
have been specified in the test cases and test procedures, the correctness
of the executions is assured from a static test point of view; that is, the
tests have been reviewed manually.

Dynamic testing, or time-dependent techniques, involves executing a
specific sequence of instructions with the computer. These techniques are
used to study the functional and computational correctness of the code.

Dynamic testing proceeds in the opposite order of the development life
cycle. It starts with unit testing to verify each program unit independently
and then proceeds to integration, system, and acceptance testing. After
acceptance testing has been completed, the system is ready for operation
and maintenance. Exhibit 9.2 briefly describes each testing type.

Unit Testing

Unit testing is the basic level of testing. Unit testing focuses separately on
the smaller building blocks of a program or system. It is the process of exe-
cuting each module to confirm that each performs its assigned function.
The advantage of unit testing is that it permits the testing and debugging of
small units, thereby providing a better way to manage the integration of
the units into larger units. In addition, testing a smaller unit of code makes
it mathematically possible to fully test the code’s logic with fewer tests.
Unit testing also facilitates automated testing because the behavior of

Exhibit 9.1. Coding Phase Defect Recording

Defect Category Missing Wrong Extra Total

1. Decision logic or sequencing is erroneous 
or inadequate

2. Arithmetic computations are erroneous or 
inadequate

3. Branching is erroneous

4. Branching or other testing is performed 
incorrectly

5. There are undefined loop terminations

6. Programming language rules are violated

7. Programming standards are violated

8. The programmer misinterprets language 
constructs

9. Typographical errors exist

10. Main storage allocation errors exist

TEAM LinG



93

Verifying the Coding Phase

smaller units can be captured and played back with maximized reusability.
A unit can be one of several types of application software. Examples
include the module itself as a unit, GUI components such as windows,
menus, and functions, batch programs, online programs, and stored proce-
dures.

Integration Testing

After unit testing is completed, all modules must be integration tested. Dur-
ing integration testing, the system is slowly built up by adding one or more
modules at a time to the core of already integrated modules. Groups of
units are fully tested before system testing occurs. Because modules have
been unit tested prior to integration testing, they can be treated as black-
boxes, allowing integration testing to concentrate on module interfaces.
The goals of integration testing are to verify that each module performs
correctly within the control structure and that the module interfaces are
correct.

Incremental testing is performed by combining modules in steps. At
each step one module is added to the program structure, and testing con-
centrates on exercising this newly added module. When it has been dem-
onstrated that a module performs properly with the program structure,
another module is added, and testing continues. This process is repeated
until all modules have been integrated and tested.

System Testing

After integration testing, the system is tested as a whole for functionality
and fitness of use based on the system/acceptance test plan. Systems are

Exhibit 9.2. Executing the Tests

TEAM LinG



94

LIFE CYCLE TESTING REVIEW

fully tested in the computer operating environment before acceptance test-
ing occurs. The sources of the system tests are the quality attributes that
were specified in the software quality assurance plan. System testing is a
set of tests to verify these quality attributes and ensure that the accep-
tance test occurs relatively trouble-free. System testing verifies that the
functions are carried out correctly. It also verifies that certain nonfunc-
tional characteristics are present. Some examples include usability testing,
performance testing, stress testing, compatibility testing, conversion test-
ing, and document testing.

Black-box testing is a technique that focuses on testing a program’s
functionality against its specifications. White-box testing is a testing tech-
nique in which paths of logic are tested to determine how well they pro-
duce predictable results. Gray-box testing is a combination of these two
approaches and is usually applied during system testing. It is a compro-
mise between the two and is a well-balanced testing approach that is
widely used during system testing.

Acceptance Testing

After systems testing, acceptance testing certifies that the software system
satisfies the original requirements. This test should not be performed until
the software has successfully completed systems testing. Acceptance test-
ing is a user-run test that uses black-box techniques to test the system
against its specifications. The end users are responsible for assuring that
all relevant functionality has been tested.

The acceptance test plan defines the procedures for executing the
acceptance tests and should be followed as closely as possible. Accep-
tance testing continues even when errors are found, unless an error itself
prevents continuation. Some projects do not require formal acceptance
testing. This is true when the customer or user is satisfied with the other
system tests, when timing requirements demand it, or when end users
have been involved continuously throughout the development cycle and
have been implicitly applying acceptance testing as the system is devel-
oped.

Acceptance tests are often a subset of one or more system tests. Two
other ways to measure acceptance testing are as follows:

1. Parallel Testing — A business transaction level comparison with the
existing system to ensure that adequate results are produced by the
new system.

2. Benchmarks — A static set of results produced either manually or
from an existing system is used as expected results for the new
system.

TEAM LinG



95

Verifying the Coding Phase

Defect Recording

Each defect discovered during the above tests is documented to assist in
the proper recording of these defects. A problem report is generated when
a test procedure gives rise to an event that cannot be explained by the
tester. The problem report documents the details of the event and includes
at least these items (see Appendix E12, Defect Report, for more details):

• Problem Identification
• Author
• Release/Build Number 
• Open Date
• Close Date
• Problem Area
• Defect or Enhancement
• Test Environment
• Defect Type
• Who Detected
• How Detected
• Assigned to
• Priority
• Severity
• Status

Other test reports to communicate the testing progress and results
include a test case log, test log summary report, and system summary
report.

A test case log documents the test cases for a test type to be executed.
It also records the results of the tests, which provides the detailed evi-
dence for the test log summary report and enables reconstructing testing,
if necessary. See Appendix E9, Test Case Log, for more information.

A test log summary report documents the test cases from the tester’s
logs in progress or completed for the status reporting and metric collec-
tion. See Appendix E10, Test Log Summary Report.

A system summary report should be prepared for every major testing
event. Sometimes it summarizes all the tests. It typically includes the fol-
lowing major sections: general information (describing the test objectives,
test environment, references), test results and findings (describing each
test), software functions and findings, and analysis and test summary. See
Appendix E11, System Summary Report, for more details.

TEAM LinG



TEAM LinG



Section III
Software 
Testing 

Methodology

TEAM LinG



98

SOFTWARE TESTING METHODOLOGY

Spiral development methodologies are a reaction to the traditional water-
fall systems development in which the product evolves in sequential
phases. A common problem with the life cycle development model is that
the elapsed time to deliver the product can be excessive with user involve-
ment only at the very beginning and very end. As a result, the system that
they are given is often not what they originally requested.

By contrast, spiral development expedites product delivery. A small but
functioning initial system is built and quickly delivered, and then enhanced
in a series of iterations. One advantage is that the users receive at least
some functionality quickly. Another advantage is that the product can be
shaped by iterative feedback, for example, users do not have to define
every feature correctly and in full detail at the beginning of the develop-
ment cycle, but can react to each iteration.

Spiral testing is dynamic and may never be completed in the traditional
sense of a delivered system’s completeness. The term “spiral” refers to the
fact that the traditional sequence of analysis-design-code-test phases are
performed on a microscale within each spiral or cycle in a short period of
time, and then the phases are repeated within each subsequent cycle.

The objectives of this section are to:

• Discuss the limitations of waterfall development.
• Describe the complications of client/server.
• Discuss the psychology of spiral testing.
• Describe the iterative/spiral development environment.
• Apply Deming’s continuous improvement quality to a spiral devel-

opment environment in terms of:
– Information gathering
– Test planning
– Test case design
– Test development
– Test execution/evaluation
– Traceability/coverage matrix
– Preparing for the next spiral
– System testing
– Acceptance testing
– Summarizing/reporting spiral test results

TEAM LinG



99

Part 10

Development 
Methodology 
Overview

Limitations of Life Cycle Development

In Section II, Life Cycle Testing Review, the waterfall development method-
ology was reviewed along with the associated testing activities. The life
cycle development methodology consists of distinct phases from require-
ments to coding. Life cycle testing means that testing occurs in parallel
with the development life cycle and is a continuous process. Although the
life cycle or waterfall development is very effective for many large applica-
tions requiring a lot of computer horsepower, for example, DOD, financial,
security-based, and so on, it has a number of shortcomings.

• The end users of the system are only involved in the very beginning
and the very end of the process. As a result, the system that they
were given at the end of the development cycle is often not what
they originally visualized or thought they requested.

• The long development cycle and the shortening of business cycles
leads to a gap between what is really needed and what is delivered.

• End users are expected to describe in detail what they want in a
system, before the coding phase. This may seem logical to develop-
ers, however, there are end users who haven’t used a computer
system before and aren’t really certain of its capabilities.

• When the end of a development phase is reached, it is often not
quite complete, but the methodology and project plans require that
development press on regardless. In fact, a phase is rarely complete
and there is always more work than can be done. This results in the
“rippling effect,” where sooner or later, one must return to a phase
to complete the work.

• Often the waterfall development methodology is not strictly fol-
lowed. In the haste to produce something quickly, critical parts of
the methodology are not followed. The worst case is ad hoc devel-
opment in which the analysis and design phases are bypassed and

TEAM LinG



100

SOFTWARE TESTING METHODOLOGY

the coding phase is the first major activity. This is an example of an
unstructured development environment.

• Software testing is often treated as a separate phase starting in the
coding phase as a validation technique and is not integrated into
the whole development life cycle.

• The waterfall development approach can be woefully inadequate for
many development projects, even if it were followed. An imple-
mented software system is not worth very much if it is not the
system the user wanted. If the requirements are incompletely doc-
umented, the system will not survive user validation procedures;
that is, it is the wrong system. Another variation is when the require-
ments are correct, but the design is inconsistent with the require-
ments. Once again, the completed product will probably fail the
system validation procedures.

• Due to the above, experts began to publish methodologies based on
other approaches, such as prototyping.

The Client/Server Challenge

The client/server architecture for application development allocates func-
tionality between a client and server so that each performs its task indepen-
dently. The client cooperates with the server to produce the required results.

The client is an intelligent workstation used as a single user, and
because it has its own operating system it can run other applications such
as spreadsheets, word processors, and file processors. The user and the
server process client/server application functions cooperatively. The
server can be a PC, mini-computer, local area network, or even a main-
frame. The server receives requests from the clients and processes them.
The hardware configuration is determined by the application’s functional
requirements.

Some advantages of client/server applications include reduced costs,
improved accessibility of data, and flexibility. However, justifying a cli-
ent/server approach and assuring quality is difficult and presents addi-
tional difficulties not necessarily found in mainframe applications. Some of
these problems include the following:

• The typical graphical user interface has more possible logic paths,
and thus the large number of test cases in the mainframe environ-
ment is compounded.

• Client/server technology is complicated and often new to the orga-
nization. Furthermore, this technology often comes from multiple ven-
dors and is used in multiple configurations and in multiple versions.

• The fact that client/server applications are highly distributed results
in a large number of failure sources and hardware/software config-
uration control problems.

TEAM LinG



101

Development Methodology Overview

• A short- and long-term cost/benefit analysis must be performed to
include the overall organizational costs and benefits to justify a
client/server.

• Successful migration to a client/server depends on matching migra-
tion plans to the organization’s readiness for a client/server.

• The effect of client/server technology on the user’s business may
be substantial.

• Choosing which applications will be the best candidates for a cli-
ent/server implementation is not straightforward.

• An analysis needs to be performed of which development technol-
ogies and tools enable a client/server.

• Availability of client/server skills and resources, which are expen-
sive, needs to be considered.

• Although the cost of client/server is more expensive than mainframe
computing, cost is not the only issue. The function, business benefit,
and the pressure from end users have to be balanced.

Integration testing in a client/server environment can be challenging.
Client and server applications are built separately. When they are brought
together, conflicts can arise no matter how clearly defined the interfaces
are. When integrating applications, defect resolutions may have single or
multiple solutions, and there must be open communication between qual-
ity assurance and development.

In some circles there exists a belief that the mainframe is dead and the
client/server prevails. The truth of the matter is that applications using
mainframe architecture are not dead, and client/server is not necessarily
the panacea for all applications. The two will continue to coexist and com-
plement each other in the future. Mainframes will not prosper as they have
in the past but should certainly be part of any client/server strategy.

Psychology of Client/Server Spiral Testing

The New School of Thought

The psychology of life cycle testing encourages testing by individuals out-
side the development organization. The motivation for this is that with the
life cycle approach there typically exist clearly defined requirements, and
it is more efficient for a third party to verify the requirements. Testing is often
viewed as a destructive process designed to break development’s work.

The psychology of spiral testing, on the other hand, encourages cooper-
ation between quality assurance and the development organization. The
basis of this argument is that, in a rapid application development environ-
ment, requirements may or may not be available, to varying degrees. With-
out this cooperation, the testing function would have a difficult task defin-
ing the test criteria. The only possible alternative is for testing and
development to work together.

TEAM LinG



102

SOFTWARE TESTING METHODOLOGY

Testers can be powerful allies to development and, with a little effort,
they can be transformed from adversaries into partners. This is possible
because most testers want to be helpful; they just need a little consider-
ation and support. In order to achieve this, however, an environment needs to
be created to bring out the best of a tester’s abilities. The tester and develop-
ment manager must set the stage for cooperation early in the development
cycle and communicate throughout the development life cycle.

Tester/Developer Perceptions

To understand some of the inhibitors to a good relationship between the
testing function and development, it is helpful to understand how each
views his role and responsibilities.

Testing is a difficult effort. It is the task that’s both infinite and indefinite.
No matter what testers do, they can’t be sure they will find all the prob-
lems, or even all the important ones.

Many testers are not really interested in testing and do not have the
proper training in basic testing principles and techniques. Testing books or
conferences typically treat the testing subject too rigorously and employ
deep mathematical analysis. The insistence on formal requirement specifi-
cations as a prerequisite to effective testing is not realistic in the real world
of a software development project.

It is hard to find individuals who are good at testing. It takes someone
who is a critical thinker motivated to produce a quality software product,
likes to evaluate software deliverables, and is not caught up in the assump-
tion held by many developers that testing has a lesser job status than devel-
opment. A good tester is a quick learner and eager to learn, is a good team
player, and can effectively communicate both verbally and in written form.

The output from development is something that is real and tangible. A
programmer can write code and display it to admiring customers who
assume it is correct. From a developer’s point of view, testing results in
nothing more tangible than an accurate, useful, and all-too-fleeting per-
spective on quality. Given these perspectives, many developers and
testers often work together in an uncooperative, if not hostile, manner.

In many ways the tester and developer roles are in conflict. A developer
is committed to building something to be successful. A tester tries to min-
imize the risk of failure and tries to improve the software by detecting
defects. Developers focus on technology, which takes a lot of time and
energy when producing software. A good tester, on the other hand, is moti-
vated to provide the user with the best software to solve a problem.

Testers are typically ignored until the end of the development cycle
when the application is “completed.” Testers are always interested in the

TEAM LinG



103

Development Methodology Overview

progress of development and realize that quality is only achievable when
they take a broad point of view and consider software quality from multiple
dimensions.

Project Goal: Integrate QA and Development

The key to integrating the testing and developing activities is for testers to
avoid giving the impression that they are out to “break the code” or
destroy development’s work. Ideally, testers are human meters of product
quality and should examine a software product, evaluate it, and discover if
the product satisfies the customer’s requirements. They should not be out
to embarrass or complain, but inform development how to make their
product even better. The impression they should foster is that they are the
“developer’s eyes to improved quality.”

Development needs to be truly motivated to quality and view the test
team as an integral player on the development team. They need to realize
that no matter how much work and effort has been expended by develop-
ment, if the software does not have the correct level of quality, it is des-
tined to fail. The testing manager needs to remind the project manager of
this throughout the development cycle. The project manager needs to
instill this perception in the development team.

Testers must coordinate with the project schedule and work in parallel
with development. They need to be informed about what’s going on in
development and included in all planning and status meetings. This less-
ens the risk of introducing new bugs, known as “side-effects,” near the end
of the development cycle and also reduces the need for time-consuming
regression testing.

Testers must be encouraged to communicate effectively with everyone
on the development team. They should establish a good communication
relationship with the software users, who can help them better understand
acceptable standards of quality. In this way, testers can provide valuable
feedback directly to development.

Testers should intensively review online help and printed manuals
whenever they are available. It will relieve some of the communication bur-
den to get writers and testers to share notes rather than saddle develop-
ment with the same information.

Testers need to know the objectives of the software product, how it is
intended to work, how it actually works, the development schedule, any
proposed changes, and the status of reported problems.

Developers need to know what problems were discovered, what part of
the software is or is not working, how users perceive the software, what
will be tested, the testing schedule, the testing resources available, what

TEAM LinG



104

SOFTWARE TESTING METHODOLOGY

the testers need to know to test the system, and the current status of the
testing effort.

When quality assurance starts working with a development team, the
testing manager needs to interview the project manager and show an inter-
est in working in a cooperative manner to produce the best software prod-
uct possible. The next section describes how to accomplish this.

Iterative/Spiral Development Methodology

Spiral methodologies are a reaction to the traditional waterfall methodol-
ogy of systems development, a sequential solution development approach.
A common problem with the waterfall model is that the elapsed time for
delivering the product can be excessive.

By contrast, spiral development expedites product delivery. A small but
functioning initial system is built and quickly delivered, and then enhanced
in a series of iterations. One advantage is that the clients receive at least
some functionality quickly. Another is that the product can be shaped by
iterative feedback; for example, users do not have to define every feature
correctly and in full detail at the beginning of the development cycle, but
can react to each iteration.

With the spiral approach, the product evolves continually over time; it
is not static and may never be completed in the traditional sense. The term
“spiral” refers to the fact that the traditional sequence of analysis-design-
code-test phases are performed on a microscale within each spiral or
cycle, in a short period of time, and then the phases are repeated within
each subsequent cycle. The spiral approach is often associated with pro-
totyping and rapid application development.

Traditional requirements-based testing expects that the product defini-
tion will be finalized and even frozen prior to detailed test planning. With
spiral development, the product definition and specifications continue to
evolve indefinitely; that is, there is no such thing as a frozen specification.
A comprehensive requirements definition and system design probably
never will be documented.

The only practical way to test in the spiral environment, therefore, is to
“get inside the spiral.” Quality assurance must have a good working rela-
tionship with development. The testers must be very close to the develop-
ment effort, and test each new version as it becomes available. Each itera-
tion of testing must be brief, in order not to disrupt the frequent delivery
of the product iterations. The focus of each iterative test must be first to
test only the enhanced and changed features. If time within the spiral
allows, an automated regression test also should be performed; this
requires sufficient time and resources to update the automated regression
tests within each spiral.

TEAM LinG



105

Development Methodology Overview

Clients typically demand very fast turnarounds on change requests;
there may be neither formal release nor a willingness to wait for the next
release to obtain a new system feature. Ideally, there should be an efficient,
automated regression test facility for the product, which can be used for at
least a brief test prior to the release of the new product version (see Sec-
tion V, Modern Software Testing Tools, for more details).

Spiral testing is a process of working from a base and building a system
incrementally. Upon reaching the end of each phase, developers reexamine
the entire structure and revise it. Drawing the four major phases of system
development — planning/analysis, design, coding, and test/deliver — into
quadrants, as shown in Exhibit 10.1, represents the spiral approach. The
respective testing phases are test planning, test case design, test develop-
ment, and test execution/evaluation.

The spiral process begins with planning and requirements analysis to
determine the functionality. Then a design is made for the base compo-
nents of the system and the functionality determined in the first step. Next,
the functionality is constructed and tested. This represents a complete
iteration of the spiral.

Having completed this first spiral, the users are given the opportunity to
examine the system and enhance its functionality. This begins the second
iteration of the spiral. The process continues, looping around and around
the spiral until the users and developers agree the system is complete; the
process then proceeds to implementation.

The spiral approach, if followed systematically, can be effective for
ensuring that the users’ requirements are being adequately addressed and
that the users are closely involved with the project. It can allow for the sys-
tem to adapt to any changes in business requirements that occurred after

Exhibit 10.1. Spiral Testing Process

TEAM LinG



106

SOFTWARE TESTING METHODOLOGY

the system development began. However, there is one major flaw with this
methodology: there may never be any firm commitment to implement a
working system. One can go around and around the quadrants, never actu-
ally bringing a system into production. This is often referred to as “spiral
death.”

Although the waterfall development has often proved itself to be too
inflexible, the spiral approach can produce the opposite problem. Unfortu-
nately, the flexibility of the spiral methodology often results in the develop-
ment team ignoring what the user really wants, and thus the product fails
the user verification. This is where quality assurance is a key component of
a spiral approach. It will make sure that user requirements are being satis-
fied.

A variation to the spiral methodology is the iterative methodology
where the development team is forced to reach a point where the system
will be implemented. The iterative methodology recognizes that the sys-
tem is never truly complete, but is evolutionary. However, it also realizes
that there is a point at which the system is close enough to completion to
be of value to the end user.

The point of implementation is decided upon prior to the start of the
system and a certain number of iterations will be specified with goals iden-
tified for each iteration. Upon completion of the final iteration, the system
will be implemented in whatever state it may be.

Role of JADs

During the first spiral the major deliverables are the objectives, an initial
functional decomposition diagram, and a functional specification. The
functional specification also includes an external (user) design of the sys-
tem. It has been shown that errors defining the requirements and external
design are the most expensive to fix later in development. It is, therefore,
imperative to get the design as correct as possible the first time.

A technique that helps accomplish this is joint application design ses-
sions (see Appendix G19, JADs, for more details). Studies show that JADs
increase productivity over traditional design techniques. In JADs, users
and IT professionals jointly design systems in facilitated group sessions.
JADs go beyond the one-on-one interviews to collect information. They
promote communication, cooperation, and teamwork among the partici-
pants by placing the users in the driver’s seat.

JADs are logically divided into phases: customization, session, and
wrap-up. Regardless of what activity one is pursuing in development, these
components will always exist. Each phase has its own objectives.

TEAM LinG



107

Development Methodology Overview

Role of Prototyping

Prototyping is an iterative approach often used to build systems that users
initially are unable to describe precisely (see Appendix G24, Prototyping,
for more details). The concept is made possible largely through the power
of fourth-generation languages (4GLs) and application generators. 

Prototyping is, however, as prone to defects as any other development
effort, maybe more so if not performed in a systematic manner. Prototypes
need to be tested as thoroughly as any other system. Testing can be difficult
unless a systematic process has been established for developing prototypes.

There are various types of software prototypes, ranging from simple
printed descriptions of input, processes, and output to completely auto-
mated versions. An exact definition of a software prototype is impossible
to find; the concept is made up of various components. Among the many
characteristics identified by MIS professionals are the following:

• Comparatively inexpensive to build (i.e., less than 10 percent of the
full system’s development cost).

• Relatively quick development so that it can be evaluated early in
the life cycle.

• Provides users with a physical representation of key parts of the
system before implementation.

• Prototypes:
– Do not eliminate or reduce the need for comprehensive analysis

and specification of user requirements.
– Do not necessarily represent the complete system.
– Perform only a subset of the functions of the final product.
– Lack the speed, geographical placement, or other physical char-

acteristics of the final system.

Basically, prototyping is the building of trial versions of a system. These
early versions can be used as the basis for assessing ideas and making
decisions about the complete and final system. Prototyping is based on the
premise that, in certain problem domains (particularly in online interac-
tive systems), users of the proposed application do not have a clear and
comprehensive idea of what the application should do or how it should
operate.

Often, errors or shortcomings overlooked during development appear
after a system is operational. Applications prototyping seeks to overcome
these problems by providing users and developers with an effective means
of communicating ideas and requirements before a significant amount of
development effort has been expended. The prototyping process results in
a functional set of specifications that can be fully analyzed, understood,

TEAM LinG



108

SOFTWARE TESTING METHODOLOGY

and used by users, developers, and management to decide whether an
application is feasible and how it should be developed.

Fourth-generation languages have enabled many organizations to
undertake projects based on prototyping techniques. They provide many
of the capabilities necessary for prototype development, including user
functions for defining and managing the user–system interface, data man-
agement functions for organizing and controlling access, and system func-
tions for defining execution control and interfaces between the application
and its physical environment.

In recent years, the benefits of prototyping have become increasingly
recognized. Some include the following:

• Prototyping emphasizes active physical models. The prototype
looks, feels, and acts like a real system.

• Prototyping is highly visible and accountable.
• The burden of attaining performance, optimum access strategies,

and complete functioning is eliminated in prototyping.
• Issues of data, functions, and user–system interfaces can be readily

addressed.
• Users are usually satisfied, because they get what they see.
• Many design considerations are highlighted and a high degree of

design flexibility becomes apparent.
• Information requirements are easily validated.
• Changes and error corrections can be anticipated and in many cases

made on the spur of the moment.
• Ambiguities and inconsistencies in requirements become visible and

correctable.
• Useless functions and requirements can be quickly eliminated.

Methodology for Developing Prototypes

The following describes a methodology to reduce development time
through reuse of the prototype and knowledge gained in developing and
using the prototype. It does not include how to test the prototype within
spiral development. This is included in the next part.

1. Develop the Prototype

In the construction phase of spiral development, the external design and
screen design are translated into real-world windows using a 4GL tool such
as Visual Basic or Power Builder. The detailed business functionality is not
built into the screen prototypes, but a “look and feel” of the user interface
is produced so the user can imagine how the application will look.

Using a 4GL, the team constructs a prototype system consisting of data
entry screens, printed reports, external file routines, specialized procedures,

TEAM LinG



109

Development Methodology Overview

and procedure selection menus. These are based on the logical database
structure developed in the JAD data modeling sessions. The sequence of
events for performing the task of developing the prototype in a 4GL is iter-
ative and is described as follows.

Define the basic database structures derived from logical data modeling.
The data structures will be populated periodically with test data as
required for specific tests.

Define printed report formats. These may initially consist of query com-
mands saved in an executable procedure file on disk. The benefit of a query
language is that most of the report formatting can be done automatically
by the 4GL. The prototyping team needs only to define what data elements to
print and what selection and ordering criteria to use for individual reports.

Define interactive data entry screens. Whether each screen is well
designed is immaterial at this point. Obtaining the right information in the
form of prompts, labels, help messages, and validation of input is more
important. Initially, defaults should be used as often as possible.

Define external file routines to process data that is to be submitted in
batches to the prototype or created by the prototype for processing by
other systems. This can be done in parallel with other tasks.

Define algorithms and procedures to be implemented by the prototype
and the finished system. These may include support routines solely for the
use of the prototype.

Define procedure selection menus. The developers should concentrate
on the functions as the user would see them. This may entail combining
seemingly disparate procedures into single functions that can be executed
with one command from the user.

Define test cases to ascertain that:

• Data entry validation is correct.
• Procedures and algorithms produce expected results.
• System execution is clearly defined throughout a complete cycle of

operation.

Reiterate this process by adding report and screen formatting options,
corrections for errors discovered in testing, and instructions for the
intended users. This process should end after the second or third iteration
or when changes become predominantly cosmetic rather than functional.

At this point, the prototyping team should have a good understanding of
the overall operation of the proposed system. If time permits, the team
must now describe the operation and underlying structure of the proto-
type. This is most easily accomplished through the development of a draft

TEAM LinG



110

SOFTWARE TESTING METHODOLOGY

user manual. A printed copy of each screen, report, query, database struc-
ture, selection menu, and catalogued procedure or algorithm must be
included. Instructions for executing each procedure should include an
illustration of the actual dialogue.

2. Demonstrate Prototypes to Management

The purpose of this demonstration is to give management the option of
making strategic decisions about the application on the basis of the proto-
type’s appearance and objectives. The demonstration consists primarily of
a short description of each prototype component and its effects and a
walkthrough of the typical use of each component. Every person in atten-
dance at the demonstration should receive a copy of the draft user manual
if one is available.

The team should emphasize the results of the prototype and its impact
on development tasks still to be performed. At this stage, the prototype is
not necessarily a functioning system, and management must be made
aware of its limitations.

3. Demonstrate Prototype to Users

There are arguments for and against letting the prospective users actually
use the prototype system. There is a risk that users’ expectations will be
raised to an unrealistic level with regard to delivery of the production sys-
tem and that the prototype will be placed in production before it is ready.
Some users have actually refused to give up the prototype when the pro-
duction system was ready for delivery. This may not be a problem if the
prototype meets the users’ expectations and the environment can absorb
the load of processing without affecting others. On the other hand, when
users exercise the prototype, they can discover the problems in proce-
dures and unacceptable system behavior very quickly.

The prototype should be demonstrated before a representative group of
users. This demonstration should consist of a detailed description of the
system operation, structure, data entry, report generation, and procedure
execution. Above all, users must be made to understand that the prototype
is not the final product, that it is flexible, and that it is being demonstrated
to find errors from the users’ perspective.

The results of the demonstration include requests for changes, correc-
tion of errors, and overall suggestions for enhancing the system. Once the
demonstration has been held, the prototyping team reiterates the steps in
the prototype process to make the changes, corrections, and enhance-
ments deemed necessary through consensus of the prototyping team, the
end users, and management.

TEAM LinG



111

Development Methodology Overview

For each iteration through prototype development, demonstrations
should be held to show how the system has changed as a result of feedback
from users and management. The demonstrations increase the users’
sense of ownership, especially when they can see the results of their sugges-
tions. The changes should therefore be developed and demonstrated quickly.

Requirements uncovered in the demonstration and use of the prototype
may cause profound changes in the system scope and purpose, the concep-
tual model of the system, or the logical data model. Because these modifica-
tions occur in the requirements specification phase rather than in the design,
code, or operational phases, they are much less expensive to implement.

4. Revise and Finalize Specifications

At this point, the prototype consists of data entry formats, report formats,
file formats, a logical database structure, algorithms and procedures, selec-
tion menus, system operational flow, and possibly a draft user manual.

The deliverables from this phase consist of formal descriptions of the
system requirements, listings of the 4GL command files for each object
programmed (i.e., screens, reports, database structures), sample reports,
sample data entry screens, the logical database structure, data dictionary
listings, and a risk analysis. The risk analysis should include the problems
and changes that could not be incorporated into the prototype and the
probable impact that they would have on development of the full system
and subsequent operation.

The prototyping team reviews each component for inconsistencies,
ambiguities, and omissions. Corrections are made and the specifications
are formally documented.

5. Develop the Production System

At this point, development can proceed in one of three directions:

1. The project is suspended or canceled because the prototype has
uncovered insurmountable problems or the environment is not
ready to mesh with the proposed system.

2. The prototype is discarded because it is no longer needed or
because it is too inefficient for production or maintenance.

3. Iterations of prototype development are continued, with each itera-
tion adding more system functions and optimizing performance until
the prototype evolves into the production system.

The decision on how to proceed is generally based on such factors as:

• The actual cost of the prototype
• Problems uncovered during prototype development

TEAM LinG



112

SOFTWARE TESTING METHODOLOGY

• The availability of maintenance resources
• The availability of software technology in the organization
• Political and organizational pressures
• The amount of satisfaction with the prototype
• The difficulty in changing the prototype into a production system
• Hardware requirements

Continuous Improvement “Spiral” Testing Approach

The purpose of software testing is to identify the differences between exist-
ing and expected conditions, that is, to detect software defects. Testing
identifies the requirements that have not been satisfied and the functions
that have been impaired. The most commonly recognized test objective is
to identify bugs, but this is a limited definition of the aim of testing. Not
only must bugs be identified, but they must be put into a framework that
enables testers to predict how the software will perform.

In the spiral and rapid application development testing environment
there may be no final functional requirements for the system. They are
probably informal and evolutionary. Also, the test plan may not be com-
pleted until the system is released for production. The relatively long lead-
time to create test plans based on a good set of requirement specifications
may not be available. Testing is an ongoing improvement process that
occurs frequently as the system changes. The product evolves over time
and is not static.

The testing organization needs to get inside the development effort and
work closely with development. Each new version needs to be tested as it
becomes available. The approach is to first test the new enhancements or
modified software to resolve defects reported in the previous spiral. If time
permits, regression testing is then performed to ensure that the rest of the
system has not regressed.

In the spiral development environment, software testing is again
described as a continuous improvement process that must be integrated
into a rapid application development methodology. Testing as an inte-
grated function prevents development from proceeding without testing.
Deming’s continuous improvement process using the PDCA model (see
Exhibit 10.2) will again be applied to the software testing process.

Before the continuous improvement process begins, the testing function
needs to perform a series of information-gathering planning steps to under-
stand the development project objectives, current status, project plans,
function specification, and risks.

Once this is completed, the formal Plan step of the continuous improve-
ment process commences. A major step is to develop a software test plan.
The test plan is the basis for accomplishing testing and should be considered

TEAM LinG



113

Development Methodology Overview

an ongoing document; that is, as the system changes, so does the plan. The
outline of a good test plan includes an introduction, the overall plan, test-
ing requirements, test procedures, and test plan details. These are further
broken down into business functions, test scenarios and scripts, func-
tion/test matrix, expected results, test case checklists, discrepancy
reports, required software, hardware, data, personnel, test schedule, test
entry criteria, exit criteria, and summary reports.

The more definitive a test plan is, the easier the Plan step will be. If the
system changes between development of the test plan and when the tests
are to be executed, the test plan should be updated accordingly.

The Do step of the continuous improvement process consists of test
case design, test development, and test execution. This step describes
how to design test cases and execute the tests included in the test plan.
Design includes the functional tests, GUI tests, and fragment system and
acceptance tests. Once an overall test design is completed, test develop-
ment starts. This includes building test scripts and procedures to provide
test case details.

The test team is responsible for executing the tests and must ensure
that they are executed according to the test design. The Do step also
includes test setup, regression testing of old and new tests, and recording
any defects discovered.

The Check step of the continuous improvement process includes metric
measurements and analysis. As discussed in Section I, Part 3, Quality
through Continuous Improvement Process, crucial to the Deming method
is the need to base decisions as much as possible on accurate and timely
data. Metrics are key to verifying if the work effort and test schedule are on
schedule, and to identify any new resource requirements.

During the Check step it is important to publish intermediate test
reports. This includes recording of the test results and relating them to the
test plan and test objectives.

The Act step of the continuous improvement process involves prepara-
tion for the next spiral iteration. It entails refining the function/GUI tests,

Exhibit 10.2. Spiral Testing and Continuous Improvement

TEAM LinG



114

SOFTWARE TESTING METHODOLOGY

test suites, test cases, test scripts, and fragment system and acceptance
tests, and modifying the defect tracking system and the version and con-
trol system, if necessary. It also includes devising measures for appropri-
ate actions relating to work that was not performed according to the plan
or results that were not what was anticipated. Examples include a reevalu-
ation of the test team, test procedures, and technology dimensions of test-
ing. All the above are fed back to the test plan, which is updated.

Once several testing spirals have been completed and the application
has been verified as functionally stable, full system and acceptance testing
starts. These tests are often optional. Respective system and acceptance
test plans are developed defining the test objects and the specific tests to
be completed.

The final activity in the continuous improvement process is summarizing
and reporting the spiral test results. A major test report should be written at

Exhibit 10.3. Spiral Testing Methodology

TEAM LinG



115

Development Methodology Overview

the end of all testing. The process used for report writing is the same
whether it is an interim or a final report, and, like other tasks in testing,
report writing is also subject to quality control. However, the final test
report should be much more comprehensive than interim test reports. For
each type of test it should describe a record of defects discovered, data
reduction techniques, root cause analysis, the development of findings,
and follow-on recommendations for current and/or future projects.

Exhibit 10.3 provides an overview of the spiral testing methodology by
relating each step to the PDCA quality model. Appendix A, Spiral Testing
Methodology, provides a detailed representation of each part of the meth-
odology. The methodology provides a framework for testing in this envi-
ronment. The major steps include information gathering, test planning,
test design, test development, test execution/evaluation, and preparing for
the next spiral. It includes a set of tasks associated with each step or a
checklist from which the testing organization can choose based on its
needs. The spiral approach flushes out the system functionality. When this
has been completed, it also provides for classical system testing, accep-
tance testing, and summary reports.

TEAM LinG



TEAM LinG



117

Part  11

Information 
Gathering (Plan)

If you will recall, in the spiral development environment, software testing is
described as a continuous improvement process that must be integrated
into a rapid application development methodology. Deming’s continuous
improvement process using the PDCA model (see Exhibit 11.1) is applied
to the software testing process. We are now in the Plan part of the spiral
model.

Exhibit 11.2 outlines the steps and tasks associated with information
gathering within the Plan part of spiral testing. Each step and task is
described along with valuable tips and techniques.

The purpose of gathering information is to obtain information relevant
to the software development project and organize it in order to understand
the scope of the development project and start building a test plan. Other
interviews may occur during the development project, as necessary.

Proper preparation is critical to the success of the interview. Before the
interview, it is important to clearly identify the objectives of the interview
to all parties, identify the quality assurance representative who will lead
the interview and the scribe, schedule a time and place, prepare any
required handouts, and communicate what is required from development.

Although many interviews are unstructured, the interviewing steps and
tasks shown in Exhibit 11.2 will be very helpful.

Step 1: Prepare for the Interview

Task 1: Identify the Participants

It is recommended that there be no more than two interviewers represent-
ing quality assurance. It is helpful for one of these to assume the role of
questioner while the other takes detailed notes. This will allow the inter-
viewer to focus on soliciting information. Ideally the interviewer should be
the manager responsible for the project testing activities. The scribe, or
note taker, should be a test engineer or lead tester assigned to the project,
who supports the interviewer and records each pertinent piece of informa-
tion and lists the issues, the assumptions, and questions.

TEAM LinG



118

SOFTWARE TESTING METHODOLOGY

The recommended development participants attending include the
project sponsor, development manager, or a senior development team
member. Although members of the development team can take notes, this
is the responsibility of the scribe. Having more than one scribe can result
in confusion, because multiple sets of notes will eventually have to be con-
solidated. The most efficient approach is for the scribe to take notes and
summarize at the end of the interview. (See Appendix F20, Project Informa-
tion Gathering Checklist, which can be used to verify the information avail-
able and required at the beginning of the project.)

Task 2: Define the Agenda

The key factor for a successful interview is a well thought-out agenda. It
should be prepared by the interviewer ahead of time and agreed upon by
the development leader. The agenda should include an introduction, spe-
cific points to cover, and a summary section. The main purpose of an
agenda is to enable the testing manager to gather enough information to
scope out the quality assurance activities and start a test plan. Exhibit 11.3
depicts a sample agenda (details are described in Step 2, Conduct the Inter-
view).

Step 2: Conduct the Interview

A good interview contains certain elements. The first is defining what will
be discussed, or “talking about what we are going to talk about.” The sec-
ond is discussing the details, or “talking about it.” The third is summariz-
ing, or “talking about what we talked about.” The final element is timeli-
ness. The interviewer should state up front the estimated duration of the
interview and set the ground rule that if time expires before completing all
items on the agenda, a follow-on interview will be scheduled. This is diffi-
cult, particularly when the interview is into the details, but nonetheless it
should be followed.

Task 1: Understand the Project

Before getting into the project details the interviewer should state the
objectives of the interview and present the agenda. As with any type of

Exhibit 11.1.  Spiral Testing and Continuous Improvement

TEAM LinG



119

Information Gathering (Plan)

interview, he should indicate that only one individual should speak, no
interruptions should occur until the speaker acknowledges a request, and
focus should be on the material being presented.

He should then introduce himself and the scribe and ask the members of
the development team to introduce themselves. Each should indicate
name, title, specific roles and job responsibilities, as well as expectations
of the interview. The interviewer should point out that the purpose of this
task is to obtain general project background information.

Exhibit 11.2.  Information Gathering (Steps/Tasks)

TEAM LinG



120

SOFTWARE TESTING METHODOLOGY

The following general questions should be asked to solicit basic infor-
mation:

• What is the name of the project?
• What are the high-level project objectives?
• Who is the audience (users) of the system to be developed?
• When was the project started?
• When is it anticipated to be complete?
• Where is the project in developing the system?
• What is the projected effort in person-months?
• Is this a new, maintenance, or package development project?
• What are the major problems, issues, and concerns?
• Are there plans to address problems and issues?
• Is the budget on schedule?
• Is the budget too tight, too loose, or about right?
• What organizational units are participating in the project?
• Is there an established organization chart?
• What resources are assigned to each unit?
• What is the decision-making structure; that is, who makes the deci-

sions?
• What are the project roles and the responsibilities associated with

each role?
• Who is the resource with whom the test team will communicate on

a daily basis?
• Has a quality management plan been developed?
• Has a periodic review process been set up?
• Has there been a representative from the user community appointed

to represent quality?

Exhibit 11.3.  Interview Agenda

I. Introductions

II. Project Overview

III. Project Objectives

IV. Project Status

V. Project Plans

VI. Development Methodology

VII. High-Level Requirements

VIII. Project Risks and Issues

IX. Summary

TEAM LinG



121

Information Gathering (Plan)

Task 2: Understand the Project Objectives

To develop a test plan for a development project it is important to under-
stand the objectives of the project. The purpose of this task is to under-
stand the scope, needs, and high-level requirements of this project.

The following questions should be asked to solicit basic information.

• Purpose:
– What type of system is being developed, for example, payroll,

order entry, inventory, or accounts receivable/payable?
– Why is the system being developed?
– What subsystems are involved?
– What are the subjective requirements, for example, ease of use,

efficiency, morale, flexibility?
• Scope:

– Who are the users of the system?
– What are the users’ job titles and roles?
– What are the major and subfunctions of the system?
– What functions will not be implemented?
– What business procedures are within the scope of the system?
– Are there analysis diagrams, such as business flow diagrams,

data flow diagrams, or data models, to describe the system?
– Have project deliverables been defined along with completeness

criteria?
• Benefits:

– What are the anticipated benefits that will be provided to the
user with this system?
• Increased productivity
• Improved quality
• Cost savings
• Increased revenue
• More competitive advantage

• Strategic:
– What are the strategic or competitive advantages?
– What impact will the system have on the organization, custom-

ers, legal, government, and so on?
• Constraints:

– What are the financial, organizational, personnel, technological
constraints, or limitations for the system?

– What business functions and procedures are out of scope of the
system?

Task 3: Understand the Project Status

The purpose of this task is to understand where the project is at this point,
which will help define how to plan the testing effort. For example, if this is

TEAM LinG



122

SOFTWARE TESTING METHODOLOGY

the first interview and the project is coding the application, the testing
effort is already behind schedule. The following questions should be asked
to solicit basic information:

• Has a detailed project work plan, including activities, tasks, depen-
dencies, resource assignments, work effort estimates, and mile-
stones, been developed?

• Is the project on schedule?
• Is the completion time too tight?
• Is the completion time too loose?
• Is the completion time about right?
• Have there been any major slips in the schedule that will have an

impact on the critical path?
• How far is the project from meeting its objectives?
• Are the user functionality and quality expectations realistic and

being met?
• Are the project work effort hours trends on schedule?
• Are the project costs trends within the budget?
• What development deliverables have been delivered?

Task 4: Understand the Project Plans

Because the testing effort needs to track development, it is important to
understand the project work plans. The following questions should be
asked to solicit basic information:

• Work breakdown:
– Has a Microsoft Project (or other tool) plan been developed?
– How detailed is the plan; for example, how many major and

bottom-level tasks have been identified?
– What are the major project milestones (internal and external)?

• Assignments:
– Have appropriate resources been assigned to each work plan?
– Is the work plan well balanced?
– What is the plan to stage resources?

• Schedule:
– Is the project plan on schedule?
– Is the project plan behind schedule?
– Is the plan updated periodically?

Task 5: Understand the Project Development Methodology

The testing effort must integrate with the development methodology. If
considered a separate function, it may not receive the appropriate
resources and commitment. Testing as an integrated function should pre-
vent development from proceeding without testing. Testing steps and

TEAM LinG



123

Information Gathering (Plan)

tasks need to be integrated into the systems development methodology
through addition or modification of tasks. Specifically, the testing function
needs to know when in the development methodology test design can
start. It also needs to know when the system will be available for execution
and the recording and correction of defects.

The following questions should be asked to solicit basic information:

• What is the methodology?
• What development and project management methodology does the

development organization use?
• How well does the development organization follow the develop-

ment methodology?
• Is there room for interpretation or flexibility?
• Standards:

– Are standards and practices documented?
– Are the standards useful or do they hinder productivity?
– How well does the development organization enforce standards?

Task 6: Identify the High-Level Business Requirements

A software requirements specification defines the functions of a particular
software product in a specific environment. Depending on the develop-
ment organization, it may vary from a loosely defined document with a gen-
eralized definition of what the application will do to a very detailed speci-
fication, as shown in Appendix C, Requirements Specification. In either
case, the testing manager must assess the scope of the development
project in order to start a test plan.

The following questions should be asked to solicit basic information:

• What are the high-level functions? The functions at a high level should
be enumerated. Examples include order processing, financial pro-
cessing, reporting capability, financial planning, purchasing, inven-
tory control, sales administration, shipping, cash flow analysis,
payroll, cost accounting, and recruiting. This list defines what the
application is supposed to do and provides the testing manager an
idea of the level of test design and implementation required. The
interviewer should solicit as much detail as possible, including a
detailed breakdown of each function. If this detail is not available
during the interview, a request for a detailed functional decomposi-
tion should be made, and it should be pointed out that this infor-
mation is essential for preparing a test plan.

• What are the system (minimum) requirements? A description of the
operating system version (Windows, etc.) and minimum micropro-
cessor, disk space, RAM, and communications hardware should be
provided.

TEAM LinG



124

SOFTWARE TESTING METHODOLOGY

• What are the Windows or external interfaces? The specification
should define how the application should behave from an external
viewpoint, usually by defining the inputs and outputs. It also
includes a description of any interfaces to other applications or
subsystems.

• What are the performance requirements? This includes a description
of the speed, availability, data volume throughput rate, response
time, and recovery time of various functions, stress, and so on. This
serves as a basis for understanding the level of performance and
stress testing that may be required.

• What other testing attributes are required? This includes such
attributes as portability, maintainability, security, and usability. This
serves as a basis for understanding the level of other system-level
testing that may be required.

• Are there any design constraints? This includes a description of any
limitation on the operating environment(s), database integrity,
resource limits, implementation language standards, and so on.

Task 7: Perform Risk Analysis

The purpose of this task is to measure the degree of business risk in an
application system to improve testing. This is accomplished in two ways:
high-risk applications can be identified and subjected to more extensive
testing, and risk analysis can help identify the error prone components of
an individual application so that testing can be directed at those compo-
nents. This task describes how to use risk assessment techniques to mea-
sure the risk of an application under testing.

Computer Risk Analysis. Risk analysis is a formal method for identifying
vulnerabilities (i.e., areas of potential loss). Any area that could be mis-
used, intentionally or accidentally, and result in a loss to the organization
is a vulnerability. Identification of risks allows the testing process to mea-
sure the potential effect of those vulnerabilities (e.g., the maximum loss
that could occur if the risk or vulnerability were exploited).

Risk has always been a testing consideration. Individuals naturally try to
anticipate problems and then test to determine whether additional
resources and attention need to be directed at those problems. Often, how-
ever, risk analysis methods are both informal and ineffective.

Through proper analysis, the test manager should be able to predict the
probability of such unfavorable consequences as:

• Failure to obtain all, or even any, of the expected benefits
• Cost and schedule overruns
• An inadequate system of internal control

TEAM LinG



125

Information Gathering (Plan)

• Technical performance of the resulting system that is significantly
below the estimate

• Incompatibility of the system with the selected hardware and software

The following reviews the various methods used for risk analysis and
the dimensions of computer risk and then describes the various
approaches for assigning risk priorities. There are three methods of per-
forming risk analysis.

Method 1 — Judgment and Instinct. This method of determining how
much testing to perform enables the tester to compare the project with
past projects to estimate the magnitude of the risk. Although this method
can be effective, the knowledge and experience it relies on are not transfer-
able but must be learned over time.

Method 2 — Dollar Estimation. Risk is the probability for loss. That proba-
bility is expressed through this formula:

(Frequency of occurrence) × (loss per occurrence) = 
(annual loss expectation).

Business risk based on this formula can be quantified in dollars. Often,
however, the concept, not the formula, is used to estimate how many dol-
lars might be involved if problems were to occur. The disadvantages of pro-
jecting risks in dollars are that such numbers (i.e., frequency of occurrence
and loss per occurrence) are difficult to estimate and the method implies
a greater degree of precision than may be realistic.

Method 3 — Identifying and Weighting Risk Attributes. Experience has
demonstrated that the major attributes causing potential risks are the
project size, experience with the technology, and project structure. The
larger the project is in dollar expense, staffing levels, elapsed time, and
number of departments affected, the greater the risk.

Because of the greater likelihood of unexpected technical problems,
project risk increases as the project team’s familiarity with the hardware,
operating systems, database, and application languages decreases. A
project that has a slight risk for a leading-edge, large systems development
department may have a very high risk for a smaller, less technically
advanced group. The latter group, however, can reduce its risk by purchas-
ing outside skills for an undertaking that involves a technology in general
commercial use.

In highly structured projects, the nature of the task defines the output
completely, from the beginning. Such output is fixed during the life of the
project. These projects carry much less risk than those whose output is
more subject to the manager’s judgment and changes.

TEAM LinG



126

SOFTWARE TESTING METHODOLOGY

The relationship among these attributes can be determined through
weighting, and the testing manger can use weighted scores to rank appli-
cation systems according to their risk. For example, this method can show
application A is a higher risk than application B.

Risk assessment is applied by first weighting the individual risk
attributes. For example, if an attribute is twice as important as another it
can be multiplied by the weight of two. The resulting score is compared
with other scores developed for the same development organization and is
used to determine a relative risk measurement among applications, but it
is not used to determine an absolute measure.

Exhibit 11.4 compares three projects using the weighted risk attribute
method. Project size has a 2 weight factor, experience with technology has
a 3 weight factor, and project structure has a 1 weight factor. When the
project scores are each multiplied by each of the three weight factors, it is
clear that project A has the highest risk.

Information gathered during risk analysis can be used to allocate test
resources to test application systems. For example, high-risk applications
should receive extensive testing; medium-risk systems, less testing; and
low-risk systems, minimal testing. The area of testing can be selected on
the basis of high-risk characteristics. For example, if computer technology
is a high-risk characteristic, the testing manager may want to spend more
time testing how effectively the development team is using that technol-
ogy.

Step 3: Summarize the Findings

Task 1: Summarize the Interview

After the interview is completed, the interviewer should review the agenda
and outline the main conclusions. If there is the need for a follow-up ses-
sion, one should be scheduled at this point while the members are present.

Typically, during the interview, the notes are unstructured and hard to
follow by anyone except the note taker. However, the notes should have at

Exhibit 11.4.  Identifying and Weighting Risk Attributes

Weighting Factor
Project A

(Score ×××× Weight)
Project B

(Score ×××× Weight)
Project C

(Score ×××× Weight)

Project size (2) 5 × 2 = 10 3 × 2 = 6 2 × 2 = 4

Experience with 
technology (3)

7 × 3 = 21 1 × 3 = 3 5 × 3 = 15

Project structure (1) 4 × 1 = 4 6 × 1 = 6 3 × 1 = 3

Total score 35 15 22

TEAM LinG



127

Information Gathering (Plan)

least followed the agenda. After the interview concludes, the notes should
be formalized into a summary report. This should be performed by the
scribe note taker. The goal is to make the results of the session as clear as
possible for quality assurance and the development organization. How-
ever, the interview leader may have to embellish the material or expand in
certain areas. (See Appendix E16, Minutes of the Meeting, which can be
used to document the results and follow-up actions for the project informa-
tion gathering session).

Task 2: Confirm the Interview Findings

The purpose of this task is to bring about agreement between the inter-
viewer and the development organization to ensure an understanding of
the project. After the interview notes are formalized, it is important to dis-
tribute the summary report to the other members who attended the inter-
view. A sincere invitation for their comments or suggestions should be
communicated. The interviewer should then actively follow up interview
agreements and disagreements. Any changes should then be implemented.
Once there is full agreement, the interviewer should provide a copy of the
summary report.

TEAM LinG



TEAM LinG



129

Part 12

Test Planning (Plan)

The purpose of the test project plan is to provide the basis for accomplishing
testing in an organized manner. From a managerial point of view it is the most
important document, because it helps manage the test project. If a test plan
is comprehensive and carefully thought out, test execution and analysis
should proceed smoothly. (See Appendix E1 for a sample unit test plan,
Appendix E4 for a sample system test plan, and Appendix F24, which can be
used to verify that unit testing has been thorough and comprehensive.)

The test project plan is an ongoing document, particularly in the spiral
environment because the system is constantly changing. As the system
changes, so does it. A good test plan is one that:

• Has a good chance of detecting a majority of the defects
• Provides test coverage for most of the code
• Is flexible
• Is executed easily and automatically and is repeatable
• Defines the types of tests to be performed
• Clearly documents the expected results
• Provides for defect reconciliation when a defect is discovered
• Clearly defines the test objectives
• Clarifies the test strategy
• Clearly defines the test exit criteria
• Is not redundant
• Identifies the risks
• Documents the test requirements
• Defines the test deliverables

Although there are many ways a test plan can be created, Exhibit 12.1
provides a framework that includes most of the essential planning consid-
erations. It can be treated as a checklist of test items to consider. Some of
the items, such as defining the test requirements and test team, are obvi-
ously required, however, others may not be. It depends on the nature of the
project and the time constraints.

The planning test methodology includes three steps: building the test
project plan, defining the metrics, and reviewing/approving the test project
plan. Each of these is then broken down into its respective tasks, as shown
in Exhibit 12.1.

TEAM LinG



130

SOFTWARE TESTING METHODOLOGY

Step 1: Build a Test Plan

Task 1: Prepare an Introduction

The first bit of test plan detail is a description of the problem(s) to be
solved by the application of the associated opportunities. This defines
the summary background describing the events or current status leading
up to the decision to develop the application. Also, the application’s

Exhibit 12.1.  Test Planning (Steps/Tasks)

TEAM LinG



131

Test Planning (Plan)

risks, purpose, objectives, and benefits, and the organization’s critical suc-
cess factors should be documented in the introduction. A critical success
factor is a measurable item that will have a major influence on whether a
key function meets its objectives. An objective is a measurable end state
that the organization strives to achieve. Examples of objectives include:

• New product opportunity
• Improved efficiency (internal and external)
• Organizational image
• Growth (internal and external)
• Financial (revenue, cost profitability)
• Competitive position
• Market leadership

The introduction should also include an executive summary descrip-
tion. The executive sponsor (often called the project sponsor) is the indi-
vidual who has ultimate authority over the project. This individual has a
vested interest in the project in terms of funding, project results, resolving
project conflicts, and is responsible for the success of the project. An exec-
utive summary describes the proposed application from an executive’s
point of view. It should describe the problems to be solved, the application
goals, and the business opportunities. The objectives should indicate
whether the application is a replacement of an old system and document
the impact the application will have, if any, on the organization in terms of
management, technology, and so on.

Any available documentation should be listed and its status described.
Examples include requirements specifications, functional specifications,
project plan, design specification, prototypes, user manual, business
model/flow diagrams, data models, and project risk assessments. In addi-
tion to project risks, which are the potential adverse effects on the devel-
opment project, the risks relating to the testing effort should be docu-
mented. Examples include the lack of testing skills, scope of the testing
effort, lack of automated testing tools, and the like. See Appendix E4, Test
Plan (Client/Server and Internet Spiral Testing), for more details.

Task 2: Define the High-Level Functional Requirements (in Scope)

A functional specification consists of the hierarchical functional decompo-
sition, the functional window structure, the window standards, and the
minimum system requirements of the system to be developed. An example
of window standards is the Windows 95 GUI Standards. An example of a
minimum system requirement could be Windows 95, a Pentium II micropro-
cessor, 24-MB RAM, 3-gig disk space, and a modem. At this point in devel-
opment, a full functional specification may not have been defined. How-
ever, a list of at least the major business functions of the basic window
structure should be available.

TEAM LinG



132

SOFTWARE TESTING METHODOLOGY

A basic functional list contains the main functions of the system with
each function named and described with a verb–object paradigm. This list
serves as the basis for structuring functional testing (see Exhibit 12.2).

A functional window structure describes how the functions will be
implemented in the windows environment. At this point, a full functional
window structure may not be available but a list of the major windows
should be (see Exhibit 12.3).

Task 3: Identify Manual/Automated Test Types

The types of tests that need to be designed and executed depend totally on
the objectives of the application, that is, the measurable end state the orga-
nization strives to achieve. For example, if the application is a financial
application used by a large number of individuals, special security and
usability tests need to be performed. However, three types of tests that are
nearly always required are function, user interface, and regression testing.
Function testing comprises the majority of the testing effort and is con-
cerned with verifying that the functions work properly. It is a black-box-ori-
ented activity in which the tester is completely unconcerned with the inter-
nal behavior and structure of the application. User interface testing, or GUI
testing, checks the user’s interaction or functional window structure. It
ensures that object state dependencies function properly and provide use-
ful navigation through the functions. Regression testing tests the applica-
tion in light of changes made during debugging, maintenance, or the devel-
opment of a new release.

Other types of tests that need to be considered include system and
acceptance testing. System testing is the highest level of testing and evalu-
ates the functionality as a total system, its performance, and overall fitness

Exhibit 12.2. High-Level Business Functions

Order processing (ex. create new order, edit order, etc.)
Customer processing (create new customer, edit customer, etc.)
Financial processing (receive payment, deposit payment, etc.)
Inventory processing (acquire products, adjust product price, etc.)
Reports (create order report, create account receivable report, etc.)

Exhibit 12.3. Functional Window Structure

The Main-Window (menu bar, customer order window, etc.)
The Customer-Order-Window (order summary list, etc.)
The Edit-Order-Window (create order, edit order, etc.)
The Menu Bar (File, Order, View, etc.)
The Tool Bar with icons (FileNew, OrderCreate)

TEAM LinG



133

Test Planning (Plan)

of use. Acceptance testing is an optional user-run test that demonstrates
the ability of the application to meet the user’s requirements. This test may
or may not be performed based on the formality of the project. Sometimes
the system test suffices.

Finally, the tests that can be automated with a testing tool need to be
identified. Automated tests provide three benefits: repeatability, leverage,
and increased functionality. Repeatability enables automated tests to be
executed more than once, consistently. Leverage comes from repeatability
from tests previously captured and tests that can be programmed with the
tool, which may not have been possible without automation. As applica-
tions evolve, more and more functionality is added. With automation, the
functional coverage is maintained with the test library.

Task 4: Identify the Test Exit Criteria

One of the most difficult and political problems is deciding when to stop
testing, because it is impossible to know when all the defects have been
detected. There are at least four criteria for exiting testing.

1. Scheduled testing time has expired — This criterion is very weak,
inasmuch as it has nothing to do with verifying the quality of the
application. This does not take into account that there may be an
inadequate number of test cases or the fact that there may not be
any more defects that are easily detectable.

2. Some predefined number of defects discovered — The problems with
this are knowing the number of errors to detect and also overesti-
mating the number of defects. If the number of defects is underes-
timated, testing will be incomplete. Potential solutions include
experience with similar applications developed by the same devel-
opment team, predictive models, and industrywide averages. If the
number of defects is overestimated, the test may never be completed
within a reasonable time frame. A possible solution is to estimate
completion time, plotting defects detected per unit of time. If the
rate of defect detection is decreasing dramatically, there may be
“burnout,” an indication that a majority of the defects have been
discovered.

3. All the formal tests execute without detecting any defects — A major
problem with this is that the tester is not motivated to design
destructive test cases that force the tested program to its design
limits; for example, the tester’s job is completed when the test
program fields no more errors. The tester is motivated not to find
errors and may subconsciously write test cases that show the pro-
gram is error free. This criterion is only valid if there is a rigorous
and totally comprehensive test case suite created that approaches
100 percent coverage. The problem with this is determining when

TEAM LinG



134

SOFTWARE TESTING METHODOLOGY

there is a comprehensive suite of test cases. If it is felt that this is
the case, a good strategy at this point is to continue with ad hoc
testing. Ad hoc testing is a black-box testing technique in which the
tester lets her mind run freely to enumerate as many test conditions
as possible. Experience has shown that this technique can be a very
powerful supplemental or add-on technique.

4. Combination of the above — Most testing projects utilize a combina-
tion of the above exit criteria. It is recommended that all the tests
be executed, but any further ad hoc testing will be constrained by
time.

Task 5: Establish Regression Test Strategy

Regression testing tests the application in light of changes made during a
development spiral, debugging, maintenance, or the development of a new
release. This test must be performed after functional improvements or
repairs have been made to a system to confirm that the changes have no
unintended side effects. Correction of errors relating to logic and control
flow, computational errors, and interface errors are examples of conditions
that necessitate regression testing. Cosmetic errors generally do not affect
other capabilities and do not require regression testing.

It would be ideal if all the tests in the test suite were rerun for each new
spiral but, due to time constraints, this is probably not realistic. A good
regression strategy during spiral development is for some regression test-
ing to be performed during each spiral to ensure that previously demon-
strated capabilities are not adversely affected by later development spirals
or error corrections. During system testing after the system is stable and
the functionality has been verified, regression testing should consist of a
subset of the system tests. Policies need to be created to decide which
tests to include. (See Appendix E21.)

A retest matrix is an excellent tool that relates test cases to functions (or
program units) as shown in Exhibit 12.4. A check entry in the matrix indi-
cates that the test case is to be retested when the function (or program
unit) has been modified due to an enhancement(s) or correction(s). No
entry means that the test does not need to be retested. The retest matrix
can be built before the first testing spiral but needs to be maintained dur-
ing subsequent spirals. As functions (or program units) are modified dur-
ing a development spiral, existing or new test cases need to be created and
checked in the retest matrix in preparation for the next test spiral. Over
time with subsequent spirals, some functions (or program units) may be
stable with no recent modifications. Consideration to selectively remove
their check entries should be undertaken between testing spirals. Also see
Appendix E14, Retest Matrix.

Other considerations of regression testing are as follows:

TEAM LinG



135

Test Planning (Plan)

• Regression tests are potential candidates for test automation when
they are repeated over and over in every testing spiral.

• Regression testing needs to occur between releases after the initial
release of the system.

Exhibit 12.4. Retest Matrix

Test Case

1 2 3 4 5

Business function

Order processing

Create new order � � � �

Fulfill order

Edit order � �

Delete order

Customer processing

Create new customer

Edit customer

Delete customer �

Financial processing

Receive customer payment � � �

Deposit payment

Pay vendor

Write a check � � � � �

Display register

Inventory processing

Acquire vendor products

Maintain stock

Handle back orders � � � � �

Audit inventory

Adjust product price

Reports

Create order report

Create account receivables report � � � � �

Create account payables report

Create inventory report

TEAM LinG



136

SOFTWARE TESTING METHODOLOGY

• The test that uncovered an original defect should be rerun after it
has been corrected.

• An in-depth effort should be made to ensure that the original defect
was corrected and not just the symptoms.

• Regression tests that repeat other tests should be removed.
• Other test cases in the functional (or program unit) area where a

defect is uncovered should be included in the regression test suite.
• Client-reported defects should have high priority and should be

regression tested thoroughly.

Task 6: Define the Test Deliverables

Test deliverables result from test planning, test design, test development,
and test defect documentation. Some spiral test deliverables from which
you can choose include the following:

• Test plan: Defines the objectives, scope, strategy, types of tests, test
environment, test procedures, exit criteria, and so on (see Appendix
E4, sample template).

• Test design: The tests for the application’s functionality, perfor-
mance, and appropriateness for use. The tests demonstrate that the
original test objectives are satisfied.

• Change request: A documented request to modify the current soft-
ware system, usually supplied by the user (see Appendix D, Change
Request Form, for more details). It is typically different from a defect
report, which reports an anomaly in the system.

• Metrics: The measurable indication of some quantitative aspect of
a system. Examples include the number of severe defects, and the
number of defects discovered as a function of the number of testers.

• Test case: A specific set of test data and associated procedures
developed for a particular objective. It provides a detailed blueprint
for conducting individual tests and includes specific input data val-
ues and the corresponding expected result(s) (see Appendix E8, Test
Case, for more details).

• Test log summary report: Specifies the test cases from the tester’s
individual test logs that are in progress or completed for status
reporting and metric collection (see Appendix E10, Test Log Sum-
mary Report).

• Test case log: Specifies the test cases for a particular testing event
to be executed during testing. It is also used to record the results
of the test performed, to provide the detailed evidence for the sum-
mary of test results, and to provide a basis for reconstructing the
testing event if necessary (see Appendix E9, Test Case Log).

• Interim test report: A report published between testing spirals indi-
cating the status of the testing effort (see Part 16, Step 3, Publish
Interim Report).

TEAM LinG



137

Test Planning (Plan)

• System summary report: A comprehensive test report after all spiral
testing that has been completed (see Appendix E11, System Sum-
mary Report).

• Defect report: Documents defect(s) discovered during spiral testing
(see Appendix E12, Defect Report).

Task 7: Organize the Test Team

The people component includes human resource allocations and the
required skill sets. The test team should comprise the highest-caliber per-
sonnel possible. They are usually extremely busy because their talents put
them in great demand, and it therefore becomes vital to build the best case
possible for using these individuals for test purposes. A test team leader
and test team need to have the right skills and experience, and be moti-
vated to work on the project. Ideally, they should be professional quality
assurance specialists but can represent the executive sponsor, users, tech-
nical operations, database administration, computer center, independent
parties, and so on. In any event, they should not represent the develop-
ment team, for they may not be as unbiased as an outside party. This is not
to say that developers shouldn’t test. For they should unit and function
test their code extensively before handing it over to the test team.

There are two areas of responsibility in testing: testing the application,
which is the responsibility of the test team, and the overall testing processes,
which is handled by the test manager. The test manager directs one or more
testers, is the interface between quality assurance and the development orga-
nization, and manages the overall testing effort. Responsibilities include:

• Setting up the test objectives
• Defining test resources
• Creating test procedures
• Developing and maintaining the test plan
• Designing test cases
• Designing and executing automated testing tool scripts
• Test case development
• Providing test status
• Writing reports
• Defining the roles of the team members
• Managing the test resources
• Defining standards and procedures
• Ensuring quality of the test process
• Training the team members
• Maintaining test statistics and metrics

The test team must be a set of team players and have these responsibil-
ities:

TEAM LinG



138

SOFTWARE TESTING METHODOLOGY

• Executing test cases according to the plan
• Evaluating the test results
• Reporting errors
• Designing and executing automated testing tool scripts
• Recommending application improvements
• Recording defects

The main function of a team member is to test the application and report
defects to the development team by documenting them in a defect tracking
system. Once the development team corrects the defects, the test team
reexecutes the tests that discovered the original defects.

It should be pointed out that the roles of the test manager and team
members are not mutually exclusive. Some of the team leader’s responsi-
bilities are shared with the team members and vice versa.

The basis for allocating dedicated testing resources is the scope of the
functionality and the development time frame; for example, a medium
development project will require more testing resources than a small one.
If project A of medium complexity requires a testing team of five, project B
with twice the scope would require ten testers (given the same resources).

Another rule of thumb is that the testing costs approach 25 percent of
the total budget. Because the total project cost is known, the testing effort
can be calculated and translated to tester headcount.

The best estimate is a combination of the project scope, test team skill
levels, and project history. A good measure of required testing resources
for a particular project is the histories of multiple projects, that is, testing
resource levels and performance compared to similar projects.

Task 8: Establish a Test Environment

The purpose of the test environment is to provide a physical framework for
testing necessary for the testing activity. For this task, the test environ-
ment needs are established and reviewed before implementation.

The main components of the test environment include the physical test
facility, technologies, and tools. The test facility component includes the
physical setup. The technologies component includes the hardware plat-
forms, physical network and all its components, operating system soft-
ware, and other software such as utility software. The tools component
includes any specialized testing software such as automated test tools,
testing libraries, and support software.

The testing facility and workplace need to be established. This may
range from an individual workplace configuration to a formal testing lab. In
any event, it is important that the testers be together and in close proxim-
ity to the development team. This facilitates communication and the sense

TEAM LinG



139

Test Planning (Plan)

of a common goal. The testing tools that were acquired need to be
installed.

The hardware and software technologies need to be set up. This
includes the installation of test hardware and software, and coordination
with vendors, users, and information technology personnel. It may be nec-
essary to test the hardware and coordinate with hardware vendors. Com-
munication networks need to be installed and tested.

Task 9: Define the Dependencies

A good source of information is previously produced test plans on other
projects. If available, the sequence of tasks in the project work plans can be
analyzed for activity and task dependencies that apply to this project.

Examples of test dependencies include:

• Code availability
• Tester availability (in a timely fashion)
• Test requirements (reasonably defined)
• Test tools availability
• Test group training
• Technical support
• Defects fixed in a timely manner
• Adequate testing time
• Computers and other hardware
• Software and associated documentation
• System documentation (if available)
• Defined development methodology
• Test lab space availability
• Agreement with development (procedures and processes)

The support personnel need to be defined and committed to the project.
This includes members of the development group, technical support staff,
network support staff, and database administrator support staff.

Task 10: Create a Test Schedule

A test schedule should be produced that includes the testing steps (and
perhaps tasks), target start and end dates, and responsibilities. It should
also describe how it will be reviewed, tracked, and approved. A simple test
schedule format, as shown in Exhibit 12.5, follows the spiral methodology.

Also, a project management tool such as Microsoft Project can format a
Gantt chart to emphasize the tests and group them into test steps. A Gantt
chart consists of a table of task information and a bar chart that graphi-
cally displays the test schedule. It also includes task time duration and
links the task dependency relationships graphically. People resources can

TEAM LinG



140

SOFTWARE TESTING METHODOLOGY

Exhibit 12.5. Test Schedule 

Test Step
Begin 
Date

End 
Date Responsible

First Spiral

Information gathering

Prepare for interview 6/1/04 6/2/04 Smith, Test Manager

Conduct interview 6/3/04 6/3/04 Smith, Test Manager

Summarize findings 6/4/04 6/5/04 Smith, Test Manager

Test planning

Build test plan 6/8/04 6/12/04 Smith, Test Manager

Define the metric objectives 6/15/04 6/17/04 Smith, Test Manager

Review/approve plan 6/18/04 6/18/04 Smith, Test Manager

Test case design

Design function tests 6/19/04 6/23/04 Smith, Test Manager

Design GUI tests 6/24/04 6/26/04 Smith, Test Manager

Define the system/acceptance

Tests 6/29/04 6/30/04 Smith, Test Manager

Review/approve design 7/3/04 7/3/04 Smith, Test Manager

Test development

Develop test scripts 7/6/04 7/16/04 Jones, Baker, Brown, 
Testers

Review/approve test development 7/17/04 7/17/04 Jones, Baker, Brown, 
Testers

Test execution/evaluation

Setup and testing 7/20/04 7/24/04 Smith, Jones, Baker, 
Brown, Testers

Evaluation 7/27/04 7/29/04 Smith, Jones, Baker, 
Brown, Testers

Prepare for the next spiral

Refine the tests 8/3/04 8/5/04 Smith, Test Manager

Reassess team, procedures, and test 
environment

8/6/04 8/7/04 Smith, Test Manager

Publish interim report 8/10/04 8/11/04 Smith, Test Manager

•

•

•

TEAM LinG



141

Test Planning (Plan)

also be assigned to tasks for workload balancing. See Appendix E13, Test
Schedule, and template file Gantt spiral testing methodology template.

Another way to schedule testing activities is with “relative scheduling” in
which testing steps or tasks are defined by their sequence or precedence. It

Last spiral…

Test execution/evaluation

Setup and testing 10/5/04 10/9/04 Jones, Baker, Brown, 
Testers

Evaluation 10/12/04 10/14/04 Smith, Test Manager

•

•

•

Conduct system testing

Complete system test plan 10/19/04 10/21/04 Smith, Test Manager

Complete system test cases 10/22/04 10/23/04 Smith, Test Manager

Review/approve system tests 10/26/04 10/30/04 Jones, Baker, Brown, 
Testers

Execute the system tests 11/2/04 11/6/04 Jones, Baker, Brown, 
Testers

Conduct acceptance testing

Complete acceptance test plan 11/9/04 11/10/04 Smith, Test Manager

Complete acceptance test cases 11/11/04 11/12/04 Smith, Test Manager

Review/approve acceptance 

Test plan 11/13/04 11/16/04 Jones, Baker, Brown, 
Testers

Execute the acceptance tests 11/17/04 11/20/04

Summarize/report spiral test results

Perform data reduction 11/23/04 11/26/04 Smith, Test Manager

Prepare final test report 11/27/04 11/27/04 Smith, Test Manager

Review/approve the final 

Test report 11/28/04 11/29/04 Smith, Test Manager 
Baylor, Sponsor

Exhibit 12.5. Test Schedule (Continued)

Test Step
Begin 
Date

End 
Date Responsible

TEAM LinG



142

SOFTWARE TESTING METHODOLOGY

does not state a specific start or end date but does have a duration, such
as days. (Also see Appendix E18, Test Execution Plan, which can be used to
plan the activities for the Execution phase, and Appendix E20, PDCA Test
Schedule, which can be used to plan and track the Plan-Do-Check-Act test
phases.)

It is also important to define major external and internal milestones.
External milestones are events that are external to the project but may
have a direct impact on the project. Examples include project sponsorship
approval, corporate funding, and legal authorization. Internal milestones
are derived for the schedule work plan and typically correspond to key
deliverables that need to be reviewed and approved. Examples include test
plan, design, and development completion approval by the project spon-
sor and the final spiral test summary report. Milestones can be docu-
mented in the test plan in table format as shown in Exhibit 12.6. (Also see
Appendix E19, Test Project Milestones, which can be used to identify and
track the key test milestones.)

Task 11: Select the Test Tools

Test tools range from relatively simple to sophisticated software. New
tools are being developed to help provide the high-quality software needed
for today’s applications.

Because test tools are critical to effective testing, those responsible for
testing should be proficient in using them. The tools selected should be
most effective for the environment in which the tester operates and the
specific types of software being tested. The test plan needs to name spe-
cific test tools and their vendors. The individual who selects the test tool
should also conduct the test and be familiar enough with the tool to use it
effectively. The test team should review and approve the use of each test
tool, because the tool selected must be consistent with the objectives of
the test plan.

Exhibit 12.6.  Project Milestones

Project Milestone Due Date

Sponsorship approval 7/1/04

First prototype available 7/20/04

Project test plan 6/18/04

Test development complete 7/1704

Test execution begins 7/20/04

Final spiral test summary report published 11/27/04

System ship date 12/1/04

TEAM LinG



143

Test Planning (Plan)

The selection of testing tools may be based on intuition or judgment.
However, a more systematic approach should be taken. Section V, Modern
Software Testing Tools, provides a comprehensive methodology for acquir-
ing testing tools. It also provides an overview of the types of modern test-
ing tools available.

Task 12: Establish Defect Recording/Tracking Procedures

During the testing process a defect is discovered. It needs to be recorded.
A defect is related to individual tests that have been conducted, and the
objective is to produce a complete record of those defects. The overall
motivation for recording defects is to correct defects and record metric
information about the application. Development should have access to the
defects reports, which they can use to evaluate whether there is a defect
and how to reconcile it. The defect form can either be manual or electronic,
with the latter being preferred. Metric information such as the number of
defects by type or open time for defects can be very useful in understand-
ing the status of the system.

Defect control procedures need to be established to control this process
from initial identification to reconciliation. Exhibit 12.7 shows some possi-
ble defects states from open to closed with intermediate states. The testing
department initially opens a defect report and also closes it. A “Yes” in a
cell indicates a possible transition from one state to another. For example,
an “Open” state can change to “Under Review,” “Returned by Develop-
ment,” or “Deferred by Development.” The transitions are initiated by
either the testing department or development.

A defect report form also needs to be designed. The major fields of a
defect form include (see Appendices E12 and E27, Defect Report, for more
details):

• Identification of the problem, for example, functional area, problem
type, and so on

• Nature of the problem, for example, behavior
• Circumstances that led to the problem, for example, inputs and steps
• Environment in which the problem occurred, for example, platform,

and so on
• Diagnostic information, for example, error code and so on
• Effect of the problem, for example, consequence

It is quite possible that a defect report and a change request form are the
same. The advantage of this approach is that it is not always clear whether
a change request is a defect or an enhancement request. The differentia-
tion can be made with a form field that indicates whether it is a defect or
enhancement request. On the other hand, a separate defect report can be
very useful during the maintenance phase when the expected behavior of

TEAM LinG



144

SOFTWARE TESTING METHODOLOGY

E
xh

ib
it

 1
2.

7.
 D

ef
ec

t 
St

at
es

O
p

en
U

n
d

er
 

R
ev

ie
w

R
et

u
rn

ed
 b

y 
D

ev
el

op
m

en
t

R
ea

d
y 

fo
r 

Te
st

in
g

R
et

u
rn

ed
 b

y 
Q

A
D

ef
er

re
d

 b
y 

D
ev

el
op

m
en

t
C

lo
se

d

O
p

en
 

 —
 

Ye
s

Ye
s

 —
 

 —
 

Ye
s

 —
 

U
nd

er
 r

ev
ie

w
 —

 
 —

 
Ye

s
Ye

s
 —

 
Ye

s
Ye

s

R
et

ur
ne

d
 b

y 
d

ev
el

op
m

en
t

 —
 

 —
 

 —
 

 —
 

Ye
s

 —
 

Ye
s

R
ea

d
y 

fo
r 

te
st

in
g

 —
 

 —
 

 —
 

 —
 

Ye
s

 —
 

Ye
s

R
et

ur
ne

d
 b

y 
Q

A
 —

 
 —

 
Ye

s
 —

 
 —

 
Ye

s
Ye

s

D
ef

er
re

d
 b

y 
d

ev
el

op
m

en
t

 —
 

Ye
s

Ye
s

Ye
s

 —
 

 —
 

Ye
s

C
lo

se
d

Ye
s

 —
 

 —
 

 —
 

 —
 

 —
 

 —
 

TEAM LinG



145

Test Planning (Plan)

the software is well known and it is easier to distinguish between a defect
and an enhancement.

Task 13: Establish Change Request Procedures

If it were a perfect world, a system would be built and there would be no
future changes. Unfortunately, it is not a perfect world and after a system
is deployed, there are change requests.

Some of the reasons for change are:

• The requirements change.
• The design changes.
• The specification is incomplete or ambiguous.
• A defect is discovered that was not discovered during reviews.
• The software environment changes, for example, platform, hard-

ware, and so on.

Change control is the process by which a modification to a software
component is proposed, evaluated, approved or rejected, scheduled, and
tracked. It is a decision process used in controlling the changes made to
software. Some proposed changes are accepted and implemented during
this process. Others are rejected or postponed, and are not implemented.
Change control also provides for impact analysis to determine the depen-
dencies (see Appendix D, Change Request Form, for more details).

Each software component has a life cycle. A life cycle consists of states
and allowable transitions between those states. Any time a software com-
ponent is changed, it should always be reviewed. While being reviewed, it
is frozen from further modifications and the only way to change it is to cre-
ate a new version. The reviewing authority must approve the modified soft-
ware component or reject it. A software library should hold all components
as soon as they are frozen and also act as a repository for approved com-
ponents.

The formal title of the organization to manage changes is a configuration
control board, or CCB. The CCB is responsible for the approval of changes
and for judging whether a proposed change is desirable. For a small
project, the CCB can consist of a single person, such as a project manager.
For a more formal development environment, it can consist of several
members from development, users, quality assurance, management, and
the like.

All components controlled by software configuration management are
stored in a software configuration library, including work products such as
business data and process models, architecture groups, design units,
tested application software, reusable software, and special test software.
When a component is to be modified, it is checked out of the repository

TEAM LinG



146

SOFTWARE TESTING METHODOLOGY

into a private workspace. It evolves through many states that are tempo-
rarily outside the scope of configuration management control.

When a change is completed, the component is checked in to the library
and becomes a new component version. The previous component version
is also retained.

Change control is based on the following major functions of a develop-
ment process: requirements analysis, system design, program design, test-
ing, and implementation. At least six control procedures are associated
with these functions and need to be established for a change control sys-
tem (see Appendix B, Software Quality Assurance Plan, for more details).

1. Initiation Procedures — This includes procedures for initiating a
change request through a change request form, which serves as a
communication vehicle. The objective is to gain consistency in doc-
umenting the change request document and routing it for approval.

2. Technical Assessment Procedures — This includes procedures for
assessing the technical feasibility and technical risks, and schedul-
ing a technical evaluation of a proposed change. The objectives are
to ensure integration of the proposed change, the testing require-
ments, and the ability to install the change request.

3. Business Assessment Procedures — This includes procedures for
assessing the business risk, effect, and installation requirements of
the proposed change. The objectives are to ensure that the timing
of the proposed change is not disruptive to the business goals.

4. Management Review Procedures — This includes procedures for eval-
uating the technical and business assessments through management
review meetings. The objectives are to ensure that changes meet
technical and business requirements and that adequate resources
are allocated for testing and installation.

5. Test Tracking Procedures — This includes procedures for tracking
and documenting test progress and communication, including steps
for scheduling tests, documenting the test results, deferring change
requests based on test results, and updating test logs. The objectives
are to ensure that testing standards are utilized to verify the change,
including test plans and test design, and that test results are com-
municated to all parties.

6. Installation Tracking Procedure — This includes procedures for track-
ing and documenting the installation progress of changes. It ensures
that proper approvals have been completed, adequate time and
skills have been allocated, installation and backup instructions have
been defined, and proper communication has occurred. The objec-
tives are to ensure that all approved changes have been made,
including scheduled dates, test durations, and reports.

TEAM LinG



147

Test Planning (Plan)

Task 14: Establish Version Control Procedures

A method for uniquely identifying each software component needs to be
established via a labeling scheme. Every software component must have a
unique name. Software components evolve through successive revisions,
and each needs to be distinguished. A simple way to distinguish compo-
nent revisions is with a pair of integers, 1.1, 1.2, . . . , that define the release
number and level number. When a software component is first identified, it
is revision 1 and subsequent major revisions are 2, 3, and so on.

In a client/server environment it is highly recommended that the devel-
opment environment be different from the test environment. This requires
the application software components to be transferred from the develop-
ment environment to the test environment. Procedures need to be set up.

Software needs to be placed under configuration control so that no
changes are being made to the software while testing is being conducted.
This includes source and executable components. Application software
can be periodically migrated into the test environment. This process must
be controlled to ensure that the latest version of software is tested. Ver-
sions will also help control the repetition of tests to ensure that previously
discovered defects have been resolved.

For each release or interim change between versions of a system config-
uration, a version description document should be prepared to identify the
software components.

Task 15: Define Configuration Build Procedures

Assembling a software system involves tools to transform the source com-
ponents, or source code, into executable programs. Examples of tools are
compilers and linkage editors.

Configuration build procedures need to be defined to identify the cor-
rect component versions and execute the component build procedures.
The configuration build model addresses the crucial question of how to
control the way components are built.

A configuration typically consists of a set of derived software compo-
nents. An example of derived software components is executable object
programs derived from source programs. Derived components must be
correctly associated with each source component to obtain an accurate
derivation. The configuration build model addresses the crucial question
of how to control the way derived components are built.

The inputs and outputs required for a configuration build model include
primary inputs and primary outputs. The primary inputs are the source
components, which are the raw materials from which the configuration is

TEAM LinG



148

SOFTWARE TESTING METHODOLOGY

built; the version selection procedures; and the system model, which
describes the relationship between the components. The primary outputs
are the target configuration and derived software components.

Different software configuration management environments use differ-
ent approaches for selecting versions. The simplest approach to version
selection is to maintain a list of component versions. Other automated
approaches allow for the most recently tested component versions to be
selected, or those updated on a specific date. Operating system facilities
can be used to define and build configurations including the directories
and command files.

Task 16: Define Project Issue Resolution Procedures

Testing issues can arise at any point in the development process and must
be resolved successfully. The primary responsibility of issue resolution is
with the project manager who should work with the project sponsor to
resolve those issues. Typically, the testing manager will document test
issues that arise during the testing process. The project manager or
project sponsor should screen every issue that arises. An issue can be
rejected or deferred for further investigation but should be considered rel-
ative to its impact on the project. In any case, a form should be created that
contains the essential information. Examples of testing issues include lack
of testing tools, lack of adequate time to test, inadequate knowledge of the
requirements, and so on.

Issue management procedures need to be defined before the project
starts. The procedures should address how to:

• Submit an issue
• Report an issue
• Screen an issue (rejected, deferred, merged, or accepted)
• Investigate an issue
• Approve an issue
• Postpone an issue
• Reject an issue
• Close an issue

Task 17: Establish Reporting Procedures

Test reporting procedures are critical to manage the testing progress and
manage the expectations of the project team members. This will keep the
project manager and sponsor informed of the testing project progress and
minimize the chance of unexpected surprises. The testing manager needs
to define who needs the test information, what information they need, and
how often the information is to be provided. The objectives of test status
reporting are to report the progress of the testing toward its objectives and
report test issues, problems, and concerns.

TEAM LinG



149

Test Planning (Plan)

Two key reports that need to be published are:

1. Interim Test Report — An interim test report is a report published
between testing spirals indicating the status of the testing effort.

2. System Summary Report — A test summary report is a comprehen-
sive test report after all spiral testing has been completed.

Task 18: Define Approval Procedures

Approval procedures are critical in a testing project. They help provide the
necessary agreement between members of the project team. The testing
manager needs to define who needs to approve a test deliverable, when it
will be approved, and what the backup plan is if an approval cannot be
obtained. The approval procedure can vary from a formal sign-off of a test
document to an informal review with comments. Exhibit 12.8 shows test
deliverables for which approvals are required or recommended, and by
whom. (Also see Appendix E17, Test Approvals, for a matrix that can be
used to formally document management approvals for test deliverables.)

Step 2: Define the Metric Objectives

“You can’t control what you can’t measure.” This is a quote from Tom
DeMarco’s book, Controlling Software Projects,20 in which he describes how
to organize and control a software project so it is measurable in the context
of time and cost projections. Control is the extent to which a manager can
ensure minimum surprises. Deviations from the plan should be signaled as
early as possible in order to react. Another quote from DeMarco’s book,
“The only unforgivable failure is the failure to learn from past failure,”

Exhibit 12.8.  Deliverable Approvals

Test Deliverable Approval Status Suggested Approver

Test plan Required Project Manager, Development 
Manager, Sponsor

Test design Required Development Manager

Change request Required Development Manager

Metrics Recommended Development Manager

Test case Required Development Manager

Test log summary report Recommended Development Manager

Interim test report Required Project Manager, Development 
Manager

System summary report Required Project Manager, Development 
Manager, Sponsor

Defect report Required Development Manager

TEAM LinG



150

SOFTWARE TESTING METHODOLOGY

stresses the importance of estimating and measurement. Measurement is a
recording of past effects to quantitatively predict future effects.

Task 1: Define the Metrics

Software testing as a test development project has deliverables such as
test plans, test design, test development, and test execution. The objective
of this task is to apply the principles of metrics to control the testing pro-
cess. A metric is a measurable indication of some quantitative aspect of a
system and has the following characteristics:

• Measurable — A metric point must be measurable for it to be a
metric, by definition. If the phenomenon can’t be measured, there
is no way to apply management methods to control it.

• Independent — Metrics need to be independent of human influence.
There should be no way of changing the measurement other than
changing the phenomenon that produced the metric.

• Accountable — Any analytical interpretation of the raw metric data
rests on the data itself and it is, therefore, necessary to save the
raw data and the methodical audit trail of the analytical process.

• Precise — Precision is a function of accuracy. The key to precision
is, therefore, that a metric is explicitly documented as part of the
data collection process. If a metric varies, it can be measured as a
range or tolerance.

A metric can be a “result” or a “predictor.” A result metric measures a
completed event or process. Examples include actual total elapsed time to
process a business transaction or total test costs of a project. A predictor
metric is an early warning metric that has a strong correlation to some
later result. An example is the predicted response time through statistical
regression analysis when more terminals are added to a system when the
amount of terminals has not yet been measured. A result or predictor met-
ric can also be a derived metric. A derived metric is one that is derived
from a calculation or graphical technique involving one or more metrics.

The motivation for collecting test metrics is to make the testing process
more effective. This is achieved by carefully analyzing the metric data and
taking the appropriate action to correct problems. The starting point is to
define the metric objectives of interest. Some examples include the following:

• Defect analysis — Every defect must be analyzed to answer such
questions as the root causes, how detected, when detected, who
detected, and so on.

• Test effectiveness — How well is testing doing, for example, return
on investment?

• Development effectiveness — How well is development fixing
defects?

TEAM LinG



151

Test Planning (Plan)

• Test automation — How much effort is expended on test automation?
• Test cost — What are the resources and time spent on testing?
• Test status — Another important metric is status tracking, or where

are we in the testing process?
• User involvement — How much is the user involved in testing?

Task 2: Define the Metric Points

Exhibit 12.9 lists some metric points associated with the general metrics
selected in the previous task and the corresponding actions to improve the
testing process. Also shown is the source, or derivation, of the metric
point.

Exhibit 12.9. Metric Points 

Metric Metric Point Derivation

Defect analysis: Distribution of defect causes Histogram, Pareto

Number of defects by cause over 
time

Multi-line graph

Number of defects by how found 
over time

Multi-line graph

Distribution of defects by module Histogram, Pareto

Distribution of defects by priority 
(critical, high, medium, low)

Histogram

Distribution of defects by functional 
area

Histogram

Distribution of defects by 
environment (platform)

Histogram, Pareto

Distribution of defects by type 
(architecture, connectivity, 
consistency, database integrity, 
documentation, GUI, installation, 
memory, performance, security, 
standards and conventions, stress, 
usability, bad fixes)

Histogram, Pareto

Distribution of defects by who 
detected (external customer, 
internal customer, development, 
QA, other)

Histogram, Pareto

Distribution by how detected 
(technical review, walkthroughs, 
JAD, prototyping, inspection, test 
execution)

Histogram, Pareto

TEAM LinG



152

SOFTWARE TESTING METHODOLOGY

Distribution of defects by severity 
(high, medium, low defects)

Histogram

Development 
effectiveness:

Average time for development to 
repair defect

Total repair time ÷ 
number of repaired 
defects

Test 
automation:

Percent of manual vs. automated 
testing

Cost of manual test effort 
÷ total test cost

Test cost: Distribution of cost by cause Histogram, Pareto

Distribution of cost by application Histogram, Pareto

Percent of costs for testing Test testing cost ÷ total 
system cost 

Total costs of testing over time Line graph

Average cost of locating a defect Total cost of testing ÷ 
number of defects 
detected

Anticipated costs of testing vs. 
actual cost

Comparison

Average cost of locating a 
requirements defect with 
requirements reviews

Requirements review 
costs ÷ number of 
defects uncovered 
during requirement 
reviews

Average cost of locating a design 
defect with design reviews

Design review costs ÷ 
number of defects 
uncovered during 
design reviews

Average cost of locating a code 
defect with reviews

Code review costs ÷ 
number of defects 
uncovered during code 
reviews

Average cost of locating a defect 
with test execution

Test execution costs ÷ 
number of defects 
uncovered during test 
execution

Number of testing resources over 
time

Line plot

Test 
effectiveness:

Percentage of defects discovered 
during maintenance

Number of defects 
discovered during 
maintenance ÷ total 
number of defects 
uncovered

Exhibit 12.9. Metric Points (Continued)

Metric Metric Point Derivation

TEAM LinG



153

Test Planning (Plan)

Percentage of defects uncovered 
due to testing

Number of detected 
errors through testing ÷ 
total system defects

Average effectiveness of a test Number of tests ÷ total 
system defects

Value returned while reviewing 
requirements 

Number of defects 
uncovered during 
requirements review ÷ 
requirements test costs

Value returned while reviewing 
design

Number of defects 
uncovered during 
design review ÷ design 
test costs

Value returned while reviewing 
programs

Number of defects 
uncovered during 
program review ÷ 
program test costs 

Value returned during test 
execution

Number of defects 
uncovered during 
testing ÷ test costs 

Effect of testing changes Number of tested changes 
÷ problems attributable 
to the changes

People’s assessment of 
effectiveness of testing

Subjective scaling (1–10)

Average time for QA to verify fix Total QA verification time 
÷ total number of 
defects to verify

Number of defects over time Line graph

Cumulative number of defects over 
time

Line graph

Number of application defects over 
time

Multi-line graph

Test extent: Percentage of statements executed Number of statements 
executed ÷ total 
statements

Percentage of logical paths 
executed

Number of logical paths ÷ 
total number of paths

Percentage of acceptance criteria 
tested

Acceptance criteria 
tested ÷ total 
acceptance criteria

Exhibit 12.9. Metric Points (Continued)

Metric Metric Point Derivation

TEAM LinG



154

SOFTWARE TESTING METHODOLOGY

Step 3: Review/Approve the Plan

Task 1: Schedule/Conduct the Review

The test plan review should be scheduled well in advance of the actual
review and the participants should have the latest copy of the test plan.

As with any interview or review, it should contain certain elements. The
first is defining what will be discussed, or “talking about what we are going
to talk about.” The second is discussing the details, or “talking about it.”
The third is summarization, or “talking about what we talked about.” The
final element is timeliness. The reviewer should state up front the esti-
mated duration of the review and set the ground rule that if time expires
before completing all items on the agenda, a follow-on review will be sched-
uled.

The purpose of this task is for development and the project sponsor to
agree and accept the test plan. If there are any suggested changes to the
test plan during the review, they should be incorporated into the test plan.

Task 2: Obtain Approvals

Approval is critical in a testing effort, for it helps provide the necessary
agreements between testing, development, and the sponsor. The best
approach is with a formal sign-off procedure of a test plan. If this is the

Number of requirements tested over 
time

Line plot

Number of statements executed 
over time

Line plot

Number of data elements exercised 
over time 

Line plot

Number of decision statements 
executed over time

Line plot

Test status: Number of tests ready to run over 
time

Line plot

Number of tests run over time Line plot

Number of tests run without defects 
uncovered

Line plot

Number of defects corrected over 
time 

Line plot

User 
involvement:

Percentage of user testing User testing time ÷ total 
test time

Exhibit 12.9. Metric Points (Continued)

Metric Metric Point Derivation

TEAM LinG



155

Test Planning (Plan)

case, use the management approval sign-off forms. However, if a formal
agreement procedure is not in place, send a memo to each key participant,
including at least the project manager, development manager, and sponsor.
In the document attach the latest test plan and point out that all their feed-
back comments have been incorporated and that if you do not hear from
them, it is assumed that they agree with the plan. Finally, indicate that in a
spiral development environment, the test plan will evolve with each itera-
tion but that you will include them in any modification.

TEAM LinG



TEAM LinG



157

Part 13
Test Case Design (Do)
If you will recall, in the spiral development environment, software testing is
described as a continuous improvement process that must be integrated
into a rapid application development methodology. Deming’s continuous
improvement process using the PDCA model is applied to the software
testing process. We are now in the Do part of the spiral model (see Exhibit
13.1).

Exhibit 13.2 outlines the steps and tasks associated with the Do part of
spiral testing. Each step and task is described along with valuable tips and
techniques.

Step 1: Design Function Tests

Task 1: Refine the Functional Test Requirements

At this point, the functional specification should have been completed. It
consists of the hierarchical functional decomposition, the functional win-
dow structure, the window standards, and the minimum system require-
ments of the system to be developed. An example of windows standards is
the Windows 2000 GUI Standards. A minimum system requirement could
consist of Windows 2000, a Pentium IV microprocessor, 1-GB RAM, 40-
gigdisk space, and a 56-KB modem.

A functional breakdown consists of a list of business functions, hierar-
chical listing, group of activities, or set of user profiles defining the basic
functions of the system and how the user will use it. A business function is
a discrete controllable aspect of the business and the smallest component
of a system. Each should be named and described with a verb–object par-
adigm. The criteria used to determine the successful execution of each
function should be stated. The functional hierarchy serves as the basis for
function testing in which there will be at least one test case for each lowest-
level function. Examples of functions include: approve customer credit,
handle order, create invoice, order components, receive revenue, pay bill,
purchase items, and so on. Taken together, the business functions consti-
tute the total application including any interfaces. A good source of these
functions (in addition to the interview itself) is a process decomposition
and/or data flow diagram, or CRUD matrix, which should be requested dur-
ing the information-gathering interview.

TEAM LinG



158

SOFTWARE TESTING METHODOLOGY

Exhibit 13.1. Spiral Testing and Continuous Improvement

Exhibit 13.2. Test Design (Steps/Tasks)

TEAM LinG



159

Test Case Design (Do)

The requirements serve as the basis for creating test cases. The follow-
ing quality assurance test checklists can be used to assure that the require-
ments are clear and comprehensive:

• Appendix E22: Clarification Request that can be used to document
questions that may arise while the tester analyzes the requirements.

• Appendix F25: Ambiguity Review Checklist that can be used to assist
in the review of a Functional Specification of structural ambiguity
(not to be confused with content reviews).

• Appendix F26: Architecture Review Checklist that can be used to
review the architecture for completeness and clarity.

• Appendix F27: Data Design Review Checklist that can be used to
review the logical and physical design for clarity and completeness.

• Appendix F28: Functional Specification Review Checklist that can be
used in Functional Specification for content completeness and clar-
ity (not to be confused with ambiguity reviews).

• Appendix F29: Prototype Review Checklist that can be used to review
a prototype for content completeness and clarity.

• Appendix F30: Requirements Review Checklist that can be used to
verify that the testing project requirements are comprehensive and
complete.

• Appendix F31: Technical Design Review Checklist that can be used
to review the technical design for clarity and completeness.

A functional breakdown is used to illustrate the processes in a hierarchi-
cal structure showing successive levels of detail. It is built iteratively as
processes and nonelementary processes are decomposed (see Exhibit
13.3).

A data flow diagram shows processes and the flow of data among these
processes. It is used to define the overall data flow through a system and
consists of external agents that interface with the system, processes, data
flow, and stores depicting where the data is stored or retrieved. A data flow
diagram should be reviewed, and each major and leveled function should
be listed and organized into a hierarchical list.

A CRUD matrix, or association matrix, links data and process models. It
identifies and resolves matrix omissions and conflicts and helps refine the
data and process models, as necessary.

A functional window structure describes how the functions will be
implemented in the windows environment. Exhibit 13.4 shows a sample
functional window structure for order processing.

Task 2: Build a Function/Test Matrix

The function/test matrix cross-references the tests to the functions. This
matrix provides proof of the completeness of the test strategies, illustrating

TEAM LinG



160

SOFTWARE TESTING METHODOLOGY

in graphic format which tests exercise which functions. See Exhibit 13.5
and Appendix E5, Function/Test Matrix for more details.

 The matrix is used as a control sheet during testing and can also be
used during maintenance. For example, if a function is to be changed, the
maintenance team can refer to the function/test matrix to determine which
tests need to be run or changed. The business functions are listed verti-
cally and the test cases are listed horizontally. The test case name is
recorded on the matrix along with the number. (Also see Appendix E24,
Test Condition Versus Test Case, Matrix I, which can be used to associate
a requirement with each condition that is mapped to one or more test
cases.)

Exhibit 13.3. Functional Breakdown

Functional Test Requirements (Breakdown)

Order Processing
Create new order
Fulfill order
Edit order
Delete order

Customer Processing
Create new customer
Edit customer
Delete customer

Financial Processing
Receive customer payment
Deposit payment
Pay vendor
Write a check
Display register

Inventory Processing
Acquire vendor products
Maintain stock
Handle back orders
Audit inventory
Adjust product price

Reports
Create order report
Create account receivable report
Create account payable report
Create inventory report

TEAM LinG



161

Test Case Design (Do)

Exhibit 13.4. Functional Window Structure

The Main Window
a. The top line of the main window has the standard title bar with Min/Max controls.
b. The next line contains the standard Windows menu bar.
c. The next line contains the standard Windows tool bar.
d. The rest of the Main-Application Window is filled with the Customer-Order 

Window.

The Customer-Order Window
a. This window shows a summary of each previously entered order.
b. Several orders will be shown at one time (sorted by order number and customer 

name). For each customer order, this window will show:
1. Order Number
2. Customer Name
3. Customer Number
4. Date
5. Invoice Number
6. Model Number
7. Product Number
8. Quantity Shipped
9. Price

c. The scroll bar will be used to select which orders are to be viewed.
d. This window is read-only for viewing.
e. Double-clicking an order will display the Edit-Order Dialog where the order can be 

modified.

The Edit-Order Window
a. This dialog is used to create new orders or for making changes to previously 

created orders.
b. This dialog will be centered over the Customer-Order Window. The layout of this 

dialog will show the following:
1. Order Number (automatically filled in)
2. Edit field for: Customer Name
3. Edit field for: Customer Number
4. Date (initialized)
5. Edit field for: Invoice Number
6. Edit field for: Model Number
7. Edit field for: Product Number
8. Edit field for: Quantity Shipped
9. Price (automatically filled in)

10. Push buttons for: OK and Cancel

The Menu Bar Will Include the Following Menus:
File:

New:
Used to create a new order file

Open:
Used to open the order file

TEAM LinG



162

SOFTWARE TESTING METHODOLOGY

Exhibit 13.4. Functional Window Structure (Continued)

Save:
Used to save the order file

Save As…:
Used to save the current order file under a new name

Exit:
Used to exit Windows

Order:
Create New Order:

Display Edit-Order Window with blank fields (except date)
Fulfill Order:

This dialog will be used to verify that the order quantity is available in inventory 
stock and validate customer credit.

The dialog will include:
1. Edit field for: Order Number
2. Edit field for: Customer Name
3. Edit field for: Customer Number
4. Date (initialized)
5. Invoice Number (initialized)
6. Model Number (initialized)
7. Product Number (initialized)
8. Quantity Shipped (initialized)
9. Price (initialized)

10. Push buttons for: OK and Cancel
a. The quantity order is checked against the inventory stock level. If the 

order cannot be filled, a back order note is sent to purchasing.
b. The customer history will be displayed (the scroll bar will be used to view 

the history information).
c. An Accept button will fulfill the order and create an invoice for shipping.
d. A Reject button deletes the order and creates a customer order rejection 

letter.

Edit an Order:
This dialog will be used to edit an existing order. The dialog will include:

1. Edit field for: Order Number
2. Edit field for: Customer Name
3. Edit field for: Customer Number
4. Push buttons for: OK and Cancel

Delete an Order:
This dialog will be used to delete an existing order. The dialog will include:

1. Edit field for: Order Number
2. Edit field for: Customer Name
3. Edit field for: Customer Number
4. Push buttons for: OK and Cancel

a. A confirmation message will be displayed with Yes, No, or Cancel options.

Order Report:
This dialog will display one or more orders based upon order number or date 

ranges.

TEAM LinG



163

Test Case Design (Do)

It is also important to differentiate those test cases that are manual and
those that are automated. One way to accomplish this is to come up with
a naming standard that will highlight an automated test case; for example,
the first character of the name is “A.”

Exhibit 13.5 shows an example of a function/test matrix.

Step 2: Design GUI Tests

The goal of a good graphical user interface (GUI) design should be consis-
tent in “look and feel” for the users of the application. Good GUI design has
two key components: interaction and appearance. Interaction relates to
how the user interacts with the application. Appearance relates to how the
interface looks to the user.

GUI testing involves the confirmation that the navigation is correct; for
example, when an ICON, menu choice, or ratio button is clicked, the
desired response occurs. The following are some good GUI design princi-
ples the tester should look for while testing the application.

Exhibit 13.4. Functional Window Structure (Continued)

The layout of the dialog will include:
1. Radio buttons for: Order, Date
2. First Order Number (if report by Order)
3. Optional last Order Number (if report by Order)
4. First Date (if report by Date)
5. Optional last Date (if report by Date)

The user is prompted with the message “Would you like a hard copy printout?”
The user is prompted with the message “Would you like another report (Y/N)?” 

after each report.

View:
Toolbar:

Used to toggle the display of the toolbar on and off.
Status bar:

Used to toggle the display of the status bar on or off.

The Tool Bar with Icons to Execute the Following Menu Commands:
File → New
File → Open
Order → Create
Order → Validate
Order → Edit
Order → Delete
File → Exit

TEAM LinG



164

SOFTWARE TESTING METHODOLOGY

Ten Guidelines for Good GUI Design

1. Involve users.
2. Understand the user’s culture and experience.
3. Prototype continuously to validate the requirements.
4. Let the user’s business workflow drive the design.

Exhibit 13.5. Functional/Test Matrix

Business Function

Test Case

1 2 3 4 5

Order processing

Create new order CNO01 CNO02

Fulfill order AO01

Edit order EO01 EO02 EO03 EO04

Delete order DO01 DO02 DO03 DO04 DO05

Customer processing

Create new customer ANC01 ANC02 ANC03

Edit customer EC01 EC02 EC03 EC04 EC05

Delete customer DC01 DC02

Financial processing

Receive customer payment RCP01 RCP02 RCP03 RCP04

Deposit payment AP01 AP02

Pay vendor PV01 PV02 PV03 PV04 PV05

Write a check WC01 WC02

Display register DR01 DR02

Inventory processing

Acquire vendor products AP01 AP02 AP03

Maintain stock MS01 MS02 MS03 MS04 MS05

Handle back orders HB01 HB02 HB03

Audit inventory AI01 AI02 AI03 AI04

Adjust product price AC01 AC02 AC03

Reports

Create order report CO01 CO02 CO03 CO04 CO05

Create account receivables report CA01 CA02 CA03

Create account payables AY01 AY02 AY03

Create inventory report CI01 CI02 CI03 CI04

TEAM LinG



165

Test Case Design (Do)

5. Do not overuse or underuse GUI features.
6. Create the GUI, help, and training concurrently.
7. Do not expect users to remember secret commands or functions.
8. Anticipate mistakes and don’t penalize the user.
9. Continually remind the user of the application status.

10. Keep it simple.

Task 1: Identify the Application GUI Components

The graphical user interface provides multiple channels of communica-
tion using words, pictures, animation, sound, and video. Five key founda-
tion components of the user interface are windows, menus, forms, icons,
and controls.

1. Windows — In a windowed environment, all user interaction with
the application occurs through the windows. These include a pri-
mary window, along with any number of secondary windows gener-
ated from the primary one.

2. Menus — Menus come in a variety of styles and forms. Examples
include action menus (push button, radio button), pull-down menus,
pop-up menus, option menus, and cascading menus.

3. Forms — Forms are windows or screens into which the user can add
information.

4. Icons — Icons, or “visual push buttons,” are valuable for instant
recognition, ease of learning, and ease of navigation through the
application.

5. Controls — A control component appears on a screen that allows
the user to interact with the application and is indicated by its
corresponding action. Controls include menu bars, pull-down
menus, cascading menus, pop-up menus, push buttons, check boxes,
radio buttons, list boxes, and drop-down list boxes.

A design approach to GUI test design is to first define and name each GUI
component by name within the application as shown in Exhibit 13.6. In the
next step, a GUI component checklist is developed that can be used to ver-
ify each component in the table above. (Also see Appendix E6, GUI Compo-
nent Test Matrix.)

Task 2: Define the GUI Tests

In the previous task the application GUI components were defined, named,
and categorized in the GUI component test matrix. In the present task, a
checklist is developed against which each GUI component is verified. The
list should cover all possible interactions and may or may not apply to a
particular component. Exhibit 13.7 is a partial list of the items to check.
(See Appendix E23, Screen Data Mapping, which can be used to document
the properties of the screen data and Appendix F32, Test Case Preparation

TEAM LinG



166

SOFTWARE TESTING METHODOLOGY

Exhibit 13.6. GUI Component Test Matrix

GUI Type

Name Window Menu Form ICON Control P/F Date Tester

Main window �

Customer-order window �

Edit-order window �

Menu bar �

Tool bar �

•

•

•

Exhibit 13.7. GUI Component Checklist

Access via Double-Click Multiple Windows Open Tabbing Sequence

Access via menu Ctrl menu (move) Push buttons

Access via toolbar Ctrl + function keys Pull-down menu and 
submenus options

Right-mouse options Color Dialog controls

Help links Accelerators and hot keys Labels

Context-sensitive help Cancel Chevrons

Button bars Close Ellipses

Open by double-click Apply Gray-out unavailability

Screen images and graphics Exit Check boxes

Open by menu OK Filters

Open by toolbar Tile horizontal/vertical Spin boxes

ICON access Arrange icons Sliders

Access to DOS Toggling Fonts

Access via single-click Expand/contract tree Drag/drop

Resize window panels Function keys Horizontal/vertical 
scrolling

Fields accept allowable 
values

Minimize the window
Maximize the window

Cascade
Window open

Fields handle invalid values Tabbing sequence

TEAM LinG



167

Test Case Design (Do)

Review Checklist, which can be used to ensure that test cases have been
prepared as per specifications.)

In addition to the GUI component checks above, if there is a GUI design
standard, it should be verified as well. GUI standards are essential to
ensure that the internal rules of construction are followed to achieve the
desired level of consistency. Some of the typical GUI standards that should
be verified include the following:

• Forms “enterable” and display-only formats
• Wording of prompts, error messages, and help features
• Use of color, highlight, and cursors
• Screen layouts
• Function and shortcut keys, or “hot keys”
• Consistently locating screen elements on the screen
• Logical sequence of objects
• Consistent font usage
• Consistent color usage

It is also important to differentiate manual from automated GUI test
cases. One way to accomplish this is to use an additional column in the GUI
component matrix that indicates if the GUI test is manual or automated.

Step 3: Define the System/Acceptance Tests

Task 1: Identify Potential System Tests

System testing is the highest level of testing and evaluates the functionality
as a total system, its performance, and overall fitness of use. This test is
usually performed by the internal organization and is oriented to systems’
technical issues rather than acceptance, which is a more user-oriented test.

Systems testing consists of one or more tests that are based on the orig-
inal objectives of the system that were defined during the project inter-
view. The purpose of this task is to select the system tests that will be per-
formed, not how to implement the tests. Some common system test types
include the following:

• Performance Testing — Verifies and validates that the performance
requirements have been achieved; measures response times, trans-
action rates, and other time-sensitive requirements.

• Security Testing — Evaluates the presence and appropriate function-
ing of the security of the application to ensure the integrity and
confidentiality of the data.

• Volume Testing — Subjects the application to heavy volumes of data
to determine if it can handle the volume of data.

• Stress Testing — Investigates the behavior of the system under con-
ditions that overload its resources. Of particular interest is the
impact that this has on system processing time.

TEAM LinG



168

SOFTWARE TESTING METHODOLOGY

• Compatibility Testing — Tests the compatibility of the application
with other applications or systems.

• Conversion Testing — Verifies the conversion of existing data and
loads a new database.

• Usability Testing — Determines how well the user will be able to use
and understand the application.

• Documentation Testing — Verifies that the user documentation is
accurate and ensures that the manual procedures work correctly.

• Backup Testing — Verifies the ability of the system to back up its
data in the event of a software or hardware failure.

• Recovery Testing — Verifies the system’s ability to recover from a
software or hardware failure.

• Installation Testing — Verifies the ability to install the system suc-
cessfully.

Task 2: Design System Fragment Tests

System fragment tests are sample subsets of full system tests that can be
performed during each spiral loop. The objective of doing a fragment test
is to provide early warning of pending problems that would arise in the full
system test. Candidate fragment system tests include function, perfor-
mance, security, usability, documentation, and procedure. Some of these
fragment tests should have formal tests performed during each spiral,
whereas others should be part of the overall testing strategy. Nonfragment
system tests include installation, recovery, conversion, and the like, which
are probably going to be performed until the formal system test.

Function testing on a system level occurs during each spiral as the sys-
tem is integrated. As new functionality is added, test cases need to be
designed, implemented, and tested during each spiral.

Typically, security mechanisms are introduced fairly early in the devel-
opment. Therefore, a set of security tests should be designed, imple-
mented, and tested during each spiral as more features are added.

Usability is an ongoing informal test during each spiral and should
always be part of the test strategy. When a usability issue arises, the tester
should document it in the defect tracking system. A formal type of usability
test is the end user’s review of the prototype, which should occur during
each spiral.

Documentation (such as online help) and procedures are also ongoing
informal tests. These should be developed in parallel with formal system
development during each spiral and not be put off until a formal system
test. This will avoid last-minute surprises. As new features are added, doc-
umentation and procedure tests should be designed, implemented, and
tested during each spiral.

TEAM LinG



169

Test Case Design (Do)

Some performance testing should occur during each spiral at a noncon-
tended unit level, that is, one user. Baseline measurements should be per-
formed on all key functions as they are added to the system. A baseline
measurement is a measurement taken for the specific purpose of determin-
ing the initial value of the state or performance measurement. During sub-
sequent spirals, the performance measurements can be repeated and com-
pared to the baseline. Exhibit 13.8 provides an example of baseline
performance measurements.

Task 3: Identify Potential Acceptance Tests

Acceptance testing is an optional user-run test that demonstrates the abil-
ity of the application to meet the user’s requirements. The motivation for
this test is to demonstrate rather than be destructive, that is, to show that
the system works. Less emphasis is placed on technical issues and more is
placed on the question of whether the system is a good business fit for the
end user. The test is usually performed by users, if performed at all. Typi-
cally, 20 percent of the time this test is rolled into the system test. If per-
formed, acceptance tests typically are a subset of the system tests. How-
ever, the users sometimes define “special tests,” such as intensive stress or
volume tests, to stretch the limits of the system even beyond what was
tested during the system test.

Step 4: Review/Approve Design

Task 1: Schedule/Prepare for Review

The test design review should be scheduled well in advance of the actual
review, and the participants should have the latest copy of the test design.

As with any interview or review, it should contain certain elements. The
first is defining what will be discussed, or “talking about what we are going
to talk about.” The second is discussing the details, or “talking about it.”
The third is summarization, or “talking about what we talked about.” The
final element is timeliness. The reviewer should state up front the estimated
duration of the review and set the ground rule that if time expires before com-
pleting all items on the agenda, a follow-on review will be scheduled.

The purpose of this task is for development and the project sponsor to
agree and accept the test design. If there are any suggested changes to the
test design during the review, they should be incorporated into the test
design.

Task 2: Obtain Approvals

Approval is critical in a testing effort, because it helps provide the neces-
sary agreements among testing, development, and the sponsor. The best
approach is with a formal sign-off procedure of a test design. If this is the

TEAM LinG



170

SOFTWARE TESTING METHODOLOGY

E
xh

ib
it

 1
3.

8.
B

as
el

in
e 

P
er

fo
rm

an
ce

 M
ea

su
re

m
en

ts

B
u

si
n

es
s 

Fu
n

ct
io

n

B
as

el
in

e 
Se

co
n

d
s 

– 
R

el
 1

.0
 

(1
/1

/0
4)

M
ea

su
re

 
an

d
 D

el
ta

 
Se

co
n

d
s –

 R
el

 
1.

1 
(2

/1
/0

4)

M
ea

su
re

 
an

d
 D

el
ta

 
Se

co
n

d
s 

– 
R

el
 

1.
2 

(2
/1

5/
04

)

M
ea

su
re

 
an

d
 D

el
ta

 
Se

co
n

d
s 

– 
R

el
 

1.
3 

(3
/1

/0
4)

M
ea

su
re

 
an

d
 D

el
ta

 
Se

co
n

d
s 

– 
R

el
 

1.
4 

(3
/1

5/
04

)

M
ea

su
re

 
an

d
 D

el
ta

 
Se

co
n

d
s 

– 
R

el
 

1.
5 

(4
/1

/0
4)

O
rd

er
 p

ro
ce

ss
in

g

C
re

at
e 

ne
w

 o
rd

er
1.

0
1.

5
(+

50
%

)
1.

3
(–

13
%

)
1.

0
(–

23
%

)
.9

(–
10

%
)

.7
5

(–
17

%
)

Fu
lfi

ll 
or

d
er

2.
5

2.
0

(–
20

%
)

1.
5

(–
25

%
)

1.
0

(–
33

%
)

1.
0

(0
%

)
1.

0
(0

%
)

Ed
it

 o
rd

er
1.

76
2.

0
(+

14
%

)
2.

5
(+

25
%

)
1.

7
(–

32
%

)
1.

5
(–

12
%

)
1.

2
(–

20
%

)

D
el

et
e 

or
d

er
1.

1
1.

1
(0

%
)

1.
4

(+
27

%
)

1.
0

(–
29

%
)

.8
(–

20
%

)
.7

5
(–

6%
)

•
•

•
•

•
•

•

•
•

•
•

•
•

•

•
•

•
•

•
•

•

•
•

•
•

•
•

•

•
•

•
•

•
•

•

TEAM LinG



171

Test Case Design (Do)
R

ep
or

ts

C
re

at
e 

or
d

er
 r

ep
or

t
60

55
(–

8%
)

35
(–

36
%

)
28

(–
20

%
)

20
(–

29
%

)
15

(–
25

%
)

C
re

at
e 

ac
co

un
t 

re
ce

iv
ab

le
s 

re
p

or
t

55
65

(+
18

%
)

55
(–

15
%

)
35

(–
36

%
)

25
(–

29
%

)
20

(–
20

%
)

C
re

at
e 

ac
co

un
t 

p
ay

ab
le

s
12

0
90

(–
25

%
)

65
(–

28
%

)
45

(–
31

%
)

65
(+

44
%

)
25

(–
62

%
)

C
re

at
e 

in
ve

nt
or

y 
re

p
or

t
85

70
(–

18
%

)
50

(–
29

%
)

39
(–

22
%

)
28

(–
28

%
)

25
(–

11
%

)

TEAM LinG



172

SOFTWARE TESTING METHODOLOGY

case, use the management approval sign-off forms. However, if a formal
agreement procedure is not in place, send a memo to each key participant,
including at least the project manager, development manager, and sponsor.
In the document attach the latest test design and point out that all their
feedback comments have been incorporated and that if you do not hear
from them, it is assumed that they agree with the design. Finally, indicate
that in a spiral development environment, the test design will evolve with
each iteration but that you will include them in any modification.

TEAM LinG



173

Part 14
Test Development (Do)

Exhibit 14.1 outlines the steps and tasks associated with the Do part of spi-
ral testing. Each step and task is described along with valuable tips and
techniques.

Step 1: Develop Test Scripts

Task 1: Script the Manual/Automated GUI/Function Tests

In a previous step, a GUI/Function Test Matrix was built that cross-refer-
ences the tests to the functions. The business functions are listed verti-
cally, and the test cases are listed horizontally. The test case name is
recorded on the matrix along with the number.

In the current task, the functional test cases are documented and trans-
formed into reusable test scripts with test data created. To aid in the devel-
opment of the script of the test cases, the GUI-based Function Test Matrix
template in Exhibit 14.2 can be used to document function test cases that
are GUI-based (see Appendix E7, GUI-Based Functional Test Matrix, for
more details).

Consider the script in Exhibit 14.2, which uses the template to create a
new customer order. The use of this template shows the function, the case
number within the test case (a variation of a specific test), the requirement
identification cross-reference, the test objective, the case steps, the
expected results, the pass/fail status, the tester name, and the date the test
was performed. Within a function, the current GUI component is also doc-
umented. In Exhibit 14.2, a new customer order is created by first invoking
the menu bar to select the function, followed by the Edit-Order Window to
enter the order number, customer number, model number, product num-
ber, and quantity.

Task 2: Script the Manual/Automated System Fragment Tests

In a previous task, the system fragment tests were designed. They are sam-
ple subsets of full system tests, which can be performed during each spiral
loop.

In this task, the system fragment tests can be scripted using the GUI-
based Function Test Matrix discussed in the previous task. The test objec-
tive description is probably more broad than the Function/GUI tests, as

TEAM LinG



174

SOFTWARE TESTING METHODOLOGY

they involve more global testing issues such as performance, security,
usability, documentation, procedure, and so on.

Step 2: Review/Approve Test Development

Task 1: Schedule/Prepare for Review

The test development review should be scheduled well in advance of the
actual review and the participants should have the latest copy of the test
design.

As with any interview or review, it should contain certain elements. The
first is defining what will be discussed, or “talking about what we are going
to talk about.” The second is discussing the details, or “talking about it.”
The third is summarization, or “talking about what we talked about.” The final
element is timeliness. The reviewer should state up front the estimated dura-
tion of the review and set the ground rule that if time expires before complet-
ing all items on the agenda, a follow-on review will be scheduled.

The purpose of this task is for development and the project sponsor to
agree and accept the test development. If there are any suggested changes
to the test development during the review, they should be incorporated
into the test development.

Task 2: Obtain Approvals

Approval is critical in a testing effort, because it helps provide the neces-
sary agreements among the testing, development, and the sponsor. The

Exhibit 14.1. Test Development (Steps/Tasks)

TEAM LinG



175

Test Development (Do)

E
xh

ib
it

 1
4.

2.
 F

u
n

ct
io

n
/G

U
I 

Te
st

 S
cr

ip
t

Fu
n

ct
io

n
 (

C
re

at
e 

a 
N

ew
 C

u
st

om
er

 O
rd

er
)

C
as

e 
N

o.
R

eq
. N

o.
Te

st
 O

b
je

ct
iv

e
C

as
e 

St
ep

s
E

xp
ec

te
d

 R
es

u
lt

s
(P

/F
)

Te
st

er
D

at
e

M
en

u
 B

a
r

15
67

C
re

at
e 

a 
va

lid
 

ne
w

 c
us

to
m

er
 

or
d

er
 

Se
le

ct
 F

ile
/C

re
at

e 
O

rd
er

 
fr

om
 t

h
e 

m
en

u 
b

ar
Ed

it
-O

rd
er

 W
in

d
ow

 a
p

p
ea

rs
P

as
se

d
Jo

ne
s

7/
21

/0
4

E
d

it
-O

rd
er

 W
in

d
o

w

1.
En

te
r 

or
d

er
 n

um
b

er
O

rd
er

 v
al

id
at

ed
P

as
se

d
Jo

ne
s

7/
21

/0
4

2.
En

te
r 

cu
st

om
er

 n
um

b
er

C
us

to
m

er
 v

al
id

at
ed

P
as

se
d

Jo
ne

s
7/

21
/0

4

3.
En

te
r 

m
od

el
 n

um
b

er
M

od
el

 v
al

id
at

ed
P

as
se

d
Jo

ne
s

7/
21

/0
4

4.
En

te
r 

p
ro

d
uc

t 
nu

m
b

er
P

ro
d

uc
t 

va
lid

at
ed

P
as

se
d

Jo
ne

s
7/

21
/0

4

5.
En

te
r 

q
ua

nt
it

y
Q

ua
nt

it
y 

va
lid

at
ed

 d
at

e,
 in

vo
ic

e 
nu

m
b

er
, a

nd
 t

ot
al

 p
ri

ce
 g

en
er

at
ed

P
as

se
d

Jo
ne

s
7/

21
/0

4

6.
Se

le
ct

 O
K

C
us

to
m

er
 is

 c
re

at
ed

 s
uc

ce
ss

fu
lly

P
as

se
d

Jo
ne

s
7/

21
/0

4

TEAM LinG



176

SOFTWARE TESTING METHODOLOGY

best approach is with a formal sign-off procedure of a test development. If
this is the case, use the management approval sign-off forms. However, if a
formal agreement procedure is not in place, send a memo to each key par-
ticipant, including at least the project manager, development manager, and
sponsor. In the document, attach the latest test development and point out
that all their feedback comments have been incorporated and that if you
do not hear from them, it is assumed that they agree with the development.
Finally, indicate that in a spiral development environment, the test devel-
opment will evolve with each iteration but that you will include them in any
modification.

TEAM LinG



177

Part 15

Test Coverage 
through Traceability

As already pointed out previously, most businesses have reconciled to the
fact that a percent error-free software cannot be produced and only over a
period of time the soft base gets stabilized. But the system cannot go live
with critical defects unresolved. Many of the companies have started stat-
ing their acceptance criteria in the test strategy document. It may range
from nonexistence of critical and medium defects to business flow accep-
tance by the end users. The ultimate aim of final testing is to prove that the
software delivers what the client requires. A trace between the different
test deliverables should ensure that the test covers the requirements com-
prehensively so that all requirements are tested without any omission.

The business requirement document (BRD), functional specification
documents (FS), test conditions/cases, test data, and defects identified
during testing are some key components of the traceability matrix. The fol-
lowing discussion illustrates how these components are integrated
through the traceability matrix as shown in Exhibit 15.1.

The requirements specified by the users in the business requirement
document may not be exactly translated into a functional specification
document. Therefore, a trace on specifications between functional specifi-
cation and business requirements is done on a one-to-one basis. This helps
in finding the gap between the documents. These gaps are then closed by
the author of the functional specifications, or deferred to the next release
after discussion. The final FS may vary from the original, as deferring or
taking in a gap may have a ripple effect on the application. Sometimes,
these ripple effects may not be properly documented. This is the first-level
traceability.

The functional specification documents are divided into smaller mod-
ules, functions, and test conditions to percolate down to the test case
where various data values are imputted to the test conditions for validat-
ing them. A test condition is an abstract extraction of the testable require-
ments from the functional specification documents. The test conditions
may be explicitly or implicitly in the requirement documents. A test condi-
tion has one or more associated test cases. Each of the test conditions is

TEAM LinG



178

SOFTWARE TESTING METHODOLOGY

traced back to its originating requirements. The second level of trace is
thus between the functional specification documents and the test condi-
tion documents.

A test case is a set of test inputs, execution conditions, and expected
results developed for a particular objective to validate a specific function-
ality in the application under test. The number of test cases for each test
condition may vary from multiple to one. Each of these test cases can be
traced back to its test conditions and through test conditions to their orig-
inating requirements. The third level of traceability is between the test
cases and test conditions and ultimately to the baseline requirements.

The final phase of traceability is with the defects identified in the test
execution phase. Tracing the defect to the test condition and the specifica-
tion will lead us to retrospection on the reason why the requirements or
the test condition has failed. Whether the requirements have not been
stated clearly or the test conditions have not been extracted properly from
the requirements will help us to correct ourselves in future assignments.
Exhibit 15.2 illustrates how the above deliverables are traced using a trace-
ability matrix.

Use Cases and Traceability

A use case is a scenario that describes the use of a system by an actor to
accomplish a specific goal. An actor is a user playing a role with respect to the
system. Actors are generally people, although other computer systems may
be actors. A scenario is a sequence of steps that describe the interactions

Exhibit 15.1. Traceability Tree Diagram

Requirements

Specifications

Test Conditions

Test Cases

Defects

TEAM LinG



179

Test Coverage through Traceability

Exhibit 15.2. Traceability Matrix

Item
No.

Ref. No. Application/
Module 
Name

Test 
Condition

Test 
Cases

Test 
Script 

ID
Defect 

ID
BRD 

Ref. No.
FS 

Ref. No.

TEAM LinG



180

SOFTWARE TESTING METHODOLOGY

between an actor and the system. Exhibit 15.3 shows a use case diagram that
consists of the collection of all actors and all use cases. Use cases:

• Capture the system’s functional requirements from the users’ per-
spective

• Actively involve users in the requirements-gathering process
• Provide the basis for identifying major classes and their relation-

ships
• Serve as the foundation for developing system test cases

The use cases should be traced back to the functional specification doc-
ument and traced forward to the test conditions and test cases documents.
The following have to be considered while deriving traceability:

• Whether the use cases unfold from highest to lowest levels
• If all the system’s functional requirements are reflected in the use

cases
• If we can trace each use case back to its requirement(s)

Summary

During the progress of the project new requirements are brought in due to
the client’s additional requirements or as a result of the review process.
These additional requirements should be appropriately traced to the test
conditions and cases.

Similarly, a change request raised during the course of testing the appli-
cation should be handled in the traceability matrix. Requirements present
in the traceability matrix document should not be deleted at any time even
when the requirement is moved for the next release. All the requirements
present in the traceability matrix should be covered with at least one test
case.

Thus, traceability serves as an effective tool to ensure that the testware
is comprehensive. This instills confidence in the client that the test team
has tested all the requirements. Various modern testing tools such as Test
Director from Mercury Interactive have the facility of creating the trace-
ability documents.

Exhibit 15.3. Use Case Diagram

Actor

Use Case

Use Case z

Included
Use Case

<<include>>

TEAM LinG



181

Part 16

Test Execution/
Evaluation 
(Do/Check)
If you will recall, in the spiral development environment, software testing is
described as a continuous improvement process that must be integrated
into a rapid application development methodology. Deming’s continuous
improvement process using the PDCA model is applied to the software
testing process. We are now in the Do/Check part of the spiral model (see
Exhibit 16.1).

Exhibit 16.2 outlines the steps and tasks associated with the Do/Check
part of spiral testing. Each step and task is described along with valuable
tips and techniques.

Step 1: Setup and Testing

Task 1: Regression Test the Manual/Automated Spiral Fixes

The purpose of this task is to retest the tests that discovered defects in the
previous spiral. The technique used is regression testing. Regression test-
ing is a technique that detects spurious errors caused by software modifi-
cations or corrections. See Appendix G27, Regression Testing, for more
details.

A set of test cases must be maintained and available throughout the
entire life of the software. The test cases should be complete enough so
that all the software’s functional capabilities are thoroughly tested. The
question arises as to how to locate those test cases to test defects discov-
ered during the previous test spiral. An excellent mechanism is the retest
matrix.

As described earlier, a retest matrix relates test cases to functions (or
program units). A check entry in the matrix indicates that the test case is
to be retested when the function (or program unit) has been modified due
to an enhancement(s) or correction(s). No entry means that the test case
does not need to be retested. The retest matrix can be built before the first

TEAM LinG



182

SOFTWARE TESTING METHODOLOGY

testing spiral, but needs to be maintained during subsequent spirals. As
functions (or program units) are modified during a development spiral,
existing or new test cases need to be created and checked in the retest
matrix in preparation for the next test spiral. Over time with subsequent
spirals, some functions (or program units) may be stable with no recent
modifications. Consideration to selectively remove their check entries
should be undertaken between testing spirals.

If a regression test passes, the status of the defect report should be
changed to “closed.”

Task 2: Execute the Manual/Automated New Spiral Tests

The purpose of this task is to execute new tests that were created at the
end of the previous testing spiral. In the previous spiral, the testing team
updated the test plan, GUI-based function test matrix, scripts, the GUI, the

Exhibit 16.1.  Spiral Testing and Continuous Improvement

Exhibit 16.2.  Test Execution/Evaluation (Steps/Tasks)

TEAM LinG



183

Test Execution/Evaluation (Do/Check)

system fragment tests, and acceptance tests in preparation for the current
testing spiral. During this task those tests are executed.

Task 3: Document the Spiral Test Defects

During spiral test execution, the results of the testing must be reported in
the defect tracking database. These defects are typically related to individ-
ual tests that have been conducted. However, variations to the formal test
cases often uncover other defects. The objective of this task is to produce
a complete record of the defects. If the execution step has been recorded
properly, the defects have already been recorded on the defect tracking
database. If the defects are already recorded, the objective of this step
becomes to collect and consolidate the defect information.

Tools can be used to consolidate and record defects depending on the
test execution methods. If the defects are recorded on paper, the consoli-
dation involves collecting and organizing the papers. If the defects are
recorded electronically, search features can easily locate duplicate defects.
A sample defect report is given in Appendix E27, which can be used to
report the details of a specific defect.

Step 2: Evaluation

Task 1: Analyze the Metrics

Metrics are used so that we can help make decisions more effectively and
support the development process. The objective of this task is to apply the
principles of metrics to control the testing process.

In a previous task, the metrics and metric points were defined for each
spiral to be measured. During the present task the metrics that were mea-
sured are analyzed. This involves quantifying the metrics and putting them
into a graphical format.

The following is the key information a test manager needs to know at the
end of a spiral:

• Test Case Execution Status — How many test cases have been exe-
cuted, how many not executed, and how many discovered defects?
This provides an indication of the tester’s productivity. If the test
cases are not being executed in a timely manner, this raises a flag
that more personnel may need to be assigned to the project.

• Defect Gap Analysis — What is the gap between the number of
defects that have been uncovered and the number that have been
corrected? This provides an indication of development’s ability to
correct defects in a timely manner. If there is a relatively large gap,
this raises the flag that perhaps more developers need to be assigned
to the project.

TEAM LinG



184

SOFTWARE TESTING METHODOLOGY

• Defect Severity Status — The distribution of the defect severity (e.g.,
critical, major, and minor) provides an indication of the quality of
the system. If there is a large percentage of defects in the critical
category, there probably exist a considerable number of design and
architecture issues, which also raises a flag.

• Test Burnout Tracking — Shows the cumulative and periodic number
of defects being discovered. The cumulative number, for example,
the running total number of defects, and defects by time period help
predict when fewer and fewer defects are being discovered. This is
indicated when the cumulative curve “bends” and the defects by
time period approach zero. If the cumulative curve shows no indi-
cation of bending, it implies that defect discovery is still very robust
and that many more still exist to be discovered in other spirals.

Graphical examples of the above metrics can be seen in Part 17, Prepare
for the Next Spiral.

Step 3: Publish Interim Report

See Appendix E25, Project Status Report, which can be used to report the
status of the testing project for all key process areas; Appendix E26, Test
Defect Details Report, which can be used to report the detailed defect sta-
tus of the testing project for all key process areas; and Appendix E28, Test
Execution Tracking Manager, which is an Excel spreadsheet that provides
a comprehensive and test cycle view of the number of test cases which
passed/failed, the number of defects discovered by application area, the
status of the defects, percentage completed, and the defect severities by
defect type. The template is located on the CD in the back of the book.

Task 1: Refine the Test Schedule

In a previous task, a test schedule was produced that includes the testing
steps (and perhaps tasks), target start dates and end dates, and responsi-
bilities. During the course of development, the testing schedule needs to
be continually monitored. The objective of the current task is to update the
test schedule to reflect the latest status. It is the responsibility of the test
manager to:

• Compare the actual progress to the planned progress.
• Evaluate the results to determine the testing status.
• Take appropriate action based on the evaluation.

If the testing progress is behind schedule, the test manager needs to
determine the factors causing the slip. A typical cause is an underestima-
tion of the test effort. Other factors could be that an inordinate number of
defects are being discovered, causing a lot of the testing effort to be
devoted to retesting old corrected defects. In either case, more testers may

TEAM LinG



185

Test Execution/Evaluation (Do/Check)

be needed and/or overtime may be required to compensate for the slip-
page.

Task 2: Identify Requirement Changes

In a previous task, the functional requirements were initially analyzed by
the testing function, which consisted of the hierarchical functional decom-
position, the functional window structure, the window standards, and the
minimum system requirements of the system.

Between spirals new requirements may be introduced into the develop-
ment process. They can consist of:

• New GUI interfaces or components
• New functions
• Modified functions
• Eliminated functions
• New system requirements, for example, hardware
• Additional system requirements
• Additional acceptance requirements

Each new requirement needs to be identified, recorded, analyzed, and
updated in the test plan, test design, and test scripts.

TEAM LinG



TEAM LinG



187

Part 17

Prepare for the Next 
Spiral (Act)

If you will recall, in the spiral development environment, software testing is
described as a continuous improvement process that must be integrated
into a rapid application development methodology. Deming’s continuous
improvement process using the PDCA model is applied to the software
testing process. We are now in the Act part of the spiral model (see Exhibit
17.1), which prepares for the next spiral.

Exhibit 17.2 outlines the steps and tasks associated with the Act part of
spiral testing. Each step and task is described along with valuable tips and
techniques.

Step 1: Refine the Tests

See Appendix F21, Impact Analysis Checklist, which can be used to help
analyze the impacts of changes to the system.

Task 1: Update the Function/GUI Tests

The objective of this task is to update the test design to reflect the new
functional requirements. The Test Change Function Test Matrix, which
cross-references the tests to the functions, needs to be updated. The new
functions are added in the vertical list and the respective test cases are
added to the horizontal list. The test case name is recorded on the matrix
along with the number. (See Appendix E5, Function/Test Matrix.)

Next, any new GUI/function test cases in the matrix need to be docu-
mented or scripted. The conceptual test cases are then transformed into
reusable test scripts with test data created. Also, any new GUI require-
ments are added to the GUI tests. (See Appendix E7, GUI-Based Functional
Test Matrix.)

Finally, the tests that can be automated with a testing tool need to be
updated. Automated tests provide three benefits: repeatability, leverage,
and increased functionality. Repeatability enables automated tests to be
executed more than once, consistently. Leverage comes from repeatability
from tests previously captured and tests that can be programmed with the

TEAM LinG



188

SOFTWARE TESTING METHODOLOGY

tool, which might not have been possible without automation. As applica-
tions evolve, more and more functionality is added. With automation, the
functional coverage is maintained with the test library.

Task 2: Update the System Fragment Tests

In a prior task, the system fragment tests were defined. System fragment
tests are sample subsets of full system tests that can be performed during
each spiral loop. The objective of doing a fragment test is to provide early
warning of pending problems that would arise in the full system test.

Candidate fragment system tests include function, performance, secu-
rity, usability, documentation, and procedure. Some of these fragment tests
should have formal tests performed during each spiral, whereas others

Exhibit 17.1. Spiral Testing and Continuous Improvement

Exhibit 17.2. Prepare for the Next Spiral (Steps/Tasks)

TEAM LinG



189

Prepare for the Next Spiral (Act)

should be part of the overall testing strategy. The objective of the present
task is to update the system fragment tests defined earlier based on new
requirements. New baseline measurements are defined.

Finally, the fragment system tests that can be automated with a testing
tool need to be updated.

Task 3: Update the Acceptance Tests

In a prior task, the initial list of acceptance tests was defined. Acceptance
testing is an optional user-run test that demonstrates the ability of the
application to meet the user’s requirements. The motivation for this test is
to demonstrate rather than be destructive, that is, to show that the system
works. If performed, acceptance tests typically are a subset of the system
tests. However, the users sometimes define “special tests,” such as inten-
sive stress or volume tests, to stretch the limits of the system even beyond
what was tested during the system test. The objective of the present task
is to update the acceptance tests defined earlier based on new require-
ments.

Finally, the acceptance tests that can be automated with a testing tool
need to be updated.

Step 2: Reassess the Team, Procedures, and Test Environment

Task 1: Evaluate the Test Team

Between each spiral, the performance of the test team needs to be evalu-
ated in terms of its quality and productivity. The test team leader directs
one or more testers to ensure that the right skill level is on the project. She
makes sure that the test cases are being executed according to the plan,
the defects are being reported and retested, and the test automation is suc-
cessful. The basis for allocating dedicated testing resources is the scope of
the functionality and the development timeframe. If the testing is not being
completed satisfactorily, the team leader needs to counsel one or more
team members and/or request additional testers. On the other hand, if the
test is coming to a conclusion, the testing manager needs to start thinking
about reassigning testers to other projects.

Task 2: Review the Test Control Procedures

In a prior task, the test control procedures were set up before the first spi-
ral. The objective of this task is to review those procedures and make
appropriate modifications. The predefined procedures include the following:

• Defect recording/tracking procedures
• Change request procedures
• Version control procedures
• Configuration build procedures

TEAM LinG



190

SOFTWARE TESTING METHODOLOGY

• Project issue resolution procedures
• Reporting procedures

The purpose of defect recording/tracking procedures is to record and
correct defects and record metric information about the application. As
the project progresses, these procedures may need tuning. Examples
include new status codes or new fields in the defect tracking form, an
expanded defect distribution list, and the addition of more verification
checks.

The purpose of change request procedures is to allow new change
requests to be communicated to the development and testing team. Exam-
ples include a new change control review board process, a new sponsor
who has ideas of how the change request process should be implemented,
a new change request database, and a new software configuration manage-
ment tool.

The purpose of version control procedures is to uniquely identify each
software component via a labeling scheme and allow for successive revi-
sions. Examples include a new software configuration management tool
with a new versioning scheme or new labeling standards.

The purpose of configuration build procedures is to provide an effective
means to assemble a software system from the software source compo-
nents into executable components. Examples include the addition of a new
4GL language, a new software configuration management tool, or a new
delta build approach.

The purpose of project issue resolution procedures is to record and pro-
cess testing issues that arise during the testing process. Examples include
a new project manager who requests a Lotus Notes approach, a newly
formed issue review committee, an updated issue priority categorization
scheme, and a new issue submission process.

The purpose of reporting procedures is to facilitate the communication
process and reporting. Examples include a new project manager who
requires weekly testing status reports, a new interim test report structure,
or an expanded reporting distribution.

Task 3: Update the Test Environment

In a prior task, the test environment was defined. A test environment pro-
vides a physical framework for testing necessary for the testing activity.
During the present task, the test environment needs are reviewed and
updated. (See Appendix F22, Environment Readiness Checklist, which can
be used to verify the readiness of the environment for testing before start-
ing test execution.)

TEAM LinG



191

Prepare for the Next Spiral (Act)

The main components of the test environment include the physical test
facility, technologies, and tools. The test facility component includes the
physical setup. The technologies component includes hardware platforms,
the physical network and all its components, operating system software,
and other software, such as utility software. The tools component includes
any specialized testing software, such as automated test tools, testing
libraries, and support software. Examples of changes to the test environ-
ment include:

• Expanded test lab
• New testing tools required
• Additional test hardware required
• Additional network facilities
• Additional test database space required
• New Lotus Notes log-ons
• Additional software to support testing

Step 3: Publish Interim Test Report

Task 1: Publish the Metric Graphics

Each spiral should produce an interim report to describe the status of the
testing. These tests are geared to the testing team, the test manager, and
the development manager, and will help them make adjustments for the
next spiral. The following minimal graphical reports are recommended
between each spiral test.

Test Case Execution Status. The objective of Exhibit 17.3 is to show the
status of testing and predict when the testing and development group will
be ready for production. Test cases run with errors have not yet been cor-
rected.

If there are a relatively large number of test cases that have not been run,
the testing group needs to increase its productivity and/or resources. If
there are a large number of test cases run with errors that have not been
corrected, the development team also needs to be more productive.

Defect Gap Analysis. The objective of Exhibit 17.4 is to show the gap
between the number of defects that has been uncovered compared to the
number that has been corrected. A large gap indicates that development
needs to increase effort and resources to correct defects more quickly.

Defect Severity Status. The objective of Exhibit 17.5 is to show the dis-
tribution of the three severity categories: critical, major, and minor. A large
percentage of defects in the critical category indicates that a problem with
the design or architecture of the application may exist.

TEAM LinG



192

SOFTWARE TESTING METHODOLOGY

Test Burnout Tracking. The objective of Exhibit 17.6 is to indicate the
rate of uncovering defects. The cumulative, for example, running total
number of defects and defects by time period help predict when fewer
defects are being discovered. This is indicated when the cumulative curve
“bends,” and the defects by time period approach zero.

Exhibit 17.3. Test Execution Status

Exhibit 17.4. Defect Gap Analysis

TEAM LinG



193

Prepare for the Next Spiral (Act)

Exhibit 17.5. Defect Severity Status

Exhibit 17.6. Test Burnout Tracking

TEAM LinG



TEAM LinG



195

Part 18

Conduct the 
System Test
System testing evaluates the functionality and performance of the whole
application and consists of a variety of tests including: performance,
usability, stress, documentation, security, volume, recovery, and so on.
Exhibit 18.1 describes how to extend fragment system testing. It includes
discussions of how to prepare for the system tests, design and script them,
execute them, and report anomalies discovered during the test.

Step 1: Complete System Test Plan

Task 1: Finalize the System Test Types

In a previous task, a set of system fragment tests was selected and exe-
cuted during each spiral. The purpose of the current task is to finalize the
system test types that will be performed during system testing.

If you will recall, systems testing consists of one or more tests that are
based on the original objectives of the system, which were defined during
the project interview. The purpose of this task is to select the system tests
to be performed, not to implement the tests. Our initial list consisted of the
following system test types:

• Performance
• Security
• Volume
• Stress
• Compatibility
• Conversion
• Usability
• Documentation
• Backup
• Recovery
• Installation

The sequence of system test type execution should also be defined in
this task. For example, related tests such as performance, stress, and vol-
ume, might be clustered together and performed early during system test-
ing. Security, backup, and recovery are also logical groupings, and so on.

TEAM LinG



196

SOFTWARE TESTING METHODOLOGY

Exhibit 18.1. Conduct System Test (Steps/Tasks)

TEAM LinG



197

Conduct the System Test

Finally, the system tests that can be automated with a testing tool need
to be finalized. Automated tests provide three benefits: repeatability, lever-
age, and increased functionality. Repeatability enables automated tests to
be executed more than once, consistently. Leverage comes from repeat-
ability from tests previously captured and tests that can be programmed
with the tool, which might not have been possible without automation. As
applications evolve, more and more functionality is added. With automa-
tion the functional coverage is maintained with the test library.

Task 2: Finalize System Test Schedule

In this task, the system test schedule should be finalized and includes the
testing steps (and perhaps tasks), target start and target end dates, and
responsibilities. It should also describe how it will be reviewed, tracked,
and approved. A sample system test schedule is shown in Exhibit 18.2.
(Also see the Gantt chart template, Gantt Spiral Testing Methodology tem-
plate.)

Task 3: Organize the System Test Team

With all testing types, the system test team needs to be organized. The sys-
tem test team is responsible for designing and executing the tests, evaluat-
ing the results, and reporting any defects to development, and using the
defect tracking system. When development corrects defects, the test team
retests the defects to ensure the correction.

The system test team is led by a test manager whose responsibilities
include:

• Organizing the test team
• Establishing the test environment
• Organizing the testing policies, procedures, and standards
• Assurance test readiness
• Working the test plan and controlling the project
• Tracking test costs
• Assuring test documentation is accurate and timely
• Managing the team members

Task 4: Establish the System Test Environment

During this task, the system test environment is also finalized. The purpose
of the test environment is to provide a physical framework for the testing
activity. The test environment needs are established and reviewed before
implementation.

The main components of the test environment include the physical
test facility, technologies, and tools. The test facility component includes
the physical setup. The technologies component includes the hardware

TEAM LinG



198

SOFTWARE TESTING METHODOLOGY

Exhibit 18.2. Final System Test Schedule 

Test Step Begin Date End Date Responsible

General Setup

Organize the system test team 12/1/04 12/7/04 Smith, Test Manager

Establish the system test 
environment

12/1/04 12/7/04 Smith, Test Manager

Establish the system test tools 12/1/04 12/10/04 Jones, Tester

Performance Testing

Design/script the tests 12/11/04 12/15/04 Jones, Tester

Test review 12/16/04 12/16/04 Smith, Test Manager

Execute the tests 12/17/04 12/22/04 Jones, Tester

Retest system defects 12/23/04 12/25/04 Jones, Tester

Stress Testing

Design/script the tests 12/26/04 12/30/04 Jones, Tester

Test review 12/31/04 12/31/04 Smith, Test Manager

Execute the tests 1/1/04 1/6/04 Jones, Tester

Retest system defects 1/7/04 1/9/04 Jones, Tester

Volume Testing

Design/script the tests 1/10/04 1/14/04 Jones, Tester

Test review 1/15/04 1/15/04 Smith, Test Manager

Execute the tests 1/16/04 1/21/04 Jones, Tester

Retest system defects 1/22/04 1/24/04 Jones, Tester

Security Testing

Design/script the tests 1/25/04 1/29/04 Jones, Tester

Test review 1/30/04 1/31/04 Smith, Test Manager

Execute the tests 2/1/04 2/6/04 Jones, Tester

Retest system defects 2/7/04 2/9/04 Jones, Tester

Backup Testing

Design/script the tests 2/10/04 2/14/04 Jones, Tester

Test review 2/15/04 2/15/04 Smith, Test Manager

Execute the tests 2/16/04 1/21/04 Jones, Tester

Retest system defects 2/22/04 2/24/04 Jones, Tester

Recovery Testing

Design/script the tests 2/25/04 2/29/04 Jones, Tester

TEAM LinG



199

Conduct the System Test

platforms, physical network and all its components, operating system soft-
ware, and other software. The tools component includes any specialized
testing software, such as automated test tools, testing libraries, and sup-
port software.

Test review 2/30/04 2/31/04 Smith, Test Manager

Execute the tests 3/1/04 3/6/04 Jones, Tester

Retest system defects 3/7/04 3/9/04 Jones, Tester

Compatibility Testing

Design/script the tests 3/10/04 3/14/04 Jones, Tester

Test review 3/15/04 3/15/04 Smith, Test Manager

Execute the tests 3/16/04 3/21/04 Jones, Tester

Retest system defects 3/22/04 3/24/04 Jones, Tester

Conversion Testing

Design/script the tests 4/10/04 4/14/04 Jones, Tester

Test review 4/15/04 4/15/04 Smith, Test Manager

Execute the tests 4/16/04 4/21/04 Jones, Tester

Retest system defects 4/22/04 4/24/04 Jones, Tester

Usability Testing

Design/script the tests 5/10/04 5/14/04 Jones, Tester

Test review 5/15/04 5/15/04 Smith, Test Manager

Execute the tests 5/16/04 5/21/04 Jones, Tester

Retest system defects 5/22/04 5/24/04 Jones, Tester

Documentation Testing

Design/script the tests 6/10/04 6/14/04 Jones, Tester

Test review 6/15/04 6/15/04 Smith, Test Manager

Execute the tests 6/16/04 6/21/04 Jones, Tester

Retest system defects 6/22/04 6/24/04 Jones, Tester

Installation Testing

Design/script the tests 7/10/04 7/14/04 Jones, Tester

Test review 7/15/04 7/15/04 Smith, Test Manager

Execute the tests 7/16/04 7/21/04 Jones, Tester

Retest system defects 7/22/04 7/24/04 Jones, Tester

Exhibit 18.2. Final System Test Schedule (Continued)

Test Step Begin Date End Date Responsible

TEAM LinG



200

SOFTWARE TESTING METHODOLOGY

The testing facility and workplace need to be established. These may
range from an individual workplace configuration to a formal testing lab. In
any event, it is important that the testers be together and near the devel-
opment team. This facilitates communication and the sense of a common
goal. The system testing tools need to be installed.

The hardware and software technologies need to be set up. This
includes the installation of test hardware and software and coordination
with vendors, users, and information technology personnel. It may be nec-
essary to test the hardware and coordinate with hardware vendors. Com-
munication networks need to be installed and tested.

Task 5: Install the System Test Tools

During this task, the system test tools are installed and verified for readi-
ness. A trial run of tool test cases and scripts should be performed to verify
that the test tools are ready for the actual acceptance test. Some other tool
readiness considerations include:

• Test team tool training
• Tool compatibility with operating environment
• Ample disk space for the tools
• Maximizing the tool potentials
• Vendor tool help hotline
• Test procedures modified to accommodate tools
• Installing the latest tool changes
• Assuring the vendor contractual provisions

Step 2: Complete System Test Cases

During this step, the system test cases are designed and scripted. The con-
ceptual system test cases are transformed into reusable test scripts with
test data created.

To aid in developing the script test cases, the GUI-based Function Test
Matrix template in the Appendix can be used to document system-level
test cases with the “function” heading replaced with the system test name.

Task 1: Design/Script the Performance Tests

The objective of performance testing is to measure the system against
predefined objectives. The required performance levels are compared
against the actual performance levels and discrepancies are documented.

Performance testing is a combination of black-box and white-box test-
ing. From a black-box point of view, the performance analyst does not have
to know the internal workings of the system. Real workloads or bench-
marks are used to compare one system version with another for perfor-
mance improvements or degradation. From a white-box point of view, the

TEAM LinG



201

Conduct the System Test

performance analyst needs to know the internal workings of the system
and define specific system resources to investigate, such as instructions,
modules, and tasks.

Some of the performance information of interest includes the following:

• CPU utilization
• IO utilization
• Number of IOs per instruction
• Channel utilization
• Main storage memory utilization
• Secondary storage memory utilization
• Percentage of execution time per module
• Percentage of time a module is waiting for IO completion
• Percentage of time module spent in main storage
• Instruction trace paths over time
• Number of times control is passed from one module to another
• Number of waits encountered for each group of instructions
• Number of pages-in and pages-out for each group of instructions
• System response time, for example, last key until first key time
• System throughput, that is, number of transactions per time unit
• Unit performance timings for all major functions

Baseline performance measurements should first be performed on all
major functions in a noncontention mode, for example, unit measurements
of functions when a single task is in operation. This can be easily done with
a simple stopwatch, as was done earlier for each spiral. The next set of
measurements should be made in a system contended mode in which mul-
tiple tasks are operating and queueing results for demands on common
resources such as CPU, memory, storage, channel, network, and so on.
Contended system execution time and resource utilization performance
measurements are performed by monitoring the system to identify poten-
tial areas of inefficiency.

There are two approaches to gathering system execution time and
resource utilization. With the first approach, samples are taken while the
system is executing in its typical environment with the use of external
probes, performance monitors, or a stopwatch. With the other approach,
probes are inserted into the system code, for example, calls to a perfor-
mance monitor program that gathers the performance information. The
following is a discussion of each approach, followed by a discussion of test
drivers, which are support techniques used to generate data for the perfor-
mance study.

Monitoring Approach. This approach involves monitoring a system by
determining its status at periodic time intervals and is controlled by an
elapsed time facility in the testing tool or operating system. Samples taken

TEAM LinG



202

SOFTWARE TESTING METHODOLOGY

during each time interval indicate the status of the performance criteria
during the interval. The smaller the time interval, the more precise the
sampling accuracy is.

Statistics gathered by the monitoring are collected and summarized in
performance.

Probe Approach. This approach involves inserting probes or program
instructions into the system programs at various locations. To determine,
for example, the CPU time necessary to execute a sequence of statements,
a problem execution results in a call to the data collection routine that
records the CPU clock at that instant. A second probe execution results in
a second call to the data collection routine. Subtracting the first CPU time
from the second yields the net CPU time used. Reports can be produced
showing execution time breakdowns by statement, module, and statement
type.

The value of these approaches is their use as performance requirements
validation tools. However, formally defined performance requirements
must be stated and the system should be designed so that the performance
requirements can be traced to specific system modules.

Test Drivers. In many cases test drivers and test harnesses are required
to make system performance measurements. A test driver provides the
facilities needed to execute a system, for example, inputs. The input data
files for the system are loaded with data values representing the test situa-
tion to yield recorded data to evaluate against the expected results. Data
are generated in an external form and presented to the system.

Performance test cases need to be defined, using one or more of the test
templates located in the appendices, and test scripts need to be built.
Before any performance test is conducted, however, the performance ana-
lyst must make sure that the target system is relatively bug-free. Other-
wise, a lot of time will be spent documenting and fixing defects rather than
analyzing the performance.

The following are the five recommended steps for any performance
study:

1. Document the performance objectives; for example, exactly what
the measurable performance criteria are must be verified.

2. Define the test driver or source of inputs to drive the system.
3. Define the performance methods or tools that will be used.
4. Define how the performance study will be conducted; for example,

what is the baseline, what are the variations, how can it be verified
as repeatable, and how does one know when the study is complete?

5. Define the reporting process, for example, techniques and tools.

TEAM LinG



203

Conduct the System Test

Task 2: Design/Script the Security Tests

The objective of security testing is to evaluate the presence and appropri-
ate functioning of the security of the application to ensure the integrity and
confidentiality of the data. Security tests should be designed to demon-
strate how resources are protected.

A Security Design Strategy. A security strategy for designing security
test cases is to focus on the following four security components: the
assets, threats, exposures, and controls. In this manner, matrices and
checklists will suggest ideas for security test cases.

Assets are the tangible and intangible resources of an entity. The evalu-
ation approach is to list what should be protected. It is also useful to exam-
ine the attributes of assets, such as amount, value, use, and characteris-
tics. Two useful analysis techniques are asset value and exploitation
analysis. Asset value analysis determines how the value differs among
users and potential attackers. Asset exploitation analysis examines differ-
ent ways to use an asset for illicit gain.

Threats are events with the potential to cause loss or harm. The evalua-
tion approach is to list the sources of potential threats. It is important to
distinguish among accidental, intentional, and natural threats, and threat
frequencies.

Exposures are forms of possible loss or harm. The evaluation approach
is to list what might happen to assets if a threat is realized. Exposures
include disclosure violations, erroneous decision, and fraud. Exposure
analysis focuses on identifying areas in which exposure is the greatest.

Security functions or controls are measures that protect against loss or
harm. The evaluation approach is to list the security functions and tasks
and focus on controls embodied in specific system functions or proce-
dures. Security functions assess the protection against human errors and
casual attempts to misuse the system. Some functional security questions
include the following:

• Do the control features work properly?
• Are invalid and improbable parameters detected and properly han-

dled?
• Are invalid or out-of-sequence commands detected and properly

handled?
• Are errors and file accesses properly recorded?
• Do procedures for changing security tables work?
• Is it possible to log in without a password?
• Are valid passwords accepted and invalid passwords rejected?
• Does the system respond properly to multiple invalid passwords?

TEAM LinG



204

SOFTWARE TESTING METHODOLOGY

• Does the system-initialed authentication function properly?
• Are there security features for remote accessing?

It is important to assess the performance of the security mechanisms as
well as the functions themselves. Some questions and issues concerning
security performance include the following:

• Availability — What portion of time is the application or control
available to perform critical security functions? Security controls
usually require higher availability than other portions of the system.

• Survivability — How well does the system withstand major failures
or natural disasters? This includes the support of emergency oper-
ations during failure, backup operations afterward, and recovery
actions to return to regular operation.

• Accuracy — How accurate is the security control? Accuracy encom-
passes the number, frequency, and significance of errors.

• Response time — Are response times acceptable? Slow response
times can tempt users to bypass security controls. Response time
can also be critical for control management, as the dynamic modi-
fication of security tables.

• Throughput — Does the security control support required use capac-
ities? Capacity includes the peak and average loading of users and
service requests.

A useful performance test is stress testing, which involves large num-
bers of users and requests to attain operational stress conditions. Stress
testing is used to attempt to exhaust limits for such resources as buffers,
queues, tables, and ports. This form of testing is useful in evaluating pro-
tection against service denial threats.

Task 3: Design/Script the Volume Tests

The objective of volume testing is to subject the system to heavy volumes
of data to find out if it can handle the volume of data. This test is often con-
fused with stress testing. Stress testing subjects the system to heavy loads
or stresses in terms of rates, such as throughputs over a short time period.
Volume testing is data oriented, and its purpose is to show that the system
can handle the volume of data specified in its objectives.

Some examples of volume testing are:

• Relative data comparison is made when processing date-sensitive
transactions.

• A compiler is fed an extremely large source program to compile.
• A linkage editor is fed a program containing thousands of modules.
• An electronic-circuit simulator is given a circuit containing thou-

sands of components.
• An operation system’s job queue is filled to maximum capacity.

TEAM LinG



205

Conduct the System Test

• Enough data is created to cause a system to span file.
• A test-formatting system is fed a massive document format.
• The Internet is flooded with huge e-mail messages and files.

Task 4: Design/Script the Stress Tests

The objective of stress testing is to investigate the behavior of the system
under conditions that overload its resources. Of particular interest is the
impact that this has on the system processing time. Stress testing is bound-
ary testing. For example, test with the maximum number of terminals
active and then add more terminals than specified in the requirements
under different limit combinations. Some of the resources that stress test-
ing subjects to heavy loads include:

• Buffers
• Controllers
• Display terminals
• Interrupt handlers
• Memory
• Networks
• Printers
• Spoolers
• Storage devices
• Transaction queues
• Transaction schedulers
• User of the system

Stress testing studies the system’s response to peak bursts of activity in
short periods of time and attempts to find defects in a system. It is often
confused with volume testing, in which the system’s capability of handling
large amounts of data is the objective.

Stress testing should be performed early in development because it
often uncovers major design flaws that can have an impact on many areas.
If stress testing is not performed early, subtle defects, which might have
been more apparent earlier in development, may be difficult to uncover.

The following are the suggested steps for stress testing:

1. Perform simple multitask tests.
2. After the simple stress defects are corrected, stress the system to

the breaking point.
3. Perform the stress tests repeatedly for every spiral.

Some stress testing examples include the following:

• Word-processing response time for a fixed entry rate, such as 120
words per minute

• Introduce a heavy volume of data in a very short period of time

TEAM LinG



206

SOFTWARE TESTING METHODOLOGY

• Varying loads for interactive, real-time, and process control
• Simultaneous introduction of a large number of transactions
• Thousands of users signing on to the Internet within the same minute

Task 5: Design/Script the Compatibility Tests

The objective of compatibility testing (sometimes called cohabitation test-
ing) is to test the compatibility of the application with other applications
or systems. This is a test that is often overlooked until the system is put
into production and in which defects are often subtle and difficult to
uncover. An example is when the system works perfectly in the testing lab
in a controlled environment but does not work when it coexists with other
applications. An example of compatibility is when two systems share the
same data or data files or reside in the same memory at the same time. The
system may satisfy the system requirements but not work in a shared envi-
ronment and may interfere with other systems.

The following is a compatibility (cohabitation) testing strategy:

1. Update the compatibility objectives to note how the application has
actually been developed and the actual environments in which it is
to perform. Modify the objectives for any changes in the cohabiting
systems or the configuration resources.

2. Update the compatibility test cases to make sure they are compre-
hensive. Make sure that the test cases in the other systems that can
affect the target system are comprehensive. And ensure maximum
coverage of instances in which one system could affect another.

3. Perform the compatibility tests and carefully monitor the results to
ensure the expected results. Use a baseline approach, which is the
system’s operating characteristics before the addition of the target
system into the shared environment. The baseline needs to be accu-
rate and incorporate not only the functioning but the operational
performance to ensure that it is not degraded in a cohabitation
setting.

4. Document the results of the compatibility tests and note any devi-
ations in the target system or the other cohabitation systems.

5. Regression test the compatibility tests after the defects have been
resolved and record the tests in the retest matrix.

Task 6: Design/Script the Conversion Tests

The objective of conversion testing is to verify the conversion of existing
data and load a new database. The most common conversion problem is
between two versions of the same system. A new version may have a dif-
ferent data format but must include the data from the old system. Ample
time needs to be set aside to carefully think of all the conversion issues
that may arise.

TEAM LinG



207

Conduct the System Test

Some key factors that need to be considered when designing conversion
tests include the following:

• Auditability — There needs to be a plan to perform before-and-after
comparisons and analysis of the converted data to ensure it was
converted successfully. Techniques to ensure auditability include
file reports, comparison programs, and regression testing. Regres-
sion testing checks to verify that the converted data does not change
the business requirements or cause the system to behave differently.

• Database Verification — Prior to conversion, the new database needs
to be reviewed to verify that it is designed properly, satisfies the
business needs, and that the support center and database adminis-
trators are trained to support it.

• Data Cleanup — Before the data is converted to the new system, the
old data needs to be examined to verify that inaccuracies or dis-
crepancies in the data are removed.

• Recovery Plan — Roll-back procedures need to be in place before
any conversion is attempted to restore the system to its previous
state and undo the conversions.

• Synchronization — It must be verified that the conversion process
does not interfere with normal operations. Sensitive data, such as
customer data, may be changing dynamically during conversions.
One way to achieve this is to perform conversions during nonoper-
ational hours.

Task 7: Design/Script the Usability Tests

The objective of usability testing is to determine how well the user will be
able to use and understand the application. This includes the system func-
tions, publications, help text, and procedures to ensure that the user com-
fortably interacts with the system. Usability testing should be performed
as early as possible during development and should be designed into the
system. Late usability testing might be impossible, because it is locked in
and often requires a major redesign of the system to correct serious usabil-
ity problems. This may make it economically infeasible.

Some of the usability problems the tester should look for include:

• Overly complex functions or instructions
• Difficult installation procedures
• Poor error messages, for example, “syntax error”
• Syntax difficult to understand and use
• Nonstandardized GUI interfaces
• User forced to remember too much information
• Difficult log-in procedures
• Help text not context sensitive or not detailed enough
• Poor linkage to other systems

TEAM LinG



208

SOFTWARE TESTING METHODOLOGY

• Unclear defaults
• Interface too simple or too complex
• Inconsistency of syntax, format, and definitions
• User not provided with clear acknowledgment of all inputs

Task 8: Design/Script the Documentation Tests

The objective of documentation testing is to verify that the user documen-
tation is accurate and ensure that the manual procedures work correctly.
Documentation testing has several advantages, including improving the
usability of the system, reliability, maintainability, and installability. In
these cases, testing the document will help uncover deficiencies in the sys-
tem and/or make the system more usable.

Documentation testing also reduces customer support costs, such as
when customers can figure out their questions with the documentation,
they are not forced to call the support desk.

The tester verifies the technical accuracy of the documentation to
assure that it agrees with and describes the system accurately. She needs
to assume the user’s point of view and act out the actual behavior as
described in the documentation.

Some tips and suggestions for the documentation tester include:

• Use documentation as a source of many test cases.
• Use the system exactly as the documentation describes it.
• Test every hint or suggestion.
• Incorporate defects into the defect tracking database.
• Test every online help hypertext link.
• Test every statement of fact and don’t take anything for granted.
• Act like a technical editor rather than a passive reviewer.
• Perform a general review of the whole document first and then a

detailed review.
• Check all the error messages.
• Test every example provided in the document.
• Make sure all index entries have documentation text.
• Make sure documentation covers all key user functions.
• Make sure the reading style is not too technical.
• Look for areas that are weaker than others and need more explana-

tion.

Task 9: Design/Script the Backup Tests

The objective of backup testing is to verify the ability of the system to back
up its data in the event of a software or hardware failure. This test is com-
plementary to recovery testing and should be part of recovery test plan-
ning.

TEAM LinG



209

Conduct the System Test

Some backup testing considerations include the following:

• Backing up files and comparing the backup with the original
• Archiving files and data
• Complete system backup procedures
• Checkpoint backups
• Backup performance system degradation
• Effect of backup on manual processes
• Detection of “triggers” to backup system
• Security procedures during backup
• Maintaining transaction logs during backup procedures

Task 10: Design/Script the Recovery Tests

The objective of recovery testing is to verify the system’s ability to recover
from a software or hardware failure. This test verifies the contingency fea-
tures of the system for handling interruptions and returning to specific
points in the application’s processing cycle. The key questions for design-
ing recovery tests are as follows:

• Have the potentials for disasters and system failures, and their
respective damages, been identified? Fire-drill brainstorming ses-
sions can be an effective method of defining disaster scenarios.

• Do the prevention and recovery procedures provide for adequate
responses to failures? The plan procedures should be tested with
technical reviews by subject matter experts and the system users.

• Will the recovery procedures work properly when really needed?
Simulated disasters need to be created with the actual system ver-
ifying the recovery procedures. This should involve the system
users, the support organization, vendors, and so on.

Some recovery testing examples include the following:

• Complete restoration of files that were backed up either during
routine maintenance or error recovery

• Partial restoration of file backup to the last checkpoint
• Execution of recovery programs
• Archive retrieval of selected files and data
• Restoration when power supply is the problem
• Verification of manual recovery procedures
• Recovery by switching to parallel systems
• Restoration performance system degradation
• Security procedures during recovery
• Ability to recover transaction logs

Task 11: Design/Script the Installation Tests

The objective of installation testing is to verify the ability to install the sys-
tem successfully. Customers have to install the product on their systems.

TEAM LinG



210

SOFTWARE TESTING METHODOLOGY

Installation is often the developers’ last activity and often receives the
least amount of focus during development. Yet, it is the first activity that
the customer performs when using the new system. Therefore, clear and
concise installation procedures are among the most important parts of the
system documentation.

Reinstallation procedures need to be included to be able to reverse the
installation process and validate the previous environmental condition.
Also, the installation procedures need to document how the user can tune
the system options and upgrade from a previous version.

Some key installation questions the tester needs to consider include the
following:

• Who is the user installer; for example, what technical capabilities
are assumed?

• Is the installation process documented thoroughly with specific and
concise installation steps?

• For which environments are the installation procedures supposed
to work, for example, platforms, software, hardware, networks, or
versions?

• Will the installation change the user’s current environmental setup,
for example, config.sys and so on?

• How does the installer know the system has been installed correctly;
for example, is there an installation test procedure in place?

Task 12: Design/Script Other System Test Types

In addition to the above system tests, the following system tests may also
be required.

• API Testing — Verify the system uses APIs correctly, for example,
operating system calls.

• Communication Testing — Verify the system’s communications and
networks.

• Configuration Testing — Verify the system works correctly in different
system configurations, for example, software, hardware, and net-
works.

• Database Testing — Verify the database integrity, business rules,
access, and refresh capabilities.

• Degraded System Testing — Verify the system performs properly with
less than full capabilities, for example, line connections down, and
the like.

• Disaster Recovery Testing — Verify the system recovery processes
work correctly.

• Embedded System Test — Verify systems that operate on low-level
devices, such as video chips.

TEAM LinG



211

Conduct the System Test

• Facility Testing — Verify that each stated requirement facility is met.
• Field Testing — Verify the system works correctly in the real envi-

ronment.
• Middleware Testing — Verify the middleware software works cor-

rectly, for example, the common interfaces and accessibility among
clients and servers.

• Multimedia Testing — Verify the multimedia system features, which
use video, graphics, and sound.

• Online Help Testing — Verify the system’s online help features work
properly.

• Operability Testing — Verify system will work correctly in the actual
business environment.

• Package Testing — Verify installed software package works correctly.
• Parallel Testing — Verify system behaves the same in the old and

new version.
• Port Testing — Verify system works correctly on different operating

systems and computers.
• Procedure Testing — Verify nonautomated procedures work properly,

for example, operation, DBA, and the like.
• Production Testing — Verify the system will work correctly during

actual ongoing production and not just in the test lab environment.
• Real-Time Testing — Verify systems in which time issues are critical

and there are response time requirements.
• Reliability Testing — Verify the system works correctly within pre-

defined expected failure duration, for example, mean time to failure
(MTF).

• Serviceability Testing — Verify service facilities of the system work
properly, for example, mean time to debug a defect and maintenance
procedures.

• SQL Testing — Verify the queries, data retrievals, and updates.
• Storage Testing — Verify that the system storage requirements are

met, for example, sizes of spill files and amount of main or secondary
storage used.

Step 3: Review/Approve System Tests

Task 1: Schedule/Conduct the Review

The system test plan review should be scheduled well in advance of the
actual review, and the participants should have the latest copy of the test
plan.

As with any interview or review, it should contain certain elements. The
first is defining what will be discussed; the second is discussing the details;
and the third is summarization. The final element is timeliness. The
reviewer should state up front the estimated duration of the review and set

TEAM LinG



212

SOFTWARE TESTING METHODOLOGY

the ground rule that if time expires before completing all items on the
agenda, a follow-on review will be scheduled.

The purpose of this task is for development and the project sponsor to
agree and accept the system test plan. If there are any suggested changes
to the test plan during the review, they should be incorporated into the test
plan.

Task 2: Obtain Approvals

Approval is critical in a testing effort because it helps provide the neces-
sary agreement among testing, development, and the sponsor. The best
approach is with a formal sign-off procedure of a system test plan. If this is
the case, use the management approval sign-off forms. However, if a formal
agreement procedure is not in place, send a memo to each key participant
including at least the project manager, development manager, and sponsor.
In the document attach the latest test plan and point out that all their feed-
back comments have been incorporated and that if you do not hear from
them, it is assumed that they agree with the plan. Finally, indicate that in a
spiral development environment, the system test plan will evolve with
each iteration but that you will include them in any modification.

Step 4: Execute the System Tests

Task 1: Regression Test the System Fixes

The purpose of this task is to retest the system tests that discovered
defects in the previous system test cycle for this build. The technique used
is regression testing. Regression testing is a technique that detects spuri-
ous errors caused by software modifications or corrections.

A set of test cases must be maintained and available throughout the
entire life of the software. The test cases should be complete enough so
that all the software’s functional capabilities are thoroughly tested. The
question arises as to how to locate those test cases to test defects discov-
ered during the previous test spiral. An excellent mechanism is the retest
matrix.

As described earlier, a retest matrix relates test cases to functions (or
program units). A check entry in the matrix indicates that the test case is
to be retested when the function (or program unit) has been modified due
to an enhancement(s) or correction(s). No entry means that the test does
not need to be retested. The retest matrix can be built before the first test-
ing spiral but needs to be maintained during subsequent spirals. As func-
tions (or program units) are modified during a development spiral, existing
or new test cases need to be created and checked in the retest matrix in
preparation for the next test spiral. Over time with subsequent spirals, some
functions (or program units) may be stable with no recent modifications.

TEAM LinG



213

Conduct the System Test

Consideration to selectively remove their check entries should be under-
taken between testing spirals.

Task 2: Execute the New System Tests

The purpose of this task is to execute new system tests that were created
at the end of the previous system test cycle. In the previous spiral, the test-
ing team updated the function/GUI, system fragment, and acceptance tests
in preparation for the current testing spiral. During this task, those tests
are executed.

Task 3: Document the System Defects

During system test execution, the results of the testing must be reported in
the defect tracking database. These defects are typically related to individ-
ual tests that have been conducted. However, variations to the formal test
cases often uncover other defects. The objective of this task is to produce
a complete record of the defects. If the execution step has been recorded
properly, the defects have already been recorded on the defect tracking
database. If the defects are already recorded, the objective of this step
becomes to collect and consolidate the defect information.

Tools can be used to consolidate and record defects depending on the
test execution methods. If the defects are recorded on paper, the consoli-
dation involves collecting and organizing the papers. If the defects are
recorded electronically, search features can easily locate duplicate defects.

TEAM LinG



TEAM LinG



215

Part 19

Conduct Acceptance 
Testing

Acceptance testing is a user-run test that demonstrates the application’s
ability to meet the original business objectives and system requirements
and usually consists of a subset of system tests (see Exhibit 19.1). It
includes discussions on how to prepare for the acceptance test, design and
script the acceptance tests, execute the acceptance tests, and report
anomalies discovered during the test.

Step 1: Complete Acceptance Test Planning

Task 1: Finalize the Acceptance Test Types

In this task the initial acceptance testing type list is refined and the actual
tests to be performed are selected.

Acceptance testing is an optional user-run test that demonstrates the
ability of the application to meet the user’s requirements. The motivation
for this test is to demonstrate rather than be destructive, that is, to show
that the system works. Less emphasis is placed on the technical issues and
more on the question of whether the system is a good business fit for the
end user. Users usually perform the test. However, the users sometimes
define “special tests,” such as intensive stress or volume tests, to stretch the
limits of the system even beyond what was tested during the system test.

Task 2: Finalize the Acceptance Test Schedule

In this task, the acceptance test schedule should be finalized and includes
the testing steps (and perhaps tasks), target begin dates and target end
dates, and responsibilities. It should also describe how it will be reviewed,
tracked, and approved. For acceptance testing, the test team usually con-
sists of user representatives. However, the team test environment and test
tool are probably the same as used during system testing. A sample accep-
tance test schedule is shown in Exhibit 19.2.

Task 3: Organize the Acceptance Test Team

The acceptance test team is responsible for designing and executing the
tests, evaluating the test results, and reporting any defects to development,

TEAM LinG



216

SOFTWARE TESTING METHODOLOGY

using the defect tracking system. When development corrects defects, the
test team retests the defects to ensure the correction. The acceptance test
team typically has representation from the user community, because this is
their final opportunity to accept the system.

Exhibit 19.1. Conduct Acceptance Testing (Steps/Tasks)

TEAM LinG



217

Conduct Acceptance Testing

The acceptance test team is led by a test manager whose responsibili-
ties include:

• Organizing the test team
• Establishing the test environment
• Organizing the testing policies, procedures, and standards
• Ensuring test readiness
• Working the test plan and controlling the project
• Tracking test costs
• Ensuring test documentation is accurate and timely
• Managing the team members

Task 4: Establish the Acceptance Test Environment

During this task, the acceptance test environment is finalized. Typically,
the test environment for acceptance testing is the same as that for system
testing. The purpose of the test environment is to provide the physical
framework necessary for the testing activity. For this task, the test environ-
ment needs are established and reviewed before implementation.

The main components of the test environment include the physical test
facility, technologies, and tools. The test facility component includes the
physical setup. The technologies component includes the hardware plat-
forms, physical network and all its components, operating system soft-
ware, and other software, such as utility software. The tools component
includes any specialized testing software: automated test tools, testing
libraries, and support software.

Exhibit 19.2. Acceptance Test Schedule

Test Step
Begin 
Date

End 
Date Responsible

General Setup

Organize the acceptance 
test team

8/1/04 8/7/04 Smith, Test Manager

Establish the acceptance 
test environment

8/8/04 8/9/04 Smith, Test Manager

Establish the acceptance 
test tools

8/10/04 8/10/04 Jones, Tester

Acceptance testing

Design/script the tests 12/11/04 12/15/04 Jones, Baker (user), Testers

Test review 12/16/04 12/16/04 Smith, Test Manager

Execute the tests 12/17/04 12/22/04 Jones, Baker (user), Tester

Retest acceptance defects 12/23/04 12/25/04 Jones, Baker (user), Tester

TEAM LinG



218

SOFTWARE TESTING METHODOLOGY

The testing facility and workplace needs to be established. It may range
from an individual workplace configuration to a formal testing lab. In any
event, it is important that the testers be together and in close proximity to
the development team. This facilitates communication and the sense of a
common goal. The testing tools that were acquired need to be installed.

The hardware and software technologies need to be set up. This
includes the installation of test hardware and software, and coordination
with vendors, users, and information technology personnel. It may be nec-
essary to test the hardware and coordinate with hardware vendors. Com-
munication networks need to be installed and tested.

Task 5: Install Acceptance Test Tools

During this task, the acceptance test tools are installed and verified for
readiness. A trial run of sample tool test cases and scripts should be per-
formed to verify that the test tools are ready for the actual acceptance test.
Some other tool readiness considerations include:

• Test team tool training
• Tool compatibility with operating environment
• Ample disk space for the tools
• Maximizing the tool potentials
• Vendor tool help hotline
• Test procedures modified to accommodate tools
• Installing the latest tool changes
• Assuring the vendor contractual provisions

Step 2: Complete Acceptance Test Cases

During this step, the acceptance test cases are designed and scripted. The
conceptual acceptance test cases are transformed into reusable test
scripts with test data created. To aid in the development of scripting the
test cases, the GUI-based Function Test Matrix template in Appendix E7 can
be used to document acceptance-level test cases, with the “function” head-
ing replaced with the acceptance test name.

Task 1: Subset the System-Level Test Cases

Acceptance test cases are typically (but not always) developed by the end
user and are not normally considered the responsibility of the develop-
ment organization, because acceptance testing compares the system to its
original requirements and the needs of the users. It is the final test for the
end users to accept or reject the system. The end users supply the test
resources and perform their own tests. They may or may not use the same
test environment that was used during system testing. This depends on
whether the test will be performed in the end user’s environment. The lat-
ter is the recommended approach.

TEAM LinG



219

Conduct Acceptance Testing

Typically, the acceptance test consists of a subset of system tests that
have already been designed during system testing. Therefore, the current
task consists of identifying those system-level tests that will be used during
acceptance testing.

Task 2: Design/Script Additional Acceptance Tests

In addition to the system-level tests to be rerun during acceptance testing,
they may be “tweaked” with special conditions to maximize the acceptabil-
ity of the system. For example, the acceptance test might require that a cer-
tain throughput be sustained for a period of time with acceptable response
time tolerance limits; for example, 10,000 transactions per hour are pro-
cessed with a mean response time of three seconds, with 90 percent less
than or equal to two seconds. Another example might be that an indepen-
dent user “off the street” sits down with the system and the document to
verify that he can use the system effectively.

The user might also envision other tests not designed during system
testing. These may become more apparent to the user than they would
have been to the developer because the user knows the business require-
ments and is intimately familiar with the business operations. She might
uncover defects that only a user would see. This also helps the user to get
ready for the real installation and production.

The acceptance test design might even include the use of live data,
because the acceptance of test results will probably occur more readily if
it looks real to the user. There are also unusual conditions that might not
be detected unless live data is used.

Step 3: Review/Approve Acceptance Test Plan

Task 1: Schedule/Conduct the Review

The acceptance test plan review should be scheduled well in advance of
the actual review and the participants should have the latest copy of the
test plan.

As with any interview or review, it should contain certain elements. The
first defines what will be discussed; the second discusses the details; the
third summarizes; and the final element is timeliness. The reviewer should
state up front the estimated duration of the review and set the ground rule
that if time expires before completing all items on the agenda, a follow-on
review will be scheduled.

The purpose of this task is for development and the project sponsor to
agree and accept the system test plan. If there are any suggested changes
to the test plan during the review, they should be incorporated into the test
plan.

TEAM LinG



220

SOFTWARE TESTING METHODOLOGY

Task 2: Obtain Approvals

Approval is critical in a testing effort because it helps provide the neces-
sary agreements among testing, development, and the sponsor. The best
approach is with a formal sign-off procedure of an acceptance test plan. If
this is the case, use the management approval sign-off forms. However, if a
formal agreement procedure is not in place, send a memo to each key par-
ticipant, including at least the project manager, development manager, and
sponsor. Attach to the document the latest test plan and point out that all
feedback comments have been incorporated and that if you do not hear
from them, it is assumed they agree with the plan. Finally, indicate that in
a spiral development environment, the system test plan will evolve with
each iteration but that you will include them in any modification.

Step 4: Execute the Acceptance Tests

Task 1: Regression Test the Acceptance Fixes

The purpose of this task is to retest the tests that discovered defects in the
previous acceptance test cycle for this build. The technique used is regres-
sion testing. Regression testing detects spurious errors caused by software
modifications or corrections.

A set of test cases must be maintained and available throughout the
entire life of the software. The test cases should be complete enough so
that all the software’s functional capabilities are thoroughly tested. The
question arises as to how to locate those test cases to test defects discov-
ered during the previous test spiral. An excellent mechanism is the retest
matrix.

As described earlier, a retest matrix relates test cases to functions (or
program units). A check entry in the matrix indicates that the test case is
to be retested when the function (or program unit) has been modified due
to an enhancement(s) or correction(s). No entry means that the test does
not need to be retested. The retest matrix can be built before the first test-
ing spiral but needs to be maintained during subsequent spirals. As func-
tions (or program units) are modified during a development spiral, existing
or new test cases need to be created and checked in the retest matrix in
preparation for the next test spiral. Over time with subsequent spirals,
some functions (or program units) may be stable with no recent modifica-
tions. Consideration to selectively remove their check entries should be
undertaken between testing spirals.

Task 2: Execute the New Acceptance Tests

The purpose of this task is to execute new tests that were created at the
end of the previous acceptance test cycle. In the previous spiral, the test-
ing team updated the function/GUI, system fragment, and acceptance tests

TEAM LinG



221

Conduct Acceptance Testing

in preparation for the current testing spiral. During this task, those tests
are executed.

Task 3: Document the Acceptance Defects

During acceptance test execution, the results of the testing must be
reported in the defect-tracking database. These defects are typically
related to individual tests that have been conducted. However, variations
to the formal test cases often uncover other defects. The objective of this
task is to produce a complete record of the defects. If the execution step
has been recorded properly, the defects have already been recorded on the
defect-tracking database. If the defects are already recorded, the objective
of this step becomes to collect and consolidate the defect information.

Tools can be used to consolidate and record defects depending on the
test execution methods. If the defects are recorded on paper, the consoli-
dation involves collecting and organizing the papers. If the defects are
recorded electronically, search features can easily locate duplicate defects.

TEAM LinG



TEAM LinG



223

Part 20

Summarize/Report 
Spiral Test Results

See Appendix F23, Project Completion Checklist, which can be used to con-
firm that all the key activities have been completed for the project.

Step 1: Perform Data Reduction

Task 1: Ensure All Tests Were Executed/Resolved

During this task, the test plans and logs are examined by the test team to
verify that all tests were executed (see Exhibit 20.1). The team can usually
do this by ensuring that all the tests are recorded on the activity log and
examining the log to confirm that the tests have been completed. When
there are defects that are still open and not resolved, they need to be pri-
oritized and deployment workarounds need to be established.

Task 2: Consolidate Test Defects by Test Number

During this task, the team examines the recorded test defects. If the tests
have been properly performed, it is logical to assume that, unless a defect
test document was reported, the correct or expected result was received.
If that defect was not corrected, it would have been posted to the test
defect log. The team can assume that all items are working except those
recorded on the test log as having no corrective action or unsatisfactory
corrective action. The test number should consolidate these defects so
that they can be posted to the appropriate matrix.

Task 3: Post Remaining Defects to a Matrix

During this task, the uncorrected or unsatisfactorily corrected defects
should be posted to a special function test matrix. The matrix indicates
which test-by-test number tested which function. The defect is recorded in
the intersection between the test and the functions for which that test
occurred. All uncorrected defects should be posted to the function/test
matrix intersection.

TEAM LinG



224

SOFTWARE TESTING METHODOLOGY

Step 2: Prepare Final Test Report

The objective of the final spiral test report is to describe the results of the
testing, including not only what works and what does not, from above, but
the test team’s evaluation regarding performance of the application when
it is placed into production.

For some projects, informal reports are the practice, whereas in others
very formal reports are required. The following is a compromise between
the two extremes to provide essential information not requiring an inordi-
nate amount of preparation (see Appendix E15, Spiral Testing Summary
Report; also see Appendix E29, Final Test Summary Report, which can be
used as a final report of the test project with key findings).

Exhibit 20.1. Summarize/Report Spiral Test Results

TEAM LinG



225

Summarize/Report Spiral Test Results

Task 1: Prepare the Project Overview

An objective of this task is to document an overview of the project in para-
graph format. Some pertinent information contained in the introduction
includes the project name, project objectives, the type of system, the tar-
get audience, the organizational units that participated in the project, why
the system was developed, what subsystems are involved, the major and
subfunctions of the system, and what functions are out of scope and will
not be implemented.

Task 2: Summarize the Test Activities

The objective of this task is to describe the test activities for the project
including such information as the following:

• Test Team — The composition of the test team, for example, test
manager, test leader, and testers, and the contribution of each, such
as test planning, test design, test development, and test execution.

• Test Environment — Physical test facility, technology, testing tools,
software, hardware, networks, testing libraries, and support soft-
ware.

• Types of Tests — Spiral (how many spirals), system testing (types of
tests and how many), and acceptance testing (types of tests and
how many).

• Test Schedule (Major Milestones) — External and internal. External
milestones are those events external to the project but which may
have a direct impact on it. Internal milestones are the events within
the project to which some degree of control can be administered.

• Test Tools — Which testing tools were used and for what purpose,
for example, path analysis, regression testing, load testing, and so
on.

Task 3: Analyze/Create Metric Graphics

During this task, the defect and test management metrics measured during
the project are gathered and analyzed. It is hoped that the defect tracking
has been automated and will be used to make the work more productive.
Reports are run and metric totals and trends are analyzed. This analysis
will be instrumental in determining the quality of the system and its
acceptability for use and also be useful for future testing endeavors. The
final test report should include a series of metric graphics. Following are
the suggested graphics.

Defects by Function. The objective of Exhibit 20.2 is to show the num-
ber and percentage of defects discovered for each function or group. This
analysis will flag the functions that have the most defects. Typically, such
functions had poor requirements or design. In the example below, the

TEAM LinG



226

SOFTWARE TESTING METHODOLOGY

Exhibit 20.2. Defects Documented by Function

Function Number of Defects Percent of Total

Order Processing

Create new order 11 6

Fulfill order 5 3

Edit order 15 8

Delete order 9 5

Subtotal 40 22

Customer Processing

Create new customer 6 3

Edit customer 0 0

Delete customer 10 6

Subtotal 16 9

Financial Processing

Receive customer payment 0 0

Deposit payment 5 3

Pay vendor 9 5

Write a check 4 2

Display register 6 3

Subtotal 24 13

Inventory Processing

Acquire vendor products 3 2

Maintain stock 7 4

Handle back orders 9 5

Audit inventory 0 0

Adjust product price 6 3

Subtotal 25 14

Reports

Create order report 23 13

Create account receivable report 19 11

Create account payable report 35 19

Subtotal 77 43

Grand totals 182 100

TEAM LinG



227

Summarize/Report Spiral Test Results

reports had 43 percent of the total defects, which suggests an area that
should be examined for maintainability after it is released for production.

Defects by Tester. The objective of Exhibit 20.3 is to show the number
and percentage of defects discovered for each tester during the project.
This analysis flags those testers who documented fewer than the expected
number of defects. These statistics, however, should be used with care. A
tester may have recorded fewer defects because the functional area tested
may have relatively fewer defects, for example, tester Baker in Exhibit 20.3.
On the other hand, a tester who records a higher percentage of defects
could be more productive, for example, tester Brown.

Defect Gap Analysis. The objective of Exhibit 20.4 is to show the gap
between the number of defects that has been uncovered and the number
that has been corrected during the entire project. At project completion
these curves should coincide, indicating that the majority of the defects
uncovered have been corrected and the system is ready for production.

Defect Severity Status. The objective of Exhibit 20.5 is to show the dis-
tribution of the three severity categories for the entire project, for exam-
ple, critical, major, and minor. A large percentage of defects in the critical
category indicate that there existed a problem with the design or architec-
ture of the application, which should be examined for maintainability after
it is released for production.

Test Burnout Tracking. The objective of Exhibit 20.6 is to indicate the
rate of uncovering defects for the entire project and is a valuable test com-
pletion indicator. The cumulative (e.g., running total) number of defects
and defects by time period help predict when fewer and fewer defects are
being discovered. This is indicated when the cumulative curve “bends”
and the defects by time period approach zero.

Root Cause Analysis. The objective of Exhibit 20.7 is to show the source
of the defects, for example, architectural, functional, usability, and so on. If
the majority of the defects are architectural, this will pervade the whole
system and require a great deal of redesign and rework. High percentage

Exhibit 20.3.  Defects Documented by Tester

Tester Number of Defects Percent of Total

Jones 51 28

Baker 19 11

Brown 112 61

Grand Totals 182 100

TEAM LinG



228

SOFTWARE TESTING METHODOLOGY

Exhibit 20.4.  Defect Gap Analysis

Exhibit 20.5. Defect Severity Status

TEAM LinG



229

Summarize/Report Spiral Test Results

Exhibit 20.6. Test Burnout Tracking

Exhibit 20.7. Root Cause Analysis

1. Architectural
2. Connectivity
3. Consistency
4. Database Integrity
5. Documentation
6. Functionality
7. GUI
8. Installation
9. Memory

10. Performance
11. Security
12. Standards
13. Stress
14. Usability

R
oo

t C
au

se
 (

%
)

1 141312111098765432

TEAM LinG



230

SOFTWARE TESTING METHODOLOGY

categories should be examined for maintainability after it is released for
production.

Defects by How Found. The objective of Exhibit 20.8 is to show how the
defects were discovered, for example, by external customers, manual test-
ing, and the like. If a very low percentage of defects was discovered
through inspections, walkthroughs, or JADs, this indicates that there may
be too much emphasis on testing and too little on the review process. The
percentage differences between manual and automated testing also illus-
trate the contribution of automated testing to the process.

Defects by Who Found. The objective of Exhibit 20.9 is to show who dis-
covered the defects, for example, external customers, development, qual-
ity assurance testing, and so on. For most projects, quality assurance test-
ing will discover most of the defects. However, if external or internal
customers discovered the majority of the defects, this indicates that qual-
ity assurance testing was lacking.

Functions Tested and Not. The objective of Exhibit 20.10 is to show the
final status of testing and verify that all or most defects have been cor-
rected and the system is ready for production. At the end of the project all
test cases should have been completed and the percentage of test cases
run with errors and/or not run should be zero. Exceptions should be eval-
uated by management and documented.

System Testing Defect Types. Systems testing consists of one or more
tests that are based on the original objectives of the system. The objective
of Exhibit 20.11 is to show a distribution of defects by system testing type.
In the example, performance testing had the most defects, followed by
compatibility and usability. An inordinate percentage of performance tests
indicates a poorly designed system.

Exhibit 20.8. Defects by How Found

1 2 3 4 5

60

50

40

30

20

10

0

P
er

ce
nt

1. Inspection
2. Walkthrough
3. JAD
4. Manual Testing
5. Automated Testing

TEAM LinG



231

Summarize/Report Spiral Test Results

Exhibit 20.9. Defects by Who Found

Exhibit 20.10. Functions Tested/Not Tested

TEAM LinG



232

SOFTWARE TESTING METHODOLOGY

Acceptance Testing Defect Types. Acceptance testing is an optional
user-run test that demonstrates the ability of the application to meet the
user’s requirements. The motivation for this test is to demonstrate rather
than destroy, for example, to show that the system works. Less emphasis
is placed on the technical issues and more is placed on the question of
whether the system is a good business fit for the end user.

There should not be many defects discovered during acceptance test-
ing, as most of them should have been corrected during system testing. In
Exhibit 20.12, performance testing still had the most defects, followed by
stress and volume testing.

Task 4: Develop Findings/Recommendations

A finding is a variance between what is and what should be. A recommen-
dation is a suggestion on how to correct a defective situation or improve a
system. Findings and recommendations from the test team constitute the
majority of the test report.

The objective of this task is to develop the findings and recommenda-
tions from the testing process and document “lessons learned.” Previously,

Exhibit 20.11.  System Testing by Root Cause

TEAM LinG



233

Summarize/Report Spiral Test Results

data reduction has identified the findings, but they must be put in a format
suitable for use by the project team and management.

The test team should make the recommendations to correct a situation.
The project team should also confirm that the findings are correct and the
recommendations reasonable. Each finding and recommendation can be
documented in the Finding/Recommendation matrix depicted in Exhibit
20.13.

Step 3: Review/Approve the Final Test Report

Task 1: Schedule/Conduct the Review

The test summary report review should be scheduled well in advance of
the actual review and the participants should have the latest copy of the
test plan.

As with any interview or review, it should contain certain elements. The
first is defining what will be discussed; the second is discussing the details;
the third is summarization; and the final element is timeliness. The
reviewer should state up front the estimated duration of the review and set

Exhibit 20.12.  Acceptance Testing by Root Cause

TEAM LinG



234

SOFTWARE TESTING METHODOLOGY

E
xh

ib
it

 2
0.

13
.

 F
in

d
in

g/
R

ec
om

m
en

d
at

io
n

s 
M

at
ri

x

Fi
n

d
in

g 
D

es
cr

ip
ti

on
a

B
u

si
n

es
s 

Fu
n

ct
io

n
b

Im
p

ac
tc

Im
p

ac
t o

n
 

O
th

er
 

Sy
st

em
sd

C
os

ts
 t

o 
C

or
re

ct
e

R
ec

om
m

en
d

at
io

n
f

N
ot

 e
no

ug
h

 t
es

te
rs

 w
er

e 
in

it
ia

lly
 a

ss
ig

ne
d

 t
o 

th
e 

p
ro

je
ct

N
/A

C
au

se
d

 t
h

e 
te

st
in

g 
p

ro
ce

ss
 t

o 
la

g 
b

eh
in

d
 t

h
e 

or
ig

in
al

 s
ch

ed
ul

e

N
/A

C
on

tr
ac

te
d

 5
 

ad
d

it
io

na
l t

es
te

rs
 

fr
om

 a
 c

on
tr

ac
t 

ag
en

cy

Sp
en

d
 m

or
e 

re
so

ur
ce

 
p

la
nn

in
g 

in
 fu

tu
re

 
p

ro
je

ct
s

D
ef

ec
t 

tr
ac

ki
ng

 w
as

 n
ot

 
m

on
it

or
ed

 a
d

eq
ua

te
ly

 b
y 

d
ev

el
op

m
en

t

N
/A

N
um

b
er

 o
f 

ou
ts

ta
nd

in
g 

d
ef

ec
ts

 g
re

w
 

si
gn

ifi
ca

nt
ly

N
/A

A
ut

h
or

iz
ed

 
ov

er
ti

m
e 

fo
r 

d
ev

el
op

m
en

t

Q
A

 n
ee

d
s 

to
 s

tr
es

s 
th

e 
im

p
or

ta
nc

e 
of

 d
ef

ec
t 

tr
ac

ki
ng

 o
n 

a 
d

ai
ly

 
b

as
is

 in
 fu

tu
re

 p
ro

je
ct

s

A
ut

om
at

ed
 t

es
ti

ng
 t

oo
ls

 d
id

 
co

nt
ri

b
ut

e 
si

gn
ifi

ca
nt

ly
 t

o 
re

gr
es

si
on

 t
es

ti
ng

N
/A

In
cr

ea
se

d
 t

es
ti

ng
 

p
ro

d
uc

ti
vi

ty
 

N
/A

N
/A

U
ti

liz
e 

te
st

in
g 

to
ol

s 
as

 
m

uc
h

 a
s 

p
os

si
b

le

Ex
ce

ss
iv

e 
nu

m
b

er
 o

f d
ef

ec
ts

 
in

 o
ne

 fu
nc

ti
on

al
 a

re
a

R
ep

or
ts

C
au

se
d

 a
 lo

t 
of

 
d

ev
el

op
er

 r
ew

or
k 

ti
m

e

N
/A

Ex
ce

ss
iv

e 
d

ev
el

op
er

 
ov

er
ti

m
e

P
er

fo
rm

 m
or

e 
te

ch
ni

ca
l 

d
es

ig
n 

re
vi

ew
s 

ea
rl

y 
in

 
th

e 
p

ro
je

ct

Fu
nc

ti
on

al
 a

re
a 

no
t 

co
m

p
at

ib
le

 w
it

h
 o

th
er

 
sy

st
em

s

O
rd

er
 

P
ro

ce
ss

in
g

R
ew

or
k 

co
st

s
H

ad
 t

o 
re

d
es

ig
n 

th
e 

d
at

ab
as

e

C
on

tr
ac

te
d

 a
n 

O
ra

cl
e 

d
at

ab
as

e 
D

B
A

P
er

fo
rm

 m
or

e 
d

at
ab

as
e 

d
es

ig
n 

re
vi

ew
s 

ea
rl

y 
in

 
th

e 
p

ro
je

ct

30
 p

er
ce

nt
 o

f d
ef

ec
ts

 h
ad

 
cr

it
ic

al
 s

ev
er

it
y

N
/A

Si
gn

ifi
ca

nt
ly

 
im

p
ac

te
d

 t
h

e 
d

ev
el

op
m

en
t 

an
d

 
te

st
in

g 
ef

fo
rt

N
/A

H
ir

ed
 a

d
d

it
io

na
l 

d
ev

el
op

m
en

t 
p

ro
gr

am
m

er
s

P
er

fo
rm

 m
or

e 
te

ch
ni

ca
l 

re
vi

ew
s 

ea
rl

y 
in

 t
h

e 
p

ro
je

ct
 a

nd
 t

ig
h

te
n 

up
 

on
 t

h
e 

si
gn

-o
ff

 
p

ro
ce

d
ur

es

TEAM LinG



235

Summarize/Report Spiral Test Results
Fu

nc
ti

on
/G

U
I h

ad
 t

h
e 

m
os

t 
d

ef
ec

ts
N

/A
R

eq
ui

re
d

 a
 lo

t 
of

 r
e-

w
or

k
N

/A
Te

st
er

s 
au

th
or

iz
ed

 
ov

er
ti

m
e

P
er

fo
rm

 m
or

e 
te

ch
ni

ca
l 

re
vi

ew
s 

ea
rl

y 
in

 t
h

e 
p

ro
je

ct
 a

nd
 t

ig
h

te
n 

up
 

on
 t

h
e 

si
gn

-o
ff

 
p

ro
ce

d
ur

es

T
w

o 
te

st
 c

as
es

 c
ou

ld
 n

ot
 b

e 
co

m
p

le
te

d
 b

ec
au

se
 

p
er

fo
rm

an
ce

 lo
ad

 t
es

t 
to

ol
 

d
id

 n
ot

 w
or

k 
p

ro
p

er
ly

St
re

ss
 t

es
ti

ng
 

or
d

er
 e

nt
ry

 
w

it
h

 1
00

0 
te

rm
in

al
s

C
an

no
t 

gu
ar

an
te

e 
sy

st
em

 w
ill

 
p

er
fo

rm
 

ad
eq

ua
te

ly
 u

nd
er

 
ex

tr
em

e 
lo

ad
 

co
nd

it
io

ns

N
/A

D
el

ay
 s

ys
te

m
 

d
el

iv
er

y 
un

ti
l n

ew
 

te
st

in
g 

to
ol

 
ac

q
ui

re
d

 (
2 

m
on

th
s 

d
el

ay
 a

t 
$8

5,
00

0 
lo

ss
 in

 
re

ve
nu

e,
 $

10
,0

00
 

fo
r 

to
ol

)

Lo
ss

 o
f r

ev
en

ue
 

ov
er

sh
ad

ow
s 

ri
sk

. 
Sh

ip
 s

ys
te

m
 b

ut
 

ac
q

ui
re

 p
er

fo
rm

an
ce

 
te

st
 t

oo
l a

nd
 c

om
p

le
te

 
st

re
ss

 t
es

t

a  
T

h
is

 i
nc

lu
d

es
 a

 d
es

cr
ip

ti
on

 o
f 

th
e 

p
ro

b
le

m
 f

ou
nd

 f
ro

m
 t

h
e 

d
ef

ec
t 

in
fo

rm
at

io
n 

re
co

rd
ed

 i
n 

th
e 

d
ef

ec
t 

tr
ac

ki
ng

 d
at

ab
as

e.
 I

t 
co

ul
d

 a
ls

o 
in

cl
ud

e
te

st
 t

ea
m

, t
es

t 
p

ro
ce

d
ur

es
, o

r 
te

st
 e

nv
ir

on
m

en
t 

fin
d

in
gs

 a
nd

 r
ec

om
m

en
d

at
io

ns
.

b
 D

es
cr

ib
es

 t
h

e 
b

us
in

es
s 

fu
nc

ti
on

 t
h

at
 w

as
 i

nv
ol

ve
d

 a
nd

 a
ff

ec
te

d
.

c  
D

es
cr

ib
es

 t
h

e 
ef

fe
ct

 t
h

e 
fin

d
in

g 
w

ill
 h

av
e 

on
 t

h
e 

op
er

at
io

na
l 

sy
st

em
. 

T
h

e 
im

p
ac

t 
sh

ou
ld

 b
e 

d
es

cr
ib

ed
 o

nl
y 

as
 m

aj
or

 (
th

e 
d

ef
ec

t 
w

ou
ld

 c
au

se
th

e 
ap

p
lic

at
io

n 
sy

st
em

 t
o 

p
ro

d
uc

e 
in

co
rr

ec
t 

re
su

lt
s)

 o
r 

m
in

or
 (

th
e 

sy
st

em
 i

s 
in

co
rr

ec
t,

 b
ut

 t
h

e 
re

su
lt

s 
w

ill
 b

e 
co

rr
ec

t)
.

d
 D

es
cr

ib
es

 w
h

er
e 

th
e 

fin
d

in
g 

w
ill

 a
ff

ec
t 

ap
p

lic
at

io
n 

sy
st

em
s 

ot
h

er
 t

h
an

 t
h

e 
on

e 
b

ei
ng

 t
es

te
d

. 
If 

th
e 

fin
d

in
g 

af
fe

ct
s 

ot
h

er
 d

ev
el

op
m

en
t 

te
am

s,
th

ey
 s

h
ou

ld
 b

e 
in

vo
lv

ed
 i

n 
th

e 
d

ec
is

io
n 

on
 w

h
et

h
er

 t
o 

co
rr

ec
t 

th
e 

p
ro

b
le

m
.

e  
M

an
ag

em
en

t 
m

us
t 

kn
ow

 b
ot

h
 t

h
e 

co
st

s 
an

d
 t

h
e 

b
en

efi
ts

 b
ef

or
e 

it
 c

an
 m

ak
e 

a 
d

ec
is

io
n 

on
 w

h
et

h
er

 t
o 

in
st

al
l 

th
e 

sy
st

em
 w

it
h

ou
t 

th
e 

p
ro

b
le

m
b

ei
ng

 c
or

re
ct

ed
.

f  D
es

cr
ib

es
 t

h
e 

re
co

m
m

en
d

at
io

n 
fr

om
 t

h
e 

te
st

 t
ea

m
 o

n 
w

h
at

 a
ct

io
n 

to
 t

ak
e.

TEAM LinG



236

SOFTWARE TESTING METHODOLOGY

the ground rule that if time expires before completing all items on the
agenda, a follow-on review will be scheduled.

The purpose of this task is for development and the project sponsor to
agree and accept the test report. If there are any suggested changes to the
report during the review, they should be incorporated.

Task 2: Obtain Approvals

Approval is critical in a testing effort, because it helps provide the neces-
sary agreement among testing, development, and the sponsor. The best
approach is with a formal sign-off procedure of a test plan. If this is the
case, use the management approval sign-off forms. However, if a formal
agreement procedure is not in place, send a memo to each key participant,
including at least the project manager, development manager, and sponsor.
In the document, attach the latest test plan and point out that all their feed-
back comments have been incorporated and that if you do not hear from
them, it is assumed that they agree with the plan. Finally, indicate that in a
spiral development environment, the test plan will evolve with each itera-
tion but that you will include them in any modification.

Task 3: Publish the Final Test Report

The test report is finalized with the suggestions from the review and dis-
tributed to the appropriate parties. The purpose has short- and long-term
objectives.

The short-term objective is to provide information to the software user
to determine if the system is ready for production. It also provides informa-
tion about outstanding issues, including testing not completed or out-
standing problems, and recommendations.

The long-term objectives are to provide information to the project
regarding how it was managed and developed from a quality point of view.
The project can use the report to trace problems if the system malfunc-
tions in production, for example, defect-prone functions that had the most
errors and which ones were not corrected. The project and organization
also have the opportunity to learn from the current project. A determina-
tion of which development, project management, and testing procedures
worked, and which didn’t work, or need improvement, can be invaluable to
future projects.

TEAM LinG



Section IV
Test Project 
Management

TEAM LinG



238

SOFTWARE TESTING METHODOLOGY

Project management, according to the American Society for Quality (ASQ),
is the application of knowledge, skills, tools and techniques to meet the
requirements of a project. Project management knowledge and practices
are best described as a set of processes. A critical project process is soft-
ware testing. The following section describes the implementation consid-
erations for managing this endeavor.

The objectives of this section are to:

• Describe basic project management principles.
• Contrast general project management and test management.
• Describe how to effectively estimate testing projects.
• Describe the defect management subprocess.
• Discuss the advantages and disadvantages of the offshore and

onshore models.
• List the steps for integrating the testing activity into the develop-

ment methodology.

TEST PROJECT MANAGEMENT

TEAM LinG



239

Part 21

Overview of General 
Project Management

Although project management is considered as a fast emerging profession
that is being consolidated with planning and execution of multiple
projects, it remained in its nondefinitional form for centuries. Project man-
agement can be traced back to the architectural monuments built in the
earlier centuries. Each one of these architectural marvels was itself a
project that was initiated, planned, executed, controlled, and closed. One
can visualize the amount of planning and execution activities involved in
the evolution of the pyramids in Egypt, the hanging gardens of Babylon, or
the battles fought by Caesar. Project management had its roots in the con-
struction industry for centuries; however, it gained the current popularity
only through software project management activities.

The shortening of software project life cycles (where the start date and
end date of the projects are determined in advance) has highlighted the
importance of project management activities. Project planning and execu-
tion have gained increased popularity because many projects had not
reached their logical goals within the stipulated time sequence, due to
poor planning There are numerous examples in the software industry
where a project that has not been systematically planned and executed has
resulted in failure and been cancelled by the project sponsors.

Although the knowledge and experience of the project team contributes
to the project’s success or failure, it is the project management approach
employed by the project manager that is the key factor influencing the suc-
cess of a project. The following are some project management characteris-
tics that dramatically improve the probability of successful projects.

Define the Objectives

The following is a well-known quote that illustrates the importance of
project objectives.

If you don’t know where you are going you are guaranteed to get there.

Lewis Carroll, Alice in Wonderland

TEAM LinG



240

TEST PROJECT MANAGEMENT

If one studies the history of unsuccessful projects, it is evident that
unclear objectives are the primary culprit in failure. Ever-changing require-
ments, without considering the effect on project objectives, may be the sin-
gle most critical factor that contributes to a project failure. A useful tech-
nique at the project manager’s disposal is a requirement change matrix
that helps synchronize the completion of the project within the planned
date.

As an example of project objectives, a company that wishes to migrate
its application to a more robust database and system with multiple plat-
forms and applications could define the project objectives as follows:

• Eliminate cost to support multiple operating systems.
• Eliminate the operational and credit risk by improving data integrity.
• Provide a robust, 24/7, downtime free service to the clients.

Define the Scope of the Project

The project manager needs to clearly define the project scope. If the
project scope is not clear, the project manager cannot define a comprehen-
sive planning and execution strategy. It is not possible to follow the water-
fall model in a spiral development environment where it is impossible to
freeze requirements before one starts planning and designing activities.
But as a prudent project manager, one should freeze the higher-level objec-
tives and deliverables of the project before pursuing major design activi-
ties. When directions are imposed to initiate the project with unclear
objectives, document the risks and assumptions, and convey it to the
project sponsors. 

Identify the Key Activities

When the project scope is reasonably clear and documented, identify all
the possible key activities that need to be accomplished to reach the objec-
tives. This is necessary because each activity that contributes to the
project is included within its own schedule and cost implications. If any
one of these key activities is not accounted for, the project manager will be
viewed as disorganized.

Estimate Correctly

Cost estimation is another critical factor that determines the success or
failure of most software projects. Some examples of cost include:

• Software
• People resources
• Infrastructure and other costs
• Training

TEAM LinG



241

Overview of General Project Management

Various estimation models have emerged to estimate the schedule and
cost for projects. From function points, COCOMO to domain-specific,
project-specific cost estimation models it is the responsibility of the
project manager to choose the best project estimation model. Unless a
project manager is able to identify the activity that needs to be performed
for completing the project, the estimate tends to explode and eventually
affect the cost, schedule, and the project deliverables. Prudence should be
exercised to choose the best model. Always apply a margin for error in the
estimation.

The basic COCOMO model estimates the effort required to develop
software in three modes of development (organic, semidetached, or
embedded mode) using only DSI (delivered source instructions) as an
input. The basic model is good for quick, early, and rough order-of-mag-
nitude estimates.

Design

As with requirements, design is one more crucial activity that requires
focus by the project managers. In the software industry, where new tech-
nology emerges every other day, the project manager should foresee the
changes while designing the application. The corporate charter and goals
should be given due consideration when identifying the advantages of the
emerging technologies. For example, when the corporate goal projects five
million users growth over the decade, if the designs are based on the exist-
ing technology, scaling up the problem may crop up after a few years and
the application will not survive the test of time. This will incur additional
expenses for the company that could have been avoided had the corporate
goals been considered while designing.

But just because new technologies emerge, the project manager should
avoid the tendency to choose it unless it is warranted by the business cor-
porate goals. For example, if the corporate growth is projected for 10 mil-
lion users over the next five years, it would be imprudent to embrace tech-
nology that caters to 100 million users.

While designing, the design constraints and alternatives should also be
considered. The project manager should identify and manage the explicit
and implicit design constraints.

Manage People

Team members’ under performance is the source of most software project
failures. This is one of the crucial factors that a project manager must con-
sider. Choosing the right people for the right job is a challenging project
management task. Motivation and retention is an art that the project man-
ager must master.

TEAM LinG



242

TEST PROJECT MANAGEMENT

Leadership

Leadership is one of the crucial factors in project management. A leader
should lead by example. There is considerable difference between a man-
ager and leader. Anybody can manage a well-established system, whereas
a leader alone will take the organization to greater heights. The leader
should align herself with the company vision and establish the direction
for the team.

Motivating and inspiring the team to accomplish its targets is one of the
primary leadership qualities. Retention is another crucial factor in the soft-
ware projects. Attrition at the critical stages of projects will certainly affect
the progress of the project. Promotions, pay hikes, and other fringe bene-
fits alone are not sufficient to motivate a leader. Challenging projects are
one of the contributing factors for retention of a leader’s talents. The
leader should have the necessary independence to choose the appropriate
resources for the project. Simply dumping an available resource into the
project may cause havoc, because projects are time-bound activities, not
training grounds.

Although there is speculation among project management experts that
project managers need not be technical specialists, the knowledge in func-
tional and technical areas is an added advantage for a project manager to
manage the projects effectively.

Leadership traits are best described by sixth-century B.C. Chinese wis-
dom as follows:

A leader is best when his [colleagues] scarcely know that he exists.

Bad when they fear him. Worse when they despise him.

But of a good leader, at the end of the day, when his work is done and
his goals achieved, they will all say, “We did this ourselves.”

Communication

Communication is another key factor in the success of all projects. When
the project leader is unable to clearly communicate the project objectives
to the project team the communication gap arises, which affects the qual-
ity of the project. It is the responsibility of the project leader to communi-
cate in a clear unambiguous manner and to communicate effectively. Simi-
larly, individuals receiving direction should also ensure that the message is
understood on the same wavelength as it is passed from the sender.

In the virtual world where global and physical distance do not make any
difference, clear communication is an essential contributor for the suc-
cessful and fruitful completion of the goals. Establishing clear communica-
tion channels is the primary responsibility of the project manager, irre-
spective of whether the communication is written, verbal, listening or
speaking, or internal or external to the projects.

TEAM LinG



243

Overview of General Project Management

Solving Problems

As projects are always constrained by time and scope, cost problems
always emerge. Project management does not end after preparing a
detailed project plan that includes the risks and assumptions for the
project. It is the primary responsibility of the project manager to continue
analyzing these risks while the project is in progress. In the event that any
of these risks are materializing, action must be taken to solve the issues.

The problem root causes should be identified and the necessary action
must be initiated to solve the problems. The project manager should be
able to distinguish between the causes and problems and should take
appropriate decisions to solve the problems before they explode to a
greater level.

Continuous Monitoring

The project manager must always align the project activities with the
project goals. The project should be monitored at the macrolevel. Several
projects that have been managed by technical gurus have failed simply for
the reason that they have been micromanaged. Continuous monitoring
helps to identify the symptoms in the earlier stages and will help to resolve
them at the proper time before further damage is caused to the project.
The following tips will help the project manager perform effective monitoring:

• Daily status and activity report
• Weekly progress report
• Risk and assumptions analysis report
• Review reports
• Downtime reports
• Defect reports

There are several software tools available for project and task schedul-
ing; however, comprehensive project progress monitoring cannot be satis-
fied by a software tool. Project monitoring requires a great deal of diligence
from the experience of the project manager and the decision needs to be
taken early rather than waiting until a problem escalates.

Manage Changes

Change management is yet another critical activity for effective project
management. There are several software tools available for the project
managers to monitor the changes to occur in the requirements, schedule,
and environments. Changes in the scope and time have far-reaching impli-
cations for the project deliverables. When the project is inundated with
scope creep, the project manager should reassess the cost and time impli-
cations and should convey them to the project sponsors.

TEAM LinG



244

TEST PROJECT MANAGEMENT

Change management in the development and test environment for code
promotion is yet another area that requires much disciplined activity from
the project team. There exist very good version control software products
such as PVCS. Although general management consists of planning, organiz-
ing, staffing, executing, and controlling an ongoing operation, a project
manager should possess additional skills of estimation and scheduling as
projects are specific activities with definite start and end dates. Functional
knowledge, technical knowledge, and management experience will help
produce better project managers.

Each project will complete with specific deliverables such as a specifica-
tion document and functional software. But within the project life cycle,
each cycle will consist of an entry and exit criterion. At the end of every
exit criterion the project manager should assess the situation and decide
whether it is appropriate to move to the next phase of the project.

A program is a group of projects managed in coordinated ways. When
the project manager matures and starts managing multiple projects he
becomes a program manager.

Program managers with technical and development experience derive
the futuristic vision of the product or company, create design, and define
strategy for all aspects. They understand the nuances of project manage-
ment and guide the project managers.

TEAM LinG



245

Part 22

Test Project 
Management

In the previous chapter we have discussed the concepts of project manage-
ment. Within project management a number of new management hierar-
chies are emerging due to the continuous expansion occurring in software
project management. The responsibilities that have been executed by a
single person are now divided into multiple positions and responsibilities
due to the size of the projects. Below are some of the familiar titles we hear:

• Program manager
• Technical project manger
• Development project manager
• Quality assurance manager
• Test project manager
• Configuration manager
• Release manager
• System manager
• Resource manager

In past years these responsibilities were shouldered by a single individual.

Test project management has emerged as a specialized management
activity in the last couple of years. This section analyzes how the test
project manager differs from the other project manager roles and how the
test project manager distinguishes himself in his roles and responsibilities.

In previous years, software testing was a subset of the software develop-
ment life cycle and developers performed the validation in the software
programs coded by them and released them to the users for verification of
the critical business flows before implementing to production. The volume
of business losses due to the inherent bugs in the systems has led the busi-
ness to consider the concept of independent verification and validation.
Verification is ensuring that the test specification is satisfied whereas vali-
dation ensures that the original business objectives are satisfied. Valida-
tion is commonly related to user acceptance testing. Inasmuch as develop-
ers do not like to break their own code, which is their brainchild, the
concept of independent testing has gained more attention.

TEAM LinG



246

TEST PROJECT MANAGEMENT

Historically, the development team construes themselves as the cre-
ators of the system and the QA community is considered a necessary evil.
This is more prevalent when there is an independent testing group. As the
test team often works in a hostile atmosphere, they need to equip them-
selves more strongly in both functional and testing skills. The test team
should be involved during the requirement and functional specification
stages. In recent years, major losses caused in the financial and insurance
sectors have forced the stakeholders to involve the test team even in the
requirement stage.

The following are some best practice testing principles to increase the
probability of the project’s success.

Understand the Requirements

Although the developers concentrate on their programs and modules, the
test team needs to understand the overall application. If a programmer
understands the functional and design specifications of the functions
assigned to her, she can perform well. But the testers should broaden their
vision to understand the entire functionality of the application. Testers
should be comfortable with the inputs, expected results, data flow
between modules, and between interfaces. Unless they have a thorough
knowledge of the functionality of the system it is difficult to gain overall
insight into the application. Thus, before the tester pursues testing an
application, considerable time needs to be spent understanding the com-
plexities of the application functionality.

The knowledge in the business and application system will help the test
managers to:

• Create a complete and meaningful test plan and test cases for the
application.

• Substantiate the tester’s arguments while defending defects discov-
ered in the system.

• Improve leadership capabilities by extending help to other testers.
• Suggest valid improvements to the system.

Unless the test project manager is comfortable in the business domain
in which he is operating, it is difficult for him to critically evaluate the test
conditions and cases. Because testing often occurs at the end of the devel-
opment cycle, unless the test project manager understands the application
requirements, it is difficult to deliver quality software to the end users.

Test Planning

The 70:30 project management concept considers two different types of
project managers. Structural planning is the basis for sound testing.
Instead of unstructured planning, a good test project manager spends 70

TEAM LinG



247

Test Project Management

percent of the time systematically planning and preparing the test strategy,
testware, and test execution methodologies. The result is a quality applica-
tion delivered within scheduled timelines.

The test project manager should be proactive to:

• Decide on the proper strategy depending upon the types of testing
required.

• Prepare the test cases and scripts.
• Review all the test documents.
• Implement traceability to ensure good test coverage.
• Plan the data requirements and availability.
• Plan the execution schedule.

Test Execution

The role of a tester is to ensure that no known bugs exist in the system. A
tester coming from a development background probably has the natural
instincts to test the positive flow and avoid exceptions, or negative condi-
tions.

To overcome this, the tester needs to change his perspective. He needs
to crack the system, create implicit test conditions and cases, and see the
system from a user perspective. The role of QA/testing is gaining more
importance because:

• Many systems in production are still afflicted with bugs.
• Defects lead to unexpected downtime for the business.
• The financial loss caused due to bugs and downtime is too high,
• The corporation’s board is becoming responsible for the unexpected

business loss due to bugs, and
• Bugs in mission-critical applications can be catastrophic.

The test manager must perform his role by verifying and validating the
system from the business user requirement. Even if hundred of defects are
discovered by the QA/tester, it is common to not be appreciated when one
unknown defect is unearthed in production.

Identify and Improve Processes

Although test managers work within defined boundaries of processes and
procedures established within their work environment, there is always
room for continuous process improvement. The following useful tips will
expedite continuous process improvement:

• Process Improvement — Identify the loose ends in the process and
bridge the gaps with innovative quality improvement processes.
However, there is always is scope when improving any process.

TEAM LinG



248

TEST PROJECT MANAGEMENT

• Defect Analysis — Analyze the pattern in the defects identified in the
previous releases of the application and tighten the test cases to
capture the hidden defects.

• Requirements Review — Perform a complete review for ambiguous
statements in the requirements documents, which may give rise to
different interpretations and ultimately to bugs in the system.

• Risk Mitigation — Expect the unexpected and always have a contin-
gency plan upon which to fall back. Continuously review the risk
and mitigation plan during the planning and execution phase of
testing.

Essential Characteristics of a Test Project Manager

The test project manager is a key element in ensuring a quality application.
The following are some key activities that will help maximize the efficiency
of the test project manager.

Requirement Analysis

Business users do not have to be superior communicators of their require-
ments. The business analysts will document the business user require-
ments. When business users sign off on the requirement documents they
implicitly assume certain conditions based on their intuitive knowledge in
the business. The test manager should consider these implicit conditions
also while writing and reviewing the test conditions. If an implicit condition
is not tested, then the testing is not complete and it will not certainly sat-
isfy the end-user requirements.

Gap Analysis

The gaps among the requirement document and functional and design
specifications may explode like a grenade when the project goes live and a
typical business scenario emerges. The test project manager should spend
considerable time analyzing the gap between the requirements and speci-
fications so that the rework costs will be minimal. A gap document will
demonstrate the gaps between these two documents and provide confi-
dence to the business regarding their final requirements. Gaps (if any) in
the other baseline documents such as use cases or architecture should
also be analyzed by the test project manager.

Lateral Thinking in Developing Test Cases

The test project manager should exhibit lateral thinking in designing the
test cases. Thinking beyond the perceived client expectations brings out
critical business issues that would otherwise crop up at a later stage in pro-
duction. The test project manager should analyze the assumptions, risks,
and design considerations and constitute special test cases to test them.

TEAM LinG



249

Test Project Management

Avoid Duplication and Repetition

The ability and efficiency of the test project manager is exhibited when the
test cases are comprehensive with full coverage and are not repetitive. Exe-
cuting the same type of test cases for different conditions reduces the effi-
ciency and consumes more effort of the testers. Equivalence clause parti-
tioning is a testing technique (see Appendix G) that should be used so that
the business functions that are tested in a set of test cases are not repeated
in multiple test cases.

Test Data Generation

It is the responsibility of the test manager to ensure that all the test cases
generated are executed with the required test data. Test data generation is
a vital part of the test planning activity. The data guidelines should be gen-
erated during the test planning stage to ensure that all the data required for
executing all the test cases is made available in the test environment.

Validate the Test Environment

The test environment should be defined in the test strategy document. All
the interfaces that are required to execute the test cases should be avail-
able or the testers will not be able to complete their tests. The test man-
ager should prepare a checklist for testing the environment before execut-
ing the test cases.

Test to Destroy

The test manager’s attitude should be to break the software with effective
test cases. Test cases should be designed in such a way that all possible
live situations are visualized and represented as test cases. As mentioned
above, the lateral thinking will help the test manager to prepare critical test
cases that may at times cause the software to crash. If the test cases are
prepared with this killer attitude, they will make hidden bugs in the system
surface.

Analyze the Test Results

The testing does not end with the successful execution of test cases and
reporting of defects as they arise. The test manager should analyze the
results of all the test cases to ensure that they behave as expected. Some-
times the analysis will bring out the hidden test cases that were not visual-
ized while preparing the test cases. For example, the specification docu-
ment will define the norms for a start date, tenure, and end date for a
product. The implicit test condition needs to be designed to check that the
start date couldn’t be greater than the end date.

For performance testing, analysis is an important step in the testing pro-
cess. The test manager should be equipped to analyze the performance

TEAM LinG



250

TEST PROJECT MANAGEMENT

test results and interpret them. The baseline performance measures
should be compared with the results and recommendations should be
made for better tuning of the system.

Do Not Hesitate to Accept Help from Others

The more a test manager is flexible, the higher the probability that the soft-
ware will be stable in a shorter testing life cycle. Instead of assuming and
taking things for granted, the test manager and team can approach the
business users to verify and validate the test results. For example, in the
case of performance tests where more technical knowledge is required to
analyze the test results, the developer can be consulted.

Convey Issues as They Arise

When they arise, the various issues during test execution should be con-
veyed to the stakeholders. This will help to reduce the test cycle time. The
status report and progressive reports on test planning and execution
should be elaborate enough to provide top management a realistic evalua-
tion of the testing processes.

Improve Communication

Communication between various teams is a vital aspect for the test project
management. The communication should convey the required message
and should be addressed to the correct person for whom it is intended. It
is the responsibility of the communicator to make the recipient understand
the view on the same wavelength. Improper communication has caused
projects to fail. Unlike development, testing requires effective communica-
tion whereby the test managers and testers need to communicate and
influence various other groups.

Always Keep Updating Your Business Knowledge

It is because of that business the software industry survives. Software
development exists to support the business. If there were no business,
most of the software companies would cease to exist. Updating business
knowledge is an ongoing process for the test managers and team. Unless
the test manager is very strong in business knowledge, she will not be able
to convince the developers or business of the importance of the defects
that have been discovered in the system.

Learn the New Testing Technologies and Tools

The software industry is one of the most fast-changing industries with new
technologies emerging every day. If the test manager does not update him-
self with the changing technologies he will be outdated. More efficient soft-
ware testing tools are emerging. From the first generation tools, now fourth

TEAM LinG



251

Test Project Management

generation software-testing tools are arriving. If the test project manager
does not update his knowledge of these tools, his effort estimation will suf-
fer which will affect the cost and time of the project.

Deliver Quality

The ultimate purpose of software testing is to deliver quality that is mea-
sured by what the user originally wanted. It is the primary responsibility of
the test project manager to also deliver quality software. The software
should be tested completely for the requirements and the test project man-
ager should ensure full coverage.

Improve the Process

With every project the test project manager learns new concepts that will
refresh her test processes. The test process should undergo continuous
process improvement.

Create a Knowledge Base

The expertise gathered in various projects should be documented so that
the knowledge is reused in other projects. It is the responsibility of the test
project manager to document the positive and negative factors that were
encountered in each test project execution.

Repeat the Success

This is illustrated in testing when the test team defines implicit conditions
from the requirements documents. Everyone thinks in a different way but
think logically. Place yourself in the shoes of the business users. The test
manager’s interaction with other project members may give rise to differ-
ent perspectives, which will fine-tune your test cases to unearth a hidden
bug.

TEAM LinG



TEAM LinG



253

Part 23

Test Estimation

When faced with a critical schedule crunch, a key question is whether one
is managing the test project or the project is managing the project manager
(PM). Testing activities, cost, and schedule are estimated by project man-
agement. Unfortunately, systematic testing efforts are not considered
when the overall project schedule is defined. The test project manager is
typically not brought into a project until the project has already pro-
gressed halfway. The project manager is normally requested to work within
the predefined schedule for the testing effort. In this case, the project is
going to manage the test project manager. Because of the late involvement,
the PM is unable to make a retrospective test estimation effort and/or read-
just the schedule. In this section some estimation best practices for the
test project management are discussed.

Effort estimation is concerned with the cost of resources needed to com-
plete a project. In software testing projects the following are the major
costs having an impact on resources:

• Number of testers required for the project and the cost of hiring
them

• Cost of hardware and other administrative requirements for them
to work during the project

• The software cost that needs to be installed on the machines, includ-
ing the cost of automation tools required for testing

Finalizing the testing scope is the primary task of the test project man-
ager to start the effort estimation. The work breakdown structure (WBS) is
the widely used technique to estimate the effort. For testing projects the
scope is divided into smaller tasks to a granular level of estimating how
much one person can accomplish in one day. This is called a person day.
The total number of person days is defined and the number of resources
required to complete the tasks is estimated.

The sample project plan shown in Exhibits 23.1 and 23.2 defines the typ-
ical tasks that are performed in a testing project.

Of the various activities in the project plan, planning and execution are
the key activities that determine the cost of resources and schedules
required for the testing projects. During these two crucial phases of test-
ing, various key deliverables are estimated. This will ensure the test team

TEAM LinG



254

TEST PROJECT MANAGEMENT

Exhibit 23.1. Sample Project Plan

Exhibit 23.2. Sample Project Plan (continued)

TEAM LinG



255

Test Estimation

will have a focused approach and the delivery of the deliverables will bring
each task to a logical end so that the project can proceed with the next task
in the plan. However, it is not always necessary that a particular task
should be completed to start the next task. The project manager should
analyze the task dependencies. A task dependency is the relationship
between two tasks in which one task depends on the start or finish of
another task in order to begin or end. The task that depends on the other
task is the successor, and the task it depends on is the predecessor.

The following describe some typical test dependencies and why they
are important to test management.

Finish-to-Start: (FS)

Task (B) cannot start until task (A) finishes. For example, if you have two
tasks “Test Script writing” and “Test Execution,” “Test Execution” can’t
start until “Test Script writing” completes. This is the most common type
of dependency.

Start-to-Start: (SS)

Task (B) cannot start until task (A) starts. For example, if we have two tasks
“Test Script writing” and “Run Plan preparation,” “Run Plan preparation”
can’t begin until “Test Script writing” starts.

Finish-to-Finish: (FF)

Task (B) cannot finish until task (A) finishes. For example, if you have two
tasks, “Test Execution complete” and “Test Closure Report,” “Test Closure
Report” can’t finish until “Test Execution complete” finishes.

Start-to-Finish (SF)

Task (B) cannot finish until task (A) starts. This dependency type can be
used for “just-in-time scheduling” up to a milestone or the project finish
date to minimize the risk of a task finishing late, if its dependent task slips.
This dependency type applies when a related task needs to finish before a
milestone or project finish date. However, it does not matter exactly when
and one does not want a late finish to affect the just-in-time task. You can
create an SF dependency between the task you want scheduled just in time
(the predecessor) and its related task (the successor). Then if you update
the progress on the successor task, it won’t affect the scheduled dates of
the predecessor task.

Critical Activities for Test Estimation

Test scope document, test strategy, test conditions, test cases, test scripts,
traceability, and test execution/run plan are the critical activities for the

TEAM LinG



256

TEST PROJECT MANAGEMENT

test planning phase. The number of test iterations and the defect manage-
ment process are the critical activities during the execution stage.

The following describe each testing activity.

Test Scope Document

The scope of testing is derived from the various baseline documents avail-
able for testing the application. They may be the business requirement
document, functional specification document, technical design docu-
ments, use case documents, documentation manuals available for the
application, or even the application under test itself. While arriving at the
scope of testing the test manager should consider the various dependent
applications that are required to execute the tests such as interfaces
between the applications. Unless the scope of testing is complete one can-
not proceed with the estimation.

Test Strategy

A test strategy outlines what to plan and how to plan it. An unrealistic test
strategy leads to untested software whereas a successful strategy is a foun-
dation for testing success. A well-planned test strategy ensures a well-
designed product can be successfully released to achieve the fullest poten-
tial in the market. The purpose of the strategy is to clarify the major tasks
and approach toward the test project. It should be very clear, unambigu-
ous, and specific to the objective of the project.

Test strategies can cover a wide range of testing and business topics,
such as:

• Overview of the client, project, and application
• Scope of testing
• Inclusions, exclusions, and assumptions
• Plans for recruitment, team structure, and training
• Test management, metrics, and improvement
• Test environment, change control, and release strategy
• Test techniques, test data, and test planning
• Roles and responsibilities
• High-level schedule
• Entry and exit criteria
• Defect reporting and monitoring process
• Regression test processes
• Performance test analysis approach
• Test automation and test tool assessment
• Approaches to risk assessment, costs, and quality through the orga-

nization
• Sign-off criteria and analysis
• Deliverable of the test project

TEAM LinG



257

Test Estimation

Test Condition

A test condition is an abstract extraction of the testable requirements from
the baseline documents. Test conditions may be explicitly or implicitly in
the requirement documents. A test condition has one or more associated
test cases.

Test Case

A test case is a set of test inputs, execution conditions, and expected
results developed for a particular objective to validate a specific function-
ality in the application under test.

The intensity of the testing is proportional to the number of test cases.
Confidence in the quality of the product and test process is established
when the number of test cases increases. This is true because each test
case reflects different condition scenarios, or business flow, in the applica-
tion under test. Test cases form the foundation to design and develop test
scripts.

Test Script

A test script is the collection or set of related test cases arranged in the
execution flow for testing specific business functionality. A test script con-
tains the conditions tested, number of test cases tested in that script, pre-
requisites, test data required for the testing, and instructions to verify the
test results.

The advantage of test scripts is that someone other than the individual
who prepared the test script can execute it. This provides repeatability
and helps when additional resources are deployed during the peak period
of test execution.

Execution/Run Plan

The execution/run plan addresses the logical sequence and dependent
execution of the test scripts. For example, the batch run dates must be
matched with the business functionalities scripts before arriving at the
execution plan. This logical sequencing of test cases and scripts will help
the test project manager effectively monitor the execution progress. The
stakeholders can be updated on the status of the project.

Factors Affecting Test Estimation

While estimating the test effort, the test project manager should consider
the following factors:

• Stability and availability of the test environment
• The turnaround time for review of testware

TEAM LinG



258

TEST PROJECT MANAGEMENT

• Availability of required test professionals and their expertise levels
in functionality and testing concepts

• Timely delivery of the software for testing
• Availability of external and internal interfaces attached to the appli-

cation under testing
• The change management process planned for promoting code after

fixing the bugs
• Risk escalation and mitigation procedure
• The turnaround time for bug fixes
• Choice of automation tools and required expertise
• Timely completion of earlier test execution such as unit testing
• Number of stakeholders and the business issue resolution turn-

around time
• Availability of estimated test data

Test Planning Estimation

Of the various test planning activities, the effort required to prepare the
test conditions, test cases, test scripts, test data, and execution plan are
critical. There are other deliverables that consume the effort of the test
team such as traceability and the dependency matrix, resource allocations,
and other project management deliverables. Normally 15 percent of total
effort is spent by these activities but may vary depending upon the project.
The project manager should use prudence when deciding the time
required for all other activities and include them in the effort. Normally the
test conditions are prepared first and mapped with the business require-
ment documents to ensure coverage. The test conditions are further
extrapolated into test cases by establishing the various data values
required to extensively test a condition.

It is recommended that the entire application be decomposed into vari-
ous modules and subapplications and for each of those applications
testware developed (conditions, cases, scripts) that will help to scope out
the estimation effort. However, the test conditions and test cases do not
always require the same effort. Test conditions/cases are further drilled
down to complex, medium, and simple conditions/cases. Deciding the
complexity of the conditions/cases requires the collective wisdom, techni-
cal, and functional expertise of the entire project team.

The number of test conditions/cases, and the time to prepare each of
those complex, medium, and simple scripts constitute the major effort of
the test planning activity. Prudence of the project manager and the exper-
tise of the team will help to define the work effort. Sometimes, sample con-
ditions/cases or scripts will be created for a particular application and the
logic will be extended to other applications for estimating the effort.

TEAM LinG



259

Test Estimation

In addition to critical testware such as conditions, cases, and scripts,
the time required for other planning activities such as the preparation of
the test plan, strategy, review of each of these testware deliverables, and
preparation of the run/execution plan should be taken into consideration
to arrive at the cumulative test planning effort.

Test Execution and Controlling Effort

The execution test estimation effort is influenced by the following factors:

• The complexity level of the test conditions and cases
• The number of iterations planned
• The defect fix turnaround time agreed upon in the strategy
• Availability of the required data
• Number and type of batch runs planned (for mainframe applications)
• Defect management and resolution process
• Change management process

The test project manager should use judgment in defining the effort
required to execute the complex, medium, and simple scripts. The follow-
ing should also be estimated to determine the total effort required in the
execution phase:

• Number of test cycles planned for the test execution phase
• Number of interfaces with the application
• Number of batch runs

Test Result Analysis

This is another critical activity that is very important to ensure that the
software is developed according to the requirement specifications. The
functional test automation and script capturing activities referred to in the
project plan should be taken into consideration in the estimation. The test
automation approach, the number of iterations of load and stress testing,
and analysis effort of the test results also should be added to the effort.
This varies among the projects and the functional area of the project and
the experience level of the test managers and team.

Effort Estimation — Model Project

The following describes how to effectively use an estimation template.

The critical activities for effort estimation involving functional testing
are defined in the model. The time for each of these activities is arrived at
based on the parameters defined and the experiences from the project
team. Exhibit 23.3 shows the tasks with which the project manager, test
lead, and test engineer are typically associated.

TEAM LinG



260

TEST PROJECT MANAGEMENT

Test cases are classified as simple, medium, and complex based upon
the time preparation and execution times for these scripts. The baseline
times required by project management activities and other project-related
activities are estimated and entered into Exhibit 23.4.

Exhibit 23.5 shows the total effort for test planning, test execution, and
test closure activities separately for test engineers and test project manag-
ers. The total person days are calculated for each of these effort parame-
ters and total person months are calculated. Normally 22 working days are

Exhibit 23.3. Activities for Estimating Effort

Test Initiation and Planning Resources

Understanding the application PMaTLb

Training the rest of the team members/ambiguity review TEc TL

Project plan/test strategy PM

Test conditions/scenarios TE

Review of test conditions PM

Test cases TE

Test scripts TE

Internal review of test scripts PM

Preparation of coverage/trace matrix TE TL

Data requirements/guidelines TE TL

Preparation of run plan TL

Internal review of run plan PM

Sign off by business

Test Execution

Day 0 verification - environment check PM

Validation of test scripts with application TE TL

Iteration 1 (100 percent) (execution & defect review) TE TL

Iteration 2 (50 percent) (execution & defect review) TE TL

Iteration 3 (50 percent) (automation) TE PM

Test Closure

Final report preparation PM

Business review and sign off

a PM Project Manager
b TL Test Lead
c TE Test Engineer

TEAM LinG



261

Test Estimation

taken for a month to arrive at a person month. The exhibit also shows that
the total number of individuals required can be calculated, which is calcu-
lated from the person months. If the test execution schedule is already
defined in the overall milestone project plan, one can estimate the number
of resources required to complete the project within the given time.

Exhibit 23.4. Baseline Effort Estimation

Condition to Case

Simple 1

Medium 3

Complex 5

Buffer 20%

Case to Script

10 1

No. of Test Cases per Day

Planning Executionb

30 15

No. of Test Scripts per Day

Planning Executiona

2 1

 

Timelines

Day-Hr 8

Week-Day 5

Month-Day 22

Project Schedule

Planning Execution

35 25

Note: Project Baselines — Values can be changed
depending on the project requirements.

a Including bug/defect regression.

TEAM LinG



262

TEST PROJECT MANAGEMENT

The project team should establish the baseline as to how many test con-
ditions, test cases, and test scripts can be prepared and executed by the
individual tester per day. This is critical to this estimate and will differ on
different projects. Similarly, review activities should be calculated as a per-
centage of the critical activity for each of those activities.

The project management activity should take into consideration the
defect management process, daily defect meeting, conference calls, and
other meetings expected during the planning and execution stages of the
projects.

Exhibit 23.5. Total Effort and Number of Individuals Required

No. Resource

Test 
Planning / 
Scripting

Test 
Execution

Test 
Closure Total

(All Effort in Person Days)

1 Test Engineers     

2 Project Manager/Test Lead     

 Total Person Days     

 Total Person Months 60.0 30.0 10.0 100.0

 Ratio 60.0% 30.0% 10.0% 100.0%

Person Months (Only TE Effort) 0 0 0 0

Team Size 4 3 0 7

TEAM LinG



263

Part 24

Defect Monitoring 
and Management 
Process

A defect is a product anomaly. Any variation found in the product that does
not comply with business requirements or business is considered a defect.
No product can be produced with 100 percent perfection and the product
becomes completely acceptable to the business with the continuous
improvements over a period of time. Each product has its own life cycle.

During the software testing, the test engineers find various types of
defects that need to be reported to the owners of the system and the devel-
opers to carry out the required modifications. Defect reporting and track-
ing the defects to closure are important activities of the test project man-
agement. During the test execution phase a defect report (see Appendix
E12) is produced and circulated to the stakeholders and a defect meeting
is arranged periodically to evaluate the “correctness” of the defect so that
the developers can modify the code.

The test strategy document (see Appendix E16) specifies the defect
management process for the project. It clearly spells out the test engineer’s
behavior when she finds a defect in the system. Numerous defect manage-
ment tools are available for logging in and monitoring the defects. Some of
the popular defect management tools are described in Section V, Modern
Software Testing Tools.

Test engineers will prepare the defect log (see Appendix E9) noting
when they encountered errors while testing. This forms the basis for
recording the defects into the database. The test log documents a chrono-
logical record of the relevant details of each test. This includes the results
of the test, including discrepancies between the expected and actual
results.

The test report (see Appendix E12) summarizes the discrepancies in the
test log. It includes the expected and actual test results, the environment,
and so forth. In addition, relevant information to help isolate the problem

TEAM LinG



264

TEST PROJECT MANAGEMENT

is documented. Every defect must be analyzed and entered into the error
tracking system. Trend curves should be published periodically. There are
many ways to graphically illustrate the error trends.

Some trend curves are:

• Total errors found over time
• Errors found by cause, for example, operator and program errors
• Errors found by how found, for example, discovered by user
• Errors found by system, for example, order entry or accounts receiv-

able
• Errors found by the responsible organization, for example, support

group or operations

Exhibit 24.1 shows a graph of time versus the number of errors detected
and the predicted error rate provides an estimate of progress toward com-
pletion. This shows whether error correction is a bottleneck in the test pro-
cess. If so, additional development resources should be assigned. It also
demonstrates the difference between the predicted and actual error rates
relative to the total number of projected errors.

Defect Reporting

A defect report describes defects in software and documentation. Defect
reports have a priority rating that indicates the impact the problem has on
the customer. The defects will be reported using a standard format with
the following information:

• Defect number
• Defect date

Exhibit 24.1.  Defect Tracking

Estimate of Total Errors to Be Found

To
ta

l E
rr

o
rs

 F
o

u
n

d Predicted
Error Rate

Errors Found
Thus Far or

Errors
Corrected
Thus Far

Testing Effort (Time)

TEAM LinG



265

Defect Monitoring and Management Process

• Module ID
• Test case/script number
• Test case description
• Expected result
• Defect description
• Severity
• Defect category
• Status
• Responsibility
• Defect log
• Developer’s comment
• Tester’s comment
• Client comment

Defect Meetings

Defect meetings are the best way to disseminate information among the
testers, analysts, development, and the business.

• Meetings are conducted at the end of every day between the test
team and development team to discuss test execution and defects.
This is where the defects are formally categorized in terms of the
defect type and severity.

• Before the defect meetings with the development team, the test team
should have internal discussions with the test project manager on
the defects reported. This process ensures that all defects are accu-
rate and authentic to the best knowledge of the test team.

Defect Classifications

Defects that are detected by the tester are classified into categories based
upon the defect. This helps test planning and prioritization. The following
are some sample classifications that can vary depending upon the require-
ment of the project:

• Showstopper (X): The impact of the defect is severe and the system
cannot be tested without resolving the defect because an interim
solution may not be available.

• Critical (C): The impact of the defect is severe; however, an interim
solution is available. The defect should not hinder the test process
in any way.

• Noncritical (N): All defects that are not in the X or C category are
deemed to be in the N category. These are also the defects that
could potentially be resolved via documentation and user training.
These can be GUI defects or some minor field-level observations.
Exhibit 24.2 depicts the life cycle flow of the defects. A defect has
the initial state of “New” and eventually has a “Closed” state.

TEAM LinG



266

TEST PROJECT MANAGEMENT

Defect Priority

Priority is allotted to the defects either while entering them in the defect
reports or on the basis of the discussions in the defect meetings. The
scheduled defect fixing is based upon the assigned priority. The following
are the most common priority types:

• High: Further development and testing cannot occur until the defect
has been repaired. The software system cannot be used until the
repair is done.

• Medium: The defect must be resolved as soon as possible because
it is impairing development and testing activities. Software system
use will be severely affected until the defect is fixed.

• Low: The defect is an irritant that should be repaired but which can
be repaired after a more serious defect has been fixed.

Defect Category

Defects are categorized into different categories as per the testing strategy.
The following are the major categories of defects normally identified in a
testing project.

• Works as Intended (WAI): Test cases to be modified. This may arise
when the tester’s understanding may be incorrect.

Exhibit 24.2. Defect Life Cycle

New

Authorized

Duplicate

WAI (works
as needed)

Fixed

Closed

Reraised

TEAM LinG



267

Defect Monitoring and Management Process

• Discussion Items: Arises when there is a difference of opinion
between the test and the development team. This is marked to the
domain consultant for final verdict.

• Code Change: Arises when the development team has to fix the bug.
• Data Related: Arises when the defect is due to data and not coding.
• User Training: Arises when the defect is not severe or technically

not feasible to fix; it is decided to train the user on the defect. This
should ideally not be critical.

• New Requirement: Inclusion of functionality after discussion.
• User Maintenance: Masters and parameter maintained by the user

causing the defect.
• Observation: Any other observation not classified in the above cat-

egories such as a user-perspective GUI defect.

Defect Metrics

The analysis of the defects can be done based on the severity, occurrence,
and category of the defects. As an example, defect density is a metric,
which gives the ratio of defects in specific modules to the total defects in
the application. Further analysis and derivation of metrics can be done
based on the various components of the defect management.

• Defect Age: Defect age is the time duration between the points of
identification of the defect to the point of closure of the defect. This
would give a fair idea on the defect set to be included for smoke
test during regression.

• Defect Density: Defect density is usually calculated per thousand
source lines of code (KSLOC) as shown below. This can be helpful
in that a measure of defect density can be used to (1) predict the
remaining defects when compared to the expected defect density,
(2) determine if the amount of testing is sufficient, and (3) establish
a database of standard defect densities.

Dd = D/KLSOC

where

D = the number of defects,
KSLOC = the number of noncommented lines of source code 

(numbered per thousand), and 
Dd = the actual defect density.

When one plots defect density versus module size, the curve is typically
U-shaped and concaved upwards (see Exhibit 24.3). Very small and very
large modules are associated with more bugs than those of intermediate
size. A different way of viewing the same data is to plot lines of code per
module versus total bugs. The curve looks roughly logarithmic and then
flattens (corresponding to the minimum in the defect density curve), after

TEAM LinG



268

TEST PROJECT MANAGEMENT

which it goes up as the square of the number of the lines of code (which is
what one might intuitively expect for the whole curve, following Brooks’
Law).

Brooks’ Law predicts that adding programmers to a late project makes
it later. More generally, it predicts that costs and error rates rise as the
square of the number of programmers on a project.

The increasing incidence of bugs at small module sizes holds across a
wide variety of systems implemented in different languages and has been
demonstrated by different studies.

Exhibit 24.3. Defect Count and Density versus Module Size

D
ef

ec
t D

en
si

ty

Module Size
0 400 800

2

4

6

8

Observed Data

TEAM LinG



269

Part 25

Integrating Testing 
into Development 
Methodology
The following describes testing as a process rather than a life cycle phase.
A quality assurance department that treats testing as a life cycle phase
should integrate the concepts in this section into its current systems
development methodology. If the development methodology provides for
testing throughout the design and maintenance methodology but is not
well defined, the testing approach must be expanded and modified to cor-
respond to the process described in this manual.

Testing must be integrated into the systems development methodology.
Considered as a separate function, it may not receive the appropriate
resources and commitment. Testing as an integrated function, however,
prevents development from proceeding without testing.

The integration of testing into a systems development methodology is a
two-part process.

1. The testing steps and tasks are integrated into the systems devel-
opment methodology through addition or modification of tasks for
developmental personnel to perform during the creation of an appli-
cation system.

2. Defects must be recorded and captured. This process is the equiv-
alent of problem reporting in operational application systems. The
test manager must be able to capture information about the prob-
lems or defects that occur; without this information, it is difficult to
improve testing.

Usually, the person responsible for the systems development methodol-
ogy integrates testing into it. In many organizations, this is the quality
assurance function. If no one is directly responsible for the systems devel-
opment methodology, the test manager should assume responsibility for
testing integration. If testing, which can take half of the total development
effort, is not already an integral part of the current systems development
methodology, the integration process will be especially time consuming.

TEAM LinG



270

TEST PROJECT MANAGEMENT

Step 1. Organize the Test Team

Integrating a new test methodology is difficult and complex. Some organi-
zations habitually acquire new design methodologies without fully imple-
menting them, a problem that can be obviated with proper planning and
management direction.

To make a test design methodology work, a team of key people from the
testing function must be appointed to manage it. The group should be rep-
resentative of the overall testing staff and respected by their peers. The
test management team should consist of three to seven individuals. With
fewer than three members, the interaction and energy necessary to suc-
cessfully introduce the testing methodology may not occur. With more
than seven members, management of the team becomes unwieldy.

Whether a supervisor is appointed chairperson of the test management
team or a natural leader assumes the role, this individual ensures that the
team performs its mission.

The project sponsor should ensure that the test management team:

• Understands testing concepts and the standard for software testing
discussed in this manual

• Customizes and integrates the standard for software testing into the
organization’s systems design and maintenance methodology

• Encourages adherence to and support of the integrated test meth-
odology and agrees to perform testing in the manner prescribed by
the methodology

Management should not attempt to tell the test management team
which aspects of the software testing standard should be adopted, how
that testing methodology should be integrated into the existing design
methodology, or the amount of time and effort needed to perform the task.
Management should set the tone by stating that this manual will be the
basis for testing in the organization but that the test management team is
free to make necessary modifications.

Step 2. Identify Test Steps and Tasks to Integrate

Section III defines the steps and tasks involved in the client/server and
Internet testing methodology. These should be evaluated to determine
which are applicable to the organization and design methodology. Some of
the tasks may already be performed through design methodology, the test
management team may consider them unnecessary, or the team may
decide to combine tasks.

The test team must consider three areas when performing this step: the
test team must agree on the general objectives of testing, it must understand

TEAM LinG



271

Integrating Testing into Development Methodology

how the design methodology works, and it must understand the test meth-
odology presented in this manual.

Step 3. Customize Test Steps and Tasks

The material covered in this text may not be consistent with the method-
ology in place and will need customizing for each organization. The test
team can either perform this task itself or assign it to others (e.g., to the
group in charge of design methodology).

Customization usually includes the following:

• Standardizing vocabulary — Vocabulary should be consistent
throughout the design methodology. If staff members understand
and use the same vocabulary, they can easily move from job to job
within the organization. Vocabulary customization may mean chang-
ing vocabulary in the testing standard or integrating the testing
vocabulary into the systems development methodology.

• Changing the structure of presentation — The way the testing steps
and tasks have been described may differ from the way other parts
of the design methodology are presented. For example, this manual
has separate sections for forms and text descriptions of the software
testing tools. If the systems development methodology integrates
them into single units, they may need to be rearranged or reordered
to make them consistent with the development manual.

Customization can work two ways. The process can be customized for
individual application systems. During test planning, the test team would
determine which tasks, worksheets, and checklists are applicable to the
system being developed and then create a smaller version of the process
for test purposes. In addition, the processes described in this manual can
be customized for a particular development function. The test standards and
procedures can be customized to meet specific application and test needs.

Step 4. Select Integration Points

This step involves selecting points in development methodology to inte-
grate test steps and tasks. It requires a thorough understanding of the sys-
tems development methodology and the software tasks. The two key crite-
ria for determining where to integrate these tasks are:

1. What data is needed — The test task can be inserted into the design
methodology only after the point at which the needed information
has been produced.

2. Where the test products are needed — The testing tasks must be
completed before the products produced by that task are needed in
the systems development methodology.

TEAM LinG



272

TEST PROJECT MANAGEMENT

If these two rules are followed, both the earliest and latest points at
which the tasks can be performed can be determined. The tasks should be
inserted into the systems development methodology within this time-
frame.

Step 5. Modify the Development Methodology

This step generally involves inserting the material into the systems devel-
opment methodology documentation and must be performed by someone
familiar with the design process. All the information needed to modify the
systems development methodology is available at this point.

Step 6. Incorporate Defect Recording

An integral part of the tester’s workbench is the quality control function,
which can identify problems and defects uncovered through testing as well
as problems in the testing process itself. Appropriate recording and analy-
sis of these defects is essential to improving the testing process.

The next part of this section presents a procedure for defect recording
and analysis when the testing process is integrated into the systems devel-
opment methodology. This procedure requires categorizing defects and
ensuring that they are appropriately recorded throughout the systems
development methodology.

The most difficult part of defect recording is convincing development
staff members that this information will not be used against them. This
information is gathered strictly for the improvement of the test process
and should never be used for performance appraisals or any other individ-
ual evaluations.

Step 7. Train in Use of the Test Methodology

This step involves training analysts, users, and programmers in use of the
test methodology. Once testing is integrated into the systems development
methodology, people must be trained and motivated to use the test meth-
odology, a more difficult job.

Test management team members play a large role in convincing their
peers to accept and use the new methodology — first, by their example,
and second, by actively encouraging co-workers to adopt the methodol-
ogy. An important part of this step is creating and conducting testing sem-
inars, which should cover the following.

• Testing concepts and methods — This part of the training recaps the
material in Appendix F.

• Test standards — Individuals responsible for testing must know the
standards against which they are measured. The standards should

TEAM LinG



273

Integrating Testing into Development Methodology

be taught first, so team members know why they are performing
certain tasks (e.g., test procedures), and the procedures second. If
they feel that the procedures are just one way of testing, they may
decide there are better ways. On the other hand, if they know the
objective for performing the test procedures (e.g., meeting test stan-
dards), they are more likely to take an interest in learning and
following the test procedures.

• Test methodology — The methodology incorporated into the systems
development methodology should be taught step by step and task
by task. An analyst, user, or programmer should initially perform
tasks under the direction of an instructor. This helps ensure that
these professionals fully understand how the task should be per-
formed and what results should be expected.

Until the individuals responsible for testing have been trained and have
demonstrated proficiency in testing processes, management should allow
for some testing errors. In addition, until individuals have demonstrated
mastery of the test procedures, they should be closely supervised during
the execution of those procedures.

TEAM LinG



TEAM LinG



275

Part 26

On-Site/Offshore 
Model

Outsourcing has emerged as a major concept in the twentieth century
when a number of manufacturing products were outsourced to China and
Japan. The offshore outsourcing model can provide exceptional value to
clients. Increased efficiency and cost advantage are the two basic elements
that increased the outsourcing approach. When the IT boom evaporated
and companies were forced to maintain their profit line, they often turned
toward outsourcing the development, testing, and maintenance to Asian
countries such as India, China, and others. This part analyzes the impor-
tant elements of outsourcing with special reference to testing and the
advantages and issues involved.

Step 1: Analysis

The corporate management should evaluate the outsourcing option. It
should not be chosen as a matter of routine. A committee should carefully
study the following aspects and submit a report to the senior management
to make a decision on outsourcing.

Some questions that need to be answered include the following:

• Could the products, development, testing, or maintenance be out-
sourced?

• What part of the project can be outsourced?
• What experiences have other companies had with outsourcing?
• Are there options available for the line of product in the market?
• What are the short-term versus long-term business processes to

outsource?
• Are the requirements defined beforehand?
• Are there frequent changes in the business process outsourcing

project?
• Does the client’s location have the extra capacity it needs?

Management should analyze the report and only if the product could be
successfully outsourced with substantial benefits to the company, should
the decision to outsource be taken.

TEAM LinG



276

TEST PROJECT MANAGEMENT

Step 2: Determine the Economic Tradeoffs

Once the management has decided to outsource the product, then the cost
benefit should be analyzed. This can be done in two ways:

1. The advantages of outsourcing in terms of cost within the same
geographical location

2. The advantages of outsourcing among the various competitive out-
sourcing clients available

Corporate should not be carried out by the cost-benefit studies pro-
jected by the vendors for outsourcing (see the Vendor Evaluation Excel
spreadsheet on the CD). These studies are prepared with the primary
objective of capturing the new business and have not taken into consider-
ation various recurring and unexpected project costs that normally form
part of any project operation. Eighty percent cost saving may land a com-
pany in the hands of a vendor who is not going to deliver a quality product
within the schedule. Normally studies show that around 40 to 50 percent
benefits are accrued to the outsourcing firms in terms of cost. Any projec-
tions above this industry average are questionable.

Step 3: Determine the Selection Criteria

Outsourcing projects have become catastrophic to many firms in the
United States for the simple reason that the selection process adopted by
them is wrong.

• Geographical Locations: The location of the outsourcing country is
a primary factor in selecting the outsourcing partner. Countries such
as India and China get more outsourcing projects due to their geo-
graphical locations because the 24/7 work atmosphere can be
ensured in these locations. When U.S. companies start their opera-
tions in the morning, already two rounds of shifts are completed for
the requirement sent to these locations as of the earlier day. The
project management group often finds it very difficult to evaluate
the output received as the day begins.

• Optimum Utilization: Maximum usage of the previous hardware
resources can be done if the work is carried out 24/7 in different
locations. Because the globe is interconnected by IPLC and VPN the
load to the server can be distributed across locations and maximum
utilization of the servers can be ensured.

• Quality Deliverables: The quality of the deliverables from the vendor
should be evaluated with respect to compliance with the company’s
quality standards and with respect to international quality standards.

Project Management and Monitoring

The success of the on-site/offshore model depends on the effective project
management practices adopted by the companies. Several companies that

TEAM LinG



277

On-Site/Offshore Model

have started experimenting with this model have retracted. Unless clear
project management practices defining the clear role and responsibilities
for the on-site/offshore team are established, this model is bound to face
several obstacles in delivering the expected cost advantages results.

Outsourcing Methodology

Having decided to outsource, the outsourcing methodology should clearly
define what to outsource, and what cannot be outsourced. Potential activ-
ities that can be outsourced are shown in Exhibit 26.1. Of course, the activ-
ities depend heavily on (1) completeness of the requirements, (2) effective-
ness of communication, (3) whether the project supports operating
effectiveness or strategy, and (4) the existence of well-defined project man-
agement, quality assurance, and development.

Operational effectiveness is concerned with working cheaper or faster.
Strategy is about the creation of a long-term competitive advantage for the
business. An application development that is strategic and has multiple
spiral iterations should not outsource the entire development and quality
assurance activities. An application development project with well-defined
requirements and that will not have a major effect on the operations of the
business is a candidate for offshoring.

Exhibit 26.1. Onshore versus Offshore Activities

On-site

Feasibility
Study

Requirement
Analysis

Outsourcing
Management

Process

Acceptance
Criteria

Business
Implementation

Offshore

Software
Development

System and
Detailed
Design

Quality
Assurance and

Software Testing

Application
Maintenance
and Support

Training and
Documentation

TEAM LinG



278

TEST PROJECT MANAGEMENT

On-Site Activities

• The initial feasibility study to decide whether the particular devel-
opment/testing/maintenance can be outsourced should be decided
by the corporation. This part of the activity cannot be outsourced
for obvious reasons. As indicated, this study will result in a cost-
benefit analysis and the advantages and disadvantages of outsourcing.

• When a decision has been made to outsource a part or whole pro-
cess, a requirement analysis should be conducted to decide the
portion of the business process that is to be outsourced. These
requirements should be clearly documented so that there is no
communication gap between the management and the vendor on
the deliveries and expectations.

• The project management process for effectively monitoring and
managing the outsourced projects should be established. The roles
and responsibilities of the on-site coordinator and offshore team
should be clearly documented. This should be decided by the man-
agement and should be signed off by both senior management and
the vendor.

• The acceptance criteria for the outsourced projects should be
clearly documented. The acceptance test should be conducted by
the corporation by involving the actual end users or business ana-
lysts. Proper guidelines and checklists should be created for the
acceptance criteria.

• The business implementation should be done by the corporate office
as this cannot be outsourced.

Offshore Activities

The following are potential activities that could be outsourced:

• Development — Outsourcing a software development process is the
major activity that has been stabilized across the globe. A number
of software companies have emerged in regions such as India, China,
and Southeast Asia, which lends effective support to businesses in
the United Kingdom and the United States in the development of
software code. These companies accredit themselves with the inter-
national standards such as CMM, ISO, and other international audit-
ing standards, and a clear process is established for verifying the
deliverables.

• High-Level and Detailed Design — With the English-speaking knowl-
edge gained by these countries and the globe shrinking with net-
works, in addition to software development, other related activities
such as high-level design and system design could be transferred
offshore.

TEAM LinG



279

On-Site/Offshore Model

• System Testing — With the global connectivity made simple and low-
cost, many of the testing activities can be outsourced. Because
system design and development happens in the offshore develop-
ment centers, system testing can be performed offshore. This
reduces the cost of hiring additional resources at higher cost or
dislocating the business resources from their routines that will indi-
rectly affect business growth.

• Quality Assurance — Quality assurance and software testing are
other important activities that are candidates for outsourcing by
U.S. companies. This directly relates to the outsourcing of develop-
ment and other related activities. Most of these vendors have devel-
oped expertise in modern software testing tools and they execute
these automated test scripts from anywhere across the globe.

• Support and Maintenance — Application maintenance and support
(AMS) is another critical activity that is extended to the long-term
outsourcing of maintenance and support of the critical applications.
Many of the call centers for critical applications such as U.K. Rail-
ways and major telecom companies have already been moved to
countries such as India.

• Follow-Up Activities — Any other related documentation work that
relates to software development, design development, or quality
assurance related documentation can easily be outsourced by the
standards for the deliverables. With the development of Internet
tools such as Placeware even training can be outsourced to remote
locations.

Implementing the On-Site/Offshore Model

Once it has been decided to outsource after the initial analysis phase, the
outsourcing should be managed in a phased manner. Phasing should not
affect the existing business activities of the company. The following five
phases define the process.

Knowledge Transfer

In this phase the offshore core team can visit the client site to understand
the application that is to be outsourced. These technical and business
resources communicate with the existing resources and internalize the
functional, technical, and operational aspects of the applications. Nor-
mally a client coordinator is nominated to plan, act as liaison, monitor, and
evaluate knowledge transfer sessions with active participation. The off-
shore team makes a reverse presentation of its understanding of the sys-
tem so that the client coordinator is convinced of its knowledge acquisi-
tion. This team will prepare training and other documentation to be passed
on to the rest of the offshore team.

TEAM LinG



280

TEST PROJECT MANAGEMENT

This team will consist of identified process experts from the outsourced
company who will document the process followed up at the client place.
These processes will be integrated or aligned with the vendor’s process so
that the client deliverables will pass any audit requirements.

Detailed Design

Once the initial knowledge acquisition phase is complete, the technical
team will prepare a detailed design for the hardware, software, and connec-
tivity requirements, which are to be in place to start the operations from
the remote locations. The technical team from the client side will authenti-
cate this design. Once the technical detail area is approved, the infrastruc-
ture team at the remote location will start to put the environment in place.
The client’s server and applications will be connected and a sanity test will
be performed for verification.

The business analyst team will prepare a migration plan for migrating
the planned activities such as development, testing, design, or mainte-
nance in a phased manner.

Milestone-Based Transfer

The on-site/offshore transition process provides a framework to shift the
responsibility of development, testing, design, support, and maintenance
from on-site to offshore with a step-by-step methodology without affecting
the normal business of the client. Key milestones for smooth transfer are:

• Establish the environment at the offshore location.
• Train the offshore resources.
• Plan the move of the identified activities in a phased manner.
• Obtain offshore stability.

Steady State

Over a period of time the offshore environment will be stabilized and the
deliverables will start flowing to the on-site location with the anticipation
of improved quality and less cost. This is a critical period where the on-site
project management activities should meticulously review the deliver-
ables and have conference calls to clarify issues with the offshore team and
other related activities. Once this steady state is achieved, the model is
established.

Application Management

Once the design, development, and testing are completed and the product
has gone live, the further enhancements (new requirements arising out of
business necessity will require change in the code) need ongoing mainte-
nance. Moreover, during the normal business cycle jobs such as data

TEAM LinG



281

On-Site/Offshore Model

backup and purging the data can be outsourced. As the vendor companies
have specialized in the business domain they will be in a better position to
offer these services on a long-term basis at low cost.

Ideally, 20 to 30 percent of work is done on-site whereas 70 to 80 percent
is outsourced offshore depending upon the criticality of the project. Usu-
ally, requirement analysis, development of detailed specifications, critical
support, and implementation are performed on-site, and development and
testing are outsourced offshore.

The following are the important prerequisites for an effective on-site/off-
shore model to deliver the desired results.

Relationship Model

The relationship model should be established for a successful on-site/off-
shore model. Exhibit 26.2 shows a higher-level relationship model. The
roles and responsibilities for the project manager at the on-site client, on-site
vendor coordinator, and the offshore team should be clearly established.

Some of the generic responsibilities of the on-site client team are:

• Initiate and participate in all status conference calls.
• Coordinate and provide information on testing requirements.
• Provide clarifications and other required data to the offshore team.
• Review offshore test deliverables and sign off on the quality of

deliverables.
• Single point of contact with offshore project manager in obtaining

clarifications raised and in obtaining approvals on the test deliver-
ables from the respective on-site departments and divisions.

• Establish and maintain optimal turnaround time for clarifications
and deliverables.

Exhibit 26.2. Relationship Model

On-site Offshore

On-site Project
Management and
Acceptance Team

On-site
Coordinator

Offshore
Team

Client Development
and Test Systems

Client Production
Systems

Offshore
Development/
Testing Center

TEAM LinG



282

TEST PROJECT MANAGEMENT

• Approve timesheets.
• Finalize timelines/schedule for test projects in consultation with

offshore project manager.
• Prepare and publish daily status of all test requests outstanding to

all parties.
• Proactively inform requests and/or any other changes that will affect

the deliverables of the offshore team.

The following are some of the generic responsibilities of the on-site cli-
ent team:

• A single point of contact for the offshore team in interacting with
the on-site coordinator should be identified.

• The time and methodology for the daily/weekly/monthly status
review calls should be decided and the participants in these various
calls should be decided at the various stakeholders’ levels.

• Overall project management activities and the review process
should be defined.

• A weekly/monthly status report formats and contents should be
decided.

• Prepare and publish daily progress through status report.
• Follow up with on-site coordinator in getting clarifications for issues

raised by development/test team engineers.
• Support and review process for all project-related test documents

and deliverables prepared by development and test team engineers.
• Allocate test projects to offshore test engineers.
• Identify resources for ramp-up and ramp-down depending upon the

project requirements at the various phases of the project.
• Prepare a project plan and strategy document.
• Convey project-related issues proactively to on-site coordinator.
• Responsibility for the quality of the deliverables should be explicitly

declared.
• Responsibility for project tracking and project control should be

decided.
• Conduct daily and weekly team meetings.
• Collect timesheets and submit them to on-site coordinator.
• Finalize timelines/schedule for test projects in consultation with on-

site coordinator.

Standards. Several companies that have experimented with on-site/off-
shore models to minimize their costs ended up in failure primarily due to
the incompatibility between the sourced and vendor companies on the
technical standards and guidelines. Although the companies are CMM
compliant and ISO certified, there are vast differences in the standards
adopted between them for the same source code or testing methodology.

TEAM LinG



283

On-Site/Offshore Model

The standards of the vendor companies should be evaluated and syn-
chronized with the standards and guidelines of the outsourced company.
The development standards, testing methodology, test automation stan-
dards, documentation standards, and training scope should be clearly
defined in advance. Apart from the above the following should also be
taken into consideration:

• Request for proposals (RFP) procedures
• Change request procedures
• Configuration management 
• Tools
• Acceptance criteria

The acceptance criterion is another critical factor that should be
defined with the help of the end users who are going to ultimately use the
system. If the standards and deliverables are not acceptable to them, the
project is going to fail in the implementation phase.

Benefits of On-Site/Offshore Methodology

The ultimate advantages of this model are time, money and communica-
tion. The case study described in Exhibit 26.3 demonstrates the ideal usage
of the offshore model and describes the approximate benefit that compa-
nies may reap with the onshore/offshore model. The statistics may vary
from 80 to 40 percent.

In the case study the XYZ Company will have the following parameters
to evaluate in this analysis (critical success factors are noted in parame-
ters 10 to 13).

1. The number of resources required — five.
2. The schedule for the project is assumed as six months.
3. The cost of on-site existing resources is assumed as $80 per hour.
4. Eight hours per day and 22 days per month are considered.
5. Offshore resources are considered at the cost of $150 per day.
6. Initial knowledge transfer is assumed for two weeks at nominal cost.
7. Project management cost is estimated at 30 percent of the resource

cost.
8. Initial environment establishment cost and subsequent maintenance

cost are assumed nominally.
9. A percentage of recurring administrative cost included.

10. The requirements have undergone complete reviews and sign-offs
by the stakeholders.

11. Communication has been established by the project manager and
is excellent.

12. The project is nonstrategic to the business.
13. Complete standards have been established and documented.

TEAM LinG



284

TEST PROJECT MANAGEMENT

Although it can be quite difficult to satisfy the expected demand for IT
resources in Western countries, it is a completely different scenario in
countries such as China and India where there are many available program-
mers with good academic backgrounds. These vendor companies typically
possess an extensive, highly specialized knowledge base.

Another advantage is that we can engage these resources only for
required times, not on a continuous basis. This has the potential of sub-
stantially saving the cost for the company.

Due to the geographic location advantage of these regions, 24/7 service
can be achieved from these vendors. The service requests that are sent at
the end of the day in the United States are delivered at the beginning of the
next working day.

Exhibit 26.3. Example Cost-Benefit Case Study: 
Cost-Benefit Analysis between On-Site and Offshore Models

# Description Rate
No. of 
Days

No. of 
Persons Total

(Amounts in $)

1 On-site for 5 persons for 6 months 640 132 5 422,400

PM effort 30 percent on the total cost 126,720

Total 549,120

2 Offshore for 5 persons for 6 months 150 132 5 99,000

PM effort 30 percent on the total cost 29,700

Knowledge transfer at on-site 25,000

Initial network connectivity 50,000

Recurring cost (maintenance) 5,000

Communication expenses 5,000

Administrative expenses 5,000

Total 218,700

Analysis: 

Diff. between on-site and offshore 330,420

Percent gain on offshore model over on-site model 60.17

TEAM LinG



285

On-Site/Offshore Model

Most of these outsourcing companies are CMM level 5 with ISO certifica-
tion with established processes and procedures that enhance the quality
of deliverables.

The onshore/offshore model can enable the organizations to concen-
trate on their core business, carry out business reengineering, and provide
information that is valid, timely, and adequate to assist decision making at
the top management level and quality and cost control at the middle and
lower levels

On-Site/Offshore Model Challenges

Out of Sight. The offshore team being out of sight intensifies the fear of
loss of control for the on-site project managers. This can be overcome by
visiting the stakeholders and vendors’ facilities offshore to provide confi-
dence. Second, the established processes, methodologies, and tools,
which are industry standard due to CMM and ISO and IEEE standards, will
provide additional confidence.

Establish Transparency. The vendors should provide the clients with
complete transparency and allow them to actively participate in recruiting
offshore resources and utilizing their own resources on-site, as they deem
appropriate.

Security Considerations. The security considerations on the secrecy of
the data can be overcome by a dedicated network set up exclusively for the
client.

Project Monitoring. The failure of established project management
practices can be attended to by tailoring the project management prac-
tices to suit to the requirements of the clients.

Management Overhead. When the overall cost benefit is evaluated, the
management overhead, which will be additional, expenses using this
model can be substantial.

Cultural Differences. Although fluency in the English language is con-
sidered an advantage for outsourcing most of the projects from Europe and
the United States, the cultural differences can create difficulties. However,
individuals tend to adapt to the different cultural environment very
quickly.

Software Licensing. This is another problem relating to global licensing
or regional restrictions on the use of software licenses that needs to be
dealt with on a case-to-case basis.

TEAM LinG



286

TEST PROJECT MANAGEMENT

The Future of Onshore/Offshore

Several companies have attempted and are attempting the onshore/
offshore model in order to reduce IT costs. Although the cost savings are
clear, the quality of the offshore deliverables depends heavily on the clear
onshore project management practices and standards. Even though there
is a 24/7 work paradigm because of the overlap in global time differences,
communication and cultural adjustments are critical success factors.

However, offshoring is a mistake when technology companies confuse
operating effectiveness and strategy. Operational effectiveness is con-
cerned with working cheaper or faster. Strategy is about the creation of a
long-term competitive advantage, which for technology companies is usu-
ally the ability to create innovative software.

Outsourcing developers and quality assurance testing is feasible when
the software developed isn’t a key part of the pipeline of innovation for
products a company actually sells. For example, when Web site design or
back-office software such as accounts payable or inventory control is out-
sourced, that can be an effective approach because it improves opera-
tional effectiveness. But writing and testing innovative software cannot be
produced on an assembly line. It requires hard-to-find development,
design, and testing skills. Farming out development efforts overseas will
not create a competitive advantage. When a technology company out-
sources a majority of its software development, that company may lose its
capacity to be innovative and grow its competitive edge.

TEAM LinG



Section V
Modern 
Software 

Testing Tools

TEAM LinG



288

TEST PROJECT MANAGEMENT

Test automation can add a lot of complexity and cost to a test team’s effort.
However, it can also provide some valuable assistance if it is done by the
right people, with the right training and right environment, and in the right
context.

The flourish of automated testing tools is a reaction to advances in Web-
based, client/server, and mainframe software development tools that have
enabled developers to build applications rapidly and with increased func-
tionality. Testing departments must cope with software that is dramatically
improved, but increasingly complex. New testing tools have been devel-
oped to aid in the quality assurance process.

The objectives of this section are to:

• Describe a brief history of software testing.
• Describe the evolution of automated software testing tools.
• Describe futuristic testing tools.
• Describe when a testing tool is useful.
• Describe when not to use a testing tool.
• Provide a testing tool selection checklist.
• Discuss types of testing tools.
• Provide descriptions of modern and popular testing tools.
• Describe a methodology to evaluate testing tools.

MODERN SOFTWARE TESTING TOOLS

TEAM LinG



289

Part 27

A Brief History 
of Software Testing

Modern testing tools are becoming more and more advanced and user-
friendly. The following discussion describes how the software testing activ-
ity has evolved, and is evolving, over time. This sets the perspective on
where automated testing tools are going.

Software testing is the activity of running a series of dynamic executions
of software programs that occurs after the software source code has been
developed. It is performed to uncover and correct as many of the potential
errors as possible before delivery to the customer. As pointed out earlier,
software testing is still an “art.” It can be considered a risk management
technique; the quality assurance technique, for example, represents the
last defense to correct deviations from errors in the specification, design,
or code.

Throughout the history of software development, there have been many
definitions and advances in software testing. Exhibit 27.1 graphically illus-
trates these evolutions. In the 1950s, software testing was defined as “what
programmers did to find bugs in their programs.” In the early 1960s the def-
inition of testing underwent a revision. Consideration was given to exhaus-
tive testing of the software in terms of the possible paths through the code,
or total enumeration of the possible input data variations. It was noted that
it was impossible to completely test an application because: (1) the
domain of program inputs is too large, (2) there are too many possible
input paths, and (3) design and specification issues are difficult to test.
Because of the above points, exhaustive testing was discounted and found
to be theoretically impossible.

As software development matured through the 1960s and 1970s the
activity of software development was referred to as “computer science.”
Software testing was defined as “what is done to demonstrate correctness
of a program” or as “the process of establishing confidence that a program
or system does what it is supposed to do” in the early 1970s. A short-lived
computer science technique that was proposed during the specification,
design, and implementation of a software system was software verification
through “correctness proof.” Although this concept was theoretically

TEAM LinG



290

MODERN SOFTWARE TESTING TOOLS

promising, in practice it was too time consuming and insufficient. For sim-
ple tests, it was easy to show that the software “works” and prove that it
will theoretically work. However, because most of the software was not
tested using this approach, a large number of defects remained to be dis-
covered during actual implementation. It was soon concluded that “proof
of correctness” was an inefficient method of software testing. However,
even today there is still a need for correctness demonstrations, such as
acceptance testing, as described in various sections of this book.

In the late 1970s it was stated that testing is a process of executing a pro-
gram with the intent of finding an error, not proving that it works. He
emphasized that a good test case is one that has a high probability of find-
ing an as-yet-undiscovered error. A successful test is one that uncovers an
as-yet-undiscovered error. This approach was the exact opposite up to this
point.

The above two definitions of testing present (prove that it works versus
prove that it does not work) the “testing paradox” with two underlying and
contradictory objectives:

1. To give confidence that the product is working well
2. To uncover errors in the software product before its delivery to the

customer (or the next state of development)

If the first objective is to prove that a program works. It was determined
that “we shall subconsciously be steered toward this goal; that is, we shall
tend to select test data that have a low probability of causing the program
to fail.”

If the second objective is to uncover errors in the software product, how
can there be confidence that the product is working well, inasmuch as it
was just proved that it is, in fact, not working! Today it has been widely

Exhibit 27.1. History of Software Testing

1950 1960 1970 1980 1990 2000

Time

Fix Bugs Exhaustive
Testing

Prove
it Works

Prove it
Does Not

Work

Defect
Prevention
& Test
Process

Early
Test

Design

Internet
(Agile)

Test
Automation

Tools

Advanced
Test

Automation

Automated
Business

Optimization

TEAM LinG



291

A Brief History of Software Testing

accepted by good testers that the second objective is more productive
than the first objective, for if one accepts the first objective, the tester will
subconsciously ignore defects trying to prove that a program works.

The following good testing principles were proposed:

• A necessary part of a test case is a definition of the expected output
or result.

• Programmers should avoid attempting to test their own programs.
• A programming organization should not test its own programs.
• Thoroughly inspect the results of each test.
• Test cases must be written for invalid and unexpected, as well as

valid and expected, input conditions.
• Examining a program to see if it does not do what it is supposed to

do is only half the battle. The other half is seeing whether the
program does what it is not supposed to do.

• Avoid throwaway test cases unless the program is truly a throwaway
program.

• Do not plan a testing effort under the tacit assumption that no errors
will be found.

• The probability of the existence of more errors in a section of a
program is proportional to the number of errors already found in
that section.

The 1980s saw the definition of testing extended to include defect pre-
vention. Designing tests is one of the most effective bug prevention tech-
niques known. It was suggested that a testing methodology was required,
specifically that testing must include reviews throughout the entire soft-
ware development life cycle and that it should be a managed process. He
promoted the importance of testing not just a program but the require-
ments, design, code, tests themselves, and the program.

“Testing” traditionally (up until the early 1980s) referred to what was
done to a system once working code was delivered (now often referred to
as system testing), however, testing today is “greater testing,” where a
tester should be involved in almost any aspect of the software develop-
ment life cycle. Once code is delivered to testing, it can be tested and
checked but if anything is wrong the previous development phases have to
be investigated. If the error was caused by a design ambiguity, or a pro-
grammer oversight, it is simpler to try to find the problems as soon as they
occur, not wait until an actual working product is produced. Studies have
shown that about 50 percent of bugs are created at the requirements (what
do we want the software to do?) or design stages, and these can have a
compounding effect and create more bugs during coding. The earlier a bug
or issue is found in the life cycle, the cheaper it is to fix (by exponential
amounts). Rather than test a program and look for bugs in it, requirements
or designs can be rigorously reviewed. Unfortunately, even today, many

TEAM LinG



292

MODERN SOFTWARE TESTING TOOLS

software development organizations believe that software testing is a back-
end activity.

In the mid-1980s automated testing tools emerged to automate the man-
ual testing effort to improve the efficiency and quality of the target applica-
tion. It was anticipated that the computer could do more tests of a program
than a human could do manually and do them more reliably. These tools
were initially fairly primitive and did not have advanced scripting language
facilities (see the following section, Evolution of Automated Testing Tools,
for more details).

In the early 1990s the power of early test design was recognized. Testing
was redefined to be “the planning, design, building, maintaining and exe-
cuting tests and test environments.” This was a quality assurance perspec-
tive of testing that assumes that good testing is a managed process, a total
life cycle concern with testability.

Also in the early 1990s more advanced capture/replay testing tools
offered rich scripting languages and reporting facilities. Test management
tools helped manage all the artifacts from requirements and test design, to
test scripts and test defects. Also, commercially available performance
tools arrived to test the performance of the system. These tools tested
stress and load-tested the target system to determine their breaking
points. This was facilitated by capacity planning.

Although the concept of a test as a process throughout the entire soft-
ware development life cycle has persisted, in the mid-1990s with the popu-
larity of the Internet, software was often developed without a specific test-
ing standard model, making it much more difficult to test. Just as
documents could be reviewed without specifically defining each expected
result of each step of the review, so could tests be performed without
explicitly defining everything that had to be tested in advance. Testing
approaches to this problem are known as “agile testing.” The techniques
include exploratory testing, rapid testing, and risk-based testing.

In the early 2000s Mercury Interactive introduced an even broader defi-
nition of testing when they introduced the concept of business technology
optimization (BTO). BTO aligns the IT strategy and execution with busi-
ness goals. It helps govern the priorities, people, and processes of IT. The
basic approach is to measure and maximize value across the IT service
delivery life cycle to ensure applications meet quality, performance, and
availability goals. Topaz is an interactive digital cockpit that reveals vital
business availability information in real-time to help IT and business exec-
utives prioritize IT operations and maximize business results. It provides
end-to-end visibility into business availability by presenting key business
process indicators in real-time, as well as their mapping to the underlying
IT infrastructure.

TEAM LinG



293

A Brief History of Software Testing

Evolution of Automated Testing Tools

Test automation started in the mid-1980s with the emergence of automated
capture/replay tools. A capture/replay tool enables testers to record inter-
action scenarios. Such tools record every keystroke, mouse movement,
and response that was sent to the screen during the scenario. Later, the
tester may replay the recorded scenarios. The capture/replay tool auto-
matically notes any discrepancies in the expected results. Such tools
improved testing efficiency and productivity by reducing the manual test-
ing efforts.

Linda Hayes, a pioneer in test automation, points out that the cost justi-
fication for test automation is simple and can be expressed in a single
exhibit (Exhibit 27.2). As this exhibit suggests, over time the number of
functional features for a particular application increases due to changes
and improvements to the business operations that use the software. Unfor-
tunately, the number of people and the amount of time invested in testing
each new release either remain flat or may even decline. As a result, the
test coverage steadily decreases, which increases the risk of failure, trans-
lating to potential business losses.

For example, if the development organization adds application enhance-
ments equal to 10 percent of the existing code, this means that the test
effort is now 110 percent as great as it was before. Because no organization
budgets more time and resources for testing than they do for development,
it is literally impossible for testers to keep up.

This is why applications that have been in production for years often expe-
rience failures. When test resources and time can’t keep pace, decisions must
be made to omit the testing of some functional features. Typically, the newest
features are targeted because the oldest ones are assumed to still work. How-
ever, because changes in one area often have an unintended impact on other
areas, this assumption may not be true. Ironically, the greatest risk is in the

Exhibit 27.2. Motivation for Test Automation (From “Why Automate,” Linda 
Hayes, Worksoft, Inc white paper, 2002, www.worksoft.com. With permission.)

GapFeatures

Tests

Time

TEAM LinG



294

MODERN SOFTWARE TESTING TOOLS

existing features, not the new ones, for the simple reason that they are
already being used.

Test automation is the only way to resolve this dilemma. By continually
adding new tests for new features to a library of automated tests for exist-
ing features, the test library can track the application functionality.

The cost of failure is also on the rise. Whereas in past decades software
was primarily found in back-office applications, today software is a com-
petitive weapon that differentiates many companies from their competi-
tors and forms the backbone of critical operations. Examples abound of
errors in the tens or hundreds of millions — even billions — of dollars in
losses due to undetected software errors. Exacerbating the increasing risk
is the decreasing cycle times. Product cycles have compressed from years
into months, weeks, or even days. In these tight timeframes, it is virtually
impossible to achieve acceptable coverage with manual testing.

Capture/replay automated tools have undergone a series of staged
improvements. The evolutionary improvements are described below.

Static Capture/Replay Tools (without Scripting Language)

With these early tools, tests were performed manually and the inputs and
outputs were captured in the background. During subsequent automated
playback, the script repeated the same sequence of actions to apply the
inputs and compare the actual responses to the captured results. Differ-
ences were reported as errors. The GUI menus, radio buttons, list boxes,
and text were stored in the script. With this approach the flexibility of
changes to the GUI was limited. The scripts resulting from this method con-
tained hard-coded values that must change if anything at all changed in the
application. The costs associated with maintaining such scripts were
astronomical, and unacceptable. These scripts were not reliable, even if
the application had not changed, and often failed on replay (pop-up win-
dows, messages, and other things can happen that did not happen when
the test was recorded). If the tester made an error entering data, the test
had to be rerecorded. If the application changed, the test had to be rere-
corded.

Static Capture/Replay Tools (with Scripting Language)

The next generation of automated testing tools introduced scripting lan-
guages. Now the test script was a program. Scripting languages were
needed to handle conditions, exceptions, and the increased complexity of
software. Automated script development, to be effective, had to be subject
to the same rules and standards that were applied to software develop-
ment. Making effective use of any automated test tool required at least one
trained, technical person — in other words, a programmer.

TEAM LinG



295

A Brief History of Software Testing

Variable Capture/Replay Tools

The next generation of automated testing tools introduced added variable
test data to be used in conjunction with the capture/replay features. The
difference between static capture/replay and variable is that in the former
case the inputs and outputs are fixed, whereas in the latter the inputs and
outputs are variable. This is accomplished by performing the testing man-
ually, and then replacing the captured inputs and expected outputs with
variables whose corresponding values are stored in data files external to
the script. Variable capture/replay is available from most testing tools that
use a script language with variable data capability. Variable capture/replay
and extended methodologies reduce the risk of not performing regression
testing on existing features and improving the productivity of the testing
process.

However, the problem with variable capture/replay tools is that they
still require a scripting language that needs to be programmed. However,
just as development programming techniques improved, new scripting
techniques emerged. The following describes two of these techniques.

Functional Decomposition Approach. The focus of the functional decompo-
sition script development approach (like structured software develop-
ment) is to reduce all test cases to their most fundamental tasks, and write
the following types of scripts: user-defined functions, business function
scripts, and “subroutine” or “utility” scripts that perform these tasks inde-
pendently of each other. Some examples of what these scripts do include:

• Navigation (e.g., “Access Loan Screen from Main Menu”)
• Specific (Business) Function (e.g., “Post a Loan”)
• Data Verification (e.g., “Verify Loan Payment Updates Current Bal-

ance”)
• Return Navigation (e.g., “Return to Main Menu”)

In order to effectively use these scripts, it is necessary to separate data
from function. This allows an automated test script to be written for a busi-
ness function, using data files to provide both the input and the expected-
results verification. A hierarchical architecture is employed, using a struc-
tured or modular design.

The highest level is the driver script, which is the engine of the test. The
driver script contains a series of calls to one or more “test case” scripts.
The test case scripts contain the test case logic, calling the business func-
tion scripts necessary to perform the application testing. Utility scripts
and functions are called as needed by drivers, main, and business function
scripts. These are described in more detail below.

• Driver Scripts: Perform initialization (if required); then call the test
case scripts in the desired order.

TEAM LinG



296

MODERN SOFTWARE TESTING TOOLS

• Test Case Scripts: Perform the application test case logic using busi-
ness function scripts.

• Business Function Scripts: Perform specific business functions within
the application.

• Subroutine Scripts: Perform application-specific tasks required by
two or more business scripts.

• User-Defined Functions: General, application-specific, and screen-
access functions.

Test Plan Driven (“Keyword”) Approach. This approach uses the test
case document developed by the tester using a spreadsheet containing
table-driven “keywords.” This approach maintains most of the advantages
of the functional decomposition approach, while eliminating most of the
disadvantages. Using this approach, the entire process is data-driven,
including functionality.

The keywords control the processing. Templates like “Loan Payments”
shown in Exhibit 27.3 are created using a spreadsheet program, such as
Microsoft Excel, and then copied to create additional test cases. This file is
read by a controller script for the application, and processed. When a key-
word is encountered, a list is created using data from the remaining col-
umns. This continues until a “null” (blank) in the field/screen name field is
encountered. The controller script then calls a utility script associated
with the keyword and passes the “list” as an input parameter. The utility
script continues processing until “end-of-list,” then returns to the “Control-
ler” script, which continues processing the file until “end-of-file” is reached.

Each of the keywords causes a utility script to be called that processes
the remaining columns as input parameters in order to perform specific
functions. An advantage of this approach is that it can be run as a manual
test. The numerical dollar data and “Check” (bold font) in the Input/Verifi-
cation Data column indicates what would need to be changed if one were
to copy this test case to create additional tests.

This approach has all of the advantages of functional decomposition, as
well as the following:

• The detail test plan can be written in spreadsheet format containing
all input and verification data. Therefore, the tester only needs to
write this once, rather than writing it in Word and then creating
input and verification files as is required by the functional decom-
position method.

• The test plan does not necessarily have to be written using Excel.
Any format can be used from which either “tab-delimited” or
“comma-delimited” files can be saved, e.g., Access Database.

TEAM LinG



297

A Brief History of Software Testing

• If someone proficient in the automated tool’s scripting language
(prior to the detail test plan being written) can create utility scripts,
then the tester can use the automated test tool immediately via the
“spreadsheet-input” method, without needing to learn the scripting
language.

• The tester need only learn the keywords required, and the specific
format to use within the test plan. This allows the tester to be
productive with the test tool very quickly, and allows more extensive
training in the test tool to be scheduled at a more convenient time.

• After a number of generic utility scripts have already been created
for testing an application, most of these can be reused to test
another. This allows the organization to get their automated testing
“up and running” (for most applications) within a few days, rather
than weeks.

Exhibit 27.3.  Test Plan Template (Loan Payments)

Key Word
Field/

Screen Name

Input/
Verification 

Data Pass/Fail Comments

Start test: Screen Main menu Pass Verify starting point

Enter: Selection 3 Pass Select loan option

Action: Press_key F4 Pass Access loan screen

Verify: Screen Loan posting Pass Verify screen accessed

Pass

Enter: Loan amount $125.87 Pass Enter loan data

Loan method Check Pass

Pass

Action: Press_key F9 Pass Process loan

Pass

Verify: Screen Loan payment 
screen

Pass Verify screen remains

Pass

Verify_data: Loan amount $125.87 Pass Verify updated data

Current balance $1,309.77 Pass

Status message Loan posted Pass

Action: Press_key F12 Pass Return to main menu

Verify: Screen Main menu Pass Verify return to menu

TEAM LinG



298

MODERN SOFTWARE TESTING TOOLS

Historical Software Testing and Development Parallels

In some ways software testing and automated testing tools are following
similar paths to traditional development. The following is a brief evolution
of software development and shows how deviations from prior best prac-
tices are also being observed in the software testing process.

The first computers were developed in the 1950s and FORTRAN was the
first 1GL programming language. In the late 1960s, the concept of “struc-
tured programming” stated that any program can be written using three
simple constructs: simple sequence, if/then/else, and do while statements.
There were other prerequisites such as the program being a “proper pro-
gram” whereby there must exist only one entry and one exit point. The
focus was on the process of creating programs.

In the 1970s the development community focused on design techniques.
They realized that structured programming was not enough to ensure
quality — a program must be designed before it can be coded. Techniques
such as Yourdon’s, Myers’, and Constantine’s structured design and com-
posite design techniques flourished and were accepted as best practice.
The focus still had a process orientation.

The philosophy of structured design was partitioning and organizing the
pieces of a system. By partitioning is meant the division of the problem into
smaller subproblems, so that each subproblem will eventually correspond
to a piece of the system. Highly interrelated parts of the problem should be
in the same piece of the system; that is, things that belong together should
go together. Unrelated parts of the problem should reside in unrelated
pieces of the system; for example, things that have nothing to do with each
other don’t belong together.

In the 1980s it was determined that structured programming and soft-
ware design techniques were still not enough: the requirements for the pro-
grams must first be established for the right system to be delivered to the
customer. The focus was on quality that occurs when the customer
receives exactly what she wanted in the first place.

Many requirement techniques emerged such as Data Flow Diagrams
(DFDs). An important part of a DFD is a “store,” a representation of where
the application data will be stored. The concept of a “store” motivated
practitioners to develop a logical view representation of the data. Previ-
ously the focus was on the physical view of data in terms of the database.
The concept of a data model was then created: a simplified description of
a real-world system in terms of data, for example, a logical view of data.
The components of this approach included entities, relationships, cardi-
nality, referential integrity, and normalization. These also created a contro-
versy as to which came first: the process or data, a chicken-and-egg argu-
ment. Until this logical representation of data, the focus was on the

TEAM LinG



299

A Brief History of Software Testing

processes that interfaced to databases. Proponents of the logical view of
data initially insisted that the data was the first analysis focus and then the
process. With time, it was agreed that both the process and data must be
considered jointly in defining the requirements of a system.

In the mid-1980s the concept of information engineering was introduced.
It was a new discipline that led the world into the information age. With this
approach there is more interest in understanding how information can be
stored and represented, how information can be transmitted through net-
works in multimedia forms, and how information can be processed for var-
ious services and applications. Analytical problem-solving techniques,
with the help of mathematics and other related theories, were applied to
the engineering design problems. Information engineering stressed the
importance of taking an enterprise view to application development rather
than a specific application. By modeling the entire enterprise in terms of
processes, data, risks, critical success factors, and other dimensions it was
proposed that management would be able to manage the enterprise in a
more efficient manner.

During this same timeframe, fourth generation computers embraced
microprocessor chip technology and advanced secondary storage with
fantastic rates with storage devices holding tremendous amounts of data.
Software development techniques had vastly improved and 4GLs made the
development process much easier and faster. Unfortunately, the emphasis
on quick turnaround of applications led to a backwards trend of fundamen-
tal development techniques to “get the code out” as quickly as possible.

Extreme Programming

Extreme programming (XP) is an example of such a trend. XP is an unortho-
dox approach to software development and it has been argued that it has
no design aspects. The extreme programming methodology proposes a
radical departure from commonly accepted software development pro-
cesses. There are really two XP rules: Do a Little Design and No Require-
ments, Just User Stories. Extreme programming disciples insist that “there
really are no rules, just suggestions. XP methodology calls for small units
of design, from ten minutes to half an hour, done periodically from one day
between sessions to a full week between sessions. Effectively, nothing gets
designed until it is time to program it.”

Although most people in the software development business under-
standably consider requirements documentation to be vital, XP recom-
mends the creation of as little documentation as possible. No upfront
requirement documentation is created in XP and very little is created in the
software development process.

With XP, the developer comes up with test scenarios before she does
anything else. The basic premise behind test-first design is that the test

TEAM LinG



300

MODERN SOFTWARE TESTING TOOLS

class is written before the real class; thus the end purpose of the real class
is not simply to fulfill a requirement, but simply to pass all the tests that are
in the test class. The problem with this approach is that independent test-
ing is needed to find out things about the product the developer did not
think about or was not able to discover in his own testing.

TEAM LinG



301

Part 28

Software Testing 
Trends

Today many companies are still struggling with the requirements phase,
which is often minimized (or bypassed) to get the application “out the
door.” Testers constantly ask, “How do you test software without a require-
ments specification?” The answer is, you can’t. This lack of good require-
ments has resulted in a loss of billions of dollars each year dealing with the
rippling effect, which occurs when one phase of the development life cycle
has not been sufficiently completed before proceeding to the next. For
example, if the requirements are not fully defined, the design and coding
will reflect the wrong requirements. The application project will have to
constantly go back to redefine the requirements and then design and code.
The efficiency of 4GLs in some ways has diminished previously learned les-
sons in software development.

Unfortunately, the above historical development trends are being fol-
lowed in software testing, as listed below.

Automated Capture/Replay Testing Tools

The original purpose of automated test tools was to automate regression
testing to verify that software changes do not adversely affect any portion
of the application already tested. This requires that a tester has developed
detailed test cases that are repeatable, and the suite of tests is run every
time after there is a change to the application. With the emergence of auto-
mated testing tools many have embraced this as the final frontier for the
testing effort.

However, on many occasions testing tools are attempted with no testing
process or methodology. A test process consists of test planning, test
design, test implementation, test execution, and defect management. Auto-
mated testing tools must integrate within the context of a testing process.
The testing process must be embedded into the development methodol-
ogy to be successful. Having some testing process is also not enough. Many
companies decline and even fail at the same time they are reforming their
processes. They are winning Baldrige Awards and creating dramatic new
efficiencies, savings, and improvements in product quality and customer

TEAM LinG



302

MODERN SOFTWARE TESTING TOOLS

service. Companies experiencing this paradox have clearly gotten a pro-
cess right. But that differs from getting the right process right. The selec-
tion and usage of an automated testing tool does not guarantee success.

Test Case Builder Tools

Companies purchase a lot of automated testing tools but soon realize that
they need a programmer or tester with programming experience to create
and maintain the scripts. The emphasis becomes getting the automated
test script (a program) to work. The scripting effort is a development
project within a development project and requires a great deal of program-
ming effort. Many testers do not have a programming background and
developers do not want to do testing.

Automated testing tools are just a delivery mechanism of the test data to
the target application under test. Automated testing tools are typically
used for function/GUI testing. Tools are the interface between the test data
and the GUI to verify the target application responds as defined by the
requirements (if there are any). The creation of the test data/scenarios is a
manual process in which a tester (or business analyst) translates the
requirements (usually written in a word processor such as Microsoft
Word) to test data.

This is a very time-consuming and difficult problem in which humans
are not very efficient. There are numerous testing techniques (see Appen-
dix G) that aid in the translation, but this translation is still a human effort
from one formalism to another, for example, an English language statement
to test data/scenarios. It is ironic that so much attention has been given to
developing test scenarios with little or no concern about the quality of the
test data.

Advanced Leading-Edge Automated Testing Tools

Because most test efforts require hundreds, if not thousands, of test cases,
this leads to an extensive development effort when a capture/replay tool
uses a scripting language. This is time consuming, as automating an hour
of manual testing can require ten hours of coding and debugging. The net
result is another development effort within the test cycle, which is not
planned, staffed, or budgeted. For testers who do not have a programming
background there is a steep learning curve to learn how to use these tools.
As the inevitable changes are made to the application, even minor modifi-
cations can have an extensive impact on the automated test library. A sin-
gle change to a widely used function can affect dozens of test cases or
more. Not only do the changes have to be mapped to the affected scripts
and any necessary modifications made, but also the results have to be
tested. Eventually the maintenance effort takes so much of the test cycle

TEAM LinG



303

Software Testing Trends

time that testers are forced to revert to manual testing to meet their dead-
lines. At this point, the tool becomes shelfware.

The focus of future automated testing tools will have a business per-
spective rather than a programming view. Business analysts will be able to
use such tools to test applications from a business perspective without
having to write test scripts.

Instead of requiring one to learn a scripting language, or to document
their tests so someone else can code them into scripts, the most advanced
automated testing tools let one document and automate in one step with
no programming effort required. Application experts with business knowl-
edge will be able to learn to develop and execute robust automated tests
using simple drop-down menus.

Original Software’s “TestBench” Suite is an example of such tools; see
Taxonomy of Testing Tools section for more details. Also see www.origsoft.
com, which describes the tool in detail.

Original Software’s TestBench suite, for example, uses a simple “point-
and-click” interface in all of its record and playback modules, to guarantee
the type of fast start required by business and the ease of use that makes
the tools accessible to even the least technical end-users. It really is as sim-
ple as using a VCR and completely bypasses the need for any scripting lan-
guage or programming knowledge. And yet, these modules still deliver
powerful high-level functions such as:

• Self Healing Script (SHS ) technology with which existing test scripts
can be run over new versions of applications without the need for
tedious and time-consuming maintenance.

• Variable data, which enables multiple business scenarios to be
tested with a single script. Integration between TestBench and the
TestSmart feature means that optimum quality variable data can be
created.

• Tracked fields to enable data created during the running of an appli-
cation (e.g., purchase reference numbers) to be captured and used
throughout playback.

• Script progression logic with which individual scripts and other test
modules can be linked into a sequence that recreates an entire
business process. The launch sequence of scripts and modules can
change automatically according to the data presented during a test
run, so a whole range of possible scenarios can be recreated from
just one sequence.

The TestBench record and playback modules all come with the ability to
integrate with a range of server-side databases — which means that all
scripts, script sequences, test results, documentation, etc. can be stored in

TEAM LinG



304

MODERN SOFTWARE TESTING TOOLS

one central repository, accessible by all authorized team members. This
vastly improves test process efficiency by eliminating the duplication of
resources and ensuring that the knowledge of how to test an application or
a particular process is not held by just one individual. Features such as
these guarantee the highest levels of availability and usability to fulfill the
requirements of the business perspective and avoid the shelfware scenario
described above, and such tools are at the core of the future of automated
software testing.

Advanced Leading-Edge Test Case Builder Tools

These tools can even accomplish advanced tasks such as providing test
data from spreadsheets without writing a single line of code.

As pointed out previously, the focus of test automation to date has been
on getting the scripts to work. But from where does the data come? How
does the tester know the right test data is being used by the script and how
does the tester know there is adequate requirement coverage by the data?

This is a fundamental element that is typically missing from cap-
ture/replay automated tools up to now. What is not being addressed is the
quality of the test data input and test scenarios to the scripts. Typically the
automated testing tool scripter is assigned to the testing department.
Because this is a very specialized and intensive programming effort, the
focus is on getting the script working correctly. It is assumed someone else
will provide the test data/test scenarios

Smartware Technology’s TestSmartTM is an example of such a tool; see
the Taxonomy of Testing Tools section for more details.

Necessary and Sufficient Conditions

Automated testing tools to date do not satisfy the necessary and sufficient
conditions to achieve quality. These tools are as good as the quality of the
test data input from the automated test scripts.

GIGO stands for Garbage In, Garbage Out. Computers, unlike humans,
will unquestioningly process the most nonsensical input data and produce
nonsensical output. Of course a properly written program will reject input
data that is obviously erroneous but such checking is not always easy to
specify and is tedious to write. GIGO is usually said in response to users
who complain that a program didn’t “do the right thing” when given imper-
fect input. This term is also commonly used to describe failures in human
decision making due to faulty, incomplete, or imprecise data. This is a sar-
donic comment on the tendency human beings have to put excessive trust
in “computerized” data.

The necessary and sufficient conditions for quality are that a robust tool
be the deliverer of quality test data/test scenarios to be exercised against

TEAM LinG



305

Software Testing Trends

the target application based upon what the application should do or not
do. For most commercial applications, the data is key to the test result. Not
just entering or verifying data values, but knowing what the state of the
data is supposed to be so you can predict expected results. Getting control
of the test data is fundamental for any test effort, because a basic tenet of
software testing is that you must know both the input conditions of the
data and the expected output results to perform a valid test. If you don’t
know either of these, it’s not a test; it’s an experiment, because you don’t
know what will happen. This predictability is important for manual testing,
but for automated testing it’s essential.

But for many systems you can’t even get started until you have enough
test data to make it meaningful and if you need thousands or millions of
data records, you’ve got a whole new problem. In an extreme case, testing
an airline fare pricing application required tens of thousands of setup
transactions to create the cities, flights, passengers, and fares needed to
exercise all of the requirements. The actual test itself took less time than
the data setup. Other necessary conditions are the people and process.
The right people need to be trained and there must be a solid testing pro-
cess in place before test automation can be attempted.

Test Data/Test Case Generation

Historically there are four basic strategies for assembling a test data envi-
ronment: production sampling, starting from scratch, seeding data, or gen-
erating it. Each strategy is considered including the advantages and disad-
vantages of each. After this discussion, a fifth, or cutting-edge, approach
generating not only test data but test scenarios and the expected results
(based upon the requirements) is discussed.

Sampling from Production

The most common test data acquisition technique is to take it from pro-
duction. This approach seems both logical and practical: production rep-
resents reality, in that it contains the actual situations the software must
deal with and it offers both depth and breadth while ostensibly saving the
time required to create new data.

There are at least three major drawbacks, however. The test platform
seldom replicates production capacity, and so a subset must be extracted.
Acquiring this subset is not as easy as taking every Nth record or some flat
percentage of the data: the complex interrelationships between files means
that the subset must be internally cohesive. For example, the selected
transactions must reference valid selected master accounts, and the totals
must coincide with balances and histories. Simply identifying these rela-
tionships and tracing through all of the files to ensure that the subset
makes sense can be a major undertaking in and of itself. Furthermore, it is

TEAM LinG



306

MODERN SOFTWARE TESTING TOOLS

difficult to know how large a sample is necessary to achieve coverage of all
critical states and combinations.

The second major drawback of this approach is that the tests them-
selves and the extracted data must be constantly modified to work
together. Going back to our basic tenet, we must know the input conditions
for a valid test, in this case, the data contents. Each fresh extraction starts
everything over. If a payroll tax test requires an employee whose year-to-
date earnings will cross over the FICA limit on the next paycheck, for exam-
ple, the person performing the test must either find such an employee in
the subset, modify one, or add one. If the test is automated, it too must be
modified for the new employee number and related information. Searching
for an employee that meets all the conditions you are interested in is like
searching for a needle in a haystack. Thus, the time savings are illusory
because there is limited repeatability: all effort to establish the proper test
conditions is lost every time the extract is refreshed.

And finally, this approach obviously cannot be employed for new sys-
tems under development, inasmuch as no production data is available.

Starting from Scratch

The other extreme is to start from scratch, in effect reconstructing the test
data each time. This approach has the benefit of complete control; the con-
tent is always known and can be enhanced or extended over time, preserv-
ing prior efforts. Internal cohesion is ensured because the software itself
creates and maintains the interrelationships, and changes to file structures
or record layouts are automatically incorporated.

But reconstructing test data is not free from hazards. The most obvious
is that, without automation, it’s highly impractical for large-scale applica-
tions. But less obvious is the fact that some files cannot be created through
online interaction: they are system generated only through interfaces or pro-
cessing cycles. Thus, it may not be possible to start from a truly clean slate.

A compelling argument also might be made that data created in a vac-
uum, so to speak, lacks the expanse of production: unique or unusual situ-
ations that often arise in the real world may not be contemplated by test
designers. Granted, this technique allows for steady and constant expan-
sion of the test data as necessary circumstances are discovered, but it
lacks the randomness that makes production so appealing.

Seeding the Data

Seeding test data is a combination of using production files and creating
new data with specific conditions. This approach provides a dose of reality
tempered by a measure of control.

TEAM LinG



307

Software Testing Trends

This was the strategy adopted by a major mutual fund to enable test
automation. Without predictable repeatable data there was no practical
means of reusing automated tests across releases. Although much of the
data could be created through the online interface, such as funds, custom-
ers, and accounts, other data had to be extracted from production. Testing
statements and tax reports, for example, required historical transactions
that could not be generated except by multiple execution cycles. So, the
alternative to acquiring the data from production and performing the nec-
essary maintenance on the tests proved to be less time consuming. Once
the data was assembled, it was archived for reuse.

It’s still not easy. You must still surmount the cohesion challenge, ensur-
ing that the subset you acquire makes sense, and you must still have an effi-
cient means of creating the additional data needed for test conditions. Fur-
thermore, you must treat the resulting data as the valuable asset that it is,
instituting procedures for archiving it safely so that it can be restored and
reused.

Although a popular and sensible concept, reuse brings its own issues.
For time-sensitive applications, which many if not most are, reusing the
same data over and over is not viable unless you can roll the data dates for-
ward or the system date back. For example, an employee who is 64 one
month may turn 65 the next, resulting in different tax consequences for
pension payouts.

Furthermore, modifications to file structures and record layouts
demand data conversions, but this may be seen as an advantage because,
it is hoped, the conversions are tested against the testbed before they are
performed against production.

Generating Data Based upon the Database

Generated test data can obviously be used to create databases with
enough information to approximate real-world conditions for testing
capacity and performance. If you need to ensure that your database design
can support millions of customers or billions of transactions and still
deliver acceptable response times, generation may be the only practical
means of creating these volumes.

Test data generators begin with the description of the file or database
that is to be created. In most cases, the tools can read the database tables
directly to determine the fields and their type, length, and format. The user
can then add the rules, relationships, and constraints that govern the gen-
eration of valid data.

Standard “profiles” are also offered, which can automatically produce bil-
lions of names, addresses, cities, states, zip codes, Social Security numbers,

TEAM LinG



308

MODERN SOFTWARE TESTING TOOLS

test dates, and other common data values such as random values, ranges,
and type mixes. User-customizable data types are also available in most
products, which can be used for generating unique SIC (standard industri-
alization classification) business codes, e-mail addresses, and other data
types.

A more critical feature, and more difficult to implement, is support for
parent/child and other relationships in complex databases. For example, a
parent record, such as a customer account master, must be linked with
multiple child records, such as different accounts and transactions. This
type of functionality is essential for relational database environments
where referential integrity is key.

Some users have found it easier to use a test data generator to create
data that is then read by an automated test tool and entered into the appli-
cation. This is an interesting combination of data generating and seeding.
The synergy between test data generation and test automation tools is a
natural, and in some cases the test data generation capability is being
embedded in test execution products.

Databases can contain more than just data, such as stored procedures
or derived foreign keys that link other tables or databases. In these cases,
it is not feasible to generate data directly into the tables. Too often main-
taining database integrity is a project unto itself.

And, of course, in the end volume is its own challenge. More is not nec-
essarily better. Too much data will take too long to generate, will require
too much storage, and may create even more issues than not enough.

Generating Test Data/Test Cases Based upon the Requirements

Functional testing is a different animal, however. If you are testing business
rules, such as whether an account whose balance owed is more than 90
days past due will permit additional credit purchases to be posted, then
you must know precisely which account number contains the condition
and which transactions will be entered against it. Although it may be easy
to generate huge volumes of accounts whose balances are all over the map
in terms of their amounts and due dates, it is not as simple to know exactly
which accounts satisfy which business rules.

In this context, the same issues exist as they do for simply sampling pro-
duction data. Even if you are comfortable that your data sample is repre-
sentative of the types of conditions that must be tested, it’s another matter
altogether to know which accounts meet which requirements. Testing
complex business rules may require knowing the exact state of several
variables that are spread over multiple databases, tables, and/or files, and
finding that precise combination may be like looking for a needle in a hay-
stack.

TEAM LinG



309

Software Testing Trends

The latest test case builder tools derive the test data directly from the
requirements. The output of such tools is the input to the automated test-
ing scripts that do not require business knowledge. How novel — test
scripts and the test data/test case scenarios are based upon the require-
ments! Such cutting edge test tools bridge the gap between the require-
ments and testing phases.

Advanced test case generation tools have the following characteristics:

• Input the raw unprocessed test data from a test grid or external
source such as Microsoft’s Excel. For GUI the data could be the
values in a drop-down list, the text field data, the radio button
conditions, and so on.

• Capability of reengineering existing parameterized test data to raw
unprocessed test data from an external source such as Microsoft Excel.

• Generate optimized “pair-wise” positive and negative test data from
the raw unprocessed parameters and values.

• Capability for the analyst/tester to build business rules in English
prose statements from the requirements when rigorous require-
ments exist, achieved by simple point and clicking. The English
prose must be able to handle any complex conditional expression.

• Based upon the condition tested against the optimized “pairwise”
test data an action or otherwise action is applied to the test data to
reflect how the application should or should not behave. This results
in a set of rows of optimized test scenarios.

• Capability for the analyst/tester to build interface rules in English
prose statements to modify the input set to (1) modify the input set
based upon one or more conditions, and (2) generate ranges for test
data that is important for performance/load/stress testing. This fea-
ture is used when rigorous requirements are not necessarily rigorous
or even available.

• Capability of exporting the optimized test data to Excel or other
medium which can then be input to an automated testing tool (cur-
rent or future automated tool).

• Provide a cross-reference matrix to associate the business rules with
the resulting test scenario test data. This is bidirectional traceability,
which is important for maintenance efforts.

• Capability of creating an expected result in the test data based upon
the application of the business rules. This allows the script to com-
pare the actual and expected values.

Smartware Technologies’s TestSmart™ is an example of this cutting edge
tool approach; TestSmart™ has also been seamlessly integrated with Origi-
nal Software’s TestBench Suite of testing tools as characterized above. With
Original’s simple “point-and-click” interface, the user can generate intelligent
test cases and replay them using the replay portion of the TestBench Suite.

TEAM LinG



310

MODERN SOFTWARE TESTING TOOLS

With TestSmart™, one does not have to manually create test cases by
entering symmetric row tests in a test grid or an Excel spreadsheet. All that
is necessary from the user are the parameters and respective column val-
ues, e.g., GUI dropdown caption name and the respective values. A set of
optimized test cases will be generated. To be even more sophisticated, the
user can easily enter business rules by point and click to constrain the
optimized test data to reflect how the application or system should or
should not behave. See www.smartwaretechnologies.com, which
describes the tool in detail. A free trial version is available from this Smart-
ware Technologies, Inc. Web site.

TEAM LinG



311

Part 29

Taxonomy of Testing 
Tools

Testing Tool Selection Checklist

Finding the appropriate tool can be difficult. Several questions need to be
answered before selecting a tool. Appendix F19, Testing Tool Selection
Checklist, lists questions to help the QA team evaluate and select an auto-
mated testing tool.

The following categorizes currently available tool types based upon their
tool objectives and features. In the section called Vendor Tool Descriptions,
popular vendors supporting each tool category are discussed.

• Function/Regression Tools — These tools help you test software
through a native graphical user interface. Some also help with other
interface types. Another example is Web test tools that test through
a browser, for example, capture/replay tools.

• Test Design/Data Tools — These tools help create test cases and
generate test data.

• Load/Performance Tools — These tools are often also GUI test drivers.
• Test Process/Management Tools — These tools help organize and

execute suites of test cases at the command line, API, or protocol
level. Some tools have graphical user interfaces, but they don’t have
any special support for testing a product that has a native GUI. Web
test tools that work at the protocol level are included here.

• Unit Testing Tools — These tools, frameworks, and libraries support
unit testing, which is usually performed by the developer, usually
using interfaces below the public interfaces of the software under
test.

• Test Implementation Tools — These tools assist with testing at runtime.
• Test Evaluation Tools — Tools that help you evaluate the quality of

your tests. Examples include code coverage tools.
• Static Test Analyzers — Tools that analyze programs without running

them. Metrics tools fall in this category.
• Defect Management Tools — Tools that track software product

defects and manage product enhancement requests. Manages defect
states from defect discovery to closure.

TEAM LinG



312

MODERN SOFTWARE TESTING TOOLS

• Application Performance Monitoring/Tuning Tools — Tools that mea-
sure and maximize value across the IT service delivery life cycle to
ensure applications meet quality, performance, and availability
goals.

• Runtime Analysis Testing Tools — Tools that analyze programs while
running them.

Vendor Tool Descriptions

Exhibit 29.1 provides an overview of some testing tools. The descriptions
are listed alphabetically and are cross-referenced to the tool description
and vendor Web site information. No tool is favored over another.

When You Should Consider Test Automation

A testing tool should be considered based on the test objectives. As a gen-
eral guideline, one should investigate the appropriateness of a testing tool
when the human manual process is inadequate. For example, if a system
needs to be stress tested, a group of testers could simultaneously log on to
the system and attempt to simulate peak loads using stopwatches. How-
ever, this approach has limitations. One cannot systematically measure the
performance precisely or repeatably. For this case, a load-testing tool can
simulate several virtual users under controlled stress conditions.

A regression testing tool might be needed under the following circum-
stances:

• Tests need to be run at every build of an application, for example,
time-consuming, unreliable, and inconsistent use of human
resources.

• Tests are required using multiple data values for the same actions.
• Tests require detailed information from system internals such as

SQL and GUI attributes.
• There is a need to stress a system to see how it performs.

Testing tools have the following benefits:

• Much faster than their human counterpart
• Run without human intervention
• Provide code coverage analysis after a test run
• Precisely repeatable
• Reusable, just as programming subroutines
• Detailed test cases (including predictable “expected results”), which

have been developed from functional specifications and/or technical
design documentation

• Stable testing environment with a test database that can be restor-
able to a known constant, so that the test cases are able to be
repeated each time there are modifications made to the application

TEAM LinG



313

Taxonomy of Testing Tools

Exhibit 29.1. Testing Tool Name versus Tool Categorya 

Fu
n

ct
io

n
/R

eg
re

ss
io

n
 T

oo
ls

Te
st

 D
es

ig
n

/D
at

a 
To

ol
s

Lo
ad

/P
er

fo
rm

an
ce

 T
oo

ls

Te
st

 P
ro

ce
ss

/M
an

ag
em

en
t 

To
ol

s

U
n

it
 T

es
ti

n
g 

To
ol

s

Te
st

 I
m

p
le

m
en

ta
ti

on
 T

oo
ls

Te
st

 E
va

lu
at

io
n

 T
oo

ls

St
at

ic
 T

es
t 

A
n

al
yz

er
 T

oo
ls

D
ef

ec
t 

M
an

ag
em

en
t 

To
ol

s

A
p

p
li

ca
ti

on
 P

er
fo

rm
an

ce
 

M
on

it
or

in
g/

T
u

n
in

g 
To

ol
s

R
u

n
ti

m
e 

A
n

al
ys

is
 T

es
ti

n
g 

To
ol

s

Abbott �

Access for DB2 �

Advanced Defect Tracking �

ALLPAIRS �

AllPairs.java �

Android �

Application Expert �

ApTest Manager �

AQtest �

ASSENT �

Assertion Definition Language (ADL) �

Astra Load Test �

Astra Quick Test �

Aunit �

AutoIt �

AutomX �

AutoPilot �

AutoTester Client/Server for use 
with SAP R/3

�

AutoTester for OS/2 �

AutoTester for Windows �

Bug Tracker Software �

BugAware �

Buggit �

TEAM LinG



314

MODERN SOFTWARE TESTING TOOLS

CAPBAK �

Certify �

Check �

CitraTest �

ClearDDTS �

ClearMaker �

ClearQuest �

ClientVantage �

Clover �

CodeSurfer �

CodeTEST � �

Compare for Servers �

cUnit �

DARTT �

Datagen2000 �

DateWise FileCompare �

Defect Agent �

DGL �

Eggplant for Mac OS X �

Elementool �

e-Monitor �

EMOS Framework �

E-SIM �

e-Tester �

Exhibit 29.1. Testing Tool Name versus Tool Categorya (Continued)

Fu
n

ct
io

n
/R

eg
re

ss
io

n
 T

oo
ls

Te
st

 D
es

ig
n

/D
at

a 
To

ol
s

Lo
ad

/P
er

fo
rm

an
ce

 T
oo

ls

Te
st

 P
ro

ce
ss

/M
an

ag
em

en
t 

To
ol

s

U
n

it
 T

es
ti

n
g 

To
ol

s

Te
st

 I
m

p
le

m
en

ta
ti

on
 T

oo
ls

Te
st

 E
va

lu
at

io
n

 T
oo

ls

St
at

ic
 T

es
t 

A
n

al
yz

er
 T

oo
ls

D
ef

ec
t 

M
an

ag
em

en
t 

To
ol

s

A
p

p
li

ca
ti

on
 P

er
fo

rm
an

ce
 

M
on

it
or

in
g/

T
u

n
in

g 
To

ol
s

R
u

n
ti

m
e 

A
n

al
ys

is
 T

es
ti

n
g 

To
ol

s

TEAM LinG



315

Taxonomy of Testing Tools

eValid � �

Eventcorder suite �

File-AID/CS �

HarnessIt �

Hindsight/SQA �

HtmlUnit �

IBM Rational Robot �

IBM Rational Test Realtime � �

imbus GUI Test Case Library �

iSTROBE �

Jacareto �

Jemmy �

Jenny �

jfcUnit �

LDRA Testbed �

LoadRunner �

LOGISCOPE toolset �

Marathon �

McCabe Coverage Server �

McCabe Enterprise Quality �

McCabe QA �

McCabe Reengineer �

McCabe Test �

McCabe TRUEtrack �

Exhibit 29.1. Testing Tool Name versus Tool Categorya (Continued)

Fu
n

ct
io

n
/R

eg
re

ss
io

n
 T

oo
ls

Te
st

 D
es

ig
n

/D
at

a 
To

ol
s

Lo
ad

/P
er

fo
rm

an
ce

 T
oo

ls

Te
st

 P
ro

ce
ss

/M
an

ag
em

en
t 

To
ol

s

U
n

it
 T

es
ti

n
g 

To
ol

s

Te
st

 I
m

p
le

m
en

ta
ti

on
 T

oo
ls

Te
st

 E
va

lu
at

io
n

 T
oo

ls

St
at

ic
 T

es
t 

A
n

al
yz

er
 T

oo
ls

D
ef

ec
t 

M
an

ag
em

en
t 

To
ol

s

A
p

p
li

ca
ti

on
 P

er
fo

rm
an

ce
 

M
on

it
or

in
g/

T
u

n
in

g 
To

ol
s

R
u

n
ti

m
e 

A
n

al
ys

is
 T

es
ti

n
g 

To
ol

s

TEAM LinG



316

MODERN SOFTWARE TESTING TOOLS

MemCheck for Windows � �

Merant Tracker �

METRIC �

Metrics Tools �

Move for Legacy �

NetworkVantage �

NUnit �

Orchid �

Ozibug �

Panorama C/C++ �

Panorama-2 �

Perl X11::GUITest �

Phantom �

Pounder �

Problem Tracker �

ProjecTrak Bug �

Protune �

PVCS Tracker �

QADirector® �

QARunTM �

QC/Replay �

QES/Architect �

QES/DatEZ (date-easy) �

QES/EZ for GUI �

Exhibit 29.1. Testing Tool Name versus Tool Categorya (Continued)

Fu
n

ct
io

n
/R

eg
re

ss
io

n
 T

oo
ls

Te
st

 D
es

ig
n

/D
at

a 
To

ol
s

Lo
ad

/P
er

fo
rm

an
ce

 T
oo

ls

Te
st

 P
ro

ce
ss

/M
an

ag
em

en
t 

To
ol

s

U
n

it
 T

es
ti

n
g 

To
ol

s

Te
st

 I
m

p
le

m
en

ta
ti

on
 T

oo
ls

Te
st

 E
va

lu
at

io
n

 T
oo

ls

St
at

ic
 T

es
t 

A
n

al
yz

er
 T

oo
ls

D
ef

ec
t 

M
an

ag
em

en
t 

To
ol

s

A
p

p
li

ca
ti

on
 P

er
fo

rm
an

ce
 

M
on

it
or

in
g/

T
u

n
in

g 
To

ol
s

R
u

n
ti

m
e 

A
n

al
ys

is
 T

es
ti

n
g 

To
ol

s

TEAM LinG



317

Taxonomy of Testing Tools

qftestJUI �

QStudio for Java Pro �

QStudio JAVA �

QtUnit �

Quick Test Professional �

QuickBugs �

Rational Functional Tester for Java 
and Web

�

Rational Performance Tester �

Rational PureCoverage �

Rational Purify � �

Rational Purify Plus �

Rational Robot �

Rational TeamTest �

Rational Test RealTime Coverage �

Rational Test RealTime System 
Testing

�

Reconcile �

Remedy Quality Management �

SAFS (Software Automation 
Framework Support)

�

SDTF - SNA Development Test Facility �

ServerVantage �

Silk Vision �

SilkPerformer �

Exhibit 29.1. Testing Tool Name versus Tool Categorya (Continued)

Fu
n

ct
io

n
/R

eg
re

ss
io

n
 T

oo
ls

Te
st

 D
es

ig
n

/D
at

a 
To

ol
s

Lo
ad

/P
er

fo
rm

an
ce

 T
oo

ls

Te
st

 P
ro

ce
ss

/M
an

ag
em

en
t 

To
ol

s

U
n

it
 T

es
ti

n
g 

To
ol

s

Te
st

 I
m

p
le

m
en

ta
ti

on
 T

oo
ls

Te
st

 E
va

lu
at

io
n

 T
oo

ls

St
at

ic
 T

es
t 

A
n

al
yz

er
 T

oo
ls

D
ef

ec
t 

M
an

ag
em

en
t 

To
ol

s

A
p

p
li

ca
ti

on
 P

er
fo

rm
an

ce
 

M
on

it
or

in
g/

T
u

n
in

g 
To

ol
s

R
u

n
ti

m
e 

A
n

al
ys

is
 T

es
ti

n
g 

To
ol

s

TEAM LinG



318

MODERN SOFTWARE TESTING TOOLS

Silkperformer lite �

SilkPilot �

Silkplan pro �

SilkRadar �

SilkRealizer �

SilkTest �

Silktest international �

Smalltalk Test Mentor �

Smart TestTM � �

SQA TeamTest: ERP Extension 
for SAP

�

SQA TestFoundation for PeopleSoft �

SSW Code Auditor �

STATIC �

Strobe/APM �

TagUnit �

Tasker �

TCAT C/C++ �

TCAT for Java �

TCAT-PATH �

TDGEN �

Test Case Manager (TCM) �

Test Mentor - Java Edition �

Test Now �

Exhibit 29.1. Testing Tool Name versus Tool Categorya (Continued)

Fu
n

ct
io

n
/R

eg
re

ss
io

n
 T

oo
ls

Te
st

 D
es

ig
n

/D
at

a 
To

ol
s

Lo
ad

/P
er

fo
rm

an
ce

 T
oo

ls

Te
st

 P
ro

ce
ss

/M
an

ag
em

en
t 

To
ol

s

U
n

it
 T

es
ti

n
g 

To
ol

s

Te
st

 I
m

p
le

m
en

ta
ti

on
 T

oo
ls

Te
st

 E
va

lu
at

io
n

 T
oo

ls

St
at

ic
 T

es
t 

A
n

al
yz

er
 T

oo
ls

D
ef

ec
t 

M
an

ag
em

en
t 

To
ol

s

A
p

p
li

ca
ti

on
 P

er
fo

rm
an

ce
 

M
on

it
or

in
g/

T
u

n
in

g 
To

ol
s

R
u

n
ti

m
e 

A
n

al
ys

is
 T

es
ti

n
g 

To
ol

s

TEAM LinG



319

Taxonomy of Testing Tools

Test Station �

TestAgent �

TestBase �

TestBed �

TestBench for iSeries �

TestDirector � �

Tester �

TestGUI �

Test Manager �

TestQuest Pro Test Automation 
System

�

TestSmith �

TestWorks �

TestWorks/Advisor �

TestWorks/Coverage �

TET (Test Environment Toolkit) �

TMS �

T-Plan Professional �

TrackRecord �

TRecorder �

TrueJ �

T-SCOPE �

Turbo Data �

Unified TestPro (UTP) �

Exhibit 29.1. Testing Tool Name versus Tool Categorya (Continued)

Fu
n

ct
io

n
/R

eg
re

ss
io

n
 T

oo
ls

Te
st

 D
es

ig
n

/D
at

a 
To

ol
s

Lo
ad

/P
er

fo
rm

an
ce

 T
oo

ls

Te
st

 P
ro

ce
ss

/M
an

ag
em

en
t 

To
ol

s

U
n

it
 T

es
ti

n
g 

To
ol

s

Te
st

 I
m

p
le

m
en

ta
ti

on
 T

oo
ls

Te
st

 E
va

lu
at

io
n

 T
oo

ls

St
at

ic
 T

es
t 

A
n

al
yz

er
 T

oo
ls

D
ef

ec
t 

M
an

ag
em

en
t 

To
ol

s

A
p

p
li

ca
ti

on
 P

er
fo

rm
an

ce
 

M
on

it
or

in
g/

T
u

n
in

g 
To

ol
s

R
u

n
ti

m
e 

A
n

al
ys

is
 T

es
ti

n
g 

To
ol

s

TEAM LinG



320

MODERN SOFTWARE TESTING TOOLS

When You Should NOT Consider Test Automation

In spite of the compelling business case for test automation, and despite
the significant investments of money, time, and effort invested in test auto-
mation tools and projects, the majority of testing is still performed manu-
ally. Why? There are three primary reasons why test automation fails: the
steep learning curve, the development effort required, and the mainte-
nance overhead.

The learning curve is an issue for the simple reason that traditional test
scripting tools are basically specialized programming languages, but the
best testers are application experts, not programmers.

Vermont High Test Plus �

VersaTest �

Visual Test �

WebKing �

WinRunner �

XMLUnit �

xrc - X Remote Control �

XRunner �

XSLTunit �

X-Unity �

ZeroDefect �

ZeroFault �

a Each tool is described on the CD.

Exhibit 29.1. Testing Tool Name versus Tool Categorya (Continued)

Fu
n

ct
io

n
/R

eg
re

ss
io

n
 T

oo
ls

Te
st

 D
es

ig
n

/D
at

a 
To

ol
s

Lo
ad

/P
er

fo
rm

an
ce

 T
oo

ls

Te
st

 P
ro

ce
ss

/M
an

ag
em

en
t 

To
ol

s

U
n

it
 T

es
ti

n
g 

To
ol

s

Te
st

 I
m

p
le

m
en

ta
ti

on
 T

oo
ls

Te
st

 E
va

lu
at

io
n

 T
oo

ls

St
at

ic
 T

es
t 

A
n

al
yz

er
 T

oo
ls

D
ef

ec
t 

M
an

ag
em

en
t 

To
ol

s

A
p

p
li

ca
ti

on
 P

er
fo

rm
an

ce
 

M
on

it
or

in
g/

T
u

n
in

g 
To

ol
s

R
u

n
ti

m
e 

A
n

al
ys

is
 T

es
ti

n
g 

To
ol

s

TEAM LinG



321

Taxonomy of Testing Tools

This creates a skills disconnect that requires an unreasonable learning
curve. Application experts, who make ideal testers because of their busi-
ness knowledge, are unlikely to have programming skills. Gaining these
skills takes months if not years, and without these skills the script libraries
are usually not well-designed for maintainability.

Most test tool vendors are aware of this shortcoming and attempt to
address it through a capture/replay facility. This is an approach that osten-
sibly allows a tester to perform the test manually while it is automatically
“recorded” into a test script that can later be replayed. Although this
approach appears to address the learning curve, in reality it often causes
more problems than it solves.

First, a recorded test script is fragile and easily subject to failure.
Because it has no error handling or logic, the smallest deviation in the
application behavior or data will cause the script to either abort or make
errors. Furthermore, it combines both script and data into a single pro-
gram, which yields no reusability or modularity. The end result is essen-
tially unstructured, poorly designed code.

Also, although it may appear easy to record a script, it is not as easy to
modify or maintain it. The reason software is tested is because something
has changed, which means the scripts must also be modified. Making
extensive script changes and debugging errors is time consuming and com-
plex.

Once companies discover that capture/replay is not a viable long-term
solution, they either give up or begin a development effort.

Contrary to popular belief, it is not always wise to purchase a testing
tool. Some factors that limit a testing tool include:

• Unrealistic expectations — The IT industry is notorious for latching
onto any new technology solution thinking that it will be a panacea.
It is human nature to be optimistic about any new technology. The
vendor salespeople present the rosiest picture of their tool offerings.
The result is expectations that are often unrealistic.

• Lack of a testing process — A prerequisite for test automation is that
a sound manual testing process exists. The lack of good testing
practices and standards will be detrimental to test automation. Auto-
mated testing tools will not automatically find defects unless well-
defined test plans are in place.

• False sense of security — Even though a set of automated test scripts
runs successfully, this does not guarantee that the automated testing
tool has located all the defects. This assumption leads to a false
sense of security. Automation is as good as the test cases and test
input data.

TEAM LinG



322

MODERN SOFTWARE TESTING TOOLS

• Technical difficulties — Automated testing tools themselves unfortu-
nately have defects. Technical environmental changes such as the
operating system can severely limit automated testing tools.

• Organizational issues — Test automation will have an impact on the
organization, for it transcends projects and departments. For exam-
ple, the use of data-driven test scripts requires test input data,
typically in the form of rows in an Excel spreadsheet. This data will
probably be supplied by another group, such as the business system
analysts, not the testing organization.

• Cost — A testing tool may not be affordable to the organization, for
example, the cost/performance tradeoff.

• Culture — The development culture may not be ready for a testing
tool, because it requires the proper skills and commitment to long-
term quality.

• Usability testing — There are no automated testing tools that can test
usability.

• One-time testing — If the test is going to be performed only once, a
testing tool may not be worth the required time and expense.

• Time crunch — If there is pressure to complete testing within a fixed
timeframe, a testing tool may not be feasible, because it takes time
to learn, set up, and integrate a testing tool into the development
methodology.

• Ad hoc testing — If there is no formal test design and test cases, a
regression testing tool will be useless.

• Predictable results — If tests do not have predictable results, a regres-
sion testing tool will be useless.

• Instability — If the system is changing rapidly during each testing
spiral, more time will be spent maintaining a regression testing tool
than it is worth.

TEAM LinG



323

Part 30

Methodology to 
Evaluate Automated 
Testing Tools

This part provides an outline of the steps involved in acquiring, implement-
ing, and using testing tools. The management of any significant project
requires that the work be divided into tasks for which completion criteria
can be defined. The transition from one task to another occurs in steps; to
permit the orderly progress of the activities, the scheduling of these steps
must be determined in advance. A general outline for such a schedule is
provided by the steps described. The actual time schedule depends on
many factors that must be determined for each specific tool use.

Step 1: Define Your Test Requirements

The goals to be accomplished should be identified in a format that permits
later determination that they have been met (i.e., Step 15). Typical goals
include reducing the average processing time of C++ programs by one fifth,
achieving complete interchangeability of programs or data sets with
another organization, and adhering to an established standard for docu-
mentation format. The statement of goals should also identify responsibil-
ities, particularly the role that headquarters staff may have, and specify
coordination requirements with other organizations. When a centralized
management method is employed, the statement of goals may include a
budget and a desired completion date. Once these constraints are speci-
fied, funding management may delegate the approval of the acquisition
plan to a lower level.

Step 2: Set Tool Objectives

The goals generated in Step 1 should be translated into desired tool fea-
tures and requirements that arise from the development and operating
environment identified. Constraints on tool cost and availability may also
be added at this step. For example, a typical tool objective for a program
format is to provide header identification, uniform indentation, and the

TEAM LinG



324

MODERN SOFTWARE TESTING TOOLS

facility to print listings and comments separately for all Pascal programs.
In addition, the program must be able to run on the organization’s specific
computer under its operating system. Only tools that have been in com-
mercial use for at least one year and at no fewer than N sites should be con-
sidered. (The value of N is predetermined by the number of sites the orga-
nization has.)

Step 3a: Conduct Selection Activities for Informal Procurement

The following tasks should be performed when an informal procurement
plan is in effect.

Task 1: Develop the Acquisition Plan

The acquisition plan communicates the actions of software management
both up and down the chain of command. The plan may also be combined
with the statement of tool objectives (Step 2). The acquisition plan
includes the budgets and schedules for subsequent steps in the tool intro-
duction, a justification of resource requirements in light of expected bene-
fits, contributions to the introduction expected from other organizations
(e.g., the tool itself, modification patches, or training materials), and the
assignment of responsibility for subsequent events within the organiza-
tion, particularly the identification of the software engineer. Minimum tool
documentation requirements are also specified in the plan.

Task 2: Define Selection Criteria

The selection criteria include a ranked listing of attributes that should sup-
port effective tool use. Typical selection criteria include:

• The ability to accomplish specified tool objectives
• Ease of use
• Ease of installation
• Minimum processing time
• Compatibility with other tools
• Low purchase or lease cost

Most of these criteria must be considered further to permit objective
evaluation, but this step may be left to the individual who does the scoring.
Constraints that have been imposed by the preceding events or are gener-
ated at this step should be summarized together with the criteria.

Task 3: Identify Candidate Tools

This is the first step for which the software engineer is responsible. The
starting point for preparing a list of candidate tools is a comprehensive
tool catalogue. Two lists are usually prepared, the first of which does not

TEAM LinG



325

Methodology to Evaluate Automated Testing Tools

consider the constraints and contains all tools that meet the functional
requirements. For the feasible candidates, literature should be requested
from the developer and then examined for conformance with the given
constraints. At this point, the second list is generated, which contains
tools that meet both the functional requirements and the constraints. If
this list is too short, some constraints may be relaxed.

Task 4: Conduct the Candidate Review

The user must review the list of candidate tools prepared by the software
engineer. Because few users can be expected to be knowledgeable about
software tools, specific questions should be raised by software manage-
ment, including the following:

• Will this tool handle the present file format?
• Are tool commands consistent with those of the editor?
• How much training is required?

Adequate time should be allowed for this review, and a due date for
responses should be indicated. Because users often view this as a low-pri-
ority, long-term task, considerable follow-up by line management is
required. If possible, tools should be obtained for trial use, or a demonstra-
tion at another facility should be arranged.

Task 5: Score the Candidates

For each criterion identified in Task 2, a numeric score should be generated
on the basis of the information obtained from the vendor’s literature, tool
demonstrations, the user’s review, observation in a working environment,
or the comments of previous users. Once weighting factors for the criteria
have been assigned, the score for each criterion is multiplied by the appro-
priate factor; the sum of the products represents the overall tool score. If
the criteria are merely ranked, the scoring will consist of a ranking of each
candidate under each criterion heading. Frequently during this process, a
single tool will be recognized as clearly superior.

Task 6: Select the Tool

This decision is reserved for software managers; they can provide a review
of the scoring and permit additional factors that are not expressed in the
criteria to be considered. For example, a report from another agency may
state that the selected vendor did not provide adequate service. If the
selected tool did not receive the highest score, the software engineer must
review the tool characteristics thoroughly to avoid unexpected installation
difficulties. (Tool selection concludes the separate procedure for informal
procurement. The overall procedure continues with Step 4.)

TEAM LinG



326

MODERN SOFTWARE TESTING TOOLS

Step 3b: Conduct Selection Activities for Formal Procurement

The following tasks should be performed when a formal tool procurement
plan is in effect.

Task 1: Develop the Acquisition Plan

This plan must include all the elements mentioned for Task 1 of Step 3a,
plus the constraints on the procurement process and the detailed respon-
sibilities for all procurement documents (e.g., statement of work and tech-
nical and administrative provisions in the request for proposal).

Task 2: Create the Technical Requirements Document

The technical requirements document is an informal description of tool
requirements and the constraints under which the tool must operate. It
uses much of the material from the acquisition plan but should add enough
detail to support a meaningful review by the tool user.

Task 3: Review Requirements

The user must review the technical requirements for the proposed pro-
curement. As in the case of Step 3a, Task 4, the user may need to be
prompted with pertinent questions, and there should be close manage-
ment follow-up for a timely response.

Task 4: Generate the Request for Proposal

The technical portions of the request for proposal should be generated
from the technical requirements document and any user comments on it.
Technical considerations typically include:

• A specification of the tool as it should be delivered, including appli-
cable documents, a definition of the operating environment, and the
quality assurance provisions;

• A statement of work for which the tool is procured. This includes
any applicable standards for the process by which the tool is gen-
erated (e.g., configuration management of the tool) and documenta-
tion or test reports to be furnished with the tool. Training and
operational support requirements are also identified in the state-
ment of work; and

• Proposal evaluation criteria and format requirements. These criteria
are listed in order of importance. Subfactors for each may be iden-
tified. Any restrictions on the proposal format (e.g., major headings,
page count, or desired sample outputs) may be included.

Task 5: Solicit Proposals

This activity should be carried out by administrative personnel. Capability
lists of potential sources are maintained by most purchasing organizations.

TEAM LinG



327

Methodology to Evaluate Automated Testing Tools

When the software organization knows of potential bidders, those bidders’
names should be submitted to the procurement office. Responses should
be screened for compliance with major legal provisions of the request for
proposal.

Task 6: Perform the Technical Evaluation

Each proposal received in response to the request for proposal should be
evaluated in light of the previously established criteria. Failure to meet
major technical requirements can lead to outright disqualification of a pro-
posal. Those deemed to be in the competitive range are assigned point
scores that are then considered together with cost and schedule factors,
which are separately evaluated by administrative personnel.

Task 7: Select a Tool Source

On the basis of the combined cost, schedule, and technical factors, a
source for the tool is selected. If this is not the highest-rated technical pro-
posal, managers should require additional reviews by software manage-
ment and the software engineer to determine whether the tool is accept-
able. (Source selection concludes the separate procedure for formal
procurement. The overall procedure continues with Step 4.)

Step 4: Procure the Testing Tool

In addition to verifying that the cost of the selected tool is within the
approved budget, the procurement process considers the adequacy of
licensing and other contractual provisions and compliance with the fine
print associated with all government procurements. The vendor must fur-
nish the source program, meet specific test and performance require-
ments, and maintain the tool. In informal procurement, a trial period use
may be considered if this has not already taken place under one of the pre-
vious steps.

If the acquisition plan indicates the need for outside training, the ability
of the vendor to supply the training and any cost advantages from the com-
bined procurement of the tool and the training should be investigated. If
substantial savings can be realized through simultaneously purchasing the
tool and training users, procurement may be held up until outside training
requirements are defined (Step 7).

Step 5: Create the Evaluation Plan

The evaluation plan is based on the goals identified in Step 1 and the tool
objectives derived in Step 2. It describes how the attainment of these
objectives should be evaluated for the specific tool selected. Typical items
to be covered in the plan are milestones for installation and dates and per-
formance levels for the initial operational capability and for subsequent

TEAM LinG



328

MODERN SOFTWARE TESTING TOOLS

enhancements. When improvements in throughput, response time, or turn-
around time are expected, the reports for obtaining these data should be
identified. Responsibility for tests, reports, and other actions must be
assigned in the plan, and a topical outline of the evaluation report should
be included.

The acceptance test procedure is part of the evaluation plan, although for
a major tool procurement it may be a separate document. The procedure
lists the detailed steps that are necessary to test the tool in accordance
with the procurement provisions when it is received, to evaluate the inter-
action of the tool with the computer environment (e.g., adverse effects on
throughput), and to generate an acceptance report.

Step 6: Create the Tool Manager’s Plan

The tool manager’s plan describes how the tool manager is selected, the
responsibilities for the adaptation of the tool, and the training that is
required. The tool manager should be an experienced systems program-
mer who is familiar with the current operating system. Training in the oper-
ation and installation of the selected tool in the form of review of documen-
tation, visits to the tool’s current users, or training by the vendor must be
arranged. The software engineer is responsible for the tool manager’s plan,
and the tool manager should work under the software engineer’s direction.
The tool manager’s plan must be approved by software management.

Step 7: Create the Training Plan

The training plan should first consider the training that is automatically
provided with the tool (e.g., documentation, test cases, and online diagnos-
tics). These features may be supplemented by standard training aids sup-
plied by the vendor for in-house training (e.g., audio- or videocassettes and
lecturers). Because of the expense, training sessions at other locations
should be considered only when nothing else is available. The personnel to
receive formal training should also be specified in the plan, and adequacy
of in-house facilities (e.g., number of terminals and computer time) should
be addressed. If training by the tool vendor is desired, this should be iden-
tified as early as possible to permit training to be procured along with the
tool (see Step 4). Users must be involved in the preparation of the training
plan; coordination with users is essential. The training plan must be pre-
pared by the software engineer and approved by software management.
Portions of the plan must be furnished to the procurement staff if outside
personnel or facilities are used.

Step 8: Receive the Tool

The tool is turned over by the procuring organization to the software
engineer.

TEAM LinG



329

Methodology to Evaluate Automated Testing Tools

Step 9: Perform the Acceptance Test

The software engineer or staff should test the tool in an as-received condi-
tion with only those modifications made that are essential for bringing the
tool up on the host computer. Once a report on the test has been issued
and approved by the software manager, the tool is officially accepted.

Step 10: Conduct Orientation

When it has been determined that the tool has been received in a satisfac-
tory condition, software management should hold an orientation meeting
for all personnel involved in the use of the tool and tool products (e.g.,
reports or listings generated by the tool). The objectives of tool use (e.g.,
increased throughput or improved legibility of listings) should be directly
communicated. Highlights of the evaluation plan should be presented, and
any changes in duties associated with tool introduction should be
described. Personnel should be reassured that allowances will be made for
problems encountered during tool introduction and reminded that the
tool’s full benefits may not be realized for some time.

Step 11: Implement Modifications

This step is carried out by the tool manager in accordance with the
approved tool manager plan. It includes modifications of the tool, the doc-
umentation, and the operating system. In rare cases, some modification of
the computer (e.g., channel assignments) may also be necessary. Typical
tool modifications involve deletion of unused options, changes in prompts
or diagnostics, and other adaptations made for efficient use in the current
environment. In addition, the modifications must be thoroughly docu-
mented.

Vendor literature for the tool should be reviewed in detail and tailored to
the current computer environment and to any tool modifications that have
been made. Deleting sections that are not applicable is just as useful as
adding material that is required for the specific programming environment.
Unused options should be clearly marked or removed from the manuals. If
the tool should not be used for some resident software (e.g., because of
language incompatibility or conflicts in the operating system interface),
warning notices should be inserted in the tool manual.

Step 12: Train Tool Users

Training is a joint responsibility of the software engineer and the tool users
and should help promote tool use. The software engineer is responsible for
the content (in accordance with the approved training plan), and the tool
user controls the length and scheduling of sessions. The tool user should
be able to terminate training steps that are not helpful and to extend por-
tions that are helpful but need further explication. Retraining or training in

TEAM LinG



330

MODERN SOFTWARE TESTING TOOLS

the use of additional options may be necessary and can provide an oppor-
tunity for users to talk about problems associated with the tool.

Step 13: Use the Tool in the Operating Environment

The first use of the tool in the operating environment should involve the
most qualified user personnel and minimal use of options. This first use
should not be on a project with tight schedule constraints. Resulting diffi-
culties must be resolved before expanded service is initiated. If the first
use is successful, use by additional personnel and use of further options
may commence.

User comments on training, first use of the tool, and the use of extended
capabilities should be prepared and furnished to the software engineer.
Desired improvements in the user interface, in the speed or format of
response, and in the use of computer resources are all appropriate topics.
Formal comments may be solicited shortly after the initial use, after six
months, and again after one year.

Step 14: Write the Evaluation Report

Using the outline generated in Step 5, the software engineer prepares the
evaluation report. User comments and toolsmith observations provide
important input to this document. Most of all, the document must discuss
how the general goals and tool objectives were met. The report may also
include observations on the installation and use of the tool, cooperation
received from the vendor in installation or training, and any other lessons
learned.

Tool and host computer modifications are also described in this report.
It may contain a section of comments useful to future tool users. The report
should be approved by software management and preferably by funding
management as well.

Step 15: Determine Whether Goals Have Been Met

Funding management receives the evaluation report and should determine
whether the goals that were established in Step 1 have been met. This writ-
ten determination should address:

• Attainment of technical objectives
• Adherence to budget and other resource constraints
• Timeliness of the effort
• Cooperation from other agencies
• Recommendations for future tool acquisitions