***- IEEE Software (March_April). Volume 19, Number 2 (2002)***




































C o p y r i g h t  ©  2 0 0 2  S t e v e n  C .  M c C o n n e l l .  A l l  R i g h t s  R e s e r v e d . M a r c h / A p r i l  2 0 0 2 I E E E  S O F T W A R E 5

from the editor
E d i t o r  i n  C h i e f :  S t e v e  M c C o n n e l l  � C o n s t r u x  S o f t w a r e  � s o f t w a r e @ c o n s t r u x . c o m

F
or decades, experts have struggled to
define quality. Edwards Deming said
that the only definition of quality that
mattered was the consumer’s.1 Joseph
Juran said that quality was fitness for
use.2 Philip Crosby provided the

strictest definition of quality as “confor-
mance to requirements.”3

Conformance to
requirements

Although they differ on the de-
tails, quality experts agree that
the customer’s view of require-
ments is critically important. For
that reason, I’ve found Crosby’s
definition of “conformance to re-
quirements” to be the most useful
definition in examining software
quality. Taking into account

many software projects’ tendency to elicit
some but not all of the customer’s complete
requirements, “requirements” cannot be in-
terpreted solely as the written requirements.
Requirements must also include implicit re-
quirements—those that the customer as-
sumes regardless of whether the develop-
ment team happens to write them down.
Thus, the working definition of quality that
I use is “conformance to requirements, both
stated and implied.”

The “ities” of software quality
In addition to specific functional require-

ments, software quality is also affected by
common nonfunctional characteristics that
are often referred to as the “ities.” The ities
that affect software’s internal quality (quality

visible to the software’s developers) include
maintainability, flexibility, portability, re-
usability, readability, scalability, testability,
and understandability. The ities that affect
the software’s external quality (visible to the
customer) include usability, reliability, adapt-
ability, and integrity, as well as correctness,
accuracy, efficiency, and robustness.4

Some of these characteristics overlap, but
all have different shades of meaning that are
desired more in some cases and less in others.
The attempt to maximize certain characteris-
tics invariably conflicts with the attempt to
maximize others. Figure 1 presents a sum-
mary of the ways in which external quality
characteristics affect each other. 

These characteristics will be prioritized
differently on different projects, which means
the software quality target is always chang-
ing. Finding an optimal solution from a set of
competing, changing objectives is challeng-
ing, but that’s part of what makes software
development a true engineering discipline. 

From product quality to project
quality 

When software people refer to quality, we
usually refer to the quality of the software
product we are producing. From a manage-
ment perspective, however, customers also
have requirements for projects. I think it’s
reasonable to draw an analogy from prod-
ucts to projects, conceiving project quality as
conformance to requirements, both stated
and implied. Customers’ functional require-
ments for projects draw from a small num-
ber of possible attributes, namely schedule,
resources, cost, and quality of the product

Real Quality For Real 
Engineers
Steve McConnell



6 I E E E  S O F T W A R E M a r c h / A p r i l  2 0 0 2

FROM THE EDITOR

produced. In some cases, a customer
might prioritize cost higher—in oth-
ers, schedule or product quality. 

Additionally, project quality includes
nonfunctional requirements such as

� Efficiency: Minimal use of sched-
ule, budget, and staff to deliver a
particular software product. 

� Flexibility: The extent to which
the project can be modified to de-
liver software other than that for
which the project was originally
intended or to respond to changes
in project goals. 

� Improvability: The degree to
which project experiences can be
fed back into the project to im-
prove project performance. 

� Predictability: The degree to which
a project’s cost, schedule, and prod-
uct quality outcomes can be fore-
cast in advance.  

� Repeatability: The degree to which
the project after the current project
can be conducted using practices
similar to those used on the cur-
rent project. 

� Robustness: The degree to which
the project will continue to func-
tion in the presence of stressful en-
vironmental conditions.

� Sustainability: The duration for
which a project can continue using
its current practices. 

� Visibility: The ability of a customer
to accurately determine project sta-
tus and progress. 

These project characteristics inter-
play with each other just as the soft-
ware quality attributes do. Figure 2
shows the interactions. In addition to
the interactions shown in Figure 2,
some of these project quality character-
istics tend to support or undermine the
various product characteristics summa-
rized in Figure 1. 

Different projects have different
priorities among efficiency, flexibility,
improvability, and the other character-
istics shown in Figure 2. An estab-
lished business might place high values
on efficiency, predictability, improv-
ability, and repeatability. A start-up
company might place a higher value
on robustness and visibility; it might
not value sustainability and repeata-
bility at all. This suggests that there
isn’t one best definition of project
quality for all projects; the best defi-
nition depends on the project’s con-
sumers and those consumers’ specific
project requirements.  

DEPARTMENT EDITORS

Bookshelf: Warren Keuffel, 
wkeuffel@computer.org

Construction: Andy Hunt and Dave Thomas, 
Pragmatic Programmers, 

{Andy, Dave}@pragmaticprogrammer.com

Country Report: Deependra Moitra, Lucent Technologies
d.moitra@computer.org

Design: Martin Fowler, ThoughtWorks,
fowler@acm.org

Loyal Opposition: Robert Glass, Computing Trends,
rglass@indiana.edu

Manager: Don Reifer, Reifer Consultants,
dreifer@sprintmail.com

Quality Time: Jeffrey Voas, Cigital, 
voas@cigital.com

STAFF

Senior Lead Editor 
Dale C. Strok

dstrok@computer.org

Group Managing Editor
Crystal Chweh

Associate Editors
Jenny Ferrero and 

Dennis Taylor

Staff Editors
Shani Murray, Scott L. Andresen,

and Kathy Clark-Fisher

Magazine Assistants
Dawn Craig

software@computer.org

Pauline Hosillos

Art Director
Toni Van Buskirk

Cover Illustration
Dirk Hagner

Technical Illustrator
Alex Torres

Production Artist
Carmen Flores-Garvey

Executive Director
David Hennage

Publisher
Angela Burgess

Assistant Publisher
Dick Price

Membership/Circulation
Marketing Manager
Georgann Carter

Advertising Assistant
Debbie Sims

CONTRIBUTING EDITORS

Greg Goth, Denise Hurst, Gil Shif, Keri Schreiner,
and Margaret Weatherford

Editorial: All submissions are subject to editing for clarity,
style, and space. Unless otherwise stated, bylined articles
and departments, as well as product and service descrip-
tions, reflect the author’s or firm’s opinion. Inclusion in
IEEE Software does not necessarily constitute endorsement
by the IEEE or the IEEE Computer Society.

To Submit: Send 2 electronic versions (1 word-processed
and 1 postscript or PDF) of articles to Magazine Assistant,
IEEE Software, 10662 Los Vaqueros Circle, PO Box 3014,
Los Alamitos, CA 90720-1314; software@computer.org. Ar-
ticles must be original and not exceed 5,400 words including
figures and tables, which count for 200 words each. 

Correctness

Usability

Efficiency

Reliabilty

Integrity

Adaptability

Accuracy

Robustness

⇑

⇓

⇑

⇑

⇓

⇑

⇑

⇑
⇑ Helps ⇓ Hurts

 ⇑ 

⇑

⇓

⇓

⇓

⇑

⇓

⇑

⇑

⇑

⇓

 

⇓

⇑

⇑

⇓

⇓

⇑

⇓

 

⇑

⇓

⇑

⇑

⇑

⇓

 ⇑ 

 

⇑

⇓

⇓

⇓

⇓

 

 ⇑ 

⇓

⇑

Co
rre

ctn
es

s

Us
ab

ilit
y

Eff
ici

en
cy

Re
lia

bil
ty

Int
eg

rity

Ad
ap

tab
ilit

y

Ac
cu

rac
y

Ro
bu

stn
es

s

How focusing
on the factor
below affects
the factor
to the right

Figure 1. Interactions between product quality external 
characteristics.



M a r c h / A p r i l  2 0 0 2 I E E E  S O F T W A R E 7

FROM THE EDITOR

Real engineering
One difference between a crafts-

man and an engineer is that a crafts-
man defines quality on his own
terms, whereas an engineer defines
quality through his customers’ eyes.
The craftsman settles into a way of
working that suits him personally,
while the engineer adapts his ap-
proach on each project to best satisfy
his customer’s requirements. 

Software engineering purists ar-
gue that software should always be
produced to the highest level of
quality, by which they mean the
highest levels of product quality.
End-user requirements certainly
should be considered, but the orga-
nization that builds and sells the
software is another consumer whose
requirements must be taken into ac-
count. The product characteristics
that constitute quality to the end
user do not necessarily satisfy the
software-developing organization’s
project quality requirements. 

As Deming pointed out in Out of
the Crisis, different consumers can
have different definitions of quality
for the same product, and this ap-
plies as much to project quality as it
does to product quality. The project

team, manager, and sponsoring orga-
nization can all be considered con-
sumers of a project. A manager
might consider a project to have high
quality if it provides good visibility,
robustness, and repeatability. The
project team might value efficiency,
improvability, and sustainability. The
sponsoring organization might value
predictability and flexibility. 

A manager who factors product
quality into the project plans but ig-
nores project goals takes an abridged
view of software quality. One hall-
mark of engineering work is the con-
stant balancing of trade-offs. With
the extensive trade-off decisions re-
quired to balance both software
product attributes and software pro-
ject goals, software personnel have
abundant opportunities to hone their
engineering skills in this area.

References
1. W. Edwards Deming, Out of the Crisis, MIT

Press, Cambridge, Mass., 2000. 
2. J.M. Juran, Juran’s Quality Handbook, Mc-

Graw-Hill, New York, 1998. 
3. P.B. Crosby, Quality Is Free: The Art of Mak-

ing Quality Certain, Mentor Books, Denver
Colo., 1992. 

4. S. McConnell, Code Complete, Microsoft
Press, Redmond, Wash., 1993. 

EDITOR IN CHIEF: 
Steve McConnell

10662 Los Vaqueros Circle
Los Alamitos, CA 90720-1314

software@construx.com

EDITOR IN CHIEF EMERITUS:
Alan M. Davis, Omni-Vista

ASSOCIATE EDITORS IN CHIEF

Design: Maarten Boasson, Quaerendo Invenietis
boasson@quaerendo.com

Construction: Terry Bollinger, Mitre Corp.
terry@mitre.org

Requirements: Christof Ebert, Alcatel Telecom
christof.ebert@alcatel.be

Management: Ann Miller, University of Missouri, Rolla
millera@ece.umr.edu

Quality: Jeffrey Voas, Cigital
voas@cigital.com

Experience Reports: Wolfgang Strigel, 
Software Productivity Center; strigel@spc.ca

EDITORIAL BOARD

Don Bagert, Texas Tech University
Richard Fairley, Oregon Graduate Institute

Martin Fowler, ThoughtWorks
Robert Glass, Computing Trends

Andy Hunt, Pragmatic Programmers
Warren Keuffel, independent consultant
Brian Lawrence, Coyote Valley Software

Karen Mackey, Cisco Systems
Deependra Moitra, Lucent Technologies, India

Don Reifer, Reifer Consultants
Suzanne Robertson, Atlantic Systems Guild

Dave Thomas, Pragmatic Programmers

INDUSTRY ADVISORY BOARD

Robert Cochran, Catalyst Software (chair) 
Annie Kuntzmann-Combelles, Q-Labs

Enrique Draier, PSINet
Eric Horvitz, Microsoft Research

David Hsiao, Cisco Systems
Takaya Ishida, Mitsubishi Electric Corp.

Dehua Ju, ASTI Shanghai
Donna Kasperson, Science Applications International

Pavle Knaflic, Hermes SoftLab
Günter Koch, Austrian Research Centers

Wojtek Kozaczynski, Rational Software Corp.
Tomoo Matsubara, Matsubara Consulting

Masao Matsumoto, Univ. of Tsukuba
Dorothy McKinney, Lockheed Martin Space Systems

Nancy Mead, Software Engineering Institute
Stephen Mellor, Project Technology

Susan Mickel, AgileTV
Dave Moore, Vulcan Northwest

Melissa Murphy, Sandia National Laboratories
Kiyoh Nakamura, Fujitsu

Grant Rule, Software Measurement Services
Girish Seshagiri, Advanced Information Services

Chandra Shekaran, Microsoft
Martyn Thomas, Praxis

Rob Thomsett, The Thomsett Company 
John Vu, The Boeing Company

Simon Wright, Integrated Chipware
Tsuneo Yamaura, Hitachi Software Engineering

MAGAZINE OPERATIONS COMMITTEE

George Cybenko (chair), James H. Aylor, Thomas J.
Bergin, Frank Ferrante, Forouzan Golshani, Rajesh
Gupta, Steve McConnell, Ken Sakamura, M. Satya-

narayanan, Nigel Shadbolt, Munindar P. Singh,
Francis Sullivan, James J. Thomas

PUBLICATIONS BOARD

Rangachar Kasturi (chair), Mark Christensen,
George Cybenko, Gabriella Sannitti di Baja, Lee

Giles, Thomas Keefe, Dick Kemmerer, 
Anand Tripathi

Efficiency

Flexibility

Improvability

Predictability

Repeatability

Robustness

Sustainability

Visibility

⇑

⇑

⇓

⇓

⇑

⇓

⇑

⇑

⇓

⇓

⇑

⇓

⇑
⇑ Helps ⇓ Hurts

 

⇑

⇑

⇑

⇓

⇑

⇑

⇑

⇑

 

⇑

⇑

⇑

⇑

⇑

⇓

⇑

⇑

⇓

 

⇑

 

⇑

⇑

⇑

 

⇑

 

⇑

⇑

⇓

⇑

⇑

 

 

⇑

Eff
ici

en
cy

Fle
xib

ilit
y

Im
pro

va
bil

ity

Pre
dic

tab
ilit

y

Re
pe

ata
bil

ity

Ro
bu

stn
es

s

Su
sta

ina
bil

ity

Vis
ibi

lity

How focusing
on the factor
below affects
the factor
to the right

Figure 2. Interactions between project quality characteristics.



1 2 I E E E  S O F T W A R E M a r c h / A p r i l  2 0 0 2 0 7 4 0 - 7 4 5 9 / 0 2 / $ 1 7 . 0 0  ©  2 0 0 2  I E E E

W
hen I think of Internet develop-
ment, I picture a handful of devel-
opers working frantically against
the clock amid the litter of take-
out food containers to churn out
code for the Web. As you might

expect, these developers tend to be recent
graduates who use the latest technology to

turn out applications that live for
weeks but not years. 

Not that this is bad. Some of
the best work I have seen in years
has come about this way. The
reason is simple. With their “We
can do anything” attitude, these
teams are willing to work on
technology’s proverbial bleeding
edge. They know what they pro-
duce is going to change, so they
aren’t afraid to try something

new. That’s because they know they can fix
problems as they occur without suffering the
severe penalties they might have faced in ear-
lier development environments. 

This article contrasts the current Internet
and intranet development climate with ear-
lier releases (see Table 1) and identifies the
10 greatest risks facing today’s Young Inter-
net/intranet Lions.

Where’s the action on today’s
Internet?

With the world economy’s recent slow-
down, you might easily think that the move
to the Net has also slackened. Don’t believe
it. As Table 2 shows, current trends in Inter-
net and intranet project work indicate oth-

erwise. As the economy fizzled last year,
large firms stopped expanding, instead con-
solidating and moving to the Web to cut
costs. Yes, lots of startups bit the dust, but
in many cases Big Business accelerated its
move to the Web. Firms such as GE have re-
portedly saved millions by pursuing such
strategies (James P. Pelz, “GE Takes to the
Net to Lower Company Costs,” Los Angeles
Times, 9 Oct. 2000, pp. C1–5). As an exam-
ple of the impact, a large financial client of
mine currently has over 2,000 programmers
involved in Web development. Five years
ago, its 3,000-member software workforce
worked solely on client-server applications.

Traditional versus Internet and
intranet development risks

Based on this background information,
you might well ask, “As firms large and
small move applications across the enter-
prise to the Web, what risks do they face?
How do these compare with risks that
they’ve previously encountered?” Table 3
answers these questions. In the first two
columns, I’ve listed the risks in terms of
their cost and schedule impacts for both tra-
ditional as well as Internet/intranet develop-
ments. You’ll see why I consider them
deadly. The noticeable differences in risks
stem from the project characteristics (large
versus small teams, technology-driven ver-
sus requirements-driven, and so forth) and
who’s moving to the Net (large firms instead
of small startups, for example). While many
of the risks are familiar to traditional proj-
ects, the impact of unrealistic expectations

manager
Ten Deadly Risks in 
Internet and Intranet 
Software Development
Donald Reifer

E d i t o r :  D o n a l d  J .  R e i f e r  � R e i f e r  C o n s u l t a n t s  � d . r e i f e r @ i e e e . o r g



M a r c h / A p r i l  2 0 0 2 I E E E  S O F T W A R E 1 3

MANAGER

and technology volatility should strike you as new and
important. Don’t get confused by Table 3. The mitiga-
tion strategies listed are aimed primarily at Internet/in-
tranet developments. 

How do I mitigate these risks?
Just identifying the risks is easy. Knowing how to

reduce their deadly consequences is much harder. The
third column of Table 3 lists my recommendations for
dealing with primarily the Internet/intranet risks.
Most of these mitigation strategies aren’t new, but
some are innovative, I believe. That’s because, until re-
cently, issues identified in Internet and intranet pro-
jects hadn’t surfaced or were deemed less important.
For example, you need to specifically align Web de-
velopment with your business goals. Else, why build
them? As another example, concepts such as nightly
builds, weekly releases to the field, and having the cus-
tomer function as a member of the programming team
are novel.

If you’re interested in risk management, you’ll find
many publications on the topic, many based on expe-
rience rather than theory. But, be warned. In most
cases, you will have to extend this experience into the
Internet/intranet world as you try to address the pecu-
liarities of Web projects. That’s not so bad when you
think about it. It’s easier to extend existing knowledge
than to plow new ground.

T he list of 10 deadly Internet and intranet riskswon’t surprise many. But based on them, softwareengineers need to strike a balance between the tech-
nologies that many in the Internet world depend

Table 1

Traditional versus Internet/intranet development 
Characteristic Traditional approaches Internet/intranet development

Primary goal Build quality software products at minimum cost Bring quality products to market as quickly as possible
Typical project size Medium to large (20 or more team members) Small (3 to 7 team members)
Typical timeline 10 to 18 months Several weeks to 6 months
Development approach Classical requirements-based paradigms; done Rapid application or spiral development 
employed phased or incrementally; document-driven with (Rational Unified Model, MBase, and so on)

focus on getting the design right before coding
Primary technologies used Traditional methods; object-oriented approaches, Agile methods; component-based approaches; multimedia;

procedural languages (C, C++); limited, multimedia, new language environments (Java, XML, HTML, and so
CASE tools; use cases; lots of modeling; and so on forth); visualization; and so on

Processes employed Software Capability Maturity Model, SPICE, Generally ad hoc, pairwise programming; refactoring
ISO-based

Products developed Code-based systems; mostly new; some reuse; lots Object-based systems; many reusable building blocks; few
of legacy involved; many external interfaces; external interfaces; little interaction; simple applications
lots of interactions; often complex applications

People involved Professional software engineers typically with 5+ Less experienced programmers; users as developers; 
years of experience in at least two pertinent graphic designers; new hires right out of school
application domains

DSDM

RAPID  A game-changing, non-
proprietary rapid and agile 
development project model for 
building business solutions of any 
size in short timeframes.  It shortens 
the clock-speed (and time to market) 
for delivery of core business 
benefits.  It is the only approach that 
can guarantee delivery on an exact 
day under tight,  Internet-time 
deadlines.
 
AGILE Created using 
agile techniques and now 
in version 4.0,  it continues 
to be evolved using DSDM, 
the only Agile Method created 
using an Agile Method.

NON-PROPRIETARY 
The non-proprietary DSDM method 
evolved collaboratively over the last 
6 years by an International nonprofit 
consortium.  It's tool and technology 
independent.  No tools to purchase 
or be hamstrung by.  

BUSINESS CENTERED  Built in 
partnership with big business for 
business.   DSDM will scale from five 
to hundreds of developers.  As the 
most business benefit focused 
method in the world, many of the 
world's largest companies and 
numerous government bodies have 
established DSDM as their standard.  
DSDM is  ISO 9000 certified.

WHAT IS DSDM?  DSDM 
(Dynamic Systems Development 
Method) is a framework that 
encompasses all aspects needed 
for successful delivery: people, 
process and technology - with 
emphasis on people; most 
projects fail as a result of 
people-based problems.  

DSDM is primarily non-
prescriptive  so it is 

synergistic with many 
software development 
methods such as XP, 
Scrum, Adaptive, Crystal, 

etc.   The framework 
provides underlying 

principles, processes,  project 
lifecycle, artifacts, key roles and 

responsibilities, and guidance on 
management techniques and 
development techniques.  It's a 
tool to effectively:
    * Understand,
    * Plan,
    * Communicate,
    * Control, and
    * Deliver all projects.

LEARN MORE  visit the DSDM 
Consortium web site at 
http://www.dsdm.org or contact 
DSDM directly at 
800-610-6776.



1 4 I E E E  S O F T W A R E M a r c h / A p r i l  2 0 0 2

MANAGER

upon, on the one hand, and the tried
and true processes that promote risk
management and help us achieve
business goals, on the other.

The “just do it” attitude of the
past few years has done as much
harm as good to the software engi-
neering profession. While a few good

practices have emerged for Internet
development, many time-tested soft-
ware engineering practices were jetti-
soned based on the faulty belief that
teams don’t have time to put these
practices to work. By looking at the
list of deadly risks, we can recognize
our mistake. My message is that

“Good software engineering pays off
on Internet applications, too.” Try
them, they’ll make your job easier.

Don Reifer is president of Reifer Consultants and visiting
associate of the Center for Software Engineering at the Univer-
sity of Southern California. Contact him at d.reifer@ieee.org.

Table 3
Traditional and Internet and intranet project risks and mitigation strategies

Traditional risks Ten deadly Internet and intranet project risks Internet/intranet Risk mitigation strategies

Personnel shortfalls Personnel shortfalls Bring on a skilled core team.
Have the team mentor new people.
Make training and teamwork part of the culture.
Hire top-notch personnel while the job market remains soft.

Unrealistic budgets and schedules Misalignment with business goals Align developments with business goals and highlight 
importance of development.

Volatile requirements Unrealistic customer and schedule expectations Make the customer part of the team.
Set schedule goals around frequent deliveries of varying 
functionality.

Shortfalls in externally Volatile technology, changing so rapidly that Introduce new technology slowly, according to a plan.
furnished components it is hard to keep up (SML, .NET, Persistence, J2EE, Use technology because it supports business goals, 

and so forth) not because it is the latest and greatest thing to do.
Gold-plating Unstable software releases (although the software Stabilize requirements and designs as much as practical.

works, it performs poorly and crashes frequently) Plan to refactor releases from the start.
Don’t deliver applications when quality is poor and system 
crashes (say “no”).

Released software has poor quality Constant changes in software functionality Manage functionality using releases.
(shooting at a moving target) Deliver working prototypes before you target new functionality.

New methods and unstable tools Even newer methods and more unstable tools (causes Introduce new methods and tools slowly, as justified by
(primarily causing schedule delays) heartburn, headaches, and schedule delays) business cases, not merely because they are new and 

appealing.
Make sure methods and tools are of production quality.

High turnover (especially of High turnover (especially of those personnel skilled Set clear expectations and measures of success.
skilled personnel) in the new technology) Make effort a learning exercise (to make staff feel they are 

learning, growing, and gaining valuable experience).
Friction between developers Friction within the team (lack of leadership, overwork, Staff the team carefully with compatible workforce.
and customers unrealistic expectations, and so forth) Build team and provide it with leadership.

Manage conflicts to ease friction.
Unproductive office space Unproductive office space Acquire dedicated workspace for the team.

Appropriate collaboration tools.
Lots of space available for meetings and pizza.

Table 2

Who’s Developing Internet/intranet Applications?
Time Period Who’s playing? What’s their primary interest?

1980s Researchers Technical issues (protocols, for example)
Early 1990s Internet service providers Meeting the demand for performance (many subscribers)
Late 1990s Internet startups Content is king as everyone moves to the Net
Now Large firms as well as yet more startups Save money, collaborate, and build an e-business infrastructure



0 7 4 0 - 7 4 5 9 / 0 2 / $ 1 7 . 0 0  ©  2 0 0 2  I E E E M a r c h / A p r i l  2 0 0 2 I E E E  S O F T W A R E 1 5

M
anaging requirements for busi-
ness value? What’s there to man-
age? Aren’t they just...there?”
Such perplexity was perfectly un-
derstandable in the good old
days, when all you had to do was
get the requirements from your
customer, design your system,
and produce one of those awful
conformance matrices that
demonstrated that you had im-
plemented each and every one of
the requirements in your system.
Against this dreary backdrop of
conformance matrices and the
like, it is indeed hard to imagine
a role for requirements in the de-
velopment and execution of busi-

ness strategy. But, as the song says, the
times, they are a changin’. We are now look-
ing outside the traditional boundaries and
developing ways of getting maximum busi-
ness value from our investment in require-
ments. This column discusses three ways of
realizing this value: managed requirements
process, requirements agility, and contrac-
tual innovation.

Toward requirements reuse
The first important change in recent years

has been the emergence of a true, actively
managed requirements process, which re-
places the passive approach of the past

where requirements simply arrived, if you
were lucky, on a one-way street from the
customer. An analyst who masters the re-
quirements process can become an active
participant in the strategic conception of a
system or product. He or she can elicit and
formulate requirements in such a way that
the path from requirement to implemented
system feature is illuminated in all of its con-
sequences, both technical and economic. Of
course, we already know that a robust re-
quirements process is a key factor in resolv-
ing problems early in the life cycle, with all
the familiar economic benefits. But an ac-
tively managed requirements process gives
you much more than that. Requirements an-
alysts and customers alike can discover a
new flexibility. Although there are always a
few nonnegotiable requirements (such as no
loss of human life), the vast majority are
suitable for examination, reformulation, ne-
gotiation, or adaptation. As unexpected
consequences are uncovered (for example,
the projected high cost of implementation),
the analyst can cooperate with the customer
to search for other, equally satisfactory re-
quirements formulations. The economic
tools of cost–benefit analysis for this process
have been well understood for years.

A full cost–benefit analysis of a require-
ment (or group of requirements) needs an
investment in time and resources. Further-
more, assessing the cost–benefit of require-

requirements

Managing Requirements
for Business Value
John Favaro

E d i t o r :  S u z a n n e  R o b e r t s o n  � T h e  A t l a n t i c  S y s t e m s  G u i l d  � s u z a n n e @ s y s t e m s g u i l d . c o m

Requirements engineers are often asked the question, “What will the investment in require-
ments give to our business?” John Favaro uses his experience in financial engineering to help
us answer this question by providing some tools for quantifying the value of requirements.

—Suzanne Robertson



1 6 I E E E  S O F T W A R E M a r c h / A p r i l  2 0 0 2

DEPT TITLE

ments is more difficult than design or
implementation, because require-
ments are the furthest upstream in
the development process. Conse-
quently there are more unknown fac-
tors; it can take a full development
cycle before the complete economic
impact of a requirement is known. 

Perhaps you can now understand
why requirements researchers are
studying techniques for creating re-
usable requirements.1 The first time
you conceive a system based on a re-
quirement, estimating the costs and
benefits might be difficult. But, after
carrying through the system to imple-
mentation, you will have a much bet-
ter idea of the costs and benefits trig-
gered by that requirement. A well-
formulated, measurable, reusable re-
quirement—including a full cost–
benefit analysis as part of its descrip-
tion—is every bit as valuable as a
reusable software module.

Agile requirements
The second important change has

been the emergence of strategies for
confronting the bête noire of the re-
quirements process: vague and chang-
ing requirements. These strategies are
best reflected in a new generation of
software life cycles known as agile
processes (see www.agilealliance.org).
Some prominent examples of agile
processes include Alistair Cockburn’s
Crystal Methods, Bob Charette’s Lean
Development, and Jim Highsmith’s
Adaptive Software Development.
The common denominator of these
agile processes is the iterative devel-
opment paradigm, which breaks up
the tradition of upfront requirements
elicitation and analysis. No longer do
you fire and forget requirements and
then move on to the next phase. Re-
quirements may be introduced, mod-
ified, or removed in successive itera-
tions. As Kent Beck (chief evangelist
of Extreme Programming, the most
visible of the agile processes) exhorts
us, requirements management should
“embrace change.”2 Business condi-
tions change, giving rise to new re-
quirements; requirements thought to
be critical turn out not to be as the
customer sees the first versions of the

system. Requirements that once were
vague become crystal clear as uncer-
tainty is resolved; a requirement once
thought to be rigid could be negoti-
ated and reformulated to permit sev-
eral alternative features that could
satisfy it.

Such changing conditions provide
opportunities for the strategist to in-
crease the value of his process. How-
ever, the traditional tools of cost–ben-
efit analysis that apply so well to the
noniterative requirements process
have proven less adequate to help the
requirements analyst examine the eco-
nomic value of his or her newfound
strategic flexibility—and this brings
me to the third important change in re-
cent years. I’d like to draw now on my
interaction with Kent Beck over the
past few years to discuss some cutting-
edge ideas about the relationship be-
tween strategy and finance and their
effect on requirements management.

Contractual innovation
Evaluating the financial impact of

strategic decisions has been the sub-
ject of great debate since the dawn of
economics as a discipline. In a list of
the Top 10 Unsolved Problems in Cor-
porate Finance first compiled by the
legendary financial authors Richard
Brealey and Stewart Myers in 1981
and unchanged in last year’s sixth edi-
tion of their classic textbook Princi-
ples of Corporate Finance, the finan-
cial impact was ranked Number 1.3 In
recent years, people have placed hope
in a new branch of financial theory
known as contingent claims analysis—
or more popularly, real options—
made possible by the breakthroughs
in the 1970s in option pricing theory.
In this approach, the opportunities
created by strategic flexibility are eval-
uated with the financial tools of op-
tion pricing theory.

Let’s take an example from XP.
Suppose the customer requires your
application to provide access to an
enterprise-wide knowledge manage-
ment system that he or she is con-
templating introducing in a few
months. A simple cost–benefit analy-
sis on the system features that would
satisfy this requirement is positive,

say, $10 implementation cost versus
$15 in benefits. But, an enormously
uncertain environment undermines the
cost–benefit analysis. The customer
admits that the uncertainty (“volatil-
ity”) of his estimate is as much as 100
percent. If the knowledge manage-
ment system is never introduced,
then it will have been a waste of time
to provide the access capability.
However, if it is introduced, the ac-
cess capability could become far
more valuable than originally envi-
sioned. The customer says that the
uncertainty will be resolved in a year.
The XP process permits the strategy
of waiting until a future iteration to
take up the requirement. Is there any
way to calculate the economics of
this alternative strategy? The tools of
option pricing theory can in fact cal-
culate the value of waiting to be
slightly less than $8—more than the
$5 of benefit accrued by immediate
development.4

The option to delay implementing
a requirement is an example of the
way that contingent claims analysis
is making a profound impact on the
requirements process in the form of
contractual innovation, a result of
the new discipline of financial engi-
neering born with the advent of op-
tion pricing theory. Beck likes to say
that unlike fixed-scope traditional
contracts, XP contracts have op-
tional scope: every iteration provides
a formal decision point in which the
customer can change direction, aban-
doning requirements, introducing
new requirements, or selecting be-
tween alternative requirements.

For example, you sign such con-
tracts not for a fixed set of functional-
ity, but for a team’s best effort for a
fixed period at a fixed price. The pre-
cise scope to be implemented will be
negotiated periodically over the life of
the contract, much as professional ser-
vices contracts are run today. Changes
in the team’s actual velocity and the
relative estimates attached to each fea-
ture are factored into these scope ne-
gotiations, as are changes in the per-
ceived relative value of these features.

Option pricing theory yields some
surprising insights to the economic

REQUIREMENTS



DEPT TITLEREQUIREMENTS

value of such contracts. For instance,
there is an exotic type of option known
as a best-of or rainbow option. The
owner possesses two options, of which
only one can be exercised. The rain-
bow option has the most value when
the alternatives are negatively corre-
lated—that is, when the same condi-
tions that increase one alternative’s
value will decrease the other’s. This
implies that a contract’s value is en-
hanced by contradictory requirements.
For example, two requirements each
specifying the application to run on a
different platform is a rainbow option.
If the choice can be delayed to some
later time, it adds value for the cus-
tomer, letting him hedge the ultimate
platform choice in the contract.

Similarly, a requirement whose util-
ity is uncertain (estimated cost and
value are close) gains value by inclusion
in an optional scope clause, because the
option to delay implementation has
demonstrable economic value. Where
contractual flexibility exists to select
among alternative requirements, add
requirements, or even to abandon re-
quirements, economic value is added.

Adding value
What does all this mean for you as a

requirements analyst? As the require-
ments process evolves to embrace in-
creased strategic flexibility, and the new
financial tools of contingent claims
analysis mature, requirements become
an important currency in system char-
acteristic negotiation. By learning to
create reusable requirements with a
companion cost–benefit analysis, you
bring valuable material to the table
from the very beginning. By studying
the new generation of agile develop-
ment processes, you become fluent in
the strategic possibilities to add value to
the requirements process over the entire
product life cycle. By learning some-
thing about the new tools of financial
analysis I’ve introduced, you can better
understand how strategic flexibility in
the requirements process adds value.

For that is what the requirements
process should be about. If you remem-
ber nothing else from this column, re-
member this—stamp it on your forehead
if necessary: the purpose of the require-
ments process should not be to “cover
all eventualities,” or to “limit the dam-

age,” or to “minimize risk,” or even to
“satisfy the customer.” The purpose of
the requirements process is to add busi-
ness value. It is a subtle shift in perspec-
tive for the requirements analyst, but it
makes all the difference because it puts
you in the position of managing re-
quirements to make the most of your
strategic opportunities.

References
1. S. Robertson and J. Robertson, “Chapter 12:

Reusing Requirements,” Mastering the Re-
quirements Process, Addison-Wesley, Reading,
Mass., 1999, pp. 218–234.

2. K. Beck, Extreme Programming Explained:
Embrace Change, Addison-Wesley, Reading,
Mass., 1999.

3. R. Brealey and S.C. Myers, Principles of Cor-
porate Finance, McGraw-Hill, New York,
2000.

4. M. Amram and N. Kulatilaka, Real Options:
Managing Strategic Investment in an Uncer-
tain World, Harvard Business School Press,
Cambridge, Mass., 1999.

John Favaro is an independent consultant based in Pisa,
Italy. He is European co-chair of the IEEE Technical Subcommittee
on Software Reuse and a founding member of the steering com-
mittee of the Society for Software Education. He has degrees in
computer science from Berkeley and Yale. Contact him at Via
Gamerra 21, 56123 Pisa, Italy; jfavaro@tin.it. 

Doing Software Right
• Demonstrate your level of ability in 

relation to your peers

• Measure your professional knowledge
and competence

The CSDP Program differentiates between 
you and others in a field that has every kind of 
credential, but only one that was developed by, 
for, and with software engineering professionals.

Register Today
Visit the CSDP web site at http://computer.org/certification

or contact certification@computer.org

Get CSDP Certified
Announcing IEEE Computer Society's new 

Certified Software Development
Professional Program

"The exam is valuable to me for two reasons:
One, it validates my knowledge in various areas of exper-
tise within the software field, without regard to specific
knowledge of tools or commercial products...
Two, my participation, along with others, in the exam and
in continuing education sends a message that software de-
velopment is a professional pursuit requiring advanced ed-
ucation and/or experience, and all the other requirements
the IEEE Computer Society has established. I also believe
in living by the Software Engineering code of ethics en-
dorsed by the Computer Society. All of this will help to im-
prove the overall quality of the products and services we
provide to our customers..."
— Karen Thurston, Base Two Solutions



1 8 I E E E  S O F T W A R E M a r c h / A p r i l  2 0 0 2 0 7 4 0 - 7 4 5 9 / 0 2 / $ 1 7 . 0 0  ©  2 0 0 2  I E E E

O
ne of the growing trends in software
design is separating interface from im-
plementation. The principle is about
separating modules into public and
private parts so that you can change
the private part without coordinating

with other modules. However, there is a fur-
ther distinction—the one between public and
published interfaces. This distinction is im-

portant because it affects how you
work with the interface.

Public versus published
Let’s assume I’m writing an ap-

plication in a modern modular
language—to make things more
concrete, let’s assume this lan-
guage is Java. My application thus
consists of several classes (and in-
terfaces), each of which has a pub-
lic interface. This public interface

of a class defines a group of methods that any
other class in the system can invoke. 

While I’m enhancing a public method, I re-
alize that one of its parameters is redundant—
I don’t need to use it in the method (maybe I
can get that value through some other route
or maybe I just don’t need it anymore). At this
point, I can eliminate that value from the
method signature, clarifying the method and
potentially saving work for its callers. Because
this method is public, any class in the system
can call it. Should I remove the parameter?

In this case, I would argue yes, because
there are benefits and it isn’t difficult. Al-

though the method might be used anywhere,
I can easily find the users with a search tool.
If I have one of the new breeds of refactor-
ing tools (see www.refactoring.com for de-
tails) available for Java, I can do it with a
simple menu click—the tool will then auto-
matically update all the callers. So, changing
a public method isn’t a big deal.

However, things rapidly change if I put
that software out on the Web as a compo-
nent, and other people, whom I don’t know,
start building applications on top of it. If I
now delete the parameter, everybody else’s
code will break when I upgrade. Now I must
do something a little more elaborate. I can
produce the new method with one less para-
meter but keep the old method—probably
recoding the old method to call the new one.
I mark the old method as deprecated, assum-
ing people will move the code over and that
I can change it in the next release or two.

The two cases are quite different, yet
there’s nothing in the Java language to tell
the difference—a gap that’s also present in a
few other languages. Yet there’s something
to be said for the public–published distinc-
tion being more important than the more
common public–private distinction.

The key difference is being able to find
and change the code that uses an interface.
For a published interface, this isn’t possible,
so you need a more elaborate interface 
update process. Interface users are either
callers or are classes that subclass or imple-
ment an interface.

design

Public versus Published
Interfaces
Martin Fowler

E d i t o r :  M a r t i n  F o w l e r  � T h o u g h t Wo r k s  � f o w l e r @ a c m . o r g



M a r c h / A p r i l  2 0 0 2 I E E E  S O F T W A R E 1 9

DESIGN

Advice on publishing
Recognizing the difference be-

tween public and published leads to
an important set of consequences.

Don’t treat interfaces as published
unless they are

If you need to change an interface
and can find and change all users,
then don’t bother going through all
the forwarding and deprecation gam-
bits. Just make the change and up-
date the users. 

Don’t publish interfaces inside a team
I once suggested to somebody

that we change a public method,
and he objected because of the prob-
lems caused by its being published.
The real problem was that although
there were only three people on the
team, each developer treated his in-
terfaces as published to the other
two. This is because the team used a
strong form of code ownership in
which each module was assigned to
a single programmer and only that
programmer could change the mod-
ule’s code. I’m sympathetic to code
ownership—it encourages people to
monitor their code’s quality—but a
strong code ownership model such
as this one causes problems by forc-
ing you to treat interperson inter-
faces as published. 

I encourage a weaker ownership
model in which one person is re-
sponsible for the module but other
people can make changes when nec-
essary. This lets other developers do
things such as alter calls to changed
methods. (You can also use collec-
tive code ownership—where anyone
can change anything—to avoid in-
ternal publishing.) This kind of
ownership usually requires a config-
uration management system that
supports concurrent writers (such as
CVS) rather than one that uses pes-
simistic locking.

There is a limit to how big a team
you can run without some form of
internal publishing, but I would err
on the side of too little publishing. In
other words, assume you don’t need
to publish interfaces, and then adjust
if you find this causes problems.

Publish as little as you can as late as
you can

Because publishing locks you into
the slower cycle of changes, limit
how much you publish. This is where
a language’s inability to distinguish
between public and published be-
comes an issue. The best you can do
is declare some modules to be the in-
terface and then counsel your soft-
ware users not to use the other mod-
ules, even if they can see them. Keep
these interfaces as thin as you can.
Publish as late as possible in the de-
velopment cycle to give yourself time
to refine the interfaces. One strategy
is to work closely with one or two
users of your components—users
who are friendly enough to cope with
sharp interface changes—before you
publish to the masses.

Try to make your changes additions
In addition to distinguishing be-

tween published and public interfaces,
we can also identify two types of in-
terface changes. Generally, changes
can alter any aspect of an interface.
However, there are some changes that
only cause additions to an interface,
such as adding a method. Additions
won’t break any of the interface’s
clients—existing clients have no prob-
lem using the old methods. Conse-
quently, when you make a change, it’s
worth considering whether you can
recast it into an addition. For exam-
ple, if you need to remove a parame-

ter from a method, instead of chang-
ing the method, try adding a new
method without the parameter. That
way, you get an addition rather than a
general alteration, and your clients re-
main compatible.

Additions can still cause prob-
lems if outside groups have their
own implementation of your inter-
face. If that happens, even adding a
method breaks the alternative im-
plementation. Thus, some compo-
nent technologies, such as COM,
use immutable interfaces. With an
immutable interface, once it’s pub-
lished, you guarantee not to change
it. If you want to change the inter-
face, you must create a second inter-
face, and components can then sup-
port this interface at their leisure.
It’s not the ideal scheme, but it cer-
tainly has its merits.

I would like to see the public–published distinction appear morein languages and platforms. It’s
also interesting that environments
don’t tend to provide the facilities to
evolve interfaces. Some can deprecate
a method that’s due to be removed:
Eiffel does this as part of the lan-
guage, and Java does it (but as part
of the built-in documentation). I
haven’t seen anyone add a marker to
a method that warns implementers of
something that’s going to be added
or would let you add something to an
interface in a provisional way.

That’s part of a more general issue
in software platforms. So far, plat-
forms haven’t sufficiently understood
that software is supposed to be soft
and thus needs facilities that allow
change. In recent years, we’ve taken
more steps in this direction with
component-packaging systems, but
these are just the early steps.

Martin Fowler is the chief scientist for ThoughtWorks, an
Internet systems delivery and consulting company. Contact him
at fowler@acm.org.

There’s something 
to be said for the
public–published
distinction being 

more important than 
the more common

public–private
distinction.



2 0 I E E E  S O F T W A R E M a r c h / A p r i l  2 0 0 2 0 7 4 0 - 7 4 5 9 / 0 2 / $ 1 7 . 0 0  ©  2 0 0 2  I E E E

T
his isn’t programming, this is archaeol-
ogy!” the programmer complained, wad-
ing through the ancient rubble of some
particularly crufty pieces of code. (One
of our favorite jargon words: www.
tuxedo.org/~esr/jargon/html/entry/crufty.

html.) It’s a pretty good analogy, actually. In
real archaeology, you’re investigating some

situation, trying to understand what you’re
looking at and how it all fits together. To do
this, you must be careful to preserve the arti-
facts you find and respect and understand the
cultural forces that produced them.

But we don’t have to wait a thousand
years to try to comprehend unfathomable ar-
tifacts. Code becomes legacy code just about
as soon as it’s written, and suddenly we have
exactly the same issues as the archaeologists:
What are we looking at? How does it fit in
with the rest of the world? And what were
they thinking? It seems we’re always in the
position of reading someone else’s code: ei-
ther as part of a code review, or trying to cus-
tomize a piece of open source software, or
fixing a bug in code that we’ve inherited.

This analogy is such a compelling and po-
tentially useful one that Dave, Andy, Brian
Marick, and Ward Cunningham held a
workshop on Software Archaeology at
OOPSLA 2001 (the annual ACM Confer-
ence on Object-Oriented Programming, Sys-
tems, Languages, and Applications). The
participants discussed common problems of
trying to understand someone else’s code
and shared helpful techniques and tips (see
the “Tools and Techniques” sidebar).

Roll up your sleeves
What can you do when someone dumps

250k lines of source code on your desk and
simply says, “Fix this”? Take your first cue
from real archaeologists and inventory the
site: make sure you actually have all the
source code needed to build the system.

Next, you must make sure the site is se-
cure. On a real dig, you might need to shore
up the site with plywood and braces to ensure
it doesn’t cave in on you. We have some
equivalent safety measures: make sure the ver-
sion control system is stable and accurate
(CVS is a popular choice; see www.cvshome.
org). Verify that the procedures used to build
the software are complete, reliable, and re-
peatable (see January/February issue’s column
for more on this topic). 

Be aware of build dependency issues: in
many cases, unless you build from scratch,
you’re never really sure of the results. If
you’re faced with build time measured in
hours, with multiple platforms, then the in-
vestment in accurate dependency manage-
ment might be a necessity, not a luxury.

Draw a map as you begin exploring the

software construction

Software Archaeology
Andy Hunt and Dave Thomas

E d i t o r s :  A n d y  H u n t  a n d  D a v e  T h o m a s � T h e  P r a g m a t i c  P r o g r a m m e r s
a n d y @ p r a g m a t i c p r o g r a m m e r. c o m � d a v e @ p r a g m a t i c p r o g r a m m e r. c o m



M a r c h / A p r i l  2 0 0 2 I E E E  S O F T W A R E 2 1

SOFTWARE CONSTRUCTION

code. (Remember playing “Colossal
Cave?” You are in a maze of twisty
little passages, all alike….) Keep de-
tailed notes as you discover priceless
artifacts and suspicious trapdoors.
UML diagrams might be handy (on
paper—don’t get distracted by a
fancy CASE tool unless you’re al-
ready proficient), but so too are sim-
ple notes. If there are more than one
of you on the project, consider using
a Wiki or similar tool to share your
notes (you can find the original Wiki
at www.c2.com/cgi/wiki?WikiWiki-
Web and a popular implementation
at www.usemod.com/cgi-bin/wiki.pl.

As you look for specific keywords,
routine names, and such, use the
search capabilities in your integrated
development environment (IDE), the
Tags feature in some editors, or
tools, such as Grep, from the com-
mand line. For larger projects, you’ll
need larger tools: you can use index-
ing engines such as Glimpse or
SWISH++ (simple Web indexing sys-
tem for humans) to index a large
source code base for fast searching.

The mummy’s curse
Many ancient tombs were ru-

mored to be cursed. In the software
world, the incantation for many of
these curses starts with “we’ll fix it
later.” Later never comes for the
original developers, and we’re left
with the curse. (Of course, we never
put things off, do we?)

Another form of curse is found in
misleading or incorrect names and
comments that help us misunder-
stand the code we’re reading. It’s
dangerous to assume that the code or
comments are completely truthful.
Just because a routine is named
readSystem is no guarantee that it
isn’t writing a megabyte of data to
the disk.

Programmers rarely use this sort
of cognitive dissonance on purpose;
it’s usually a result of historical acci-
dent. But that can also be a valuable
clue: How did the code get this way,
and why? Digging beneath these lay-
ers of gunk, cruft, and patch upon
patch, you might still be able to see
the original system’s shape and gain

insight into the changes that were re-
quired over the years.

Of course, unless you can prove oth-
erwise, there’s no guarantee that the
routine you’re examining is even being
called. How much of the source con-
tains code put in for a future that never
arrived? Static analysis of the code can
prove whether a routine is being used in
most languages. Some IDEs can help
with this task, or you can write ad hoc
tools in your favorite scripting lan-
guage. As always, you should prove as-
sumptions you make about the code. In
this case, adding specific unit tests helps
prove—and continue to prove—what a
routine is doing (see www.junit.org for
Java and www.xprogramming.org for
other languages).

By now, you’ve probably started
to understand some of the terminol-
ogy that the original developers used.
Wouldn’t it be great to stumble
across a Rosetta stone for your pro-
ject that would help you translate its
vocabulary? If there isn’t one, you
can start a glossary yourself as part
of your note-taking. One of the first
things you might uncover is that
there are discrepancies in the mean-
ing of terms from different sources.
Which version does the code use?

Duck blinds and aerial views
In some cases, you want to ob-

serve the dynamics of the running
system without ravaging the source
code. One excellent idea from the
workshop was to use aspects to sys-
tematically introduce tracing state-
ments into the code base without

editing the code directly (AspectJ for
Java is available at www.aspectj.org).
For instance, suppose you want to
generate a trace log of every database
call in the system. Using something
like AspectJ, you could specify what
constitutes a database call (such as
every method named “db*’” in a
particular directory) and specify the
code to insert. 

Be careful, though. Introducing
any extra code this way might pro-
duce a “Heisenbug,” a bug intro-
duced by the act of debugging. One
solution to deal with this issue is to
build in the instrumentation in the
first place, when the original devel-
opers are first building and testing
the software. Of course, this brings
its own set of risks. One of the par-
ticipants described an ancient but
still-used mainframe program that
only works if the tracing statements
are left in.

Whether diagnostic tracing and
instrumentation are added originally
or introduced later via aspects, you
might want to pay attention to what
you are adding, and where. For in-
stance, say you want to add code that
records the start of a transaction. If
you find yourself doing that in 17
places, this might indicate a struc-
tural problem with the code—and a
potential answer to the problem
you’re trying to solve.

Instead of hiding in a “duck blind”
and getting the view on the ground,
you might want to consider an aerial
view of the site. Synoptic, plotting,
and visualization tools provide quick,
high-level summaries that might visu-
ally indicate an anomaly in the code’s
static structure, in the dynamic trace
of its execution, or in the data it han-
dles. For instance, Ward Cunning-
ham’s Signature Survey method
(http://c2.com/doc/SignatureSurvey)
reduces each source file to a single
line of the punctuation. It’s a surpris-
ingly powerful way of seeing a file’s
structure. You can also use visualiza-
tion tools to plot data from the vol-
umes of tracing information languish-
ing in text files.

As with real archaeology, it pays to
be meticulous. Maintain a deliberate

Instead of hiding in a
“duck blind” and getting
the view on the ground,

you might want to
consider an aerial view

of the site.



2 2 I E E E  S O F T W A R E M a r c h / A p r i l  2 0 0 2

SOFTWARE CONSTRUCTION

pace; and keep careful records. Even
for a short-term, quick patch that
doesn’t require you to understand the
whole code base, keep careful records
of what you’ve learned, what you’ve
tried, what worked, and what didn’t.

What were they thinking?
Archaeologists generally don’t

make wisecracks about how stupid a
particular culture was. In our indus-
try, we generally don’t show such re-
straint. But it’s important when read-
ing code to realize that apparently
bone-headed decisions that appear to
be straight out of a “Dilbert” cartoon
seemed perfectly reasonable to the de-
velopers at the time. Understanding
“what they were thinking” is critical
to understanding how and why they
wrote the code the way they did. 

If you discover they misunderstood
something, you’ll likely find that mis-
take in more than one place. But rather
than simply “flipping the bozo bit” on
the original authors, try to evaluate
their strengths as well as weaknesses.
You might find lost treasure—buried
domain expertise that’s been forgotten.

Also, consider to which “school of

programming” the authors belonged.
Regardless of implementation lan-
guage, West-Coast Smalltalkers will
write in a different style from Euro-
pean Modula programmers, for in-
stance. In a way, this approach gets us
back to the idea of treating code as
literature. What was the house style?

By understanding what the devel-
opers were thinking, what influenced
them, what techniques they were fond
of, and which ones they were unaware
of, you will be much better positioned
to fully understand the code they pro-
duced and take it on as your own.

Leaving a legacy
Given that today’s polished code

will inevitably become the subject of
some future developer’s archaeologi-
cal dig, what can we do to help
them? How can we help them com-
prehend “what we were thinking”
and work with our code?

� Ensure the site is secure when you
leave. Every file related to the pro-
ject should be under version con-
trol, releases should be appropri-
ately identified, and the build

should be automatic and reliable.
� Leave a Rosetta stone. The project

glossary was useful for you as you
learned the domain jargon; it will
be doubly useful for those who
come after you.

� Make a simple, high-level treasure
map. Honor the “DRY” principle:
Don’t duplicate information that’s
in the code in comments or in a
design document. Comments in
the code explain “why,” the code
itself shows “how,” and the map
shows where the landscape’s main
features are located, how they re-
late to each other, and where to
find more detailed information.

� Build in instrumentation, tracing,
and visualization hooks where ap-
plicable. This could be as simple
as tracing “got here” messages or
as intricate as an embedded HTTP
server that displays the applica-
tion’s current status (our book,
The Pragmatic Programmer [Ad-
dison-Wesley, 2000] discusses
building testable code).

� Use consistent naming conven-
tions to facilitate automatic static
code analysis and search tools. It
helps us humans, too.

� No evil spells. You know what the
incantations sound like. Don’t let
the mummy’s curse come back to
haunt programmers later. The
longer a curse festers and grows,
the worse it is when it strikes—
and curses often backfire against
their creators first.

Acknowledgments
Many thanks to the other workshop orga-

nizers, Brian Marick and Ward Cunningham,
and to the attendees: Ken Anderson, Vladimir
Degen, Chet Hendrickson, Michael Hewner,
Kevin Johnson, Norm Kerth, Dominik
Kuropka, Dragos Manolescu, John McIntosh,
Walter Risi, Andy Schneider, Glenn Vander-
burg, and Charles Weir.

Andy Hunt and Dave Thomas are partners in The
Pragmatic Programmers, LLC. They feel that software consul-
tants who can’t program shouldn’t be consulting, so they keep
current by developing complex software systems for their
clients. They also offer training in modern development tech-
niques to programmers and their management. They are 
coauthors of The Pragmatic Programmer and Programming
Ruby, both from Addison-Wesley. Contact them through www.
pragmaticprogrammer.com.

Tools and Techniques

The workshop identified these analysis tools and techniques:

� Scripting languages for
–ad hoc programs to build static reports (included by and so on) 
–filtering diagnostic output

� Ongoing documentation in basic HTML pages or Wikis
� Synoptic signature analysis, statistical analysis, and visualization tools
� Reverse-engineering tools such as Together’s ControlCenter
� Operating-system-level tracing via truss and strace
� Web search engines and tools to search for keywords in source files
� IDE file browsing to flatten out deep directory hierarchies of the source
� Test harnesses such as Junit and CPPUnit
� API documentation generation using Javadoc, doxygen, and so on
� Debuggers

Participants also identified these invasive tools:

� Hand-inserted trace statements
� Built-in diagnostic instrumentation (enabled in production code as well)
� Instrumentation to log history of data values at interface calls
� Use of AspectJ to introduce otherwise invasive changes safely



focus

0 7 4 0 - 7 4 5 9 / 0 2 / $ 1 7 . 0 0  ©  2 0 0 2  I E E E M a r c h / A p r i l  2 0 0 2 I E E E  S O F T W A R E 2 3

The real departure for Web-based enter-
prise applications is the possibility of wide-
ranging accessibility. A system that might be
deployed in-house at a manufacturing com-
pany can now be deployed to all the dealers
of that manufacturer’s products. Indeed, it
can be deployed to all the customers. In the
past, such a wide deployment was a major
hurdle. With Web-based technologies, pro-
viding external access to internal systems is
as easy as publicizing a URL.

Without doubt, the Web has significantly
affected the users of computer systems, but
what effect has it had on enterprise applica-
tion designers? That’s the question we wanted
to explore with this special issue. What things
are the same, what things are new, and what
things have changed in some subtle but inter-
esting ways? The activities that people need to
do to build Web software do not differ from
those we do with any software. We still ana-
lyze requirements, design, write code, test,
and deploy. Yet the pressures of building Web

applications add some new variations of fa-
miliar activities. None of these changes are
necessarily massive, but they suffice to suggest
some new directions.

Quality, scalability, usability,
maintainability

Jeff Offutt’s article “Quality Attributes of
Web Software Applications” is a good start-
ing point for this discussion; he explores qual-
ity attributes for Web-based systems. Al-
though the Web hasn’t necessarily introduced
any new quality issues, Web applications of-
ten combine more quality challenges than
previous generations of applications. Further-
more, the Web’s capabilities introduce new
twists on traditional ideas of quality.

The Web also brings the question of scal-
ability to the fore. Whereas a system might
have had a few thousand users a decade
ago, today Web system designers must con-
sider the possibility that hundreds of thou-
sands of users might log in at once. Colleen

The Software Engineering
of Internet Software

Elisabeth Hendrickson, Quality Tree Software

Martin Fowler, ThoughtWorks

W
hen we first started working with the Web, it was a publish-
ing medium. The Web was essentially a poorly edited and
rather erratic encyclopedia. Although much of the Web is still
that, the Web is increasingly doing things that we’ve typically

seen in enterprise applications. Rather than simply presenting static infor-
mation, Web sites invite user interaction. The Web plays host to a new gen-
eration of systems following green screens and client-server GUIs.

guest editors’ introduction



Roe and Sergio Gonik dig into the issue of
scalability in “Server-Side Design Principles
for Scalable Internet Systems.” Their article
culls principles from their experiences with
software that is both behaviorally complex
and that needs that elusive scalability.

Additionally, many Web users are end cus-
tomers. Web applications have a greater need
for usability than many enterprise applica-
tions previously demanded. Larry Constan-
tine and Lucy Lockwood tackle this problem
with techniques for designing user interfaces
through user models, task models, and con-
tent models in their article, “Usage-Centered
Engineering for Web Applications.”

When we talk about user interfaces, we
must remember that as well as designing them
from the outside to be usable, we must also
design them from the inside to be maintain-
able. Web applications need to change as
business changes, and even the most usable
site can become a problem if the interface’s in-
ternal design is difficult to change and fix. In
their article, “Objects and the Web,” Alan
Knight and Naci Dai peer inside a design to
reveal that one of the oldest patterns for user
interfaces, the Model-View-Controller, is a
key part of maintainable Web systems. In-
deed, MVC plays a bigger-than-ever part in
the Web.

Agile, international, testing,
lessons learned

Web applications must be up quickly to
compete. In “Going Faster: Testing the Web
Application,” Edward Hieatt and Robert
Mee explore this dilemma and suggest that

high quality is not just compatible with high
speed—quality enables speed. By writing the
tests before the code, the authors discovered
that they not only produced a more solid end
product but also could do it more quickly.

It’s hard to write an introduction this far
without raising the spirit of another recent fad:
agile methods such as Extreme Programming.
Throughout many of these articles, you’ll see
that theme. Web applications seem particu-
larly suited to agile processes’ emphasis on
quality and speed. One feature of agile meth-
ods is their de-emphasis on modeling. In
“Lessons in Agility from Internet-Based De-
velopment,” Scott Ambler describes a case
study of how a Web development team used
modeling within an agile process. He con-
cludes that there is less emphasis on documen-
tation and tools but more emphasis on com-
munication and quality. Modeling is there, it’s
recognizable, but a little different.

The Web’s wide reach plays particular
havoc with those who like to think that
world revolves around the English language.
Native English speakers often run into Web
pages they can’t read and get a taste of what
it’s like for many in the world who don’t have
a dominant grasp of English. This leads to a
new challenge: enabling Web applications for
international use. In “Software Localization
for Internet Software: Issues and Methods,”
Rosann Webb Collins focuses on the needs of
internationalization, providing one step to-
ward a checklist for Web applications that
need a global reach.

Eric Altendorf, Moses Hohman, and Ro-
man Zabicki provide a nice summary of les-
sons learned from Web development in “Us-
ing J2EE on a Large, Web-Based Project.”
This is a case study of a moderately large
Web application using the popular enter-
prise Java technologies. The authors discuss
the challenges they faced designing dynamic
HTML user interfaces, tackling internation-
alization, and integrating third-party sys-
tems that were developed before the Web
was around.

O ur challenge as a profession, both inWeb development and in wider soft-ware development, is to better under-
stand and communicate these lessons. We
hope this issue helps to do just that.

2 4 I E E E  S O F T W A R E M a r c h / A p r i l  2 0 0 2

About the Authors

Elisabeth Hendrickson is an independent consultant who specializes in software quality as-
surance and management. She has 14 years of experience working with leading software com-
panies and is the founder of Quality Tree Software. An award-winning author, she has pub-
lished more than 20 articles and is a frequently invited speaker at major software quality and
software management conferences. Contact her at esh@qualitytree.com.

Martin Fowler is the chief scientist for ThoughtWorks: an enterprise application devel-
opment and integration company. He has spent many years applying object oriented technol-
ogy to enterprise applications. He edits the Design column for IEEE Software and is the author
of several books on software design. He is currently working on a book on enterprise applica-
tion architecture, which you can find at www.martinfowler.com.



focusengineering Internet software

0 7 4 0 - 7 4 5 9 / 0 2 / $ 1 7 . 0 0  ©  2 0 0 2  I E E E M a r c h / A p r i l  2 0 0 2 I E E E  S O F T W A R E 2 5

systems. The software that powers Web ap-
plications is distributed, is implemented in
multiple languages and styles, incorporates
much reuse and third-party components, is
built with cutting edge technologies, and
must interface with users, other Web sites,
and databases.

Although the word “heterogeneous” is
often used for Web software, it applies in so
many ways that the synonymous term “di-
verse” is more general and familiar, and
probably more appropriate. The software
components are often distributed geograph-
ically both during development and deploy-
ment (diverse distribution), and communi-
cate in numerous distinct and sometimes
novel ways (diverse communication). Web
applications consist of diverse components
including traditional and nontraditional
software, interpreted scripting languages,
plain HTML files, mixtures of HTML and

programs, databases, graphical images, and
complex user interfaces.

As such, engineering an effective Web site
requires large teams of people with very di-
verse skills and backgrounds. These teams
include programmers, graphics designers,
usability engineers, information layout spe-
cialists, data communications and network
experts, and database administrators. This
diversity has led to the notion of Web site
engineering.1

The tremendous reach of Web applica-
tions into all areas of communication and
commerce makes this one of the largest and
most important parts of the software indus-
try. Yet a recent National Research Council
study2 found that the current base of science
and technology is inadequate for building
systems to control critical software infra-
structure. The President’s commission on
critical infrastructure protection reached

Quality Attributes of Web
Software Applications

Jeff Offutt, George Mason University

Web applications
have very high
requirements for
numerous quality
attributes. This
article discusses
some of the
technological
challenges of building
today’s complex 
Web software
applications, their
unique quality
requirements, and
how to achieve them.

T
he World Wide Web was originally designed to present information
to Web surfers using simple sites that consisted primarily of hyper-
linked text documents. Modern Web applications run large-scale
software applications for e-commerce, information distribution, en-

tertainment, collaborative working, surveys, and numerous other activities.
They run on distributed hardware platforms and heterogeneous computer



this same conclusion in the President’s In-
formation Technology Advisory Committee
report.3 This inadequacy is particularly se-
vere in the novel, high-risk area of Web ap-
plication software. Although Web software
development uses cutting-edge, diverse tech-
nologies, little is known about how to en-
sure quality attributes such as Web applica-
tion reliability.

Unique aspects of Web application
software

Several factors inherent to Web develop-
ment contribute to the quality problem. De-
velopers build Web-based software systems
by integrating numerous diverse components
from disparate sources, including custom-
built special-purpose applications, cus-
tomized off-the-shelf software components,
and third-party products. In such an environ-
ment, systems designers choose from poten-
tially numerous components, and they need
information about the various components’
suitability to make informed decisions about
the software’s required quality attributes.

Much of the new complexity found with
Web-based applications also results from
how the different software components are
integrated. Not only is the source unavail-
able for most of the components, the exe-
cutables might be hosted on computers at
remote, even competing organizations. To
ensure high quality for Web systems com-
posed of very loosely coupled components,
we need novel techniques to achieve and
evaluate these components’ connections.

Finally, Web-based software offers the sig-
nificant advantage of allowing data to be trans-
ferred among completely different types of
software components that reside and execute
on different computers. However, using multi-
ple programming languages and building com-
plex business applications complicates the flow
of data through the various Web software
pieces. When combined with the requirements
to keep data persistent through user sessions,
persistent across sessions, and shared among
sessions, the list of abilities unique to Web soft-
ware begins to get very long.

Thus, software developers and managers
working on Web software have encountered
many new challenges. Although it is obvi-
ous that we struggle to keep up with the
technology, less obvious is our difficulty in
understanding just how Web software de-

velopment is different, and how to adapt ex-
isting processes and procedures to this new
type of software. 

Economic changes
We evaluate software by measuring the

quality of attributes such as reliability, us-
ability, and maintainability, yet academics
often fail to acknowledge that the basic eco-
nomics behind software production has a
strong impact on the development process.
Although the field of software engineering
has spent years developing processes and
technologies to improve software quality at-
tributes, most software companies have had
little financial motivation to improve their
software’s quality. Software contractors re-
ceive payment regardless of the delivered
software’s quality and, in fact, are often
given additional resources to correct prob-
lems of their own making. So-called “shrink
wrap” vendors are driven almost entirely by
time to market; it is often more lucrative to
deliver poor-quality products sooner than
high-quality products later. They can deliver
bug fixes as new releases that are sold to
generate more revenue for the company. For
most application types, commercial devel-
opers have traditionally had little motiva-
tion to produce high-quality software.

Web-based software, however, raises new
economic issues. When I recently surveyed a
number of Web software development man-
agers and practitioners, I found that compa-
nies that operate through the Web depend
on customers using and, most importantly,
returning to their sites. Thus, unlike many
software contractors, Web application devel-
opers only see a return on their investment if
their Web sites satisfy customer needs. And
unlike many software vendors, if a new com-
pany puts up a competitive site of higher
quality, customers will almost immediately
shift their business to the new site once they
discover it. Thus, instead of “sooner but
worse,” it is often advantageous to be “later
and better.” Despite discussions of “sticky
Web sites” and development of mechanisms
to encourage users to return,4 thus far the
only mechanism that brings repeat users to
Web sites has been high quality. This will
likely remain true for the foreseeable future.

In software development, a process driver
is a factor that strongly influences the process
used to develop the software. Thus, if soft-

Instead of
“sooner but

worse,” 
it is often

advantageous 
to deliver Web
applications
“later and

better.”

2 6 I E E E  S O F T W A R E M a r c h / A p r i l  2 0 0 2



ware must have very high reliability, the de-
velopment process must be adapted to ensure
that the software works well. When I sur-
veyed the important quality process drivers
for traditional software, developers always
gave a single answer that stands far above the
rest: time to market. But when I recently
made the same survey of Web software de-
velopment managers and practitioners, they
claim that time to market, although still im-
portant, is no longer the dominant process
driver. They see the three most important
quality criteria for Web application success
(and thus, the underlying software) as

� Reliability
� Usability
� Security

Additional important criteria include

� Availability
� Scalability
� Maintainability
� Time to market

Of course, this is hardly a complete list of
important or even relevant quality attributes,
but it provides a solid basis for discussion. Cer-
tainly speed of execution is also important, but
network factors influence this more than soft-
ware does, and other important quality attrib-
utes such as customer service, product quality,
price, and delivery stem from human and or-
ganizational rather than software factors. That
said, the quality attributes I just listed track
closely with those cited in other books and ar-
ticles,1, 5–8 suggesting wide agreement that suc-
cessful Web software development depends on
satisfying these quality attributes.

Reliability
Extensive research literature and a collec-

tion of commercial tools have been devoted
to testing, ensuring, assuring, and measuring
software reliability. Safety-critical software
applications such as telecommunications,
aerospace, and medical devices demand
highly reliable software, but although many
researchers are reluctant to admit it, most
software currently produced does not need to
be highly reliable. I have been teaching soft-
ware testing in various forms for 15 years yet
have always felt like I was selling something
that nobody wants.

Many businesses’ commercial success de-
pends on Web software, however—if the
software does not work reliably, the busi-
ness will not succeed. The user base for Web
software is very large and expects Web ap-
plications to work as reliably as if they were
going to the grocery store or calling to order
from a catalog. Moreover, if a Web applica-
tion does not work well, the users do not
have to drive further to reach another store;
they can simply point their browser to a dif-
ferent URL. Web sites that depend on unre-
liable software will lose customers, and the
businesses could lose much money. Compa-
nies that want to do business over the Web
must spend resources to ensure high relia-
bility. Indeed, they cannot afford not to.

Usability
Web application users have grown to ex-

pect easy Web transactions—as simple as
buying a product at a store. Although much
wisdom exists on how to develop usable
software and Web sites (Jakob Nielsen’s
text9 being a classic example), many Web
sites still do not meet most customers’ us-
ability expectations. This, coupled with the
fact that customers exhibit little site loyalty,
means unusable Web sites will not be
used—customers will switch to more usable
Web sites as soon as they come online.

Security
We have all heard about Web sites being

cracked and private customer information
distributed or held for ransom. This is only
one example of the many potential security
flaws in Web software applications. When
the Web functioned primarily to distribute
online brochures, security breaches had rela-
tively small consequences. Today, however,
the breach of a company’s Web site can cause
significant revenue losses, large repair costs,
legal consequences, and loss of credibility
with customers. Web software applications
must therefore handle customer data and
other electronic information as securely as
possible. Software security is one of the
fastest growing research areas in computer
science, but Web software developers cur-
rently face a huge shortfall in both available
knowledge and skilled personnel.

Availability
In our grandparents’ time, if a shopkeeper

M a r c h / A p r i l  2 0 0 2 I E E E  S O F T W A R E 2 7

Companies that
want to do

business over
the Web must

spend resources
to ensure high

reliability. 



in a small town wanted to take a lunch break,
he would simply put a sign on the front door
that said “back at 1:00.” Although today’s
customers expect to be able to shop during
lunchtime, we do not expect stores to be open
after midnight or on holidays. But that only
works for “brick and mortar” stores. When
customers can visit our stores online, 2:00
a.m. in North America is the middle of the af-
ternoon in Asia, and national or religious hol-
idays fall on different days in different coun-
tries. On the Web, customers not only expect
availability 24 hours a day, seven days a
week, they expect the Web site to be opera-
tional every day of the year—24/7/365. Even
a 10-minute downtime can be damaging; I re-
cently purchased $50 worth of books online
but switched companies because the first Web
site gave a “missing file” error message. Ten
minutes later, that Web site was operational
again but had lost my sale. Although this was
only one sale, many customers would never
come back.

Availability means more than just being
up and running 24/7/365; the Web software
must also be accessible to diverse browsers.
In the seemingly never-ending browser wars
of the past few years, some software ven-
dors actively sought to make sure their soft-
ware would not work under competitors’
browsers. By using features only available
for one browser or on one platform, Web
software developers become foot soldiers in
the browser wars, sometimes unwittingly.
As an example, one major Web site at my
organization uses Shockwave flash, which is
only compatible with Microsoft IE and
Netscape under Windows. Thus, the many
Unix and Netscape users in my building
cannot view their own Web site. To be avail-
able in this sense, Web sites must adapt their
presentations to work with all browsers,
which requires significantly more knowl-
edge and effort on developers’ part.

Scalability
A recent television advertisement showed a

small group of young men and women nerv-
ously launching their Web site. The celebra-
tion started when the site got its first hit, but
their faces quickly turned gray when the num-
ber of hits went into the thousands, then mil-
lions. As with a small corner store, as few as
three or four people can create a commercial
Web site but, unfortunately (or fortunately

for the profit margin), virtually an unlimited
number of customers can visit the Web site.
We must therefore engineer Web software ap-
plications to be able to grow quickly in terms
of both how many users they can service and
how many services they can offer.

The need for scalability has driven many
technology innovations of the past few years.
The industry has developed new software lan-
guages, design strategies, and communication
and data transfer protocols in large part to al-
low Web sites to grow as needed. Scalability
also directly influences other attributes. Any
programming teacher knows that any design
will work for small classroom exercises, but
large software applications require discipline
and creativity. Likewise, as Web sites grow,
small software weaknesses that had no initial
noticeable effects can lead to failures (reliabil-
ity problems), usability problems, and security
breaches. Designing and building Web soft-
ware applications that scale well represents
one of today’s most interesting and important
software development challenges.

Maintainability
One novel aspect of Web-based software

systems is the frequency of new releases, or the
update rate. Traditional software involves
marketing, sales, and shipping or even per-
sonal installation at customers’ sites. Because
this process is expensive, software manufac-
turers usually collect maintenance modifica-
tions over time and distribute them to cus-
tomers simultaneously. For a software product
released today, developers will start collecting
a list of necessary changes. For a simple
change (say, changing a button’s label), the
modification might be made immediately. But
the delay in releases means that customers
won’t get more complex (and likely impor-
tant) modifications for months, perhaps years.

Web-based software, however, gives cus-
tomers immediate access to maintenance up-
dates—both small changes (such as changing
the label on a button) and critical upgrades can
be installed immediately. Instead of mainte-
nance cycles of months or years, Web sites can
have maintenance cycles of days or even hours.
Although other software applications have
high maintenance requirements, and some re-
search has focused on “on the fly” mainte-
nance10 for specialized applications, frequent
maintenance has never before been necessary
for such a quantity of commercial software.

Designing and
building Web

software
applications

that scale well
represents one
of today’s most
interesting and

important
software

development
challenges.

2 8 I E E E  S O F T W A R E M a r c h / A p r i l  2 0 0 2



Another ramification of the increased
update rate has to do with compatibility.
Users do not always upgrade their soft-
ware; hence, software vendors must ensure
compatibility between new and old ver-
sions. Companies can control the distribu-
tion of Web software to eliminate that
need, although Web applications must still
be able to run correctly on several Web
browsers and multiple versions of each
browser. 

Another possible consequence of the
rapid update rate is that developers might
not feel the same need to fix faults before re-
lease—they can always be fixed later. I have
seen no data to indicate this is happening.

Time to market
This has always been a key business

driver and remains important for Web soft-
ware, but it now shares the spotlight with
other quality attributes. Most of the soft-
ware industry continues to give priority to
first to market. Given the other factors dis-
cussed here, however, the requirement for
patience can and must impact the process
and management of Web software projects.

Analysis
Software researchers, practitioners, and

educators have discussed these criteria for
years. No type of application, however, has
had to satisfy all of these quality attributes at
the same time, and Web software components
are coupling more loosely than any previous
software applications. In fact, these criteria
have until recently been important to only a
small fraction of the software industry. They
are now essential to the bottom line of a large
and fast growing part of the industry, but we
do not yet have the knowledge to satisfy or
measure these criteria for the new technolo-
gies used in Web software applications.

Technology changes
The commercial use of the Internet and

Web has grown explosively in the past five
years. In that time, the Internet has evolved
from primarily being a communications
medium (email, files, newsgroups, and chat
rooms) to a vehicle for distributing infor-
mation to a full-fledged market channel for
e-commerce. Web sites that once simply dis-
played information for visitors have become
interactive, highly functional systems that

let many types of businesses interact with
many types of users.

These changes have had an enormous im-
pact on software engineering. As the use of
the Internet and Web has grown, the amount,
type, and quality of software necessary for
powering Web sites has also grown. Just a
few years ago, Web sites were primarily com-
posed of static HTML files, so-called “soft
brochures,” usually created by a single Web-
master who used HTML, JavaScript, and
simple CGI scripts to present information
and obtain data from visitors with forms.

Figure 1 illustrates the early Web, a typi-
cal client-server configuration in which the
client is a Web browser that people use to
visit Web sites that reside on different com-
puters, the servers, and a software package
called a Web server sends the HTML files to
the client. HTML files contain JavaScripts,
which are small pieces of code that are in-
terpreted on the client side. HTML forms
generate data that are sent back to the
server to be processed by CGI programs.

This very simple model of operation can
support relatively small Web sites. It uses
small-scale software, offers little security,
usually cannot support much traffic, and of-
fers limited functionality. This was called a
two-tier system because two separate com-
puters were involved.

The Web’s function and structure have
changed drastically, particularly in the past 24
months, yet most software engineering re-
searchers, educators, and practitioners have
not yet grasped how fully this change affects
engineering principles and processes. Web sites
are now fully functional software systems that
provide business-to-customer e-commerce,
business-to-business e-commerce, and many

M a r c h / A p r i l  2 0 0 2 I E E E  S O F T W A R E 2 9

Browser
HTML
forms

Helpers
Audio
Video

Client side Server side

GIF images

Internet
HTTP server

CGI programs

Database Files

Figure 1. First-
generation Web 
sites followed a
client-server model
that suffices to 
support simple 
page viewing and
limited traffic but 
does not scale well. 



services to many users. Instead of referring to
visitors to Web sites, we now refer to users,
implying much interaction. Instead of Web-
masters, large Web sites must employ Web
managers leading diverse teams of IT profes-
sionals that include programmers, database
administrators, network administrators, us-
ability engineers, graphics designers, security
experts, marketers, and others. This team uses
diverse technologies including several varieties
of Java (Java, servlets, Enterprise JavaBeans,
applets, and JavaServer Pages), HTML,
JavaScript, XML, UML, and many others.
The growing use of third-party software com-
ponents and middleware represents one of the
biggest changes.

The technology has changed because the
old two-tier model did not support the high
quality requirements of Web software applica-
tions. It failed on security, being prone to
crackers who only need to go through one
layer of security on a computer that is, by def-
inition, open to the world to provide access to
all data files. It failed on scalability and main-
tainability because as Web sites grow, a two-
tier model cannot effectively separate presen-
tation from business logic, and the
applications thus become cumbersome and
hard to modify. It failed on reliability: whereas
previous Web software generations relied on
CGI programs, usually written in Perl, many
developers have found that large complex Perl
programs can be hard to program correctly,
understand, or modify. Finally, it failed on
availability because hosting a site on one Web
server imposes a bottleneck: any server prob-
lems will hinder user access to the Web site.

Figure 2 illustrates current Web site soft-
ware. Instead of a simple client-server model,
the configuration has expanded first to a
three-tier model and now more generally to
an N-tier model. Clients still use a browser to
visit Web sites, which are hosted and deliv-

ered by Web servers. But to increase quality
attributes such as security, reliability, avail-
ability, and scalability, as well as functional-
ity, most of the software has been moved to a
separate computer—the application server.
Indeed, on large Web sites, a collection of ap-
plication servers typically operates in paral-
lel, and the application servers interact with
one or more database servers that may run a
commercial database.

The client-server interaction, as before, uses
the Internet, but middleware—software that
handles communication, data translation, and
process distribution—often connects the Web
and application servers, and the application
and database servers. New Web software lan-
guages such as Java are easier to modify and
program correctly and permit more extensive
reuse. These features enhance maintainability,
reliability, and scalability. The N-tier model
also permits additional security layers between
potential crackers and the data and applica-
tion business logic. The ability to separate
presentation (typically on the Web server tier)
from the business logic (on the application
server tier) makes Web software easier to
maintain and expand in terms of customers
serviced and services offered. Distributed com-
puting, particularly for the application servers,
allows the Web application to tolerate failures
and handle more customers, and allows devel-
opers to simplify the software design.

Newer design models11,12 have extended
these goals even further. JavaServer Pages
(JSPs) and Enterprise JavaBeans (EJBs) let de-
velopers separate presentation from logic,
which helps make software more maintain-
able. To further subdivide the work, develop-
ers can create a software dispatcher that ac-
cepts requests on the Web server tier, then
forwards the request to an appropriate hard-
ware or software component on the applica-
tion tier. Such design strategies lead to more

3 0 I E E E  S O F T W A R E M a r c h / A p r i l  2 0 0 2

Client Web server Application servers

Network

Database server

Middleware Middleware

Figure 2. Modern 
Web sites generally 
follow an N-tier model 
that, by separating 
presentation from
business logic, 
supports much
greater application
complexity, higher
traffic, and stronger
site security.



reliable software and more scalable Web sites.
Of course the technology keeps chang-

ing, with the latest major addition to the
technology being Microsoft’s .NET.  As of
this writing, it is too early to say what type
of effect .NET will have, although it does
not seem to provide additional abilities be-
yond what is already available.

Clearly, modern Web sites’ increased func-
tionality creates a need for increasingly com-
plex software, system integration and design
strategies, and development processes. This
leads to two exciting conclusions: 

� One of the largest and fastest-growing
software industry segments finds itself
in dire need of the high-end software en-
gineering practices and processes that
researchers and educators have been de-
veloping and teaching for years.

� The new models for Web-based soft-
ware production and deployment re-
quire that we adapt or replace many of
the research solutions available now. 

These conclusions imply that we need
significant research progress, education,
and training in diverse software engineering
areas. The software that drives the Web has
become a critical part of the world’s infra-
structure. Although Web software’s imma-
turity poses significant risk to both industry
and government, it also represents an op-
portunity for software engineering re-
searchers and educators. 

Planning for the future
One more important issue remains: the

lack of engineers skilled in Web software de-
velopment. A recent study from the US Na-
tional Research Council found that the cur-
rent base of science and technology is
inadequate for building systems to control
critical software infrastructure.2 Much of
Web software is built from existing systems
and involves complicated analysis to effec-
tively compose and integrate these compo-
nents into systems. Yet the combination of a
shortage of skilled IT engineers and the large
number of IT jobs means companies often re-
sort to hiring engineers who have less skills
and education than desired. As a small ex-
ample, US universities graduate approxi-
mately 25,000 bachelor degrees in computer
science every year, but industry recently esti-

mated that it needs more than 200,000 IT
professionals.2

Even with the current economic down-
turn, the university output is not enough. If
universities could double the production of
computer scientists, we still could not put a
dent in the need. (Most economists and busi-
ness leaders currently believe last year’s many
layoffs and bankruptcies in the e-commerce
sector resulted from temporary problems,
and expect significant growth in the near fu-
ture. I optimistically accept this prognosis; if
it is wrong, then this article will be irrelevant
anyway.) We can only meet this need by

� Retraining experienced engineers to
work with the new technology

� Applying knowledge and technology to
increase efficiency, thereby reducing the
number of engineers needed 

� Finding ways to let people with less ed-
ucation and skills contribute

We are already seeing some progress in all
three of these directions:

� Training classes and university courses
in Web software engineering technolo-
gies are increasing and experiencing
very high enrollment. The Web software
engineering courses at George Mason
University are oversubscribed every se-
mester, with some students being non-
degree professionals seeking to improve
their marketability.

� New textbooks, tools, languages, and
standards are emerging to make Web
software engineering knowledge more ac-
cessible and easier to learn, use, and de-
ploy. For example, several XML innova-
tions in the past year have made it a more
useful and accessible language, while new
development tools and refinements in the
standards for JSPs and EJBs allow more
software to be created with less effort. 

� Automated technologies have recently
allowed nonprogrammers to contribute
more to Web software development.
When HTML was first introduced, an
HTML writer needed to fully know the
language and be proficient with a text
editor to create Web pages. Recent tools
provide point-and-click ability to create
Web pages that can even be enhanced
with dynamic HTML and JavaScripts

M a r c h / A p r i l  2 0 0 2 I E E E  S O F T W A R E 3 1

Although Web
software’s
immaturity

poses significant
risk to both
industry and

government, it
also represents
an opportunity
for software
engineering
researchers 

and educators. 



while requiring very little knowledge of
HTML and programming.

Achieving the high quality requirements of
Web software represents a difficult challenge.
Although other segments of the software in-
dustry have already mastered some of these,
such as the need for reliability in telecommu-
nications and network routing, aerospace,
and medical devices, they have typically done
so by hiring the very best developers on the
market, using lots of resources (time, devel-
opers, and testing), or relying on old, stable
software and technologies.

Unfortunately, these solutions will not
work for Web applications. There are sim-
ply not enough of the best developers to im-
plement all the Web software needed today,
and few but the largest companies can af-
ford to invest extra resources in the form of
time, developers, and testing. Finally, it
should be obvious that old, stable software
technologies will not suffice as a base for
the Web—it relies on the latest cutting-edge
software technology. Although the use of
new technology involves some risk, it allows
us to achieve otherwise unattainable levels
of scalability and maintainability.

Indeed, technological innovations in just
the past three years have greatly advanced the
field, both in breadth and depth. And research
progresses: New conferences appear almost
monthly, and traditional conferences feature
more tracks and papers on Web software is-
sues. Every new PhD student, it seems, wants
to write a thesis on some aspect of Web soft-
ware engineering, and more textbooks and
classes teach Web software application mate-
rial. When George Mason University offered a
graduate course in Web software engineering
in Fall 2000, the class immediately became
popular, with a large waiting list.

W eb software engineering presentschallenging and unique researchproblems. We currently lack the
knowledge to create Web software of suffi-
cient complexity or quality and that can be
updated quickly and reliably. Although Web
software engineering has significant differ-
ences from traditional software engineering,
we can adapt much of what we already
know to understanding and resolving these
differences. Not only are we making ex-
traordinary progress, we are also bringing
much research of the past 20 years to
fruition. Indeed, this is an exciting time to
be a software engineer.

Acknowledgments
This work is supported in part by the US National

Science Foundation under grant CCR-98-04111. 

References
1. T.A. Powell, Web Site Engineering: Beyond Web Page

Design, Prentice Hall, Upper Saddle River, N.J., 2000.
2. F.B. Schneider, Trust in Cyberspace, National Academy

Press, Washington, D.C., 1999.
3. President’s Information Technology Advisory Commit-

tee, Information Technology Research: Investing in our
Future, tech. report, Nat’l Coordination Office for
Computing, Information, and Communications, Wash-
ington, D.C., 1999; www.ccic.gov/ac/report.

4. D.A. Menascé, Scaling for E-Business: Technologies,
Models, Performance, and Capacity Planning, Prentice
Hall, Upper Saddle River, N.J., 2000.

5. E. Dustin, J. Rashka, and D. McDiarmid, Quality Web
Systems: Performance, Security, and Usability, Addison-
Wesley, Reading, Mass., 2001.

6. L.L. Constantine and L.A.D. Lockwood, Software for
Use: A Practical Guide to the Models and Methods of
Usage Centered Design, ACM Press, New York, 2000.

7. S. Murugesan and Y. Deshpande, “Web Engineering: A
New Discipline for Development of Web-Based Sys-
tems,” Web Engineering 2000, Lecture Notes in Com-
puter Science 2016, Springer-Verlag, Berlin, 2001, pp.
3–13.

8. N. Kassem and the Enterprise Team, Designing Enter-
prise Applications with the Java 2 Platform, Enterprise
Edition, Sun Microsystems, Palo Alto, Calif., 2000.

9. J. Nielsen, Designing Web Usability, New Riders Pub-
lishing, Indianapolis, Ind., 2000.

10. M.E. Segal and O. Frieder, “On-the-Fly Program Modi-
fication: Systems for Dynamic Updating,” IEEE Soft-
ware, vol. 10, no. 2, Mar. 1993, pp. 53–65.

11. Wrox Multi Team, Professional Java Server Program-
ming, J2EE edition, Wrox Press, Chicago, 2000.

12. A. Scharl, Evolutionary Web Development, Springer-
Verlag, Berlin, 2000.

For more information on this or any other computing topic, please visit our
Digital Library at http://computer.org/publications/dlib.

3 2 I E E E  S O F T W A R E M a r c h / A p r i l  2 0 0 2

About the Author

Jeff Offutt is an associate professor of information and software engineering at George Mason
University. His current research interests include software testing, analysis and testing of Web appli-
cations, object-oriented program analysis, module and integration testing, formal methods, and soft-
ware maintenance. He received a PhD in computer science from the Georgia Institute of Technology.
He served as program chair for ICECCS 2001 and is on the editorial boards for IEEE Transactions on
Software Engineering, Journal of Software Testing, Verification and Reliability, and Journal of Soft-
ware and Systems Modeling. He is a member of the ACM and IEEE Computer Society. Contact him at
the Dept. of Information and Software Eng., Software Eng. Research Lab, George Mason Univ., Fair-
fax, VA 22030-4444; ofut@ise.gmu.edu; www.ise.gmu.edu/faculty/ofut. 



3 4 I E E E  S O F T W A R E M a r c h / A p r i l  2 0 0 2 0 7 4 0 - 7 4 5 9 / 0 2 / $ 1 7 . 0 0  ©  2 0 0 2  I E E E

Any application can essentially be char-
acterized by its consumption of four pri-
mary system resources: CPU, memory, file
system bandwidth, and network band-
width. Scalability is achieved by simultane-
ously optimizing the consumption of these
resources and designing an architecture that
can grow modularly by adding more re-
sources. This article looks at the underlying
principles needed to achieve such designs
and discusses some specific strategies that
exploit these principles.

The principles of scalable
architecture

Relatively few design principles are re-
quired to design scalable systems. The list is
limited to

� divide and conquer (D&C)
� asynchrony

� encapsulation
� concurrency
� parsimony

We have used these principles in system de-
sign over several years with good success.
As is evident in the discussion that follows,
there is some degree of overlap in the prin-
ciples. Despite this, each presents a concept
that is important in its own right when de-
signing scalable systems. There are also ten-
sions between these principles; one might
sometimes be applied at the cost of another.
The crux of good system design is to strike
the right balance.

Divide and conquer
D&C means that the system should be

partitioned into relatively small subsystems,
each carrying out some well-focused func-
tion. This permits deployments that can

focus
Server-Side Design
Principles for Scalable
Internet Systems

Colleen Roe and Sergio Gonik, GemStone Systems

Today’s Internet systems face real challenges in trying to serve many concurrent users.

This article explores
the design principles
guiding the creation
of scalable systems.
Each principle is
discussed and
followed by
examples of its
application.

I
n today’s Internet systems, nothing is constant but change. If you build
a successful site they will come—and boy, will they come. Consequently,
designers today are faced with building systems to serve an unknown
number of concurrent users with an unknown hit rate and transactions-

per-second requirement. The key to designing and maintaining such systems
is to build in scalability features from the start, to create Internet systems
whose capacity we can incrementally increase to satisfy ramping demand.

engineering Internet software



leverage multiple hardware platforms or
simply separate processes or threads,
thereby dispersing the load in the system
and enabling various forms of load balanc-
ing and tuning.

D&C varies slightly from the concept of
modularization in that it addresses the parti-
tioning of both code and data and could ap-
proach the problem from either side. One
example is replicated systems. Many appli-
cations can, in fact, be broken into repli-
cated instances of a system. Consider an in-
surance application that is deployed as five
separate but identical systems, each serving
some specific geography. If load increases,
more instances are deployed, and all running
instances now service a smaller geography.

Another example is functional–physical
partitioning. A system that processes orders
is broken into two components: an order-
taking component and an order-satisfaction
component. The order-taking component
acquires order information and places it in a
queue that is fed into the order-satisfaction
component. If system load increases, the
components might run on two or more sep-
arate machines. 

Asynchrony
Asynchrony means that work can be car-

ried out in the system on a resource-avail-
able basis. Synchronization constrains a 
system under load because application com-
ponents cannot process work in random or-
der, even if resources exist to do so. Asyn-
chrony decouples functions and lets the
system schedule resources more freely and
thus potentially more completely. This lets
us implement strategies that effectively deal
with stress conditions such as peak load.

Asynchrony comes at a price. Asynchro-
nous communications are generally more dif-
ficult to design, debug, and manage. “Don’t
block” is probably the most important ad-
vice a scalable-system designer can receive.
Blocking = bottlenecks. Designing asynchro-
nous communications between systems or
even between objects is always preferable. 

Moreover, use background processing
where feasible. Always question the need to
do work online in real time. A little lateral
thinking can sometimes result in a solution
that moves some work into background
processing. For example, order satisfaction
can include a background process that

emails an appropriate notification to the
user on completion. 

Encapsulation
Encapsulation results in system compo-

nents that are loosely coupled; ideally, there
is little or no dependence among compo-
nents. This principle often (but not always)
correlates strongly with asynchrony. Highly
asynchronous systems tend to have well-en-
capsulated components and vice versa. Loose
coupling means that components can pursue
work without waiting on work from others. 

Layers and partitions are an example of
the application of encapsulation. Layers and
partitions within layers are the original prin-
ciples that drive software architecture. The
concepts might be old, but they are still legit-
imate. Layers provide well-insulated bound-
aries between system parts, and they permit
reimplementation of layers without perturba-
tion to surrounding layers. Work can be car-
ried out independently within layers.

Concurrency
Concurrency means that there are many

moving parts in a system. Activities are split
across hardware, processes, and threads and
can exploit the physical concurrency of
modern symmetric multiprocessors. Con-
currency aids scalability by ensuring that
the maximum possible work is active at all
times and addresses system load by spawn-
ing new resources on demand (within pre-
defined limits). One example is to exploit
multithreading. Question the need to carry
out work serially. Look for opportunities to
spawn threads to carry out tasks asynchro-
nously and concurrently. You can also ac-
commodate expansion by adding more
physical platforms. Concurrency also maps
directly to the ability to scale by rolling in
new hardware. The more concurrency an
application exploits, the better the possibil-
ities to expand by adding more hardware.

Parsimony
Parsimony means that a designer must be

economical in what he or she designs. Each
line of code and each piece of state infor-
mation has a cost, and, collectively, the
costs can increase exponentially. A devel-
oper must ensure that the implementation is
as efficient and lightweight as possible. Pay-
ing attention to thousands of microdetails in

M a r c h / A p r i l  2 0 0 2 I E E E  S O F T W A R E 3 5

The more
concurrency an

application
exploits, the
better the

possibilities to
expand by

adding more
hardware.



a design and implementation can eventually
pay off at the macrolevel with improved sys-
tem throughput. 

Parsimony also means that designers
must carefully use scarce or expensive re-
sources. Such resources might be cached or
pooled and multiplexed whenever possible.
This principle basically pervades all the oth-
ers. No matter what design principle a de-
veloper applies, a parsimonious implementa-
tion is appropriate. Some examples include

� Algorithms. Ensure that algorithms are
optimal to the task at hand. Everyone
knows that O(n) algorithms are prefer-
able to, say, O(n2), but sometimes this is
overlooked. Several small inefficiencies
can add up and kill performance.

� Object models. Pare object models to the
bone. Most large object models with
many object-to-object relationships are
expensive to instantiate, traverse, process,
or distribute. We sometimes have to com-
promise a model’s purity for a simpler
and more tractable model to aid system
performance.

� I/O. Performing I/O, whether disk or
network, is typically the most expensive
operation in a system. Pare down I/O
activities to the bare minimum. Con-
sider buffering schemes that collect data
and do a single I/O operation as op-
posed to many. 

� Transactions. Transactions use costly re-
sources. Applications should work out-
side of transactions whenever feasible
and go into and out of each transaction
in the shortest time possible. 

Strategies for achieving scalability
Strategies are high-level applications of

one or more design principles. They are not
design patterns but rather entities that en-
compass a class of patterns. Given a strat-

egy, we can articulate multiple patterns that
embody its semantics. 

Careful system partitioning
A scalable system’s most notable charac-

teristic is its ability to balance load. As the
system scales up to meet demand, it should do
so by optimally distributing resource utiliza-
tion. For Java-based Internet systems, load
balancing maps to a well-distributed virtual
machine (VM) workload from Web client
hits. For other systems, the workload is dis-
tributed among operating system processes.

Partitioning breaks system software into
domain components with well-bounded
functionality and a clear interface. In object
terms, a domain component is “a set of
classes that collaborate among themselves to
support a cohesive set of contracts that you
can consider black boxes.”1 Each compo-
nent defines part of the architectural concep-
tual model and a group of functional blocks
and connectors.2 Ultimately, the goal is to
partition the solution space into appropriate
domain components that map onto the sys-
tem topology in a scalable manner. Princi-
ples to apply in this strategy include D&C,
asynchrony, encapsulation, and concurrency.

Service-based layered architecture
During design, a service-oriented perspec-

tive facilitates the definition of appropriate
components and data-sharing strategies. A
service encapsulates a subset of the applica-
tion domain into a domain component and
provides clients contractual access to it. By
making services available to a client’s appli-
cation layer, service-based architectures (see
Figure 1) offer an opportunity to share a sin-
gle component across many different sys-
tems. Having a services front end lets a com-
ponent offer different access rights and
visibility to different clients. 

In a sense, services do for components
what interfaces do for objects. Domain

3 6 I E E E  S O F T W A R E M a r c h / A p r i l  2 0 0 2

Presentation
Transcoding
Personalize 

content management

Application
Handle exceptions

Invoke services
Maintain

conversational state

Mediation between
presentation and service

Services
Domain service

Single behavior services
External services

Object distribution
Transaction control

Use case control

Application stack

Domain
Business behavior

Semantic validation

Persistence
Basic CRUD services

OR mapping
Transaction support

Profiling

Figure 1. Layers and their responsibilities in a service-based architecture.



components are shared by different remote
systems, each having its own contractual
view of the component. This paradigm is
helpful in Web-enabled enterprise applica-
tion integration (EAI) solutions.3

A service-based architecture not only aids
scalability by statically assisting in proper
component design; it also offers dynamic
benefits. For example, Enterprise JavaBeans’
stateless session beans help implement a serv-
ice layer that supports dynamic scalability by
enabling multiplexed access from different
clients—through bean implementation shar-
ing (see http://java.sun.com/products/ejb/
docs.html). We would gain further benefits if
we pooled stateless session bean interfaces in
the Web server tier. Each interface would be
associated with a preactivated bean imple-
mentation living in a middle-tier VM. We
could then use these interfaces to load balance
across the VMs. Principles to apply in this
strategy include D&C and encapsulation.

Just-enough data distribution 
The primary principle for object distribu-

tion is parsimony: distribute out as little
data as possible. For an object to be distrib-
uted outward, it must be serialized and
passed through memory or over a network.
This involves three system resources:

� CPU utilization and memory in the
server to serialize the object and possi-
bly packetize it for travel across the net-
work

� Network bandwidth or interprocess com-
munication activity to actually transmit
to the receiver

� CPU utilization and memory in the re-
ceiver to (possibly) unpacketize, deseri-
alize, and reconstruct the object graph

Hence, an object’s movement from server to
receiver comes at a fairly high cost.

There are two benefits of just-enough data
distribution: it diminishes the bandwidth
needed to flow data through the system, and
it lessens the amount of data a process con-
tains at any particular point in time. 

Let’s step back and look at the big pic-
ture. Suppose a large e-commerce applica-
tion services 10,000 concurrent users. Each
user must receive data. Now suppose the
amount of information sent to a client in-
creases by 10 Kbytes per hit. The total

amount of additional information the sys-
tem would thus have to send to service all
its clients would increase by 100 Mbytes.
Because large-scale systems serve many
users, relatively small increases in the
amount of data sent to an individual client
are magnified thousands of times. Seem-
ingly small increases can have a significant
impact on total system throughput.

There are quite a few guidelines for keep-
ing just the right amount of data in the right
place at the right time. The next several
paragraphs describe some techniques.

Use lazy initialization strategies to fetch
only the frequently used fields when an object
is first instantiated and initialized. Lazy ini-
tialization schemes can save significant data-
base querying, but a designer must under-
stand what fields are most commonly used in
a class to devise a good scheme. Be fore-
warned that too much lazy initialization is a
bad thing. A design must balance the costs of
bringing more data over at object initializa-
tion time against the need to go back to the
database again and again to fetch more fields.

Use state holders to pass requested data
from the back end to the presentation layer.
State holders represent a flattening of the
object graph into one object containing all
the pertinent data for a particular business
case. A service-based layered architecture
supports this concept well because services
tend to be associated with subsets of data
from domain model graphs.

In C and C++, data is commonly passed
around by reference because having a
shared memory view is easy. VMs have pri-
vate memory, and Java does not offer
pointer functionality directly, so application
data tends to be passed by value. Current
Java technologies offer a shared memory
view across VMs, and some offer transac-
tional access to shared data and even per-
sistence in native object format. This kind of
technology can help optimize performance.

If direct reference is not possible, use a key
system for passing data identity. Keys can, for
example, define catalog data associated with
a Web page. Instead of sending the whole cat-
alog to the user’s browser, an initial view is
sent and follow-up views are populated as
necessary based on returned request keys. 

Sharing read-only data can significantly
improve scalability by cutting down on data-
base queries and subsequent I/O. In a Java 2

M a r c h / A p r i l  2 0 0 2 I E E E  S O F T W A R E 3 7

A service-
based

architecture
not only aids
scalability by

statically
assisting in

proper
component

design: it also
offers dynamic

benefits.



Enterprise Edition application, many users
might execute in the same VM. Each user
can avoid acquiring its own copy of data by
sharing read-only copies across all users.

Do not underestimate the impact of ob-
ject distribution. In our experience, it is of-
ten the primary determinant of system via-
bility in distributed systems. Principles that
apply in this strategy include parsimony and
D&C.

Pooling and multiplexing
Pooling is an effective way to share and

reuse resources, which can be expensive in
terms of memory usage (for example, large
object graphs) or overhead (such as object
instantiation, remote object activation, and
relational database [RDB] connections).
Initialization for remotely accessed re-
sources is especially expensive because var-
ious protocols at different layers come into
play. Furthermore, these resources have
hard limits (such as the availability of
ports) and scalability constraints on the re-
mote server software itself. Pooling in gen-
eral and multiplexing connections in partic-
ular are solutions that optimize resource
sharing and reuse. Some examples of re-
sources that might be pooled include Java
database connectivity (JDBC) connections,
RDB connections, buffers, EJBs, and ports.

Multiplexing lets many actors share one
resource. The paradigm is especially valu-
able when accessing back-end systems using
JDBC connections or the like. 

Here are some approaches to pooling:

� Manage pool resources dynamically,
creating resources on an as-needed basis
and letting resources be reaped after a
set time. One option is to use a back-
ground process to wake up at preset
epochs and remove unused resources.
The pool can then dynamically meet
changing system load requirements
throughout the day.

� Always test for resource validity before
use when managing resources whose life
cycle you do not fully control.

The main principle that applies in pooling is
parsimony.

Queuing work for background processes
Queuing lets a foreground resource serv-

ing an interactive user delegate work to a
background process, which makes the fore-
ground resource more responsive to the
user. Queuing can also permit work prioriti-
zation. Data warehouse searches, object
cache synchronization with a back-end
RDB, and message-broker-based EAI inte-
gration are all examples of candidates for
asynchronous decoupled interaction.

If a designer uses a priority scheme, he or
she can map it to various queues to increase
throughput by using concurrent dispatch-
ing. Careful management of the number of
queues and their relative priority can en-
hance scalability. Queue content dispatched
to domain components in other processes,
perhaps using a messaging subsystem, af-
fords an opportunity for load balancing.
Designers can accomplish load balancing by

� Replicating functionality into various
processes and apportioning the number
of queued requests sent to each process,
possibly with a weighting factor for
each request based on its memory, I/O,
and computational resource usage

� Partitioning functionality into various
processes and apportioning according
to type the queued requests sent to each
process, again possibly with a weighting
factor

� Combining these schemes to apportion
requests according to type and number

The key here is to always try to tease
apart a design issue so that it can be handled
asynchronously. Resist the trap of assuming
everything must be synchronous. Consider
that even customer access from a Web
browser does not necessarily require syn-
chronous response (response at the applica-
tion level, not the HTTP level). For exam-
ple, an online customer need not necessarily
wait for credit card approval. A designer
could set up a merchandise payment use
case so that an email message is sent with
the purchase confirmation sometime after
the purchase. The world is moving in the di-
rection of alerting and notification, which
are push-based paradigms. Consequently,
even transactional requests are increasingly
handled asynchronously.

Often it makes sense to run queued jobs
on a scheduled basis. In this kind of queue-
based batching, a background process is

The world is
moving in the
direction of
alerting and
notification,
which are

push-based
paradigms.

Consequently,
even

transactional
requests are
increasingly

handled
asynchronously.

3 8 I E E E  S O F T W A R E M a r c h / A p r i l  2 0 0 2



scheduled to wake up during nonpeak pro-
duction system hours and service all out-
standing requests. This enhances scalability
by spreading system usage across time. Prin-
ciples to apply in this strategy include
D&C, asynchrony, and concurrency.

Near real-time synchronization of data
It is common for designers to assume that

in Web applications transactional changes
must be instantly reflected in all federated
databases of record. The problem is that
synchronous distributed transactions are
costly. Solutions such as two-phase commits
increase network load, create multiple
points of failure, and generate multiple wait
states where the transaction time is bounded
by the slowest system. 

Transactions rarely have to be synchro-
nously distributed across all involved sys-
tems. For example, a data warehouse 
update can typically withstand some transac-
tional delay, because it is not part of a real-
time business process. In this case, the delay
could be on the order of a few seconds (or
more), but even critical systems might be able
to handle synchronization latencies of a few
hundred milliseconds. Near real-time syn-
chronization assists scalability by spreading
the system’s transactional load across time.

In converting real-time synchronous dis-
tributed transactions into near real-time
asynchronous ones, the best recourse is to
choose a single database as the primary
database of record. This primary database
serves as the synchronization resource for all
others.4 Principles that apply in this strategy
include D&C, asynchrony, and concurrency.

Distributed session tracking 
Generally, session tracking is maintained

on a Web server either through cookies or
by server-specific internal mechanisms. This
limits load balancing across servers because
clients must always be routed back through
the same server so that their session state is
available. Routing users to any Web server
on a request-by-request basis is preferable
because HTTP is designed to optimize this
kind of resource usage. One architectural
solution, called distributed session tracking,
places the shared view of session state in a
persistent store visible to all Web servers. A
client can be routed to any server, and the
session state will still be available. Typically,

the persistent store is an RDB.
Distributed session tracking results in bet-

ter load balancing, but it comes at a cost. If
a designer uses the same RDB system for ses-
sion tracking and business-related transac-
tions, the load on the RDB increases consid-
erably. There is also the overhead of having
to object-to-relational map session state. A
better solution for implementing distributed
session tracking is to use a secondary light-
weight RDB, an object database (ODB), or,
better yet, an object cache with shared visi-
bility across VMs. Principles that apply in
this strategy include D&C and concurrency.

Intelligent Web site load distribution
Perhaps the prime characterization of an

Internet application is the requirement to
transport files of all kinds around the net-
work in an efficient manner. End-user satis-
faction with a site is highly correlated with
the speed at which information is rendered.
However, rendering covers a whole plethora
of possibilities—simple HTML pages, pic-
tures, streaming audio or video, and so on. 

A well-designed Web site can handle
many concurrent requests for simple HTML
files, but entities such as streaming video in-
volve much larger demands. For a busy Web
site, it could be literally impossible to han-
dle peak loads in a reasonable manner. The
solution is to replicate these static, high-
bandwidth resources to better manage the
load. Incoming HTTP requests redirect to
the mirrored facilities based on some com-
bination of available server and network ca-
pacity. This can be accomplished internally
or by subscribing to one of the commercial
providers who specialize in this type of serv-
ice. Principles that apply in this strategy in-
clude D&C, asynchrony, and concurrency.

Keep it simple
Donald A. Norman warns in the preface

to The Design of Everyday Things, “Rule
of thumb: if you think something is clever
and sophisticated, beware—it is probably
self-indulgence.”5

The software development cycle’s elabo-
ration phase is an iterative endeavor, and
customer requirements need constant
reevaluation. Complicated solutions make
refactoring harder. In accordance with Oc-
cam’s Razor, when faced with two design
approaches, choose the simpler one. If the

M a r c h / A p r i l  2 0 0 2 I E E E  S O F T W A R E 3 9

End-user
satisfaction
with a site is

highly
correlated with

the speed at
which

information is
rendered. 



simpler solution proves inadequate to the
purpose, consider the more complicated
counterpart in the project’s later stages.

Putting it all together
Although the complete mapping of a

problem domain into a solution space is be-
yond this article’s scope, this section offers
up a process for system partitioning. Each
step in the process uses strategies presented
in the previous sections. 

The nature of design is decidedly nonlin-

ear: it is iterative, recursive, and sometimes
anticipatory. Consequently, design does not
progress through the following steps in a
linear fashion. Rather it loops and cycles
through them (see Figure 2).

Design conceptual view
Create a conceptual view of the architec-

ture, defining appropriate domain-specific
functional blocks and connectors that fol-
low good responsibility-driven design prin-
ciples.6 This breaks down a solution space
into functional components with an encap-
sulated set of responsibilities, appropriate
communications protocols, and connectors
to manage those protocols.

Map basic topology
Map the conceptual view into an initial

system topology. This exposes all system
stakeholders in the architecture—legacy sys-
tems, data sources, server hardware, mid-
dleware, and so on. Identify the role each
system plays in the overall design and its re-
sponsibilities, needs, and constraints. For
legacy systems, identify usage patterns. In
keeping with the KISS principle, start with a
minimal setup, such as one VM or process
per tier, one Web server, and so on. Identify
systems that are candidates for using repli-
cation, pooling, data-partitioning schemes,
and so forth.

Define domain components
For each system in your topology, group

functional blocks into domain components.
It is at this stage that a designer should con-
sider existing layering paradigms to assist in
the grouping. In particular, consider a serv-
ice-based layered architecture.7 Also, at this
point, third-party software components
should be considered because they present
previously defined interfaces and usage con-
straints. Domain components should also be
designed to best use available standards, so
match a domain component with a stan-
dard’s APIs. In terms of initial scalability con-
cerns, group the functional blocks initially to
minimize communication bandwidth and
coupling. Check created domain component
candidates for possible queued background
processing or, possibly, batched processing.
Identify transaction flow between domain
components and, whenever possible, avoid
distributed transactions.

4 0 I E E E  S O F T W A R E M a r c h / A p r i l  2 0 0 2

<<synchronize>>

<<redefine>>

Design
conceptual

view

Map
basic

topology

Define
domain

components

Scale
domain

components

Scale
topology

Tune
system

<<redesign>>

Figure 2. The scalability design process. To simplify the 
diagram, we chose stereotypes instead of branches. The
stereotypes <<redesign>> and <<redefine>> should be clear
from the main text. The <<synchronize>> stereotype 
guarantees that new domain component functionality 
translates back to the conceptual view.



Scale domain components
Now relax the single VM or process per

tier constraint. Refactor the domain compo-
nents to optimize for data distribution and re-
source use. This might mean redefining func-
tional blocks or their grouping into new
domain components, placing different do-
main components in different processes, or
replicating domain components into two or
more processes. Load-balance work among
the processes, and benchmark and measure
process memory, CPU usage, and I/O band-
width to identify well-balanced configura-
tions. Analyze where some I/O-bound or
computationally intensive domain compo-
nents should have their own processes. 
Balance refactoring metrics against communi-
cation bandwidth between processes. Inter-
process communication should not cause a
huge performance hit compared to the single
process phase (at some point, the price of
communication outweighs the advantages of
domain component distribution). Identify
candidates for multiplexed resources. Keep an
eye on the back end and see how it plays
transactionally across your processes. Are
transactional deadlines being met? Are re-
sources across processes participating cor-
rectly in each transactional use case? 

Scale topology
Now move back to your initial system

topology definition and double the number of
systems that you qualified as good candidates
in mapping a basic topology. At this point, it
might be necessary to incorporate some hard-
ware routing scheme or some new component
such as a Java messaging system queue. Repli-
cating systems such as those contained in Web
servers most likely will lead to replication of
associated processes. For Web server replica-
tion in particular, consider distributed session
tracking as an option for load balancing.

Tune system
Finally, based on previous refactoring ob-

servations, increase the number of qualified
systems (domain servers, content servers,
Web servers, databases of record, and so
forth) and domain components to meet
your production load demand. Ask how the
current system and domain component dis-
tribution should be refactored to handle fu-
ture load demand increases. At this point,
the application space is partitioned in a scal-

able manner, so there should be a clear path
to handling load increases. If no such clear
path exists, restart the process by redefining
the architectural functional blocks or their
grouping into domain components.

T his article addresses many strategiesthat are important when designingscalable architectures, but our list is
not exhaustive. Rather, it represents what
we believe to be the most critical strategies
for server-side scalability. Unfortunately,
space limitations made it impossible to in-
clude an actual example of a well-designed
scalable system built on these principles.

The principles are important guidelines,
but they are not a substitute for measure-
ment. Throughout a system’s design and im-
plementation, testing and benchmarking
what you’re building to ensure that desired
scalability is achieved is important. Software
projects should always follow a motto—
“benchmark early and benchmark often.”

Acknowledgments
We thank Anita Osterhaug and John Cribbs for

their insightful comments and suggestions. We never
would have made it without them. 

References
1. S.W. Ambler, “Distributed Object Design,” The Unified

Process: Elaboration Phase, R&D Books, Lawrence,
Kan., 2000. 

2. C. Hofmeister, R. Nord, and D. Soni, Applied Software
Architecture, Addison-Wesley, Reading, Mass., 2000. 

3. D.S. Linthicum, Enterprise Application Integration, Ad-
dison-Wesley, Reading, Mass., 2000. 

4. C. Britton, IT Architectures and Middleware: Strategies
for Building Large Integrated Systems, Addison-Wesley,
Reading, Mass., 2001. 

5. D. Norman, The Design of Everyday Things, Double-
day, New York, 1988.

6. R. Wirfs-Brock, B. Wilkerson, and L. Wiener, Designing
Object-Oriented Software, Prentice Hall, Englewood
Cliffs, N.J., 1990.

7. GemStone A3Team, “I-Commerce Design Issues and
Solutions,” GemStone Developer Guide, GemStone Sys-
tems, Beaverton, Ore., 2000.

M a r c h / A p r i l  2 0 0 2 I E E E  S O F T W A R E 4 1

About the Authors

Colleen Roe is chief architect at GemStone Systems and has been designing and develop-
ing software systems since kindergarten (or at least that’s how it feels). She has experience in
several industry sectors including telecommunications, oil, manufacturing, software, and phar-
maceuticals, and has developed everything from embedded systems to expert systems. Her
current research focus is scalable architectures for large e-commerce J2EE-based systems. Con-
tact her at GemStone Systems, 1260 NW Waterhouse Ave., Ste. 200, Beaverton, OR 97006;
colleenroe@earthlink.net.

Sergio Gonik is a lead architect at GemStone Systems, where he currently focuses on
next-generation large-scale e-business distributed systems. He has 13 years of experience in
the field and has designed diverse software architectures for scientific, embedded, medical,
music, and J2EE e-commerce systems. He is also a professional composer and performer who
has written and executed musical pieces for theater, dance, and film. Contact him at GemStone
Systems, 1260 NW Waterhouse Ave., Ste. 200, Beaverton, OR 97006; sergiog@gemstone.com.



4 2 I E E E  S O F T W A R E M a r c h / A p r i l  2 0 0 2 0 7 4 0 - 7 4 5 9 / 0 2 / $ 1 7 . 0 0  ©  2 0 0 2  I E E E

Nevertheless, development for the Web
faces difficulties that standard development
does not. Three challenges come to the
fore. First, the Web development process
must be highly accelerated—“Web time”
has become synonymous with headlong
desperation and virtually impossible dead-
lines. Second, many Web projects are
“green-field” applications, which means
they have few if any standards or prece-
dents. Finally, much more so than standard
software, Web applications must focus on
the user experience. 

Systematic, disciplined practices might
help developers face these challenges, but
traditional approaches proliferate docu-
ments and deliverables and emphasize
analysis of established practices—a poor
match for novel applications with Web-time
delivery schedules. Furthermore, user inter-
face design and usability are generally weak
points in both the heavyweight software en-

gineering processes, such as the Unified
Process,1 and the lightweight competitors,
such as Extreme Programming.2 This article
describes a flexible, model-driven approach
for engineering Web applications that suc-
ceeds through a focus on user interface de-
sign and usability. Its simple, model-driven
techniques work well for novel applications
and integrate readily with lightweight or ag-
ile development processes3 under com-
pressed development schedules.

Web applications for use
Usability and the user experience are

emerging as critical determinants of success
in Web applications. If customers can’t find
what they’re looking for, they can’t buy it; a
site that buries key information impairs
business decision making. Poorly designed
interfaces increase user errors, which can be
costly. Mistakes made entering credit card
billing information, for example, can re-

focus
Usage-Centered
Engineering for Web
Applications

Larry L. Constantine and Lucy A.D. Lockwood, Constantine & Lockwood

This model-driven
approach to software
engineering, focusing
on user interface
design and usability,
proves especially
appropriate for Web
applications.

D
espite breathless declarations that the Web represents a new par-
adigm defined by new rules, professional developers are realizing
that lessons learned in the pre-Internet days of software develop-
ment still apply. Web pages are user interfaces, HTML program-

ming is programming, and browser-deployed applications are software sys-
tems that can benefit from basic software engineering principles. 

engineering Internet software



quire costly manual follow-up or lead to
lost sales. Web-usability guru Jakob Nielsen
estimates that billions of dollars in lost Web
sales can be traced to usability problems.

Technical and customer support costs in
all areas of business are skyrocketing. Every
time usability problems on a site or applica-
tion prompt a telephone call or an email
message to customer service, an inexpensive
Web session becomes an expensive support
incident. On corporate intranets, hard-to-
use applications require extensive training
or go completely unused.

Although this article focuses on Web-
based applications—systems that deploy
functional capability on the Web through a
browser instance as a thin client—it is diffi-
cult to draw a hard and fast line separating
Web-based applications from those Web
sites that are intended for use, that is, for
something more than entertainment,
“branding,” or the like. The common thread
is the delivery of useful services and capabil-
ities over the Web. 

In addition to usability, aesthetic design
aspects also figure in the user experience.
Unfortunately, graphic design and aesthetic
considerations are often established early
and drive the entire development process at
the expense of usability. For example, based
on an “aesthetic brief” approved by a
client’s CEO, a prominent graphic design
house developed an innovative Web site for
a major analyst group. Although the design-
ers consulted extensively with the firm’s cus-
tomers and conducted numerous market-
oriented focus groups, the design proved a
usability disaster, in no small part because
of the aesthetic design. Last-ditch efforts to
make improvements without modifying the
basic graphic design enabled the site to go
live but ultimately proved merely palliative
and insufficient to satisfy customers. In the
end, the firm redesigned the site completely,
this time approaching the project with us-
age-centered design.4

Usage-centered design
Usage-centered design uses abstract mod-

els to systematically design the smallest, sim-
plest system that fully and directly supports
all the tasks users need to accomplish. De-
veloped in the early 1990s, the approach has
been applied to systems ranging from indus-
trial automation systems and consumer elec-

tronics to banking and insurance applica-
tions. The streamlined process, driven by
simple models, scales readily; developers
have used it on projects ranging from a few
person-months to the five-designer, 19-
developer, 23-month project that produced
Step7lite, a sophisticated integrated devel-
opment environment from Siemens AG
(see http://foruse.com/pcd). On the Web, groups
have employed usage-centered design success-
fully for applications in fields including e-
commerce, membership support, education,
and medical informatics.5

User-centered or usage-centered?
Usage-centered design evolved as a soft-

ware engineering alternative to user-cen-
tered design. Table 1 summarizes some of
the salient differences. User-centered de-
sign is a loose collection of human-factors
techniques united under a philosophy of
understanding users and involving them in
design. It relies primarily on three tech-
niques: user studies to identify users’ de-
sires, rapid paper prototyping for user
feedback on user interface design itera-
tions, and usability testing to identify
problems in working prototypes or sys-
tems. Although helpful, none of these tech-
niques can replace good design. User stud-
ies can easily confuse what users want with
what they truly need. Rapid iterative pro-
totyping can often be a sloppy substitute
for thoughtful and systematic design. Most
importantly, usability testing is a relatively
inefficient way to find problems you can
avoid through proper design.4,6

Driving models. Role, task, and content mod-
els—simple, closely related abstract mod-

M a r c h / A p r i l  2 0 0 2 I E E E  S O F T W A R E 4 3

Table 1 
User-centered and usage-centered design: 

A comparison
User-centered design Usage-centered design

Focus is on users: User experience Focus is on usage: Improved tools 
and user satisfaction supporting task accomplishment
Driven by user input Driven by models and modeling
Substantial user involvement Selective user involvement

• User studies • Exploratory modeling
• Participatory design • Model validation
• User feedback • Usability inspections
• User testing

Design by iterative prototyping Design by modeling
Highly varied, informal, or unspecified processes Systematic, fully specified process
Design by trial and error, evolution Design by engineering



els—drive usage-centered design. The role
model captures the salient characteristics of
the roles that users play in relation to a sys-
tem. The task model represents the structure
of the work users need to accomplish with
the system. The content model represents the
user interface’s contents and organization.

A user role represents one kind of rela-
tionship users could have with a system.
Among numerous possible aspects of this
relationship are the purpose and frequency
of interaction, the volume and direction of
information exchange, and the attitude to-
ward the system of users in the role.

A task model comprises a set of task cases

and a map of the interrelationships among
those task cases. Task cases are a form of use
cases,7 which conventionally are models of
systems—namely, the “sequence of actions a
system performs that yields an observable
result to a particular actor.”1 As commonly
written, use cases express the concrete ac-
tions and responses taken by the system and
an actor with which it interacts. Figure 1, a
published example of a conventional use
case,1 tellingly omits the user’s single most
important step: actually taking the cash.

Task cases—also called essential use
cases4—are essential models8 that are ab-
stract, simplified, and technology-free—that
is, they have no built-in assumptions about
the user interface. For example, the task
case of Figure 2, written in the style used in
agile usage-centered design, is shorter, sim-
pler, and closer to the essence of the user
task than that of Figure 1. The advantages
of essential use cases are widely recognized,
and a consensus is building that this style is
best for both user interface design and re-
quirements modeling in general.9

The third model, the content model,4 is
sometimes called an abstract prototype: It
abstractly represents the user interface’s
contents—independent of actual appear-
ance or behavior—and how these are or-
ganized into the contexts within which users
interact, called interaction contexts. Ab-
stract prototypes can range from a simple
inventory of a site’s or application’s accessi-
ble contents to a highly structured abstract
model based on a canonical set of abstract
components (see http://foruse.com/articles/
canonical.pdf). Regardless of the form, ab-
stract prototypes are a bridge between task
models and realistic or representational pro-
totypes, such as conventional paper proto-
types or design sketches. By adhering to a
user perspective and leaving open the vari-
ous implementation possibilities, abstrac-
tion encourages innovative visual and inter-
action designs.4,10

Process overview. Figure 3 shows a logical
overview of usage-centered design. The
process separates system actors—other soft-
ware and hardware systems—from human
actors who are users. The role, task, and
content models are developed in coordina-
tion with and interlinked with other models.
These include a domain model (such as a

4 4 I E E E  S O F T W A R E M a r c h / A p r i l  2 0 0 2

Figure 1. Conventional use case for getting cash from an ATM.9

Withdraw Money
The use case begins when the client inserts an ATM card. The system
reads and validates the information on the card.

1.  System prompts for PIN. The client enters PIN. The system vali-
dates the PIN.

2.  System asks which operation the client wishes to perform. Client
selects “Cash withdrawal.”

3.  System requests amounts [sic]. Client enters amount.
4. System requests type. Client selects account type (checking, sav-

ings, credit).
5. The system communicates with the ATM network to validate ac-

count ID, PIN, and availability of the amount requested.
6.  The system asks the client whether he or she wants a receipt.

This step is performed only if there is paper left to print the receipt.
7. System asks the client to withdraw the card. Client withdraws

card. (This is a security measure to ensure that Clients do not
leave their cards in the machine.)

8. System dispenses the requested amount of cash.
9.  System prints receipt.

10. The use case ends.

Figure 2. Abstract use case in essential form—the “getting
cash” task case.



glossary or another more elaborate data
model), a business rules model that embod-
ies the application’s underlying logic and
constraints,11 and an operational model
that captures salient aspects of the working
environment or operational context.3

Conceptually, a straightforward and di-
rect derivation links the final design back to
task cases supporting user roles. Each page,
form, or other interaction context corre-
sponds to an abstract prototype supporting
a cluster of interrelated task cases. The de-
signer derives the actual buttons, links, ta-
bles, displays, and other features directly
from abstract components that realize spe-
cific steps within supported task cases.
These task cases, in turn, support the roles
that users can perform.

Usage-centered Web design
Over successive Web projects, usage-

centered design has evolved a lightweight
form that takes certain modeling and design
shortcuts (see http://foruse.com/articles/
agiledesign.pdf). Agile usage-centered de-
sign employs rapid-fire modeling techniques
using ordinary index cards for brainstorm-
ing, sorting, and clustering,12 as in Extreme
Programming.2 When practical, collabora-
tive modeling with end users, application
domain experts, and clients can save addi-
tional time. Figure 4 outlines an agile usage-
centered design process.

Essential purpose and preconception. Web
projects often suffer from vague objectives
and unrealistic ambitions. Early clarifica-
tion of objectives and exploration of fan-
tasies can pay off enormously.13 We prefer

to kick off the process by involving man-
agers, users, developers, and other stake-
holders in framing the application’s pur-
poses as external users and from a business
perspective. The group can brainstorm onto
cards and sort them to establish priorities.
Similarly, we encourage participants to
share their fantasies about the system’s fea-
tures, functions, facilities, content, and ca-
pabilities, but we clearly label these ideas
preconceptions and set them aside to be
treated as negotiable fantasies rather than
requirements.

Exploratory modeling. Exploratory model-
ing jumpstarts the design process by iden-
tifying questions and areas of uncertainty
or ambiguity in user requirements. The
team does this by sketching out rough ver-
sions of the role and task models that it
will develop in more refined form later. In
this way, the team can identify needed, but
missing, information.

User interface architecture. Many agile
processes3 abjure up-front design in favor of
a more evolutionary approach—construct-
ing a limited system and then elaborating it
through successive iterations. Unfortu-
nately, when code starts small and gradually
grows with expanding requirements, the
original architecture—the code’s underly-
ing organization—often proves insufficient
or inappropriate to support emerging needs.
Refactoring,14 in which existing code is re-
organized and rewritten to fit evolving and
emerging requirements, is a key to success-
ful evolution of complex software systems
through iterative expansion.

M a r c h / A p r i l  2 0 0 2 I E E E  S O F T W A R E 4 5

1. Asdhf asdf
2. Wertw rt bzc
3. Ouiaa ero

Step2Step1

Behavior

1. Asdhf asdf
2. Wertw rt bzc
3. Ouiaa ero

Step2Step1

Behavior

1.  Loquom ipso
2. 
3. Ouiaa ero

Step2Step1 Step2Step1

Behavior
Actors

System actors

User roles

Task cases Abstract prototypes

Visual and
interaction

design

Figure 3. Logical 
process of
usage-centered 
design.



User interfaces are a different story. Late
refinement of the user interface’s basic
structure is not acceptable because it
changes the system for users who have al-
ready learned or mastered an earlier ver-
sion. Whereas refactored internal software
components don’t necessarily affect users,
a redesigned user interface architecture is
unavoidably disruptive. For highly usable
user interfaces, we design the overall or-
ganization, navigation, and look and feel
to fit the full panoply of tasks to be cov-
ered. You don’t have to design every aspect
of the user interface in advance, however.
To avoid radical refactoring or inconsis-
tency across design increments, you need
to work out a well-conceived overall map
of the site or application in advance.15 To
insure that the basic visual style and be-

havior are effective across the entire appli-
cation, you must devise a comprehensive
design scheme.

Navigation architecture specifies how the
overall user interface breaks down into in-
teraction contexts, collections, and groups;
how these are presented to users; and how
users navigate them. Minimally, the naviga-
tion architecture identifies all the interac-
tion contexts and their interconnections,
but having more is advantageous. For ex-
ample, the navigation architecture might or-
ganize parts of the user interface into tabbed
notebooks—accessed through a set of com-
mand buttons on a central dispatch dia-
log—with sections within notebook pages
reached through shortcuts in a menu-like
bar across the top of each page.

Another crucial facet of overall user in-
terface design is the visual and interaction
scheme, a sort of abstract style guide. It
briefly describes the basic visual elements
that will recur throughout the design as well
as the common layouts, visual arrange-
ments, or templates that apply to various in-
teraction contexts.16 The scheme might, for
example, specify a basic layout grid with a
modified tree-view in the left-most column
for primary navigation and a set of view
controls across the top for secondary navi-
gation. At a more detailed level, it might
specify that a distinct color should identify
all editable fields, which can be edited in
place by double clicking. It might further
spell out how certain collections of controls
will be placed on slide-out tool panels that
automatically open on mouse-over and
close after use.

Clearly, a sound navigation architecture
and visual and interaction scheme requires a
complete task model that includes all identi-
fied tasks, not just those to be tackled on the
first or early iterations.

Card-based modeling. The initial user role
model is a simple inventory of roles users
can play in relation to the site or applica-
tion. One of the quickest ways to develop
such an inventory is to brainstorm it di-
rectly onto index cards. Once we’ve agreed
on the initial inventory, we succinctly de-
scribe roles directly on their index cards us-
ing the characteristics most relevant to the
design problem: the role’s context, charac-
teristic patterns of interaction within the

4 6 I E E E  S O F T W A R E M a r c h / A p r i l  2 0 0 2

Figure 4. Rough outline of steps in an agile usage-centered 
design process.

Preliminary steps
1. Essential purpose and preconception—Clarify business and user

purposes, then fantasize and set aside preconceptions of features,
facilities, content, and capabilities.

2. Exploratory modeling—Identify questions, ambiguities, and areas
of risk and uncertainty.

First iteration
3. Role modeling—Inventory and prioritize all user roles and select

initially targeted subset.
4. Task modeling—Inventory and prioritize all tasks and select ini-

tially targeted subset.
5. Task clustering—Group all tasks by affinity and draft overall navi-

gation architecture.
6. Design—Draft visual and interaction scheme; review and revise

aesthetic design.
7. Abstract prototyping—Develop content model for interaction con-

texts supporting selected subset of tasks.
8. Design—Develop detailed user interface design for selected interac-

tion contexts.
9. Construction—Program designed portions of user interface.

Successive iterations (similarly)
3a. Review role model and revise as needed, select next subset.
4a. Review task model and revise as needed, select next subset.
5a. Review navigation architecture and revise as needed.
6a. Review visual and interaction scheme and revise as needed.
7a. Develop content model for interaction contexts supporting next se-

lected task subset.
8a. Design user interface for selected interaction contexts.
9a. Construct newly designed portions.



role, and special support criteria. We then
sort the completed cards to rank them by
overall importance.

After reviewing the roles in order of im-
portance, we brainstorm an inventory of
task cases, again onto index cards. As we re-
view and refine this inventory—condensing,
combining, eliminating, or adding tasks—
we standardize the names to convey the ba-
sic purpose from a user’s perspective, such
as “finding a replacement part for a specific
product” or “adding an assessment to a les-
son plan.” Once we’ve established the initial
set of task cases, we sort the cards to rank
them for expected frequency and for overall
importance. Based on these two rankings,
we deal them into three piles: required (do
first), desired (do if there’s time), and de-
ferred (do next time).

At this point, we start to fill in the de-
tails, but not for all task cases. We skip task
cases for which the interaction appears ob-
vious or routine. For task cases that are crit-
ical, complex, unclear, or interesting, we
write the interaction narrative on the index
card. Working on small cards favors good
modeling discipline. If the process narrative
doesn’t fit on one card, either the narrative
is not sufficiently abstract and simplified, or
the card really covers multiple task cases
that we must identify.

Using all the knowledge built from creat-
ing, refining, and interacting with the task
cases, we group the cards into affinity clus-
ters on the basis of how strongly they seem
to be related—particularly, how likely users
in a role will perform tasks together. Each
cluster represents a set of capabilities that
the application must offer together—within
the same page, form, or browser window, or
at least within a set of closely connected in-
teraction contexts.

Each task cluster becomes the guide for
designing a part of the user interface. We
prefer to construct abstract prototypes but
under pressure could move directly into
sketching realistic paper prototypes. We
then evaluate the paper prototypes, aug-
mented by a description of how the various
elements behave, to identify usability prob-
lems and areas for improvement. For speed
and convenience, we prefer collaborative
usability inspections,3 involving users and
clients as well as designers and developers,
but recognize the effectiveness of heuristic

inspections and other techniques.

In practice
The approach we took to designing a

browser-resident classroom information
management system illustrates the ad hoc
accommodations an agile process often
needs for development in Web time. A so-
phisticated performance-support system to
empower K–12 classroom teachers, this sys-
tem simplified key administrative, planning,
and teaching tasks in an environment fully
integrated with existing administrative sys-
tems and software and with Web-based
learning resources. The application was de-
ployed through a Web-based technology
with the client-side user interface realized in
HTML, XML, and Java JFC/Swing.

Risk factors were numerous, including a
new and unproven development team in a
new company, an outside consulting team
for user interface design, the need for tech-
nical breakthroughs in several areas, vague
and ambiguous requirements exacerbated
by frequent changes in scope, and manage-
ment obsessed with early delivery, frequent
demonstrations, and unpredictable changes
of direction.

The design team collaborated with a team
of PhD-level educators with extensive class-
room experience. This education team served
as both domain experts and representa-
tive end users, while also functioning in what
amounted to an XP-style customer role.2

Collaboratively with the education team,
the design team developed the initial user
role model and inventory of task cases. In
total, we identified and prioritized 14 user
roles. The initial inventory of 142 task cases
included 35 task cases related to lesson
planning, which emerged as a critical capa-
bility for the system. We wrote process nar-
ratives for only a handful of interesting task
cases.1 We constructed a draft navigation
map with just over 40 interaction contexts,
based on task case clustering (see Figure 5).

To resolve problems with the design and
omissions, irregularities, and ambiguities
in the requirements, we held frequent
meetings and consulted continually with
the education team—a pattern we now re-
fer to as JITR, just-in-time requirements.
One of the two designers concentrated on
requirements and scope issues, while the
other began working on the initial design.

M a r c h / A p r i l  2 0 0 2 I E E E  S O F T W A R E 4 7

Each task
cluster

becomes the
guide for

designing a part
of the user
interface.



The design team devised drafts of a navi-
gation architecture, navigation map, and
visual and interaction scheme, validated
them with the education team, then revised
them.

Not entirely by plan, the design pro-
ceeded in two- to two-and-a-half-week cy-
cles. Once full-time design work started, we
completed an initial navigation architecture
and visual and interaction scheme within
the first cycle. We finished the first version
of the complete visual and interaction de-
sign in the next cycle, after which we in-
spected it for usability defects, and we un-
dertook a major design revision on the
second iteration. In the third iteration, we
continued to extend and refine the design,
spread over the remainder of the project’s
17-week usage-centered design portion.
Programming followed an agile, light-
weight process that proceeded in parallel
with visual and interaction design.

We needed a design that would be im-
mediately usable by ordinary teachers

with little or no special training. It had to
be flexible enough to accommodate a
wide range of working styles and efficient
to use, given that teachers may typically
have less than 15 minutes a day to plan
an entire day’s lessons. Technical goals
that we identified based on the usability
criteria included

� Minimize window management over-
head.

� Minimize modal behavior.
� Allow simple and direct switching

among contexts.
� Maximize available screen real estate

for document and data display.

A discussion of the complete design is
beyond this article’s scope (for details, see
our design studies on the Web, www.
foruse.com/articles/designstudy1.pdf, 
/designstudy2.pdf, and /designstudy3.pdf).
However, the first design iteration’s results
clearly illustrate the importance of early,

4 8 I E E E  S O F T W A R E M a r c h / A p r i l  2 0 0 2

Figure 5. Web-
deployed classroom
information 
management system:
Main portion of the
navigation map.



up-front consideration of the overall navi-
gation architecture and the visual and in-
teraction scheme based on a full under-
standing of user roles and tasks. Neither
traditional software engineering practices
nor the newer lightweight methods make
this possible.

Figure 5 shows the main portion of the
navigation map, as finalized in the second
iteration. The mock-up in Figure 6,
adapted directly from actual design docu-
ments produced in the project’s first cycle,
illustrates how we documented the overall
navigation and visual and interaction de-
sign scheme. Together, these design docu-
ments specified much of the user interface’s
overall architecture.

Understanding the various roles class-
room teachers play and thoroughly explor-
ing their tasks revealed the importance of a
fast, flexible, and easy-to-learn scheme for
navigating the application’s various work-
ing contexts. Once we understood the task
structure, we organized the various groups
and interaction contexts for single-click
switching among tasks within any given ac-
tivity. Using simple visual metaphors based
on thumb tabs, hanging folders, and file
folders, we devised a three-level visible hi-
erarchy that proved far easier to learn and

use than a conventional Windows-style tree
view.

Screen real estate, always a scarce re-
source, is particularly valuable in an appli-
cation running within a browser instance
on low- to moderate-resolution displays. To
address this, the design scheme included
components that would expand and con-
tract or reveal and hide themselves auto-
matically as needed. For example, the top-
level navigation panel (containing thumb
tabs) contracts to provide more working
space once the user selects a starting point.
Figure 6 shows another example of screen
space conservation in the dynamic work-
space, which anticipated rather than mim-
icked the Apple OS-X dock. The workspace
provides a compact holding bin that lets the
user gather various documents or frag-
ments from a variety of personal and pub-
lic resources for assembly into a lesson plan
or assignment.

Any product design’s success is ulti-
mately measured by its acceptance. Class-
room teachers in a variety of settings made
immediate use of this application with only
minimal instruction. In fact, we were able
to achieve the stringent design objective of
enabling immediate, productive use of the
system based on a single-page tutorial.

M a r c h / A p r i l  2 0 0 2 I E E E  S O F T W A R E 4 9

Figure 6. Navigation
architecture and 
design scheme, 
classroom 
information 
management 
system.



O ur experiences in agile usage-cen-tered design have made clear that im-proved usability does not come
without cost or risk, even when the methods
are streamlined and the schedule is com-
pressed. Special training and skills are nec-
essary, and these may not be available to
every development group, especially the
lean teams typical in agile development.
Moreover, the best design is easily corrupted
by casual or even well-intentioned alter-
ations and elaborations during coding.
Close coordination between user interface
designers and programmers is essential, and
programmers must be fully committed to
the process and convinced of the benefits
even if they do not understand every design
decision.

A flexible and condensed usage-centered
design process is a good starting point for
collaboration, but for agile practices such as
Extreme Programming, which abjures any-
thing resembling BDUF (big design up
front), some of the rules and philosophy can
get in the way. Just which rules need to be

bent or rewritten and what philosophy may
need to be compromised is the subject of
much current debate and experimentation.
The outline presented here should be re-
garded more as a draft than as a definitive
model.

References
1. P. Kruchten, The Rational Unified Process: An Intro-

duction. Addison-Wesley, Reading, Mass., 1999.
2. K. Beck, Extreme Programming Explained, Addison-

Wesley, Reading, Mass., 2000.
3. M. Fowler, “Put Your Processes on a Diet,” Software

Development, vol. 8, no. 12, Dec. 2000. Expanded ver-
sion at www.martinfowler.com/articles/newMethodol-
ogy.html.

4. L.L. Constantine and L.A.D. Lockwood, Software for
Use: A Practical Guide to the Models and Methods of
Usage-Centered Design, Addison-Wesley, Reading,
Mass., 1999.

5. J. Anderson et al., “Integrating Usability Techniques
into Software Development,” IEEE Software, vol. 18,
no. 1, Jan./Feb. 2001, pp. 46–53.

6. A. Parush, “Usability Design and Testing,” ACM Inter-
actions, vol. 8, no. 5, Sept./Oct. 2001.

7. I. Jacobson et al., Object-Oriented Software Engineer-
ing: A Use Case Driven Approach, Addison-Wesley,
Reading, Mass., 1992.

8. S.M. McMenamin and J. Palmer, Essential Systems
Analysis, Prentice Hall, Englewood Cliffs, N.J., 1984.

9. L.L. Constantine and L.A.D. Lockwood, “Structure and
Style in Use Cases for User Interface Design.” M. van
Harmelan, ed., Object Modeling an User Interface De-
sign, Addison Wesley, Reading, Mass., 2001.

10. D. Corlett, “Innovating with OVID,” ACM Interac-
tions, vol. 7, no. 4, July/Aug. 2000. 

11. E. Gottesdiener, “Rules Rule: Business Rules as Re-
quirements,” Software Development, vol. 7, no. 12,
Dec. 1999. Reprinted in L.L. Constantine, ed., Beyond
Chaos: The Expert Edge in Managing Software Devel-
opment, Addison-Wesley, Reading, Mass., 2001.

12. R. Jeffries, “Card Magic for Managers: Low-Tech Tech-
niques for Design and Decisions,” Software Develop-
ment, vol. 8, no. 12, Dec. 2000. Reprinted in L.L. Con-
stantine, ed., Beyond Chaos: The Expert Edge in
Managing Software Development, Addison-Wesley,
Reading, Mass., 2001.

13. L.A.D. Lockwood, “Taming the Wild Web: Business
Alignment in Web Development,” Software Develop-
ment, vol. 7, no. 4, Apr. 1999. Reprinted in L.L. Con-
stantine, ed., Beyond Chaos: The Expert Edge in Man-
aging Software Development, Addison-Wesley, Boston,
2001.

14. M. Fowler, Refactoring: Improving the Design of Exist-
ing Code, Addison-Wesley, Reading, Mass., 1999.

15. M.H. Cloyd, “Designing User-Centered Web Applica-
tions in Web Time,” IEEE Software, vol. 18, no. 1,
Jan./Feb. 2001, pp. 62–69.

16. J. Pokorny, “Static Pages Are Dead: How a Modular
Approach Is Changing Interaction Design,” ACM Inter-
actions, vol. 8, no. 5, Sept./Oct. 2001.

For more information on this or any other computing topic, please visit our
Digital Library at http://computer.org/publications/dlib.

5 0 I E E E  S O F T W A R E M a r c h / A p r i l  2 0 0 2

L. Constantine, Constantine on Peopleware, Prentice Hall, Upper Saddle 
River, N.J., 2001.

R. Jeffries, A. Anderson, and C. Hendrickson, Extreme Programming In-
stalled, Addison-Wesley, Reading, Mass., 2001.

J. Nielsen and R.L. Mack, eds., Usability Inspection Methods, John Wiley & 
Sons, New York, 1994.

Further Reading

About the Authors

Larry L. Constantine is an adjunct professor of information technology, University of
Technology, Sydney, Australia, and director of research, Constantine & Lockwood, Ltd. A
methodologist who pioneered fundamental concepts and techniques underlying modern soft-
ware engineering theory and practice, he now focuses on methods for improving the usability
of software and hardware. A consultant and designer with clients around the world, he shared
the 2001 Performance-Centered Design award for his work with Siemens AG. He is a graduate
of MIT’s Sloan School of Management, a member of the IEEE Computer Society and the ACM,
and he has served on the boards of numerous professional journals, including IEEE Software.
Contact him at lconstantine@foruse.com.

Lucy A.D. Lockwood is president of Constantine & Lockwood, Ltd., the design, train-
ing, and consulting firm she cofounded. One of the developers of usage-centered design, she
is the coauthor of a Jolt Award-winning book on the subject. With a passion for teaching and
professional communication, she has chaired several professional conferences and is co-organ-
izer of forUSE 2002, the first international conference on usage-centered, performance-cen-
tered, and task-oriented design. She is a graduate of Tufts University and a member of the
IEEE and the ACM. Contact her at llockwood@foruse.com.



focusengineering Internet software

0 7 4 0 - 7 4 5 9 / 0 2 / $ 1 7 . 0 0  ©  2 0 0 2  I E E E M a r c h / A p r i l  2 0 0 2 I E E E  S O F T W A R E 5 1

technology-independent reuse and sharing
of content, data, and messaging but at the
expense of encapsulation and the associa-
tion of behavior with state, which is central
to OO.

Scripting languages, common on the
Web, are often optimized for rapidly creat-
ing simple functionality rather than for
modular construction of large programs.
Also, such languages typically lack the rich
development environments of general-pur-
pose languages. Some Web developers even
deliberately disregard software engineering
principles. They argue that if we’re just
writing scripts, they don’t merit the kind of
engineering techniques—object or other-
wise—we apply to “real” systems.

However, software engineering and OO
techniques are gaining importance in Web
development as Web applications become
more complex and integrated with tradi-
tional server-side applications. To motivate

the need for these techniques, we examine
some representative Web technologies and
the issues they present in naive use. We de-
scribe a layered, OO architecture, based on
the Model-View-Controller (MVC) pattern,
which can overcome these issues to produce
large, well-structured systems.  

Motivations
If we consider scripts from an OO and

layering perspective, the most immediate
problem is that a single script has responsi-
bilities spanning several layers (see the “Def-
initions” sidebar for an explanation of our
terminology). The script must

� Accept input
� Handle application logic
� Handle business logic
� Generate output (presentation logic)

This couples all the layers together, making

Objects and the Web

Alan Knight, Cincom Systems

Naci Dai, ObjectLearn

Good design
practices are
increasingly
important in Web
development. Here,
the authors apply
such practices 
using a framework
for layered
architectures based
on the Smalltalk GUI
development pattern
of Model-View-
Controller.

A
pplying software engineering principles, particularly object-ori-
ented techniques, to the Web can be difficult. Many current Web
technologies lend themselves to—or even encourage—bad prac-
tices. Scripting and server-page technologies can encourage cut-

and-paste reuse, direct-to-database coding, and poor factoring. Component
models such as the Component Object Model (COM) and Enterprise Java-
Beans (EJB) seek to construct building blocks for application assembly, but in
doing so they sacrifice many of the advantages of objects. XML emphasizes



it harder to modify or test any particular
aspect in isolation. In addition, there are
significant issues related to handling these
responsibilities, as described below. For
server pages used alone, the same issues ap-
ply (because we can consider a server as a
script with some additional embedded
HTML), and mixing code with the HTML
also presents code management and debug-
ging issues. 

Accepting input 
When accepting input, a script receives ei-

ther the raw HTTP input stream or a mini-
mally parsed representation of it. HTTP sup-
ports several different mechanisms for
passing parameters (encoding into the URL,
query values, or form data), and all of these
pass the data as simple strings. Each script
must know or determine the parameter-pass-
ing mechanism, convert the parameters to

5 2 I E E E  S O F T W A R E M a r c h / A p r i l  2 0 0 2

Here we define our terminology and goals. In the main text,
we present a layered architecture, implemented using both scripts
and server pages. Most of these techniques can be applied to
any technology in these categories, but where the details of a
specific technology are important, we use servlets and Java-
Server Pages as representative technologies. Both, while nomi-
nally Java specifications, can easily be applied to other lan-
guages, and implementations in both Smalltalk and C++ are
commercially available.

Layered architecture
A layered architecture is a system containing multiple,

strongly separated layers, with minimal dependencies and in-
teractions between the layers. Such a system has good separa-
tion of concerns, meaning that we can deal with different areas
of the application code in isolation, with minimal or no side ef-
fects in different layers. By separating the system’s different
pieces, we make the software adaptable so that we can easily
change and enhance it as requirements change. The layers we
are concerned with here include input, application logic, busi-
ness logic, and presentation.

Input 
The input layer contains the code concerned with processing

and syntactically validating input. In a Web context, this pro-
cessing and syntactically validating input includes HTTP input
parsing and extracting parameters from an HTTP request. In the
Model-View-Controller framework, this corresponds to the input
controller.

Application logic
The application logic code is concerned with the applica-

tion’s overall flow. We often refer to it as the glue layer, sepa-
rating business logic from input and presentation logic and
managing the interface between the two. This requires some
knowledge of both layers. For example, this layer would be in-
volved in converting between presentation-level inputs and out-
puts as strings and the corresponding business object messages
or state. This layer might also manage a multipage Web inter-
action as a sequence of steps. In the Model-View-Controller
framework, this corresponds to the application controller.

Business logic
The business logic code is concerned only with the underly-

ing business functionality. This code should be entirely unaware
of the presentation being used. We also refer to business ob-
jects, which implement the business logic. In a complex appli-
cation, business logic is likely to be the largest component and
can include code that accesses external systems such as data-
bases, external components, and other services. In the Model-
View-Controller framework, this corresponds to the model.

Presentation
This layer contains code and noncode resources (such as

HTML text and images) used to present the application. It typi-
cally contains little code—code concerned only with formatting
and presenting data. An example of this in a Web context is a
server page’s code fragments that print values into a dynami-
cally generated Web page. In the Model-View-Controller
framework, this corresponds to the view.

Scripts
Many basic Web technologies can be grouped together in

the category of scripts—small programs that perform HTTP pro-
cessing. This term encompasses, among others, compiled CGI
programs, files of scripting language code (such as Perl,
Python, Ruby, and VBScript), and Java servlets. While there are
significant differences among these technologies, all of them
have the fundamental characteristic of being programs that ac-
cept an HTTP request and send back a response. In basic us-
age, each script is stateless and independent of all others. An
important distinction within this category is whether scripts can
share memory with other scripts (for example, servlets) or are
entirely independent (for example, CGI programs). Shared
memory allows more effective use of system resources through
pooling, and a simpler programming model through persistent
states at the cost of a more complex infrastructure.

Server pages
An alternative mode of HTML scripting is that of an HTML

page annotated with small amounts of code. Many scripting lan-
guages support this kind of facility in addition to pure scripts, and
representative examples include JavaServer Pages (JSP), Mi-
crosoft’s Active Server Pages, PHP, and Zope. There are also vari-
ations on this approach in which the pages are annotated with
application-specific HTML or XML tags, and developers can spec-
ify code to be run when these tags are encountered. Examples of
this include JSP bean and custom tags and Enhydra’s XMLC. 

Definitions



appropriate types, and validate them. This
causes code duplication between scripts.

Handling application logic 
Another issue, which affects both input

and application logic, is the lack of informa-
tion hiding when accessing request and ses-
sion data. The script must retrieve input data
from the request by name. HTTP is a state-
less protocol, so data used in multiple scripts
must be either stored in a session identified
with the user or reread from an external data
source in each script requiring the data. For
example, if a script passes login information
as form data, the code to store that informa-
tion in the session might be

password = request.getParameter

(“passwordField”);

decrypted = this.decode

(password);

request.getSession().putValue

(“password”,decrypted);

Both storage in the session and storage in
an external data source are effectively global
in scope, and the application accesses them in
dictionary-like fashion using strings as keys.
Normal programming mechanisms for con-
trolling variable access do not apply to this
data, and any scripts or server pages that wish
to use this data must be aware of the naming
conventions. We cannot easily find all accesses
to the variables using programming-language
mechanisms, so modifications become more
difficult. If the script does not encapsulate
these conventions, knowledge of them and of
the HTTP protocol’s details can spread
throughout the application, greatly hindering
adaptation to new uses. Furthermore, this is a
potential source of errors because of both
spelling errors and different scripts using the
same name for different purposes. As the
number of scripts or server pages increases,
these problems become overwhelming.

When using server pages for application
logic, we are adding potentially significant
amounts of code to the page. Code manage-
ment techniques are not usually available for
code inside server pages. With many tech-
nologies, debugging code inside the server
pages is difficult. Features of modern devel-
opment environments for code authoring and
interactive debugging might not be available,
and for compiled languages we might need to

debug inside complex generated code. For
these reasons, it is a good idea to minimize the
amount of code in server pages and to keep
application logic out of the pages.

Handling business logic 
Because all of the code is grouped to-

gether, it is difficult to isolate the business
logic from the other layers, particularly ap-
plication logic. Furthermore, unless we can
run portions of the scripts separately, it is
impossible to test the business logic (or any
of the other layers) independently.

With server pages, business logic presents
the same issues that we discussed for appli-
cation logic. We can very quickly have too
much code in the pages, and even pages
with minimal code are difficult to manage
and debug. 

Generating output
In producing output, simple scripts mix

the HTML encoding of the result with the
dynamic data. This couples the page’s look
and feel with the other layers. Changing the
Web site’s look or adapting the application to
multiple output devices becomes extremely
difficult. The latter is becoming increasingly
important as the Web expands to include de-
vices such as WAP (Wireless Application Pro-
tocol)-connected mobile phones and other
small devices.

Server pages help address this last issue by
letting Web designers design and maintain
the pages and by letting programmers pro-
vide annotations. This is generally considered
the most appropriate use for server pages.

Model-View-Controller
To overcome these difficulties, we can use

a combination of scripts, server pages, and
application code. The approach we present
here is part of a family of possible approaches
we have used1,2 to properly partition respon-
sibilities and overcome weaknesses in the un-
derlying technologies (see the “A Related Ap-
proach” sidebar for a comparison). MVC
strongly influences our approach, so we first
examine its history.

MVC concepts
MVC originated in the Smalltalk-80 system

to promote a layered approach when develop-
ing graphical user interfaces.3 It emerged in the
late 1970s and early 1980s, long before such

M a r c h / A p r i l  2 0 0 2 I E E E  S O F T W A R E 5 3

It is a good idea
to minimize the
amount of code
in server pages

and to keep
application logic
out of the pages.



interfaces had become mainstream. It defined
three different components (see Figure 1a):

� The model handles application and
business logic.

� The view handles presentation logic.
� The controller accepts and interprets

keyboard and mouse input.

The intention was to separate the model
(meaning nonGUI) code from its presentation.
The model code didn’t contain any GUI infor-
mation, but it broadcast notification of any

state changes to dependents, which were typi-
cally views. This is similar to many current
GUI component models, of which JavaBeans
is perhaps the most widely known.

This scheme provided a good separation
between these three layers but suffered from
two weaknesses. First, it had a simplistic
view of the model and did not account for
any difference between application logic (for
example, flow of control and coordination
of multiple widgets in a GUI) and business
logic (for example, executing a share pur-
chase). Second, most GUI libraries and win-
dowing systems combined the view and con-
troller functions in a single widget, making
the logical separation into view and con-
troller less useful. Later versions of Smalltalk
with operating system widgets chose not to
use a separate controller. All of the Smalltalk
versions eventually introduced an additional
layer to handle application logic, distinct
from the business objects. Perhaps the most
elegant of these is Presenter, as used in, for
example, Taligent and Dolphin Smalltalk.4,5

Together, these revisions changed the
common understanding of the framework,
such that the term controller now refers to
the object handling application logic and
the term model is reserved for business ob-
jects (see Figure 1b). To distinguish these
two interpretations of MVC, we use model
to refer to business objects, and we use in-
put controller and application controller to
refer to the two types of controllers.

MVC for the Web
To apply MVC to the Web, we use a com-

bination of scripts, server pages, and ordi-
nary objects to implement the various com-
ponents in a framework we call Web
Actions. In this context, both versions of the

5 4 I E E E  S O F T W A R E M a r c h / A p r i l  2 0 0 2

There are many frameworks and design patterns influenced
by layering, the Model-View-Controller pattern, and object
principles. In general, using servlets and server pages together
and keeping code out of the server pages as much as possible
is referred to in Java circles as model 2 Web programming
(see http://java.sun.com/j2ee). The most widely known frame-
work using these principles is the open source Jakarta Struts
(see http://jakarta.apache.org/struts).

Struts is a controller framework for Java to build JavaServer
Pages-based views that are linked to business models using a
single controller servlet. Struts is very close to many of our con-
cepts and also uses a single input controller servlet, but it dif-
fers in some significant areas. Most notably, it does not distin-
guish a separate application controller, distinguishing only

input, action, presentation, and model. 
Application controller responsibilities are assigned to ac-

tions, model objects, or declarative aspects of the framework.
Actions are command objects, and Struts suggests that they
should delegate as much behavior as possible to the model, but
coding examples include application control or even model be-
havior in some actions. The transition to the next page after an
action is controlled by the developer by editing configuration
files. One other difference is that Struts classifies model objects
into “bean forms” that have only state, and more normal ob-
jects with both function and behavior. These bean forms are
used to store data for input or presentation processing. We
have no corresponding layer and are unclear as to the role of
pure state objects in an OO framework.

A Related Approach

Model View

Controller

(a)

Notifications

Display

Keyboard,
mouse

Application
controller

Business
objects

View

Input
controller

(b)

Notifications

Notifications

Business logic layer Tool layer

Problem space User interface

View layer

Figure 1. (a) The
original Smalltalk-80
Model-View-Controller
pattern and (b) the
revised Model-View-
Controller. 



MVC are relevant, particularly the dual uses
of the term controller. For HTTP applica-
tions, input and presentation are entirely
separate, so an input controller distinct from
the view is useful. For applications of any
complexity, we also need an application con-
troller to separate the details of application
flow from the business logic.

Figure 2 shows the framework’s basic ob-
ject structure. 

Input controller. We implement the input con-
troller as a script. One of the framework’s
important features is that there is a single in-
put controller for all pages in a Web appli-
cation. The input controller parses input, de-
termines the parameter-passing mechanisms,
extracts any necessary information from the
request, cooperates with the application
controller to determine the next action, and
invokes that action in the correct context.

By having a single script as an input con-
troller, we localize any knowledge of HTTP
or naming conventions at the request level,
reduce code duplication and the total num-
ber of scripts, and make it easier to modify
any of the input processing, because there is
a single point of modification.

Note that the input controller class is
shown as an abstract class, with one imple-
mentation for accessing the applications
over HTTP with a regular Web browser and
another implementation for accessing the
applications using a WAP-enabled device.

Application controller. We implement the ap-
plication controller as a regular object—not
as a script or server page. It coordinates
logic related to the application flow, handles
errors, maintains longer-term state (includ-
ing references to the business objects), and
determines which view to display. We store
the application controller in the session us-
ing a key known to the input controller.
This relies on a naming convention, with
the disadvantages described earlier, but be-
cause this is the only thing stored directly in
the session, the impact is minimized.

A single application controller is respon-
sible for multiple Web pages. In a simple
application, it might be responsible for all
pages; in a complex application, there are
multiple application controllers for different
areas of the application.

By using a single, well-encapsulated ob-

ject as the central point of reference for any
persistent information, the application con-
troller resolves the issues of information
hiding and naming conventions. Rather
than storing isolated pieces of information
in the session, we store them in business ob-
jects and access them using messages from
the application controller. Programming-
language mechanisms let us track use of the
application controller and business objects
and more easily modify our code. If we are
using a statically typed language, we also
get static type checking as an additional val-
idation of data usage.

Action. The single input controller will in-
voke one of many possible actions on each
request. One of its responsibilities is to de-
termine which one. This depends on both
the input from the user and on the applica-
tion’s current state, so it must be determined
in conjunction with the application con-
troller. We represent the result of this deter-
mination as an Action object (an implemen-
tation of the Command pattern described
elsewhere6).

Business objects. We implement business ob-
jects as normal objects that contain only
business logic, so they should have no knowl-
edge of any other layers. The application
controller is the only thing that manipulates
the business objects, and for this reason they
are not shown in Figure 2 but are inside the
application controller. Both of these attri-
butes make it much easier to develop and test
the business logic in isolation from the Web
infrastructure. Because the business objects

M a r c h / A p r i l  2 0 0 2 I E E E  S O F T W A R E 5 5

HTTP

Finds and executes

JavaServer
Page view

WAP

Action

Input
controller

HTTP input
controller

WAP input
controller

Application
controller

Java-
Server
Page

XML

View

XML view

Figure 2. The Web
Actions framework.



might be isolated, we should be able to use
the same implementation for a thin-client
Web application, a more rich-client imple-
mentation, or even a traditional GUI.

View. We implement views as server pages,
which can access the application controller
and business objects. Views should contain as
little code as possible, delegating most func-
tionality to the application controller or busi-
ness objects. Only code directly related to
presentation in the current page should be
used in a page. If supported, we prefer to use
a tag mechanism such as JavaServer Pages
(JSP) custom tags to remove code from the
pages altogether.

Figure 2 shows two different view mecha-
nisms. The first uses a server page implemen-
tation, appropriate for a Web browser or
WAP device. The second generates the same
information in an XML format, which could
be sent to an applet, an ActiveX control, or a
GUI application.

Web actions: Control flow
By organizing the scripts, server pages, and

regular objects as we’ve described, we’ve over-
come many of the issues associated with sim-
ple Web development. We have minimized
code duplication and reliance on the proto-
col’s details or naming conventions by using a
single input script. We have also achieved
good information hiding by maintaining the
data in objects rather than as flat session data.
We have confined our use of server pages to
the view layer, maximizing the amount of code
we can manage and debug using standard pro-
gramming tools and methods. By keeping each
layer separate, we can test each in isolation.
Overall, our application remains well-factored
and easy to maintain and extend.

To see how this works in more detail,
let’s examine the flow of control from a sin-
gle Web request in an online banking appli-

cation. First, we see how the input con-
troller accepts input, finds and sets up for
the action, executes the appropriate applica-
tion controller code, and then delegates to
the view. Figure 3 shows sample code for
this, and here we examine each of the steps.

Find the controller
The input controller must determine

which application controller is responsible
for the current request. Active application
controllers are stored in the session. We as-
sume that we can determine the controller
using a lookup based on the request’s path.
For example, in our banking application,
we might have an account maintenance ap-
plication controller, which is used for any
pages related to account maintenance.

Accept input
Once we have determined the application

controller, we must extract the appropriate
information from the request and transfer
that information to the application con-
troller. Most notably, we need to find any
input parameters. These can be encoded as
part of the URL, as query parameters listed
after the URL, or as form data. Regardless
of the transmission mechanism, this input
consists entirely of strings and must be con-
verted into the appropriate types and vali-
dated. The input controller extracts the in-
formation and, in cooperation with the
application controller, performs basic syn-
tactic validation and informs the applica-
tion controller of the values.

For example, we might have a user re-
quest to transfer funds such as

https://objectbank.com/

InputController/transfer?from=

123&to=321&amt=$50.00

The string /transfer identifies the action to

5 6 I E E E  S O F T W A R E M a r c h / A p r i l  2 0 0 2

Figure 3. Java code
for an input 
controller servlet.

public void service( HttpServletRequest req,

HttpServletResponse res )

throws ServletException, IOException {

ApplicationController controller = this.appControllerFor(req);

this.setValuesIn(controller, request);

String actionID = req.getPathInfo();

Action action = controller.actionFor(actionID);

appController.performAction(action);

View view = this.viewFor(req);

view.forwardPage(controller.nextPage());

}



the input controller. The remainder of the
string holds query parameters for the two ac-
count numbers and the amount to transfer. 

On the other hand, a request to update ac-
count holder data might be submitted from
an HTML form and would carry many pieces
of data as form data, invisible in the URL:

https://objectbank.com/

InputController/updateAccountData

Find the action
The application controller can keep track

of a set of acceptable sequences of operations
and the previous steps the user has taken. On
the basis of this and the information submit-
ted in the request, we can determine which ac-
tion to take and whether this action is legal in
the current context. This logic is particularly
important because Web users can use the
Back button to throw off the sequence of op-
erations in a way that is not possible in a GUI.

In the simplest version, we might define
actions for login, logout, transfer between ac-
counts, account update, and bill payments.
Any account activity is allowed once the user
has logged in, but actions transferring money
have a confirmation page. We only let the
user confirm a request if the immediately
previous operation was the request. We also
detect the situation in which the user backs
up and resubmits the same request or hits the
Submit button repeatedly while waiting for
the first submission to be processed.

Perform the action
Once we have determined which action

to take, we must execute it. The exact pro-
cedure for this varies depending on which
implementation we choose. We might
choose to have heavyweight actions, imple-
mented as objects inheriting from a class
Action and implementing a trigger method. 

public void trigger(Controller 

controller){

BankController ctrlr=

BankController)controller;

Account acct=ctrlr.readAccount

(context.getAccountNo()); 

ctrlr.setCurrentAccount(account);

ctrlr.setNextPage(“account

Summary”);

}

This lets us separate the different actions, but
it does not give us as much encapsulation in
the application controller as we might like.
Rather than the standard OO usage of mul-
tiple methods on a single object, we have
many action objects that manipulate the con-
troller’s state, providing little encapsulation. 

As an alternative, we can represent the
action simply as an indicator of the applica-
tion controller method to be performed.
This is particularly easy in a system such as
Smalltalk, with simple reflection techniques.
Given a Smalltalk servlet and JSP frame-
work,7 we can simply set the action

action := #deposit.

and then execute

controller perform: action.

In this case, the action is simply the action
method’s name, and we invoke it with the
standard perform: method. No action class
or trigger operation is necessary. 

Which of these choices is preferred depends
on the situation. Keeping code and data to-
gether in the application controller provides
better encapsulation but raises potential team
issues of having many people collaborating on
a single object. In a GUI presentation, it might
also be desirable to use the full Command pat-
tern to better support Undo functionality.

Forward the request
We use forwarding to divide responsibil-

ity among different components. Once the
action is complete, we determine the URL of
the next page to be displayed and forward
the request. In a simple system, the actions
can directly determine the next page. In a
more complex system, the application con-
troller might coordinate this using internal
state management such as a state machine.

Variations
We have described one particular imple-

mentation of this type of framework. Many
others are possible, and in our uses of this
framework, we have made significant varia-
tions depending on the application’s precise
needs. 

Complex views
We have described views as mapping di-

M a r c h / A p r i l  2 0 0 2 I E E E  S O F T W A R E 5 7

Keeping code
and data

together in the
application
controller

provides better
encapsulation

but raises
potential team

issues of having
many people

collaborating on
a single object.



rectly to server pages, but in more complex
applications, this can become more sophis-
ticated. First, we can assemble larger views
from multiple smaller views. So, we might
have a main page with subsections in frames
or with subsections determined by including
the results of other Web requests. We could
model this structure either by explicitly is-
suing new requests or by using an internal
mechanism to forward the request to an-
other partial page.

We might also want to interpose another
layer of objects at the view level. For exam-
ple, if handling multiple possible forms of
output, we might need an explicit View ob-
ject that handles the output details on a par-
ticular device. For example, we might store a
key that lets the View identify the appropri-
ate output. This might delegate to different
server pages for HTML and WAP presenta-
tion and to a screen identifier for a GUI. Us-
ing a View object helps isolate the presenta-

tion of an action’s results from the action it-
self. It is also an appropriate place to imple-
ment functionality such as internationaliza-
tion of the result.

Action context
We have described an architecture with

very lightweight actions in which state is as-
sociated directly with the application con-
troller. In some situations, it might be useful
to dissociate some of this state from the
controller. In particular, if we do not have a
simple mapping from the URL to the appli-
cation controller, we might need to extract
more of the request state to determine the
correct controller. We can do this by intro-
ducing an ActionContext object, which
stores the request state in a standardized
form. In this case, we would create the con-
text, use it to find the appropriate con-
troller, and then apply actions to the combi-
nation of controller and context.

5 8 I E E E  S O F T W A R E M a r c h / A p r i l  2 0 0 2

PURPOSE The IEEE Computer Society is the world’s
largest association of computing professionals, and is the

leading provider of technical information in the field.

MEMBERSHIP Members receive the monthly
magazine COMPUTER, discounts, and opportunities

to serve (all activities are led by volunteer mem-

bers). Membership is open to all IEEE members,

affiliate society members, and others interested in

the computer field.

B O A R D  O F  G O V E R N O R S
Term Expiring 2002: Mark Grant, Gene F. Hoff-
nagle, Karl Reed, Kathleen M. Swigger, Ronald
Waxman, Michael R. Williams, Akihiko Yamada 

Term Expiring 2003: Fiorenza C. Albert-
Howard, Manfred Broy, Alan Clements, Richard
A. Kemmerer, Susan A. Mengel, James W. Moore,
Christina M. Schober

Term Expiring 2004: Jean M. Bacon, Ricardo
Baeza-Yates, Deborah M. Cooper, George V. Cy-
benko, Wolfgang K. Giloi, Haruhisha Ichikawa, 
Thomas W. Williams
Next Board Meeting: 10 May 02, Portland OR

I E E E  O F F I C E R S
President: RAYMOND D. FINDLAY

President-Elect: MICHAEL S. ADLER

Past President: JOEL B. SYNDER

Executive Director: DANIEL J. SENESE

Secretary: HUGO M. FERNANDEZ VERSTAGEN

Treasurer: DALE C. CASTON

VP, Educational Activities: LYLE D. FEISEL

VP, Publications Activities: JAMES M. TIEN

VP, Regional Activities: W. CLEON ANDERSON

VP, Standards Association: BEN C. JOHNSON

VP, Technical Activities: MICHAEL R. LIGHTNER

President, IEEE-USA: LeEARL A. BRYANT

EXECUTIVE COMMITTEE
President: WILLIS K. KING*
University of Houston
Dept. of Comp. Science
501 PGH
Houston, TX 77204-3010
Phone: +1 713 743 3349 Fax: +1 713 743 3335
w.king@computer.org

President-Elect: STEPHEN L. DIAMOND*
Past President: BENJAMIN W. WAH*
VP, Educational Activities: CARL K. CHANG *
VP, Conferences and Tutorials: GERALD L. ENGEL*
VP, Chapters Activities: JAMES H. CROSS†

VP, Publications: RANGACHAR KASTURI†

VP, Standards Activities: LOWELL G. JOHNSON
(2ND VP)*

VP, Technical Activities: DEBORAH K. SCHER-
RER(1ST VP)*

Secretary: DEBORAH M. COOPER*
Treasurer: WOLFGANG K. GILOI*
2001–2002 IEEE Division VIII Director:
THOMAS W. WILLIAMS

2002–2003 IEEE Division V Director:
GUYLAINE M. POLLOCK†

Executive Director: DAVID W. HENNAGE†

*voting member of the Board of Governors

COMPUTER SOCIETY WEB SITE
The IEEE Computer Society’s Web site, at 
http://computer.org, offers information and samples
from the society’s publications and conferences, as
well as a broad range of information about technical
committees, standards, student activities, and more.

COMPUTER SOCIETY O F F I C E S
Headquarters Office

1730 Massachusetts Ave. NW 
Washington, DC 20036-1992
Phone: +1 202 371 0101 • Fax: +1 202 728 9614
E-mail: hq.ofc@computer.org

Publications Office
10662 Los Vaqueros Cir., PO Box 3014
Los Alamitos, CA 90720-1314
Phone:+1 714 821 8380
E-mail: help@computer.org
Membership and Publication Orders:
Phone: +1 800 272 6657 Fax: +1 714 821 4641
E-mail: help@computer.org

European Office
13, Ave. de L’Aquilon
B-1200 Brussels, Belgium
Phone: +32 2 770 21 98 • Fax: +32 2 770 85 05
E-mail: euro.ofc@computer.org

Asia/Pacific Office
Watanabe Building
1-4-2 Minami-Aoyama, Minato-ku,
Tokyo 107-0062, Japan
Phone: +81 3 3408 3118 • Fax: +81 3 3408 3553
E-mail: tokyo.ofc@computer.org

E X E C U T I V E  S T A F F
Executive Director: DAVID W. HENNAGE
Publisher: ANGELA BURGESS
Assistant Publisher: DICK PRICE
Director, Volunteer Services: ANNE MARIE KELLY
Chief Financial Officer: VIOLET S. DOAN
Director, Information Technology & Services:
ROBERT CARE
Manager, Research & Planning: JOHN C. KEATON

11-FEB-2002



Business logic and components
We have not devoted a great deal of dis-

cussion to the business logic layer, because
we consider it to be a normal OO program.
However, it can be quite complex and in-
volve any number of other technologies.
One that frequently comes up is the rela-
tionship between this layer and compo-
nents, particularly Enterprise JavaBeans.
Some frameworks (when using Java) use en-
tity EJBs as a standards-based solution for
the business object layer.

We do not generally consider this to be a
good solution. Although the business logic
layer might have good reasons for accessing
components—most notably session beans en-
capsulating access to transactional or legacy
systems—using entity beans to represent the
business logic seems ill-advised. Entity beans
offer some automation potential for issues
such as transactions, security, and persistence,
but they impose serious limits on our design
and have significant performance constraints,
even relative to normal Java performance. In
particular, the absence of inheritance is a bur-
den, and other frameworks exist to deal with
transactions, security, and performance in the
context of normal business objects. Finally, it
is harder to test EJBs in isolation because they
require a container to run; this imposes in-
creased overhead on development. Using ses-
sion beans to wrap business objects or forgo-
ing EJBs altogether are both reasonable
alternatives.

U sing our framework, developers pri-marily focus on writing applicationcode rather than dealing with
servlets, requests, or session variables. We
have used this in a variety of different Web
applications, using both Smalltalk and Java.
In this, we have achieved better software
quality because good OO principles were
not dictated but followed naturally from the
structure of the framework. Paradoxically, it
also allowed developers inexperienced with
OO techniques to produce high-quality code
without knowing the details of the frame-
work too well. These frameworks could be
extended in several different areas, including
customization to particular domains, con-
version into a full-fledged framework rather
than a set of patterns, and more detailed
adaptation to particular technologies. 

Acknowledgments
The authors thank Michael Ellis and Martin Fowler

for their contributions to the ideas expressed here.

References
1. M. Ellis and N. Dai, “Best Practices for Developing

Web Applications Using Java Servlets,” OOPSLA 2000
tutorial; www.smalltakchronicles.net/papers/Practices.
pdf.

2. N. Dai and A. Knight, “Objects versus the Web,” OOPSLA
2001 tutorial; www.smalltalkchronicles.net/papers/
objectsXweb10152001.pdf.

3. E. Gamma et al., Design Patterns, Addison-Wesley,
Reading, Mass., 1994.

4. G.E. Krasner and S.T. Pope, “A Description of the
Model-View-Controller User Interface Paradigm in the
Smalltalk-80 System,” J. Object-Oriented Program-
ming, vol. 1, no. 3, Aug. 1988, pp. 26–49. 

5. M. Potel, MVP: Model-Viewer-Presenter, tech. report,
IBM, 1996; www-106.ibm.com/developerworks/
library/mvp.html.

6. A. Bower and B. MacGlashan, “Model-View-Presenter
Framework,” 2001, www.object-arts.com/Education-
Centre/Overviews/ModelViewPresenter.htm.

7. VisualWave Application Developer’s Guide, tech. re-
port, Cincom Systems, 2001.

For more information on this or any other computing topic, please visit our
Digital Library at http://computer.org/publications/dlib.

M a r c h / A p r i l  2 0 0 2 I E E E  S O F T W A R E 5 9

About the Authors

Alan Knight is a senior software developer
at Cincom Systems, where he works on Smalltalk
Web tools. He was also recently the technical
lead on a high-performance Smalltalk Web appli-
cation server supporting full implementations of
ASP, JSP, and servlets. His research interests in-
clude Web development, object-relational map-
ping, and team programming systems. He re-
ceived his BS and MS in computer science from

Carleton University in Ottawa, Canada. He is a member of the ACM. He
coauthored Mastering ENVY/Developer (Cambridge Univ. Press, 2001). 
Contact him at 594 Blanchard Cr., Ottawa, Ontario, Canada K1V 7B8;
knight@acm.org.

Naci Dai is an independent mentor and an
educator. He teaches object technology, Java, de-
sign patterns, and distributed computing. He
leads and mentors Web development projects. He
has a background in applied engineering and
computational physics. He has received his PhD
in mechanical engineering from Carleton Univer-
sity. He is a member of the ACM. Contact him at
Acarkent C75, 81610 Beykoz, Istanbul, Turkey;
nacidai@acm.org.



6 0 I E E E  S O F T W A R E M a r c h / A p r i l  2 0 0 2 0 7 4 0 - 7 4 5 9 / 0 2 / $ 1 7 . 0 0  ©  2 0 0 2  I E E E

it, the market changes, or the money runs
out. Despite developers’ hard work, the re-
sulting code is often precisely the opposite of
what is desired: unstable and fragile, pro-
foundly slowing development. Furthermore,
adapting to new requirements is difficult be-
cause existing functionality must be manu-
ally retested whenever a change is made. We
know that testing is the answer, but is there
a way to adhere to doing it while still devel-
oping at a fast pace?

We have found that a radical approach cen-
tered on testing proves highly effective in
achieving rapid development. We have spent
the last two years using the Extreme Program-
ming methodology to develop e-commerce
software at Evant, a Web-based ASP company.
Focusing on testing and testability results in a
code base that can be built on quickly and that
is malleable to the extent that it can easily be
changed to accommodate new customer re-

quirements and the latest Internet technologies.
We describe how testing helped and how we
established it as the development centerpiece.

Testing first
First, let us briefly describe what we mean by

“test-first” programming.  To test-first means
to write a unit test for a new piece of function-
ality before the new code is written.  For exam-
ple, if one were writing a “shoot-’em-up” com-
puter game, one might write a test asserting that
the player initially has three lives before the
code is implemented to make that the case.

Using unit testing to drive development
rather than relying solely on a quality assur-
ance team profoundly changes several aspects
of software engineering. Designs often differ
from what they might have otherwise been,
technology choices are influenced, and the way
in which the application code is integrated with
third-party frameworks is significantly altered.

focus
Going Faster: Testing The
Web Application

Edward Hieatt and Robert Mee, Evant

This article
documents test-first
design and the
creation of testable
code for Web
applications. The
authors explain how
testing has been
critical to building
Evant’s application 
at speed while
maintaining a high
degree of quality.

T
esting is a fundamental aspect of software engineering, but it is a
practice that too often falls by the wayside in today’s fast-paced
Web application development culture. Often, valuable software
engineering principles are discarded, simply because they are per-

ceived as being too time-consuming and lacking a significant payoff, and
testing is a common casualty. Testing is often last in developers’ minds when
pressured to deliver something, anything, before the competition jumps on

engineering Internet software



Though it might seem a tautology, prac-
ticing test-first programming leads to a per-
vasive quality of testability throughout the
code base. By having tests in place, the over-
all code design is positively affected. More
tests, and more features, can easily be added
to designs and beneficial characteristics arise
as a natural consequence of its testability.

One such beneficial characteristic is re-
usable code. Code that is developed for a sin-
gle purpose usually services a single client
(meaning client code); a test provides a second
client for the interface. Thus we force reuse by
testing. This secondary interface exercise, and
its resultant refactoring, tends to flush out de-
sign flaws that might exist in single-use code.
For instance, a class B written to support class
A might make inappropriate assumptions.
Writing unit tests specifically for B, independ-
ent of its relationship with A, removes these as-
sumptions, which leaves classes A and B more
loosely coupled and therefore more reusable.

Testable code tends to have a cleaner inter-
face, and classes tend to be defined at a more
appropriate granularity than they might other-
wise be. Writing tests simply makes it more ap-
parent when a single class’s scope is too ambi-
tious or too reliant on the workings of another.

Testing the servlet
Unit testing has beneficial effects on re-

moving dependence on a particular technol-
ogy. Often software teams are directed to
minimize their reliance on a single tool, tech-
nology, or vendor in an effort to maintain
long-term flexibility. This is easier said than
done. Take the ubiquitous Java servlet as an
example. The instructions are simple: over-
ride the “service()” method and have it do
what you want. Several outcomes are possible
as the complexity of request processing grows
or the types of different requests increase. The
“service()” method might become long or
break up into several methods in the class,
there might be many utility methods added to
the servlet subclass, or a whole hierarchy of
different subclasses might arise in an attempt
to specialize and still reuse code. The problem
is that servlets are extremely awkward to test,
primarily because they are used as a compo-
nent in a specific environment—the servlet
container. Creating instances of servlets and
testing individual methods is quite difficult.

The test-first approach leads to a different
implementation. When it comes to servlets, it

is quite apparent that the easiest way to test
the code that handles requests is to remove it
from the servlet entirely, and put it into its
own class. In fact, not only is that the easiest
thing to do with respect to the tests, but the
resulting design itself is then more flexible
and quite independent of the servlet technol-
ogy. Our code in “service()” is three lines
long. It has just one responsibility: create an-
other object that will do the work and then
hand control to it. In our case, we call this
object a dispatcher. Its job is to decide what
kind of request is being submitted and create
yet another object (an instance of a com-
mand pattern) to handle the request. The dis-
patcher and each command object have their
own unit tests, in which creating the needed
fixture is trivial. Now we not only have a
testable design, we have one with a more ap-
propriate division of labor and one that is
more technology-independent.

Aggressive refactoring: Swapping 
technologies

With the support of extensive unit tests it
is possible to radically change code without
breaking existing functionality.  This gives the
developer the ability to make major changes,
even in integral frameworks, very quickly.
For example, it allowed us to refactor our
persistence code just weeks before we went
live with our first client. 

We used Enterprise JavaBeans, specifically
Entity beans, to manage our database persist-
ence. We wanted to remove the entire EJB
layer because of poor performance and diffi-
culties in testing the internals of the Beans,
among other reasons. Without tests, we would
not have attempted to remove the Beans, but
given that our unit test suite included extensive
coverage for testing persistence, and that we
had experience in performing this kind of ma-
jor change, none of us was too concerned. In
fact, we would be getting a performance im-
provement, more testable code, and the re-
moval of an expensive technology.

We executed the refactoring in less than a
week, and performance did increase markedly.
We saved money by removing the EJB tech-
nology and ended up with more testable code
because we had removed a part of our system
that was essentially a black box.

Bugs
Testing has radically changed the way we

M a r c h / A p r i l  2 0 0 2 I E E E  S O F T W A R E 6 1

Though it might
seem a

tautology,
practicing 
test-first

programming
leads to a
pervasive
quality of
testability

throughout the
code base.



deal with controlling bugs. We think of bugs
differently from the traditional view—we
use our bug count to help pinpoint areas
that need more testing. That is, we consider
bugs as feedback about how we are doing
with our testing.

When a bug is found, we write a test that
“catches” the bug—a test in the area where the
bug was found that asserts what should be the
case and fails. We then change the code until
the test runs successfully, thus fixing the bug. In
this way, we fill in holes in our suite of tests.

We do not stop there, though. We look for
the case where several bugs in a related area en-
tered our bug tracking system during a short
time period. This information tells us that
something more seriously wrong exists with
our tests—perhaps the testing framework in
that area needs some work. For example, per-
haps three bugs are found one day all in the
area of logging out of the application. Rather
than trying to “pinpoint fix” each bug—that
is, assign each bug to a different developer,
thinking of each as an independent problem—
we assign the group of bugs to a single pair of
programmers. This pair looks at existing tests
for that area, which clearly did not do their job
well. The existing tests might not be extensive
enough, in which case the programmers would
add more tests. Or they might refactor the test-
ing framework in that area to allow better,
more accurate support for the kinds of tests
that we can then proceed to write.

The result is a suite of tests that constantly
grows and changes to cover as much of the
application as possible. The tests are run
whenever code is changed, catching prob-
lems before the changes are even deemed
worthy of being added to the main code base.
Armed with such a weapon, as little time as
possible is spent maintaining existing func-
tionality. In short, development occurs faster.

Tests as documentation
Our methodology does not encourage

writing documentation for code or even
writing comments in the code. Instead, we
rely on tests to document the system. This
might seem strange, but because tests are
written first, they completely define what
the code should do. (In fact, the definition
of the code being in working order is that
the tests all run.) We write our tests with
readability in mind. The best way to learn
what the code is supposed to do is to read

the tests. New developers, or developers
who are inexperienced in a particular area,
can get up to speed far quicker this way
than by trying to wade through requirement
and design documents. It is more effective to
learn by reading a short, simple test than to
try and decipher code that might contain
problems or is very complex.

This method of learning through reading
tests is especially important in our company as
the development team grows. As new devel-
opers come on, they are paired with existing
developers to gain knowledge, but a large part
of their learning comes from reading tests.
New team members come up to speed quickly
because of well-factored, readable tests.

Acceptance tests and their extensions
In addition to writing unit tests, our

process has a mechanism called acceptance
tests that allow product managers to ex-
press tests at the scenario level. 

We began by writing a testing tool that ex-
pressed operations in the system and the ex-
pected results as XML. Product managers,
with the assistance of programmers and qual-
ity assurance team members, wrote extensive
acceptance tests that, along with our suite of
unit tests, are run at every integration.

The acceptance tests provided a clear
value, but it was soon apparent that the
supporting frameworks for the XML had
several other benefits. As we neared deploy-
ment for our first customer, the need arose
to integrate with other e-commerce systems
and with the client’s legacy mainframes. We
employed an industry-standard EAI (Enter-
prise Application Integration) tool config-
ured to generate XML as an adaptor to
transform data from these external systems
to our own. The XML test framework re-
quired little modification to act as the inter-
face to this integration tool. We even used
the framework assertion capabilities to ver-
ify incoming transactions.

As our need for acceptance tests grew,
writing them more quickly and making them
easily maintainable became important. XML,
though workable, is awkward for a human to
write, especially for nontechnical staff. To
solve this, we implemented a domain-specific
language, called Evant Script Programming
(ESP), which is more compact and readable
than XML. Once ESP was available, we rec-
ognized uses beyond acceptance tests and

The existing
tests might not
be extensive
enough, in

which case the
programmers

would add 
more tests.

6 2 I E E E  S O F T W A R E M a r c h / A p r i l  2 0 0 2



data loading, including an easy way to con-
figure system parameters, and that we could
use a suite of ESP scripts to create a tailored
demo environment.  

Testing challenges and solutions
In the Web environment, the client and

the server are very different beasts. The
user’s machine runs a Web browser, which
understands HTML and JavaScript (we will
not discuss the extra complications of ap-
plets, plug-ins, or other client-side technolo-
gies here). The only ability this browser has
is to send information through a map of
strings over a stateless protocol to a Web
server, which deals with the submission by
passing it off to an engine running applica-
tion code (for example a servlet container
running Java code or a Perl program). We
would typically like to test the client and the
application server part of this set up, but it
would be difficult to come up with a testing
framework that could do both at once—to
test JavaScript and DHTML on the client
and Java on the application server. However,
splitting tests into two distinct parts is not a
good idea because we need to test how the
two sides interact.

This is the classic Web application testing
problem. We recommend the following remedy.

First, test those parts of the server-side
code that are not directly concerned with
being part of a Web application, without in-
volving the Web peculiarities. For example,
if one were testing the manipulation of a
tree object, one would test manipulating the
object directly, ignoring the fact that the vi-
sual representation of the tree is collapsed
and expanded by the user through the UI.
Test the business logic with such low-granu-
larity unit tests. That is, of course, the goal
no matter what the UI, but it becomes par-
ticularly important to be strict about it
when working with a Web application.

Second, test those parts of the client-side
code that have no server interaction.  This is
typically code that contains little or no busi-
ness logic.  Examples of such code, typically
written in JavaScript, might include func-
tions to pop up error messages, perform
simple field validations, and so on.  The im-
mediate problem we faced in writing tests
for this sort of code was that no testing
framework existed for JavaScript inside the
browser. In fact, we had already heard from

other developers that testing JavaScript was
too hard, and that JavaScript itself should
therefore be avoided entirely.  Undeterred,
we wrote a testing framework for JavaScript
called JsUnit, now a member of the XUnit
family of unit testing frameworks (www.
jsunit.net). JavaScript is notoriously diffi-
cult to write and debug, but using JsUnit
has greatly helped our rapid development of
client-side code.

Third, write functional tests of a low grain
in the server-side language (for example, Java
or C++) that simulate the request/response
Web environment. We wrote a framework for
such tests that extends our usual unit-testing
framework to allow the test author to think of
himself or herself as being in the Web browser
position, putting keys and values into a Map
object (which corresponds to <form> in an
HTML document).  The framework simulates
a submission of the Map to the server by in-
voking the server code, passing in the Map as
a parameter.  This invocation is coded in such
a way as to follow as closely as possible the
path a request would take were it submitted
through the true UI.  (We take advantage here
of the refactoring of our servlet discussed
above: We cannot “invoke” the servlet from a
test, but we can easily talk to the objects that
the servlet spins off.) The server’s response can
then be inspected and verified by the test (see
the next section for more on this). Because the
test code explicitly invokes the server-side
code, the test can inspect the state of the code,
rather than just the HTML output resulting
from the submission.  It is for this reason es-
pecially that this method of testing is more
useful than a framework such as HTTPUnit,
which views the situation purely from the
client’s viewpoint.

M a r c h / A p r i l  2 0 0 2 I E E E  S O F T W A R E 6 3

Books
Extreme Programming Explained, Kent Beck, especially Chapter 18

(”The introduction to Extreme Programming“); a good place to start.
Extreme Programming Installed, Ron Jeffries et al., Chapters 13, 14,

29, and 34; contains a lot of information addressing common problems
and questions about unit testing.

Web sites
Testing frameworks:
www.xprogramming.com/software.htm 
www.junit.org

Extreme Programming sites:
www.xprogramming.com
www.extremeprogramming.org

For More Information



Using this framework, you can write walk-
through tests that simulate the user moving
through the UI. For example, we have meth-
ods available in the test framework with
names such as “clickSaveButton().” A test
case might call that method and then perform
assertions about the domain object that
should have been persisted as a result of the
call. Thus, we have the ability to perform an
action that the user might execute and then to
test specifics about the domain. Walkthrough
tests are particularly useful when writing tests
to reproduce bugs that manual testing has ex-
posed. In fact, there has not yet been a case
when we could not write a walkthrough test
that catches a bug found by a member of the
quality assurance team.

Testing server output
How to test output from the server is an-

other classic problem because of the nature of
Web applications. How do you test HTML?
Certainly we don’t want tests that assert that
the output is some long string of HTML. The
test would be almost unreadable. Imagine
tracking down a mistake in such a test, or
changing it when new functionality is re-
quired. Testing the Web page look is not usu-
ally the goal; testing the data in the output is.
Any slight change to the output—for example,
a cosmetic change such as making a word ap-
pear in red—should not break an output test.
We came to understand these issues as we pro-
gressed through several technologies used to
generate our HTML output. In fact, this pro-
gression and the choice on which we finally
settled were influenced in large part by what
was most easily testable.

At the outset we worked with JavaServer
Pages (JSPs). We found that although we got
up and running quickly using them, testing
them was hard. They are another Web tech-
nology that runs in a container environment
that is difficult to reproduce in a test. In addi-
tion, it was challenging for our UI designers
to work with JSPs because of the amount of
Java code interspersed among the HTML.
Soon we were spinning off testable objects to
which the JSPs delegated that did the HTML
generation. When we arrived at the point
where the JSPs were literally one line long, we
dispensed with them altogether and produced
our HTML purely using Java. With that
arrangement we could easily test the state of
objects that produced the HTML—for exam-

ple, that the object knew to make a button
disabled. We could also test the process of
producing pieces of HTML—for example,
the production of an HTML <table> ele-
ment. However, we were still not testing the
output. The same question dogged us: How
do you test the data in the HTML without
getting confused with the cosmetics? Further-
more, our designers were even less interested
in delving into Java code to change text color
than they had been with JSPs.

A further change of technology solved
both these problems. We generated our UI
by outputting XML—pure data—from our
domain objects. Then we transformed the
XML into HTML for the client using Ex-
tensible Stylesheet Language Transforma-
tion (XSLT) pages, written and maintained
by our developers and UI designers.

XML is easily testable. In our walkthrough
tests, we can ask for the current XML out-
put at any time and then make assertions
about its structure and contents using the
standard XPath syntax. The XML consists
strictly of data from the domain; there is no
knowledge even that it will be used in the UI.
Our XSLT contains only information about
how the UI should look and how to use the
data in the XML.

Thus we achieved an extremely clean sep-
aration of testable business data from HTML
generation that helped us develop the UI
much faster than the other methods we tried.
We could test that part of the output that
contained data without getting it confused
with the HTML.  Conversely, our UI design-
ers were happier because their work was not
confused by Java among the HTML. Again
we found a way to separate the untestable
Web technology, in this case HTML, from
the data.

A first test
A question we are often asked is how we

started promoting testing and testability at the
beginning of our development. The first step
in making our Web application testable was to
establish a culture of testing among the devel-
opers. Following the methodology of writing
tests first, our first task as a team was to write
the first test. Most team members were new to
the concept of test-first programming, so it
was not obvious to them how to go about this. 

It seems almost comical thinking back on
it, but it was appropriate at the time: We pro-

6 4 I E E E  S O F T W A R E M a r c h / A p r i l  2 0 0 2

How to test
output from 

the server is
another classic

problem
because of the
nature of Web
applications.



grammed in a group for the first couple of
days. Having the entire team of eight huddled
around a monitor, achieving consensus on
every line of code, let us iron out right from the
start what a typical test looked like, how we
write a little bit of test code and then some
code to make the test run, when to go back
and add another test case, and so on. We also
quickly settled other issues that often plague
development teams: we agreed on coding stan-
dards, established a way to perform integra-
tions, and settled on a standard workstation
configuration.

T hus testing has been an integral part ofour development process since the firstday, and remains so now.   Of course,
our testing frameworks and practices change
and grow daily, and we are by no means fin-
ished with their evolution. For example, we
feel that we have a long way to go in finding
more appropriate ways to expose testing to

our business users. We would also like to ex-
tend our walkthrough test framework to simu-
late even more closely what happens in real sit-
uations. Our company supports us when we
ask for time to develop such abilities because
they have seen the benefits of testing. They
agree that our focus on tests and testability has
helped us go faster than any project they have
worked on in the past.

For more information on this or any other computing topic, please visit our
Digital Library at http://computer.org/publications/dlib.

M a r c h / A p r i l  2 0 0 2 I E E E  S O F T W A R E 6 5

About the Authors

Edward Hieatt is a lead developer at Evant, a software vendor in San Francisco that
observes a strict practice of XP. He became involved with Extreme Programming in 1999. His
research interests include the rapid development and testing of Web-enabled software. He is
also the author of JsUnit (www.jsunit.net), a unit-testing framework for JavaScript, which is
one of the XUnit testing frameworks. Contact him at edward@edwardh.com.

Robert Mee is a software programming and process consultant in a variety of indus-
tries, helping companies apply the practices of Extreme Programming. He regularly gives
lectures on XP for corporations, their investors, and others. During 2000 and 2001, he was
director of engineering at Evant, where he helped introduce XP. His research interests include
human and computer languages, and the application of domain-specific languages to the au-
tomated testing of software. He has a BA in oriental languages from the University of Cali-
fornia at Berkeley. Contact him at robmee@
hotmail.com.

What do software engineering professionals need to know, ac-
cording to those who hire and manage them? They must be able to
produce secure and high-quality systems in a timely, predictable,
and cost-effective manner.

This special issue will focus on the methods and techniques for
enhancing software education programs worldwide—academic,
re-education, alternative—to give graduates the knowledge and
skills they need for an industrial software career.

Potential topics include
• balancing theory, technology, and practice
• experience reports with professional education
• software processes in the curriculum
• teaching software engineering practices (project management,

requirements, design, construction, …)
• quality and security practices
• team building
• software engineering in beginning courses
• computer science education vs. SE education
• undergraduate vs. graduate SE education
• nontraditional education (distance education, asynchronous

learning, laboratory teaching, …)
• innovative SE courses or curricula
• training for the workplace

For more information about the focus, contact the guest 
editors; for author guidelines and submission details, contact the
magazine assistant at software@computer.org or go to http://
computer.org/software/author.htm.

Submissions are due at software@computer.org on or before 
1 April 2002. If you would like advance editorial comment on a
proposed topic, send the guest editors an extended abstract by 
1 February; they will return comments by 15 February.

Manuscripts must not exceed 5,400 words including figures and 
tables, which count for 200 words each. Submissions in excess of these
limits may be rejected without refereeing. The articles we deem within
the theme's scope will be peer-reviewed and are subject to editing for
magazine style, clarity, organization, and space. We reserve the right to
edit the title of all submissions. Be sure to include the name of the
theme for which you are submitting an article.

Guest Editors:
Watts S. Humphrey, Software Engineering Institute
Carnegie Mellon University, watts@sei.cmu.edu
Thomas B. Hilburn, Dept. of Computing and Mathematics
Embry-Riddle Aeronautical University hilburn@db.erau.edu



6 6 I E E E  S O F T W A R E M a r c h / A p r i l  2 0 0 2 0 7 4 0 - 7 4 5 9 / 0 2 / $ 1 7 . 0 0  ©  2 0 0 2  I E E E

In April 2000, the Internet bubble burst and
the business world was brought back to re-
ality, discovering that business fundamen-
tals hadn’t changed. However, it isn’t so
clear whether the fundamental rules of soft-
ware development have changed. 

I worked as a software process consultant
to two Internet startups during the boom—
let’s call them XYZ.net and PQR.com to pro-
tect their identities, as both are still in busi-
ness. XYZ.net focused on fundamental
infrastructure services for Internet users and,
when I joined them, was about to start work
on its third generation of software to support
its upcoming initial public offering. PQR.com
was an online electronic retailer in the process
of re-architecting its system to support its pro-
jected growth and to position itself for a fu-
ture IPO. The two common factors between
the companies were my involvement and the
fact that both organizations were facing phe-

nomenal business growth and thus needed to
re-architect and redevelop their systems. They
were also hiring new developers to help them
deliver this new software: XYZ.net had
grown from three to 30 developers in less
than a year, and PQR.com had grown from
the original two founders to 25 developers in
a similar time frame. Both companies felt they
needed a more mature software process that
included software modeling. Both organiza-
tions had very young staff—the average age
was in the mid-20s—and both had team-
oriented cultures.

Furthermore, both companies wanted to
define, and then train their staff in, a version
of the Rational Unified Process1 tailored to
meet their specific situations, with extensions
from other software processes.2,3 Both or-
ganizations wanted to be able to claim to po-
tential investors that they were using an ac-
credited software process yet didn’t want

focus
Lessons in Agility from
Internet-Based
Development

Scott W. Ambler, Ronin International

A
t the peak of the Internet revolution hype, we were inundated by
claims that the fundamental rules of business had changed. To
support this new business paradigm, new companies sprang up
virtually overnight, often delivering very good software very

quickly. I wondered whether the rules of software development had also
changed. Were we witnessing a paradigm shift in the way we develop soft-
ware? Was there something to the concept of “development in Internet time”?The author

describes two
Internet startup
companies that
adopted effective
and efficient
modeling and
documentation
practices, one taking
a communal, team-
based approach and
the other a chief-
architect approach.
They found a 
“sweet spot” where
modeling efforts can
provide significant
benefit without
incurring the costs
of onerous
documentation.

engineering Internet software



unnecessary bureaucracy to slow them down.
Both organizations needed to improve their
modeling practices; senior management felt
that although they had bright, talented devel-
opers, they weren’t as effective as they could
be. The developers at the two organizations
had mixed feelings about this, with attitudes
ranging from “it’s about time” to “modeling
is a waste of time; all I need is source code.”  

The tailored version of the RUP needed to
include an enterprise architecture modeling
process that would define their technical and
application infrastructures. This enterprise ar-
chitecture effort was beyond the RUP’s scope,
which focused on the software process for a
single project instead of a portfolio of proj-
ects. Furthermore, both organizations needed
effective approaches to their modeling and
documentation on individual projects. Al-
though the RUP clearly included sophisti-
cated modeling processes, each organization
felt the RUP approach to modeling was too
cumbersome for Internet-based development
and wanted to find a better way to work.

Communal architecture
To architect the new generation of

XYZ.net’s software, the company intro-
duced an architecture team comprising the
technical leads from each development
team, the vice president of systems, the chief
technology officer, the quality assurance
and testing manager, and several senior de-
velopers. The first week the team met for
several afternoons to identify where they
were going, what their fundamental require-
ments were, and then to propose an initial
architecture to support these needs. When
they weren’t modeling the new architecture,
everyone worked on their normal day-to-
day jobs. XYZ.net was a round-the-clock
operation: no one could simply stop evolv-
ing the existing software. This had the side
benefit of giving people time to reflect on
the proposed architecture as well as to dis-
cuss ideas with co-workers. 

During the architecture modeling sessions,
they drew diagrams on whiteboards. At first,
the team tried to use a CASE tool connected
to a projector but found this to be insuffi-
cient—the person operating the tool could
not keep up with the conversation, and the
tool wasn’t flexible enough to meet our needs
(they needed more than the Unified Model-
ing Language4 diagrams that it supported).

The whiteboard also required little training
to work with and allowed them to swiftly
switch back and forth between modelers. As
the team modeled, people would provide in-
put and insights, question assumptions, share
past experiences, and even talk about poten-
tial implementation strategies. The simpler
tool enabled them to work together as a
team. Whenever they needed permanent doc-
umentation, they would simply draw the
models on paper and then scan them to pro-
duce electronic copies. Occasionally the team
would capture the model in a CASE tool, but
the scanner approach was more common be-
cause few developers had access to the CASE
tool due to platform incompatibilities.

The team drew UML diagrams but found
they needed a much wider range of tech-
niques—some of the software was written in
procedural languages, so they needed nonob-
ject diagrams such as structure charts. They
also drew dataflow diagrams to depict work-
flow issues and free-form diagrams to depict
the system’s technical architecture. 

Once the initial architecture model was in
place, they continued to meet almost weekly
(only when they needed to). The meetings
were typically one or two hours long and al-
ways included an informal agenda that iden-
tified one or more issues to be addressed. The
owner of each issue was responsible for ex-
plaining it, what he or she saw as the likely
solution, and potential repercussions of this
approach. Often, someone would come to an
architecture session with the issue mostly
solved; the group would simply review it and
provide appropriate feedback. Presentations
were typically done in a “chalk talk” manner,
in which the owner would describe an issue
by drawing on the whiteboard. This worked
well because many people would already be
familiar with the issue, having been involved
in earlier discussions with the owner outside
of the architecture modeling sessions. 

Introducing a chief architect
Taking a different approach, PQR.com

decided to hire an experienced senior devel-
oper as a chief architect, an “old guy” in his
early 30s who had built scalable systems be-
fore. He spent his initial week with the
company learning about the business and
the current status of the systems, mostly
through discussions with senior IT and op-
erations staff. He then ran small modeling

M a r c h / A p r i l  2 0 0 2 I E E E  S O F T W A R E 6 7

Both
organizations

needed
effective

approaches 
to their

modeling and
documentation
on individual

projects.



sessions to identify architectural require-
ments as well as potential architectural so-
lutions. His approach was to hold one-hour
“chalk talk” sessions involving two or three
other people, followed by several hours of
modeling privately with a diagramming tool
and a word processor to explore and docu-
ment what he had learned. He then posted
his models as HTML pages on the internal
network where everyone, developers and
business people alike, could view them and
potentially provide feedback. It was quite
common for people to schedule an im-
promptu meeting with him to discuss his
work or to simply send him an email.

Documenting the architecture
Both organizations chose to document

their architecture to make it available to all
their software developers. Their approaches
had several interesting similarities:

� Official documentation was HTML based.
Both companies chose HTML (and JPEG
or GIF pictures) to support the wide range
of operating systems the developers used,
thus making the documentation accessible
to everyone. Furthermore, the developers
were intimately familiar with HTML and
had existing tools to view and edit the
documents. 

� They created only overview documenta-
tion. To remain effective, both compa-
nies documented only overview infor-
mation pertaining to the architecture,
recording the critical information that
developers needed to know to under-
stand how it worked. The documenta-
tion was primarily focused around an
architecture diagram showing major
software services deployed within the
technical environment, with overview
descriptions for each software service or
system and appropriate links to the doc-
umentation pages maintained by indi-
vidual project teams. A printout of the
architectural documentation would
have been no more than 15 pages.

� The information architecture wasn’t
special. The databases and file storage
systems appeared on the system archi-
tecture diagram along with applications
and software services. The data team
was treated the same as any other proj-
ect team: all the teams provided func-

tionality that other teams needed, and
each team in effect was the customer of
several other teams. This required
everyone to work together to resolve
any configuration and release manage-
ment issues. This was unlike many or-
ganizations where the data team is con-
sidered a corporate infrastructure team
that supports application development
teams. At both companies, every team
was an infrastructure team.

� Modeling and documentation were sepa-
rate efforts. Both companies realized that
the act of modeling and the act of writing
documentation are two distinct efforts.
In the past, most developers did not have
significant experience modeling, so they
equated it to using sophisticated CASE
tools that generated detailed documenta-
tion. After experiencing several modeling
sessions using simple tools such as white-
boards, after which someone wrote up
the appropriate summary documentation
(if any), they learned they could achieve
the benefits of modeling without incur-
ring the costs of writing and maintaining
cumbersome documentation.

The main difference in their approaches
was how architectural documentation was
updated over time. PQR.com had a desig-
nated owner of the documentation, the chief
architect, whereas XYZ.net took a more
communal approach and allowed anyone to
update the documentation. At PQR.com,
people were required to first discuss their sug-
gestions with the chief architect, who then
acted accordingly. At XYZ.net, people would
first talk with the perceived owner of a part of
the documentation, typically the person ini-
tially assigned to that aspect of the architec-
ture, and then update it accordingly. No one
seemed to abuse this privilege by changing the
architectural documentation to further their
own political goals. One reason was that the
XYZ.net developers were a closely knit team.
Second, because all HTML pages were man-
aged under their version control system, there
was a record of who changed each page.

Comparing the two modeling
approaches

Let’s compare the two approaches to en-
terprise architectural modeling with regard
to several critical factors:

The main
difference 

in their
approaches

was how
architectural

documentation
was updated
over time.

6 8 I E E E  S O F T W A R E M a r c h / A p r i l  2 0 0 2



� Calendar time. XYZ.net’s architecture
team required less calendar time to de-
velop their initial architecture and then
to evolve it over time, because several
people worked on it in parallel.

� Acceptance. The communal approach
taken by XYZ.net clearly resulted in
quicker acceptance by the developers
for two reasons. First, representatives of
each development team participated in
the architectural team, and they effec-
tively “sold” the architecture to their
teammates, often simply by asking for
their help when they were working on
portions of the architecture. Second,
everyone could update the architecture
documentation as they required, thus
promoting a sense of ownership.

� Cost. PQR.com’s approach required less
overall effort because the chief architect
performed most of the architectural
modeling, enabling his co-workers to
focus on other tasks. 

� Control. The chief architect at PQR.com
provided a single source of control, al-
beit one that became a bottleneck at
times. The team approach provided sev-
eral sets of eyes working on any given
aspect of the architecture, although its
shared ownership of documentation
provided opportunities for mischief 
(although to my knowledge none ever
occurred).

� Correctness. Both approaches resulted
in scalable architectures that met the
needs of their respective organizations.

� Process compatibility. Both approaches
worked well within a RUP environment,
although the chief-architect approach
seemed to be better attuned to the 
RUP than the communal-architecture
approach.

Project-specific modeling
The project process was similar at both

organizations: systems were developed in a
highly iterative manner and released incre-
mentally in short cycles ranging from six to
12 weeks, depending on the project. The
marketing department wrote the initial re-
quirements documents, typically high-level
needs statements. The IT department heads
and the project leads did a reality check on
these documents to rework unrealistic ideas
before presenting them to the teams. Mod-

eling sessions occurred as each project team
required, with some team members meeting
with appropriate project stakeholders to
work through an issue in detail. These mod-
eling efforts were similar to those performed
by the XYZ.net architecture team: people
met and worked on the models as a team
around a whiteboard. Often, they would de-
velop several models at once—perhaps sev-
eral use cases along with some screen
sketches, some sequence diagrams, and a
class diagram. They did not have “use case
modeling sessions” or “class modeling ses-
sions”—just modeling sessions. 

At XYZ.net, the development teams had
one to eight members, each of whom fo-
cused on one part of the overall infrastruc-
ture. At any given time, each team would be
at a different place in the project life cycle.
Sometimes two or more teams would need
to schedule their releases simultaneously to
enable various subsystem dependencies. The
individual teams knew of those dependen-
cies because they worked closely with the
developers from other teams, often walking
across the floor or up the stairs to sit down
and talk with them.

These modeling sessions differed from
traditional ones. First, they iteratively did
the minimal amount required that let them
start coding, with most modeling sessions
lasting from 10 minutes to an hour. Second,
each modeling session’s goal was to explore
one or more issues to determine a strategy
for moving forward; the goal wasn’t simply
to produce an artifact. Their goal was to
model, not to produce models.

Developers worked with technical writers
to produce the release notes, installation
procedures, and user documentation. The
release notes were the formal documentation
for the subsystems, written as HTML pages
and posted online so that all internal devel-
opers could see them. The release notes typ-
ically summarized the newly implemented
requirements, the design (diagrams were
typically scans of their primary sketches and
were posted as JPEG files), and a listing of
the files making up the subsystem. Luckily,
XYZ.net developers were already producing
release notes following this process in their
premodeling days, so the only change they
needed to implement was to add the design
diagrams that presented an overview of their
work. At first, some developers included de-

M a r c h / A p r i l  2 0 0 2 I E E E  S O F T W A R E 6 9

The project
process was

similar at both
organizations:
systems were
developed in a
highly iterative

manner.



tailed diagrams in their release notes, but
they quickly found that they provided little
value. When the developers wanted detailed
information, they went straight to the source
code, which they knew was up to date.

Developers also worked closely with test
engineers. The developers needed help unit-
testing their software, and the test engineers
needed help learning the software to de-
velop acceptance tests for it. Before soft-
ware could be put into production, the
quality assurance department first had to
test it to ensure that it fulfilled its require-
ments, it was adequately documented, and
it integrated with the other subsystems.

The development process worked similarly
at PQR.com, with teams of three to six peo-
ple. Developers from the various teams
worked closely together, helping one another
where necessary. PQR.com developers took a
more formalized approach to documentation,
writing requirements documents, design doc-
uments, and technical release notes. The re-
quirements documents were based on the ini-
tial marketing requirements documents and
the requirements models produced from
them. The requirements document was writ-
ten in parallel with development of the design
document and source code and was typically
finalized during the last week of an iteration:
the teams worked iteratively, not serially, so
they could react to new and changing require-
ments as needed. A requirements document
included a use case diagram and detailed use
cases for the release as well as related business
rules, technical (nonfunctional) requirements,
and constraints as required. Design docu-
ments included screen specifications when ap-
plicable (typically just a JPEG picture of an
HTML page) and applicable overview dia-
grams; also, the developers preferred source
code to detailed design diagrams.

An interesting similarity between the two
companies was their approach to data: their
data efforts were treated just like develop-
ment projects. However, the data teams
worked differently than the other project
teams, investing more time in detailed mod-
eling. At both companies, the data groups
used a CASE tool to create physical data
models of their relational database schemas.
The tool generated and reverse-engineered
data definition language (DDL) code, includ-
ing database triggers, thereby making the ad-
ditional detailed modeling effort worthwhile.

Dissension within the ranks
Several individuals at XYZ.net didn’t ap-

preciate the need for an increased effort in
modeling and documentation. This was due
to one or more factors: they perceived them-
selves as hard-core programmers, they pre-
ferred to work alone or with others of simi-
lar mindsets, or their people skills needed
improvement. Moreover, they typically didn’t
have good modeling and documentation
skills, nor did most of their co-workers. For
all but one person, a combination of peer
pressure and the ability to see “which way
the winds were blowing” motivated them to
go along with adopting a tailored version of
the RUP, albeit with a little grumbling. The
lone holdout required minor disciplinary ac-
tion: the head of development had a few per-
sonal chats with him, he wasn’t allowed to
move on from his current project until he had
written the documentation and the quality
assurance department had accepted it, and
his promotion to team lead was put on hold
for a few months. 

There were few problems at PQR.com,
likely because its staff was more aware of
modeling’s benefits to begin with. People
had various levels of modeling skills, so they
received training, but nobody was actively
antagonistic toward increased levels of
modeling as long as they received training.

Comparison with traditional project
development

How was this different from traditional
development practices, particularly of typical
RUP instantiations?  From the 50,000-foot
level, it wasn’t different at all. Both compa-
nies identified requirements and then archi-
tected, designed, coded, tested, and deployed
their software. However, at ground level,
there were some interesting differences.

First, development started with the enter-
prise architecture model. To fit in with and ex-
ploit other systems, all project teams started
with the existing enterprise architecture
model because it showed how their efforts fit
into the overall whole. When the project
teams delved into the details, they sometimes
discovered the need to update the architec-
ture, which they did following their organiza-
tion’s chosen practices, described earlier.

Development was also highly iterative.
Project teams did a little modeling, coding,
and testing and then repeated as necessary.

An interesting
similarity

between the
two companies

was their
approach to

data.

7 0 I E E E  S O F T W A R E M a r c h / A p r i l  2 0 0 2



They did not take the serial approach of
completely developing and accepting the re-
quirements model, then completely develop-
ing and accepting an analysis model, and so
on. This let them react swiftly to changes in
their highly competitive markets and rap-
idly deliver software to their end users.

Also, modeling was streamlined. They
modeled just enough and then got on with
implementation, avoiding analysis paralysis
while at the same time showing that their
models actually worked (or didn’t) by writ-
ing code based on them.

Moreover, both organizations used sim-
ple modeling tools. With the exception of
the two data teams, the projects preferred
simple sketches on whiteboards over dia-
grams created using sophisticated CASE
tools. This sped up the modeling process, al-
though it might have slowed development
when developers could not automatically
generate code from their models—a feature
of many modern tools—or reverse-engineer
existing code to create diagrams to help
them understand it.

Finally, documentation in the two proj-
ects was minimal. Both companies im-
proved the quality of their system documen-
tation and managed to find the “sweet
spot” where it was just barely enough to
meet their needs and no more. Many tradi-
tional projects produce far more documen-
tation than they need, reducing their ability
to implement new features and sometimes
even causing the project to fail completely.5

Setting the foundation for agile
modeling

My experiences on these two projects
provided significant insights for what was to
become the Agile Modeling methodology.6

AM is a practice-based methodology for ef-
fective modeling and documentation of soft-
ware-based systems. Principles and values
guide the AM practices, intended for daily
application by software professionals. AM is
not hard and fast, nor is it a prescriptive
process—in other words, it does not define
detailed procedures for how to create a given
type of model. Instead, it provides advice for
how to be effective as a modeler.

This is different from modeling methodolo-
gies such as Iconix7 or Catalysis8 that suggest
a specific collection of modeling artifacts and
describe how to create those artifacts. Where

Iconix promotes applying use case models, ro-
bustness diagrams, UML sequence diagrams,
and UML class diagrams, AM suggests that
you apply multiple models on your projects
and the right artifacts for the situation. AM is
complementary to most traditional modeling
methodologies—it does not replace Iconix and
Catalysis but instead provides complementary
advice for improving your effectiveness with
them. AM’s scope is simply modeling and doc-
umentation. It is meant to be used in conjunc-
tion with other agile software processes9,10

such as Extreme Programming,11 Scrum,12

and the Dynamic System Development
Method,13 as well as “near-agile” instantia-
tions of the Unified Process.

AM adopts the values of XP—simplicity,
feedback, communication, and courage—and
extends it with humility. These values are
used to derive a collection of principles (see
Table 1) that in turn drive AM’s practices (see
Table 2). Nothing about it is truly new. It is a
reformulation of concepts that many software
professionals have been following for years.

M a r c h / A p r i l  2 0 0 2 I E E E  S O F T W A R E 7 1

Table 1
The principles of agile modeling

Core principles Supplementary principles

Assume simplicity Content is more important than 
representation

Embrace change Everyone can learn from everyone else
Incremental change Know your models
Model with a purpose Know your tools 
Multiple models Local adaptation
Quality work Maximize stakeholder investment 
Rapid feedback Open and honest communication 
Software is your primary goal Work with people’s instincts 
Enabling the next effort is your secondary goal
Travel light

Table 2
Agile modeling practices

Core practices Supplementary practices

Active stakeholder participation Update only when it hurts 
Apply the right artifacts Use the simplest tools 
Collective ownership Apply modeling standards 
Consider testability Apply patterns gently 
Create several models in parallel Discard temporary models 
Create simple content Formalize contract models 
Depict models simply Model to communicate 
Display models publicly Model to understand 
Iterate to another artifact Reuse existing resources 
Model in small increments
Model with others
Prove it with code



Lessons learned
XYZ.net and PQR.com were successful

because they followed a collection of com-
monsense approaches. Let’s explore the les-
sons learned at XYZ.net and PQR.com and
see how they relate to AM.

� People matter. Everyone worked to-
gether to make the effort succeed—it
was as simple as that. We operated un-
der a philosophy along the lines of the
Agile Alliance’s “value of individuals
and interactions over processes and
tools”9 and Alan M. Davis’s 131st prin-
ciple, “People are the key to success.”14

� You don’t need nearly as many docu-
ments as you think. Both companies
built mission-critical software very
quickly, software that is still in opera-
tion several years later and that has been
modified significantly since—and we
did so without creating mounds of doc-
umentation. Fred Brooks provides simi-
lar advice with what he calls the docu-
mentary hypothesis: “Amid a wash of
paper, a small number of documents be-
come the critical pivots around which
every project’s management evolves.”15

In short, the teams followed the principle
“Travel light,” producing just enough
documentation and updating it only
when they needed to, much along the
lines of AM’s practice “Update only
when it hurts.” 

� Communication is critical. The amount
of required documentation was reduced
because both organizations had a high-
communication environment. Develop-
ers worked closely together, face-to-face
with their co-workers and customers
(either marketing and sales staff or the
chief technology officer). Models were
left on the walls so that everyone could
see them, as AM suggests with its prac-
tice “Display models publicly.” By do-
ing this, developers could refer to the
models when explaining their strategies
to others and often received valuable
feedback, often simply in the form of
questions, from people who had taken
the time to look over the models. We
had rediscovered Davis’s 136th princi-
ple, “Communication skills are essen-
tial” and his 8th principle “Communi-
cate with customers/users.”14

� Modeling tools aren’t nearly as useful as
you think. Although we honestly tried
to use the leading UML modeling tool at
the time, as well as an open source soft-
ware offering, the tools simply didn’t of-
fer a lot of value. We weren’t interested
in creating a lot of documentation, and
the tools didn’t generate code for our
deployment environment. The only use-
ful CASE tools were the data-modeling
tools from which diagrams were printed
and DDL code generated. We did, how-
ever, take advantage of some simple
tools, whiteboards and flip charts, and
were very effective doing so. This expe-
rience reflects the findings and tech-
niques of the user-centered design com-
munity in the late 1980s16 and is
captured in AM’s practice “Use the sim-
plest tools.”

� You need a wide variety of modeling
techniques in your intellectual toolkit.
The techniques of the UML, although
useful, were not sufficient—both com-
panies needed to perform process, user
interface, and data modeling. Further-
more, each developer understood some
but not all of the modeling techniques
that we were using, resulting in the
teams stumbling at times because some-
one would try to model something using
an ill-suited technique. These concepts
are captured in AM’s principle “Multi-
ple models” and its practice “Apply the
right artifact(s)”—advice similar to
what everyone’s grandfather has likely
said at some point, “Use the right tool
for the job.” The concept of requiring
multiple modeling techniques is nothing
new; in the 1970s, structured methods
such as those promoted by Chris Gane
and Trish Sarson17 supported this con-
cept. As the teams modeled, they would
often work on two or three diagrams in
parallel, each of which offered a differ-
ent view on what they were discussing,
and would move back and forth be-
tween the diagrams as appropriate, sim-
ilar to AM’s practices “Create several
models in parallel” and “Iterate to an-
other artifact,” respectively.

� Big up-front design isn’t required. Al-
though XYZ.net spent a few afternoons
formulating their initial enterprise ar-
chitectural strategy, they quickly got

XYZ.net and
PQR.com were

successful
because they

followed a
collection of

commonsense
approaches.

7 2 I E E E  S O F T W A R E M a r c h / A p r i l  2 0 0 2



down to work. Similarly, the chief ar-
chitect at PQR.com also created an ini-
tial high-level model. Then he started to
work with individual teams to imple-
ment portions of it, updating his model
as required. Both companies discovered
that they didn’t need weeks or months
of detailed modeling and documenta-
tion to get it right, that hours were suf-
ficient. Instead, they worked along the
lines of AM’s practice “Model in small
increments,” modeling the architecture
a little and then either exploring strate-
gies by proving them with code or sim-
ply starting work on the actual software
itself. In his essay “Passing the Word,”15

Brooks provides similar advice—that
architects must be able to show an im-
plementation of their ideas, that they
must be able to prove that their archi-
tecture works in practice.

� Reuse the wheel, don’t reinvent it. At
XYZ.net, we took advantage of open
source software whenever we could,
building mission-critical systems quickly
and effectively as a result. Through open
source we had rediscovered Davis’s 84th
principle: “You can reuse without a big
investment.” We reused more than just
open source software; we also adopted
industry standards wherever possible,
existing architectural strategies wher-
ever possible, and applied design pat-
terns when applicable. We discovered
there was a wealth of material available
to us when we chose to reuse it, sup-
porting AM’s practice “Reuse existing
resources.”

Agile approaches to software develop-ment worked for these two Internet-based companies. Both XYZ.net and
PQR.com applied existing, proven principles
of software development in new ways to de-
liver complex, mission-critical software in a
highly iterative and incremental manner. The
lessons they learned apply beyond Internet de-
velopment: most organizations can benefit
from a shared architecture across projects as
well as more effective and efficient modeling
and documentation practices. Many of the
“modern day” principles and practices of

leading-edge development processes are the
same (or slightly modified) principles and
practices of yesteryear. Although the funda-
mentals are still the fundamentals, the way in
which the fundamentals are applied has
changed—agilely, not prescriptively.

For more information on this or any other computing topic, please visit our
Digital Library at http://computer.org/publications/dlib.

References
1. P. Kruchten, The Rational Unified Process: An Intro-

duction, 2nd ed., Addison Wesley Longman, Reading,
Mass., 2000.

2. S.W. Ambler, “Enterprise Unified Process,” 2001,
www.ronin-intl.com/publications/unifiedProcess.htm.

3. S.W. Ambler, Process Patterns—Building Large-Scale
Systems Using Object Technology, Cambridge Univ.
Press, New York, 1998.

4. Object Management Group, “The Unified Modeling
Language (UML), Version 1.4,” 2001, www.omg.org/
technology/documents/formal/uml.htm.

5. J.A. Highsmith III, Adaptive Software Development: 
A Collaborative Approach to Managing Complex 
Systems, Dorset House, New York, 2000.

6. S.W. Ambler, Agile Modeling, John Wiley & Sons, New
York, 2002.

7. D. Rosenberg and K. Scott, Use Case Driven Object
Modeling with UML: A Practical Approach, Addison
Wesley Longman, Reading, Mass., 1999.

8. D.F. D’Souza and A.C. Wills, Objects, Components,
and Frameworks with UML: The Catalysis Approach,
Addison Wesley Longman, Reading, Mass., 1999.

9. K. Beck et al., “Manifesto for Agile Software Develop-
ment,” 2001, www.agilealliance.org.

10. A. Cockburn, Agile Software Development, Addison
Wesley Longman, Reading, Mass., 2002.

11. K. Beck, Extreme Programming Explained—Embrace
Change, Addison Wesley Longman, Reading, Mass.,
2000.

12. M. Beedle and K. Schwaber, Agile Software Develop-
ment with SCRUM, Prentice Hall, Upper Saddle River,
N.J., 2001.

13. J. Stapleton, DSDM, Dynamic Systems Development
Method: The Method in Practice, Addison-Wesley,
Reading, Mass., 1997.

14. A.M. Davis, 201 Principles of Software Development,
McGraw Hill, New York, 1995.

15. F.P. Brooks, The Mythical Man Month: Essays on Soft-
ware Engineering, Anniversary Edition, 2nd ed., Addi-
son-Wesley, Reading, Mass., 1995.

16. J. Greenbaum and M. Kyng, eds., Design at Work: Co-
operative Design of Computer Systems, Lawrence Erl-
baum Assoc., Hillsdale, N.J., 1991.

17. C. Gane and T. Sarson, Structured Systems Analysis:
Tools and Techniques, Prentice Hall, Upper Saddle
River, N.J., 1979.

M a r c h / A p r i l  2 0 0 2 I E E E  S O F T W A R E 7 3

About the Author

Scott W. Ambler is president of and a principal consultant for Ronin International, Inc.
(www.ronin-intl.com) and author of several software development books, including The Object
Primer 2/e (Cambridge Univ. Press, 2001) and Agile Modeling (John Wiley & Sons, 2002).
His professional interests focus on identifying and exploring techniques to improve software
developers’ effectiveness. He received a masters in information science and a BSc in computer
science, both from the University of Toronto.  He is a member of the IEEE and ACM. Contact
him at scott.ambler@ronin-intl.com.



7 4 I E E E  S O F T W A R E M a r c h / A p r i l  2 0 0 2 0 7 4 0 - 7 4 5 9 / 0 2 / $ 1 7 . 0 0  ©  2 0 0 2  I E E E

few US sites serve foreign customers well,
with 46 percent of orders placed by individ-
uals from outside the US unfilled.4 Clearly,
Web sites that are usable in only one country
cannot exploit the Internet’s global reach.

Software localization is the process by
which computer applications, such as Web
sites, are analyzed and adapted to the re-
quirements of other countries, making the
software more usable for customers in those
countries. Although translating Web site
content is a major aspect of localization, it
also involves changing many other aspects of
the Web application, such as color, graphics,
and structure. Localization can occur after
the software application has been success-
fully used or marketed in one country and its
owner wants to make the necessary changes
for a new market. Although this produces
software that is tailored to that specific mar-
ket, the application is still not ready for easy

adaptation to other markets.5 Increasingly,
localization is a concern from the beginning
of application development, with the busi-
ness strategy pushing to introduce the appli-
cation in multiple markets. In this second
approach, the application is designed from
the beginning to support international con-
ventions, languages, formats, and process-
ing. This approach makes localization easier
and less costly,6 even though it requires a
larger initial cost.5

Differences between groups arise when-
ever individuals who live or work together
come from differing traditions and environ-
ments. When a country contains several dis-
tinct cultures, Web sites might need to ad-
dress and market to the specific needs,
desires, tastes, and consciousness of those
particular cultural or ethnic groups.1

Whereas localization efforts tend to view
each group of users as quite homogeneous,

focus
Software Localization 
for Internet Software: 
Issues and Methods

Rosann Webb Collins, University of South Florida

Web sites must adapt to many local requirements to be usable for a global audience. This
article examines key localization issues and presents an approach for analyzing and Web 

Web sites must 
adapt to many local
requirements to be
usable for a global
audience. This
article examines key
localization issues
and presents an
approach for
analyzing and
documenting
localization projects.
Examples from a
global “telehealth”
company illustrate
the main issues 
and their possible
outcomes or
solutions.

M
ost US corporations have a Web presence, but less than 15 per-
cent offer non-English content.1 This is surprising, considering
that in 2001, 57 percent of online users were non-English
speakers, and by 2003 that percentage is estimated to increase

to 65 percent of the Web’s one billion users worldwide.2 Moreover, studies
have shown that retail customers are as much as three times more likely to
purchase products from Web sites that use their native language.3 However, 

engineering Internet software



it is also important to recognize that cul-
tures are difficult to describe and measure,
and many individuals are multilingual or
culturally heterogeneous. Developers of de-
sign rules for localization should avoid
stereotypical views that do not adequately
represent the richness and variation in cul-
tural meaning in any single group.7 In-
creases in international travel for education,
work, and leisure as well as transnational
media and corporations are likely to create
transnationals—individuals who are com-
fortable in multicultural and multilingual
settings. 

This article identifies the key technical is-
sues in Web site interface localization and de-
scribes an approach for analyzing and docu-
menting a localization project. The article is
based on a review of relevant literature,
meetings with localization industry represen-
tatives, and an ongoing observation of par-
ticipants in a global “telehealth”  company
(GTC). This company provides health care
information, services, and products to health
care professionals and patients in several
countries. I will use examples from this com-
pany to illustrate the issues and their possible
outcomes or solutions.

Technical issues
Software localization is based on re-

search on each country as well as on input
from usability testing in the target country.
Testing global software in-country leverages
developers’ knowledge of the locale, and
such well-defined tasks with little interde-
pendence are easily distributed.8 Many of
the aspects of the country knowledge are
obvious and immutable characteristics, such
as language; other aspects are subtler and
subject to cultural shifts, such as the mean-
ing of colors. Such small changes in mean-
ing make it particularly important to use in-
dividuals in the target country who have
current knowledge rather than individuals
who once lived or worked there, whose
knowledge might be dated. 

A country’s technology infrastructure
presents a particular design challenge, as
that infrastructure will change and might be
controlled by individual governments or
companies. Therefore, the local technology-
driven parts of a design must be revisited
regularly. This means that the reuse possi-
bilities and maintenance demands of the 

localized software solutions differ by type
and country. 

The following discussion is an expanded
version of the guide prepared for GTC’s de-
velopers. Shirley Becker and Florence Mot-
tay provide a more general list of usability
factors for Web sites.9 I have organized the
factors critical to localization into four cat-
egories identified by Becker and Mottay: in-
formation content, page layout, navigation,
and performance.

Information content
In some cases, local differences are easily

identified, fairly stable, and easy to imple-
ment in a Web site—for instance, time and
date formatting, measurements, formats, col-
lating sequences, and numeric formats.

Time and date formatting. Conventions for
the formats of dates and time vary by loca-
tion. For example, if you read 02/03/04, do
you think it indicates 3 February 2004, 2
March 2004, or 4 March 2002?  The inter-
national Gregorian calendar standard is
YYYY-MM-DD, so the internal system rep-
resentation of dates should probably follow
that standard. Local presentation of the
date on an interface might use the local for-
mat or be converted into another calendar,
such as Hijri (Middle Eastern), Japanese,
Korean, or Thai. The international standard
notation for time is hh:mm:ss. To eliminate
time zone difference problems, Universal
Time (formerly Greenwich mean time) is in-
dicated by hh:mm:ssZ. Local times can be
shown as a deviation (+ or -) from Universal
Time. There is a database of current time
zone-related algorithms at www.twinsun.
com/tz/tz-link.htm. 

Measurements. Most of the world uses the
metric system of measurement and ex-
presses temperature in degrees Celsius, but
the US uses the British Imperial and Fahren-
heit systems. Fortunately, the conversion
from metric to British Imperial and temper-
atures from Celsius to Fahrenheit is straight-
forward, and algorithms for making the
conversion are in the public domain.

Formats. Countries differ in the type of pa-
per most commonly used (letter, legal, folio,
or A4) and paper and envelope sizes (which
can be specified in either or both inches and

M a r c h / A p r i l  2 0 0 2 I E E E  S O F T W A R E 7 5

Many of the
aspects of 

the country
knowledge are

obvious and
immutable

characteristics,
such as

language; other
aspects are
subtler and
subject to

cultural shifts.



millimeters). The arrangement of the differ-
ent components of an address also varies
among countries. For example, the address
format in China is

Country
Province City
Address
Last Name First Name Honorific

If a system includes printing of forms or in-
put of names and addresses, the design must
account for these differences. A good source
for these variations in format, as well as
other aspects of localization, are the appen-
dices in Developing International Software
by Nadine Kano.10

Collating sequences. Languages as well as
countries use different alphabet-based sort-
ing sequences. This is particularly true when
languages include diacritics and ligatures as
well as letter combinations that sort in var-
ious sequences. For example, in Swedish,
vowels with diacriticals sort after Z. Also,
list order might not be based on the alpha-
bet; for example, in traditional Chinese, the
number of strokes it takes to draw each
character determines a list’s arrangement. In
Latin America, the double character ch is
treated as a single character and sorts be-
tween the letters c and d. Sometimes the col-
lating sequence depends on what is being
listed, as in Spanish family names.11

Numeric formats. Again, numeric formats
differ among countries, with the major dif-
ferences arising from whether a comma, pe-
riod, or blank space is used to separate
thousands and higher numbers and whether
a decimal or a comma is used to separate
the decimal part of a number. Even words
used to express numbers can vary. For ex-
ample, in the US a “billion” is a 1 followed
by nine zeros, whereas in Latin America and
Europe, it means a 1 followed by 12 zeros.6

Language. Of course, language is a more dif-
ficult issue in localization. Translation soft-
ware partially supports translation of con-
tent from one language to another, but in
most cases human intervention is needed be-
cause direct translation might change the
content’s meaning. An example of this prob-
lem is from the US Milk Processor Board’s

television commercial that uses the tag line
“Got Milk?” If translated directly into
Spanish, the tag line becomes “Are You Lac-
tating?” To market to Latina mothers, the
tag line was changed to “And You, Have
You Given Them Enough Milk Today?”
The images show mothers cooking flan and
other milk-rich Latin foods.12

The translation ratio of words and sen-
tences is not one to one. Because the space
required for a body of text can dramatically
differ between languages—for instance,
when English text is translated to French, its
length increases 15 to 20 percent, but when
it is translated into Hindi, its length might
increase 80 percent—interface design must
account for the differing space requirements.
Individual words used in menus and on tabs
can be especially problematic for design: 
for example, the “Preferences” selection
common on Windows interfaces translates 
to “Bildschirmeinstellungen” in German.
Therefore, menus, boxes, logos, and other
graphics containing embedded words must
be self-sizing, or there will be additional costs
in redesigning those elements for each local-
ized interface.

Some terms, especially technology terms,
might not exist in some languages. The orig-
inal word is often used, but sometimes a
new word is coined based on a phonetic
translation. The commercial translation
services create and maintain dictionaries of
these specialized terms to reduce redundant
translation efforts and to increase the cover-
age of automatic translation software. 

Punctuation marks also vary between lan-
guages. For example, interrogatory sentences
in English end with a question mark,
whereas in Greek they end with a mark that
looks like an English semicolon. Content
must also be examined to see if it is legal in
a particular country. For example, compar-
ative advertising (brand a is better than
brand b) is now legal in the US, but not in
other countries such as Germany. In many
European countries, the collection of per-
sonal data is sometimes subject to local laws
about data privacy.

Content might be culturally loaded, in
that the idea being expressed might only
work in some cultures. For example, meet-
ings in Japan are a public ritual of consen-
sus: all brainstorming has been done and all
conflict has been worked out in discussions

Some terms,
especially
technology

terms, might
not exist 
in some

languages. 
The original

word is often
used, but

sometimes 
a new word 

is coined based
on a phonetic
translation.

7 6 I E E E  S O F T W A R E M a r c h / A p r i l  2 0 0 2



after work, often conducted in a social set-
ting. So, translating the word for meeting
might be easy, but the meaning will defi-
nitely be different.13 In the travel industry,
specific travel-related differences between
nationalities require companies such as
Lastminute to tailor descriptions of accom-
modations—for example, the meaning of a
three-star hotel.14

Asia presents some of the greatest chal-
lenges to localization. One and a third bil-
lion people speak CJK (Chinese, Japanese
and Korean), and in Japan alone more than
17 million people are online. These ideo-
graphic languages (as opposed to character-
oriented languages such as English) convey
meaning through symbols, and the number
of symbols is large. Text might include a
mix of writing systems: Japanese and Ko-
rean commonly use some Chinese charac-
ters as well as limited Roman (English)
characters.15

There are two main technology issues for
language. First, the words seen while using
a computer come from an assortment of sys-
tems, although users tend to see the interac-
tion holistically. The main system might
have all text translated, but users sometimes
see nontranslated messages (especially error
messages) from the operating system, the
browser, or a companion system (such as a
database management system). Developers
might not be in total control of a user’s
workstation, but they must realize that
users might be confused about the source of
a problem when calling for support. Second,
many languages require a two-byte charac-
ter set because of the large number of char-
acters in the language. For example, many
Asian languages use more than 50,000 char-
acters. Unicode (www.unicode.org), based
on ISO 10646, enables the input, display,
and output of all known human languages
without data corruption. However, not all
systems currently support Unicode. Devel-
opers can expect to deal with a difficult and
changing environment for character sets for
some time.

I recommend supporting only a few lan-
guages initially, especially for Internet inter-
faces. For example, seven languages (Eng-
lish, Japanese, German, French, Spanish,
Portuguese, and Swedish) account for 90
percent of online users (as of 1999). An-
other approach might be to minimize some

of your language and other issues by creat-
ing unique content for each target market or
by making the content a subset of the con-
tent for the original site. GTC’s approach
was phased introduction of the portal, with
phase one including Costa Rica, United
Arab Emirates, and Australia. This reduced
the size of the problem while still providing
experience in multiple languages, including
one with a non-Roman alphabet.

Currency. Multinational transactions often
involve displaying the appropriate local cur-
rency, indicating the applicable local tax, in-
cluding any tariff or other duties that might
be incurred, calculating and timing the cur-

M a r c h / A p r i l  2 0 0 2 I E E E  S O F T W A R E 7 7

The initial and ongoing costs of localization are significant. eTranslate, a
Web-based service bureau, charges US$50 per page for European lan-
guages and US$60 per page for Asian languages for translation of content
only. Microsoft currently spends more than $300 million per year on local-
ization of their products into 46 languages. Berlitz Translation Services esti-
mates that fully localizing a software product for one additional language
costs at least $50,000 to $100,000, whereas Microsoft estimates its costs
are $300,000 or more per product.1 Forrester Research estimates that lo-
calizing a new Web site adds 10 to 60 percent of the original development
cost. Internet sites typically change twice a year,2 so localized Web sites
also have significant, ongoing maintenance costs.

Detailed accounting of localization costs is critical for planning future ef-
forts, especially if the company sells localized products or localization serv-
ices. In such cases, the company needs that information to accurately price
development services and product maintenance. High development and
maintenance costs, and their impact on product pricing, argue for a system-
atic approach to localization that includes cost measurement.

GTC initially developed its portal without adequate cost accounting, and,
although it considered localization needs from the beginning, a technical
review found the initial portal to be nonscalable and not easily adaptable.
When the decision was made to redesign the portal for the new technology
environment (which was concurrent with a change in the company’s techni-
cal leadership), no data existed on the initial portal development with which
to estimate the time needed and cost of a redesign. This lack of information
reinforced the need to collect such information, and the new technical man-
agement instituted procedures to collect metrics on all development activi-
ties. In some ways, this initial mistake was serendipitous, because the im-
pact was relatively low (there was little basis for planning the portal re-
design effort) and because managers and developers are now committed to
measuring all the costs associated with the main software product (now in
development for several medical practices). This product will have to be lo-
calized extensively to the needs of the individual practices as well as lo-
cales, and the company must have solid cost information to price the prod-
uct appropriately.

References
1. H.H. Preston and U. Flohr, “Global from Day One,” Byte, vol. 22, no. 3, Mar. 1997, pp.

97–104.
2. J.E. Yunker, “A Hands-on Guide to Asia and the Internet,” Intercom, May 2000, pp.

14–19.

Localization Costs and Impact on Pricing



rency conversion, and determining how and
when the accounting for that transaction
will occur. Although the International Ac-
counting Standards Committee of the Inter-
national Federation of Accountants is cur-
rently trying to develop international
standards for accounting, the most recent
proposal was rejected by the US Financial
Accounting Standards Board, which identi-
fied more than 200 differences between the
international and US standards. Until an in-
ternational standard is created and ac-
cepted, each company must determine how
accounting is to be done when the transac-
tion involves parties in different countries.

The problem’s scope depends on the nature
of the Web site. In the case of GTC, where or-
ders are taken via the Web site, the in-country
offices use their accounting firms (employed
for all their accounting needs) as sources of
knowledge about financial transactions.

Images and sounds. Countries differ in their
interpretation and use of images and
sounds. In some countries, the flag is promi-
nently displayed on products, but Saudi
Arabia’s flag includes holy symbols associ-
ated with the Koran, and thus anything
with a Saudi flag on it cannot be thrown
away. Symbols like a US-style mailbox or a
shopping cart are not meaningful every-
where. The “thumbs-up” and “OK” hand
signals used in Western countries are re-
garded as sexual gestures in others. Many
cultures do not accept the use of images of
women or women and men together. Sound
as a system feature (for example, a beep in-
dicating an error) might be appropriate in
the US, but it might embarrass Asian users
in front of colleagues working nearby.

Pictures and graphics should be as cul-
turally neutral as possible, or localized. At
GTC, when patient–doctor interactions
needed to be shown, pictures were changed
to eliminate male–female combinations and
states of undress that are not acceptable in
the United Arab Emirates. All sounds were
eliminated as unnecessary.

Page layout
In Latin-based languages, documents are

laid out left to right, with the next line ap-
pearing below the previous one. Arabic and
Hebrew read from right to left. East Asian
languages follow a different layout—for in-

stance, Japanese text typically is displayed
top-to-bottom, with lines right to left. De-
velopment of international style sheets
might provide part of the solution to this is-
sue, but designers should be aware that the
habitual pattern of scanning a page will dif-
fer between languages.

Cultures also differ dramatically on the
meaning of any one color. In Western coun-
tries, red is alarm, white is pure, black is
somber, and green indicates growth or
money. In Asia, red is joy, white expresses
mourning, and black is a lucky color. In
Arabic countries, green is a holy color. 

In the West, the most important object
on a Web page should appear in the upper
left corner, whereas in East Asia, Arabic
countries, and Israel, that object should be
positioned in the upper right. If you use
color as part of a design, you should exam-
ine each color’s meaning when converting to
other cultures and adapt as necessary.

Conformity is the key for the easy infor-
mation content issues: identify and conform
to the local standards, and use technology to
provide translation functionality where ap-
propriate (for example, measurement con-
version) and translation of data to a global,
common standard (such as displaying an ad-
dress in the local format but having one data
definition for address information). In other
cases, content issues regarding language,
currency, images, and sounds are much more
difficult to identify and address.

Navigation
Navigation between Web site pages can

be structured as a network or a hierarchy.
Cultures that are high in uncertainty avoid-
ance16 might not be comfortable with links
that do not follow a strict order, especially
when a user can get “lost” at the site. A re-
cent comparison of German and US Web
sites found that German sites have a much
more hierarchical navigational structure
with few links to related sites, whereas US
sites exploit the use of hyperlinks exten-
sively within the site and to related sites.17

Although Becker and Mottay point out
such inconsistencies as a design failure,9

they might, in fact, represent important lo-
calized structures.

One suggestion is to explore with in-
country experts whether there is a national
preference for navigational structure. If not,

Cultures 
also differ

dramatically 
on the meaning

of any one
color.

7 8 I E E E  S O F T W A R E M a r c h / A p r i l  2 0 0 2



then consistency in navigation makes good
design sense.

Performance
Great disparity exists between countries’

telecommunication infrastructures, so the
capability of the infrastructure and cost of
access in each country must be examined.
Fortunately, the importance of information
technology to socioeconomic development is
currently driving policy changes in countries
with poor infrastructure. Most users world-
wide do not have fast connections, and some
pay high per-minute access charges. In gen-
eral, the price of Internet access is higher in
developing countries; for example, the price
of access in Australia is 1.5 percent of the
country’s gross domestic production per
capita, in Mexico it is 14.8 percent of GDP
per capita, and in Ethiopia it is 76.8 percent
of GDP per capita.18

Although there is general agreement that
bandwidth capability will improve and
costs will lessen, in the near term it is im-
portant to minimize slow-to-load features
such as graphics in Internet and intranet ap-
plications. Many sites have a text-only ver-
sion available as an option.

Analysis and design for localization
The key idea in developing global sys-

tems is that “the culture, language, and
user-dependent parts of the software appli-
cation are isolated from the software core
during development.”19 GTC uses the fol-
lowing sequence once it has determined and
modeled the user requirements:

1. Isolate the interface and business objects
and their relationships. Interface objects
are granular components that make up
the page, including toolbars, content
blocks, graphic blocks, background de-
sign, links, input fields, sounds, and
cookies. Business objects are the process-
ing (code and utilities) and the data stor-
age and retrieval (data model) needed to
produce the interface objects.

2. Examine the interface objects to deter-
mine if you need to change or delete
them or if you need to add new objects.

3. Design the new interface objects.
4. Conduct usability tests of the new inter-

face objects (described later).
5. Identify what new or changed business

objects you need in order to implement
the interface objects.

6. Design the new business objects.
7. Create a maintenance plan for the new,

localized design, in concert with any ex-
isting designs.

M a r c h / A p r i l  2 0 0 2 I E E E  S O F T W A R E 7 9

Outsourcing some or all localization development and maintenance is
possible. Decisions about whether and what to outsource are based on the
traditional tradeoff issues relevant to any make-or-buy decision. The advan-
tages of in-house localization are that the organization builds expertise, it
can control process and quality, and competitors might not be able to easily
reproduce the localization results. However, as discussed elsewhere in this
article, in-house localization has significant costs, and the expertise required
is not always available. The advantages of outsourcing localization are
speed of delivery and cost, but no in-house expertise is built, the organiza-
tion must depend at least to some extent on the company providing the out-
sourcing service, and competitive advantage cannot be maintained. 

Although translation software and careful preparation of source text min-
imizes translation costs, human intervention is still needed to handle words,
especially technical terms, for which there is no equivalent in some lan-
guages, as well as to deal with differences in meaning from direct transla-
tions and cultural differences.

The services offered by the localization industry are quite varied. Tradi-
tional translation services include human, human-enhanced, and automatic
approaches. Some companies offer translation management systems to
store and maintain translations, and some focus on specific industries. Auto-
matic translation is especially effective when the source text has been written
in an international style, sometimes called controlled language. This style

� Focuses on writing simple, clear copy  (for instance, using a limited
number of nouns to qualify a noun)

� Avoids humor and analogies that don’t translate to other cultures
� Uses formatting rules that facilitate translation (for example, no dashes

or slashes as punctuation marks)

GTC reviewed the range of services localization companies offered and
decided to purchase automatic-translation–enabled email and discussion
room functions for the portal. The main software product and the portal are
localized in-house to leverage the market research GTC did to select coun-
tries in which to operate and to build in-house expertise in this area. In each
country of operation, GTC maintains offices that support system development
and testing. The company’s partners that have “real estate” on the portal are
responsible for their own localization, but GTC acts as an advisor when a
partner has no experience with localization. This consultation has created an
additional revenue stream for GTC, but not all partners use GTC’s assistance
or localize their sites. Although GTC would prefer that their partners localize
their pieces of the portal, it has no control over them; it fears pressing the
point will cause its partners to shift to its competitors.

A major source of information about localization and outsourcing ven-
dors is the Localization Industry Standards Association (www.lisa.org): for
example, in 1999, localization accounted for 32 percent of the US$11 bil-
lion world market for translation services.1

Reference
1. “Going Global,” Washington Times, 24 Apr. 2000, www.lisa.org/press/washington_times.html.

The Localization Industry 



Localization
teams in 
target

countries play 
a major role 

in initial 
design testing. 

Localization teams in target countries
play a major role in initial design testing.
For example, Yahoo!’s localization team in
Asia discovered that its online address book
that sorts names alphabetically should, in
the Asian version, sort names by the number
of keystrokes required to produce them.20

In addition, the local presence prevents the
image that the company is “the big, bad
wolf, the outsider that everyone hopes will
lose.”14 At GTC, the staff of the local com-
pany office did the initial software testing,
with one of the developers from the central
company office present to record issues and
problems. Local IT developers likely will be
brought in to assist in the testing and
changes when the company and number of
clients grow.

Implementation of localized software to
date at GTC has required complete localiza-
tion of the interfaces and some localization
of the business objects. The developers im-
plemented the system’s user interface layer
with Java applets. However, although Java
can support better user interfaces, it tends to
consume more client system resources and
takes longer to load than HTML-based in-
terfaces.21 Therefore, an important part of
GTC’s usability testing was to carefully ex-
amine resource usage and download times.
Although problems have not yet arisen in
this area, the company anticipates having to
redo some interface implementations in
HTML to minimize system resource use and
download times.

T his review of the technical aspectsof localization, the outsourcing re-sources available, and GTC’s local-
ization effort summarizes what will con-
front developers who must localize Web
sites. Because this review is based on one
global telehealth company, additional re-

search is needed to create a complete list of
Web site localization issues and identify best
localization practices.

References

1. Foreign Exchange Translations, “Secrets of Successful
Web Site Globalization,” 2000, www.fxtrans.com/
resources/web_globalization.html.

2. Global Reach, “Global Internet Statistics (by Lan-
guage),” 2001, www.glreach.com/globstats/index/php3.

3. “Going Global,” Washington Times, 24 Apr. 2000,
www.lisa.org/press/washington_times.html.

4. M. Sawhney and S. Mandel, “Go Global,” Business
2.0, 1 May 2000, www.business2.com/articles/mag/
0,1640,13580,FF.html.

5. D. Johnston, “Culture Shock: Taking Your Software
Products International,” AS/400, May–June 1996,
reprint.

6. L.C. Miller, “Transborder Tips and Traps,” Byte, vol.
19, no. 6, June 1994, pp. 93–102.

7. P. Bourges-Waldegg and S.A.R. Scrivener, “Meaning,
the Central Issue in Cross-Cultural HCI Design,” Inter-
acting with Computers, vol. 9, no. 3, Jan. 1998, pp.
287–309.

8. E. Carmel, Global Software Teams, Prentice Hall PTR,
Upper Saddle River, N.J., 1998.

9. S.A. Becker and F.E. Mottay,  “A Global Perspective on
Web Site Usability,” IEEE Software, vol. 18, no. 1,
Jan./Feb. 2001, pp. 54–61.

10. N. Kano, Developing International Software, Microsoft
Press, Redmond, Wash., 1995.

11. D.W. Johnston, “Critical Business Issue: Is Your Strat-
egy a Winner?” Software Strategies, 1 June 1999,
www.softwarestrategies.com.

12. R. Wartzman, “Read Their Lips: When You Translate
‘Got Milk’ for Latinos, What Do You Get?” Wall Street
J., vol. 233, no. 107, 3 June 1999, p. 1.

13. K. Nakakoji, “Crossing the Cultural Boundary,” Byte,
vol. 19, no. 6, June 1994, pp. 107–109.

14. R.M. Spiegler, “Globalization: Easier Said than Done,”
The Industry Standard, 9 Oct. 2000, pp. 136–155;
www.thestandard.com.

15. J.E. Yunker, “A Hands-on Guide to Asia and the Inter-
net,” Intercom, May 2000, pp. 14–19.

16. G. Hofstede, Cultures and Organizations: Software of
the Mind, McGraw-Hill, London, 1991.

17. E. Lehmair, The Commercial Use of the World Wide
Web: A Comparison of German and US Web Sites, un-
published master’s thesis, Univ. Karlsruhe, Karlsruhe,
Germany, 1996; available from the author, lehmair@
wiwi.uni-regensburg.de.

18. Int’l Telecommunication Union, “Challenges to the 
Network: Internet for Development,” Oct. 1999, www.
itu.int/ITU-D/ict/publications/inet/1999/index.html.

19. E.A. Karkaletsis, C.D. Spyropoulous, and G.A. Vouros,
“A Knowledge-Based Methodology for Supporting
Multilingual and User-Tailored Interfaces,” Interacting
with Computers, vol. 9, no. 3, Jan. 1998, pp. 311–333.

20. M. Lerner, “Building Worldwide Web Sites,” 1999, www.
ibm.com/software/developer/library/web-localization.
html.

21. J.Z. Gao et al., “Engineering on the Internet for Global
Software Production,” Computer, vol. 32, no. 5, May
1999, pp. 38–47.

For more information on this or any other computing topic, please visit our
Digital Library at http://computer.org/publications/dlib.

8 0 I E E E  S O F T W A R E M a r c h / A p r i l  2 0 0 2

About the Author

Rosann Webb Collins is an associate professor of information systems and decision
sciences at the University of South Florida. Her research focuses on global information systems
and cognitive issues in systems development and testing. She is the author of Crossing Bound-
aries: The Deployment of Global IT Solutions. She received her PhD in business administration
with a specialization in management information systems from the University of Minnesota
and an MLS in library science from the University of North Carolina at Greensboro. Contact her
at the Dept. of Information Systems and Decision Sciences, College of Business Administration,
Univ. of South Florida, Tampa, FL 33620-7800; rcollins@coba.usf.edu.



The Java 2 Enterprise
Edition enables a
component-based
approach to
developing enterprise
applications. Here,
the authors describe
their experiences
and lessons learned
in using J2EE on a
large, Web-based
custom development
and enterprise
integration project. 

focus

0 7 4 0 - 7 4 5 9 / 0 2 / $ 1 7 . 0 0  ©  2 0 0 2  I E E E M a r c h / A p r i l  2 0 0 2 I E E E  S O F T W A R E 8 1

competing implementations. This in turn lets
application developers build software using
their choice of service implementations, which
frees them to concentrate on application-spe-
cific code. (For pointers to more detailed back-
ground information on the J2EE technology
family, see the “Further Reading” sidebar.)

The project we describe in this article is a
Web-based enterprise software system for a
Fortune 100 client, developed using J2EE tech-
nologies including Enterprise JavaBeans (EJBs)
and JavaServer Pages (JSPs). The system com-
prises approximately 700 kLOC, 5,600 classes,
and 500 JSPs. We have been developing it con-
tinuously for three years, and our client has
been running it live for two. 

We partitioned the system horizontally
into three tiers:

� A presentation layer, which drives the
user interface 

� A session layer, which manages the user
workspace and session business logic

� A domain object layer, which handles
persistence and entity business logic 

The system’s front end is a set of JSPs, each
with a “display bean” that provides data and
defines presentation logic. The display beans
access and update the persisted data by
means of XML data transfer objects that the
session layer loads and manages. The system
also integrates several external services ac-
cessed over the network.

Here, we discuss our experiences and les-
sons learned in dealing with five key devel-
opment topics: J2EE HTML-rendering tech-
nologies, JSP internationalization, external
system integration, sharing information be-
tween tiers using XML data transfer objects,
and maintaining complicated domain object
hierarchies using code generation.

Using J2EE on a Large,
Web-Based Project

Eric Altendorf, Independent Consultant

Moses Hohman and Roman Zabicki, ThoughtWorks

B
uilding large, multitier, Web-based applications requires an array
of software services, from dynamic HTML generation to business
domain object persistence. Rather than implement these services
from scratch, application developers find it more efficient to use

standard, publicly available implementations. Sun’s popular J2EE platform
provides a specification and common set of interfaces for these services 
in the Java programming language. J2EE lets software vendors offer

engineering Internet software



J2EE HTML-rendering technologies

Most Internet applications render the user
interface in HTML. Java systems generally
use servlets, JSPs, or both to dynamically gen-
erate the HTML.

Our solution
We decided to use just JSPs for several rea-

sons. First, there are the oft-quoted benefits of
separate development roles: Readily available
and relatively inexpensive HTML designers
can write static HTML templates (like JSPs),
while more expensive enterprise Java pro-
grammers provide the back end for dynamic
content. However, separation proved to be less
important during actual development. An in-
ternal business application’s graphic design re-
quirements are typically simpler than a public
Web site’s, so we didn’t need a graphic design
team. Also, we encourage our consultants to
maintain skills and responsibilities at all lev-
els—from HTML to architecture—and thus
few specialize in graphic design. 

On the other hand, we planned to build
the pages so that the client could modify the
design once the project was over. So, while the
role separation was not so important during

development, we anticipated that later, during
maintenance, the client would find it easier to
make look and feel and other HTML-only
changes that businesses typically require.

Finally, a template-style front end is easier
to work with. When HTML and dynamic
data are mixed in Java code, seeing the flow
of both the program and the HTML output is
often difficult. It can also be hard to debug
the static HTML or make code changes
without breaking the HTML. 

Experience
After more than two years of JSP usage, our

experiences are mixed. Although JSP technology
permits separation of Java from HTML, it does
not enforce it. Developers can still embed sections
of Java code (called scriptlets) in the JSP or write
display-bean methods that generate HTML and
call them from the JSPs. In fact, prior to custom
tags, many tasks were impossible without resort-
ing to scriptlets or HTML-producing methods.
We are still refactoring some of our older JSPs and
display beans to separate the HTML and Java.

We did gain the definite advantage of
rapid prototyping. Using standard HTML-
producing editors, we can quickly create pro-
totype screens, then simply take the gener-
ated HTML file, rename it as a JSP, and hook
it up to a display bean.

We also learned something about the ap-
plicability of JSP technology to different types
of pages. The JSP model is excellent for “fill
in the blank” pages composed of a static
HTML template populated with data drawn
from the application (strings, numbers, dates,
and so on). However, JSPs are inadequate for
highly dynamic pages that require different
layouts or designs under different conditions.
For pages with moderately varying structure,
you must use either HTML-producing dis-
play-bean methods or scriptlets that control
the page’s content display. For greater varia-
tions, you often need imports or includes,
which create other difficulties, such as main-
taining syntactically valid HTML output.
They also force you to deal with the implicit
dependencies resulting from bean- and vari-
able-sharing or cascading stylesheet usage.

Scriptlets are not necessarily bad if you use
them in moderation for basic tasks, such as
simple conditional logic. In our experience,
however, such simple usage can easily accrete
modifications and lead to scriptlet bloat.
Worse still, if you need a similar task on sev-

8 2 I E E E  S O F T W A R E M a r c h / A p r i l  2 0 0 2

For the reader unfamiliar with J2EE and other technologies discussed in
this article, we recommend the following sources.

Java 2 Enterprise Edition
� Information about Java 2 Enterprise Edition is available at http://

java.sun.com/j2ee/overview.html.

Enterprise JavaBeans
� Mastering Enterprise JavaBeans, 2nd ed., by Ed Roman et al., John

Wiley & Sons, New York, 2001.
� An Enterprise JavaBean overview is available at http://java.sun.com/

products/ejb/index.html.
� Enterprise JavaBean specifications and document downloads are

available at http://java.sun.com/products/ejb/docs.html.

JavaServer Pages
� An overview of JSPs is available at http://java.sun.com/products/

jsp/index.html.
� Specifications for JSP 1.2 and Servlet 2.3 are available at www.

jcp.org/aboutJava/communityprocess/final/jsr053.

Other resources
� A discussion of HTML templating and scriptlets is available at Martin

Fowler’s “Template View ISA Pattern,” www.martinfowler.com/isa/
serverPage.html.

� Descriptions, tutorials, and downloads of custom tag libraries are
available at http://java.sun.com/products/jsp/taglibraries.html.

Further Reading



eral pages, independently bloating scriptlets
will likely perform that task in slightly differ-
ent ways, making maintenance difficult. 

Custom tags are a good alternative, be-
cause they do not accrete modifications in
this way. They offer a standardized solution
to a common problem with a well-defined in-
terface. This encourages developers to use
them carefully, leading to cleaner templates.
Our most notable successes with custom tags
include tags for conditionally including or
hiding country-specific content, which gave
us a single JSP that serves all countries, and
iterator tags for generating table rows, which
we previously created using an HTML-pro-
ducing method on the display bean.

Internationalization issues
Many business applications must deliver

content to users residing in multiple countries,
and must thus present data in multiple formats.
Sometimes this is as simple as using different
currency and date formatting, but in other
cases, the presentation is so different that the re-
sult seems like an entirely separate application.

The internationalization required to ex-
tend our application to Australia and Canada
was only moderate. The presentation-layer
functionality we needed was similar to the US
version. Most screens had minor differences
due to variations in each country’s business
processes and legal requirements. Nonethe-
less, the changes surpassed the type of inter-
nationalization tasks typically discussed in
J2EE tutorials, such as presenting text in dif-
ferent languages or data in different formats.  

Our main design decision was whether to

� Write each JSP to handle every variation
for every country (the monolithic ap-
proach) or 

� Write one version of each JSP for each
country (the tailor-made approach)

Our solution
Initially, we picked the tailor-made ap-

proach. We had three main reasons for this:
tight time constraints, the lead developer’s per-
sonal preference, and our temporary inability
to upgrade to a JSP container that supported
custom tags. Without custom tags, a mono-
lithic approach would have forced us to use
scriptlets around every localized form input,
table, and hyperlink. As we noted earlier,
scriptlets can be messy and quickly render a

large JSP unmanageable. If we grew to support
five or 10 different countries, the monolithic
JSPs would likely be so complex and difficult
to read that we would have to split them up
into a single JSP for each country anyway. Fur-
thermore, once it became that complex, break-
ing up the JSP without error would be difficult. 

Experience
We quickly realized that the tailor-made ap-

proach had serious problems: Screen updates
often neglected one country’s version of the
page; because our system had so many JSPs,
keeping them in sync was virtually impossible.
Even with only three countries, maintenance
was becoming a nightmare.

Once we upgraded to a servlet container
that supported custom tags, we found a solu-
tion to our problem. Because custom tags can
selectively include or exclude sections of a JSP,
we did not need lots of increasingly compli-
cated scriptlets. This let us unify each screen’s
localized versions into one easy-to-read, easy-
to-modify JSP.  

So, instead of creating a monolithic JSP
riddled with scriptlets such as

<%if(bean.getLocalization()

.equals(Localization.CANADA) ||

bean.getLocalization()

.equals(Localizations.US)){%>

Canada or US-only HTML

<% } %>

we use a simple custom tag:

<localize show=“CA,US”>

Canada or US-only HTML

</localize>

This example is a typical tag usage. Not
only does the custom tag avoid scriptlet bloat,
but we’d hazard a guess that extending this
custom tag example to 10 countries would be
much easier to understand. So far, the custom
tag has proven sufficiently powerful for our
screen internationalization needs.

However, we can certainly imagine pages
with variations that would be difficult to cap-
ture clearly using this kind of in-page condi-
tional logic. In such cases, we could create
one JSP per country for the page portions that
varied dramatically and include these JSPs in
the surrounding, unvarying template using a
custom tag. The problem is that a developer

M a r c h / A p r i l  2 0 0 2 I E E E  S O F T W A R E 8 3

Custom tags
offer a

standardized
solution to a

common
problem with a
well-defined

interface.



would have to keep track of multiple files’
content when editing a single page’s static
template. Perhaps researchers will develop
tools some day that let programmers edit an
entire country’s template at once (either stor-
ing it in multiple files or supplying the neces-
sary conditional inclusion logic behind the
scenes), while still keeping invariable content
the same across all countries.

Integrating with third-party software
Very few enterprise applications run au-

tonomously; most must connect to existing
systems. Given that such systems are often ar-
chaic, use proprietary interfaces, or are simply
built with a somewhat incompatible technol-
ogy, this integration rarely comes easily. 

Our client’s business domain requires intri-
cate calculations. Our application provides this
through integration with third-party financial
software, which we’ll refer to here as TPS (for
third-party software). The third-party vendor
originally wrote TPS for use as a stand-alone
application. The client wanted to integrate TPS
within our larger application to ease version
management and radically simplify what had
been human-powered interfaces between TPS
and other business processes.

Our solution 
At first, the third-party vendor shipped us a

dynamic linked library (DLL) file that exposed
a subset of TPS’s objects via Microsoft’s Com-
ponent Object Model (COM). To drive the
DLL, we wrote an extensive adapter composed
of two types of components: a server and a
pool of translators. 

To perform financial calculations, the main
application sends XML describing financial pa-
rameters and the type of calculation required to
the TPS adapter’s server component. The multi-
threaded server component then forwards this
XML request to a translator component. This
component translates the XML into a series of
COM calls into TPS’s library, and returns the
result in XML form. The server component
maintains a pool of translator components,
each of which must run in its own separate Java
virtual machine (JVM) process, since the TPS
DLL is not thread-safe. We found that using
multiple threads to serve more than one request
at a time improved performance.

Experience
Many challenges arose in the process of

integrating TPS into an Internet application.
The main problem is that TPS has thousands
of possible input parameters. Although we
don’t use all of them, the subset we do use
can itself produce numerous parameter com-
binations. Reproducing this much function-
ality without error is difficult, especially be-
cause we do not use the same interface to the
TPS library as the stand-alone TPS.

Because no one had used the COM interface
to the complicated TPS library before, our
adapter inevitably used the software in unin-
tended ways. One result was that the translator
component processes occasionally crashed,
sometimes in an unrepeatable manner. To keep
these crashes from bringing the TPS application
down, we gave the server component the ability
to resurrect failed child processes. In such cases,
it repeats the request once, returning a service
failure only if the child process fails twice on the
same request. This provides two kinds of ro-
bustness. For unrepeatable crashes, the re-
peated request returns the desired result. For re-
peatable crashes, this strategy avoids corrupting
the pool with dead translator components.

In addition, our server component restarts
translator processes after each successful cal-
culation as well. This eliminates any problems
due to memory leaks or variable initialization
in the third-party DLL (which seldom, but
unpredictably, occur). The cost is a small per-
formance hit. However, this choice, coupled
with requiring that the server component res-
urrect child processes, ensures that the TPS fi-
nancial calculation engine remains available
and correct (for all previously tested requests)
over long time periods.

Testing the application gave rise to other
challenges. Because the COM objects have
complex interdependencies—which are fairly
common for the problem domain—writing
independent, fine-grained unit tests is diffi-
cult. We opted instead for a set of regression
tests, each composed of XML input and the
expected XML output. These tests are fairly
simple to automate and natural to generate,
because we can retrieve the XML from our
application’s logs. They are also easy to main-
tain using Extensible Stylesheet Language
Transformations (XSLTs). However, they do
take longer to run than unit tests, so we run
them less often—daily instead of hourly,
which means we catch regression errors only
on a daily basis. However, because only one
or two people work on the TPS adapter at a

Using multiple
threads to

serve more
than one

request at a
time improved
performance.

8 4 I E E E  S O F T W A R E M a r c h / A p r i l  2 0 0 2



time, we have not found this to be a problem.
Because of the complicated TPS COM ob-

ject interdependencies, the vendor-supplied
DLL requires a lot of quality assurance: The
vendors add features, send the results to us,
and we use our suite of regression tests to en-
sure nothing else has changed. Fortunately,
we have a good relationship with the vendor,
and they’ve always been quick to respond to
our bug-fix requests. Nevertheless, this back
and forth communication takes a lot of time.
Ideally, testing should be collocated with the
vendor’s development work so that feedback
is more immediate.

To address this problem, we are moving
away from the TPS COM interface toward
an XML interface. With the XML interface,
there is just one COM object with one
method that accepts XML input and returns
XML output. Now, the vendor’s software
translates XML to library function calls.
When we find a bug, we simply send them
the input XML that exposed it. They can eas-
ily process that XML within their debugging
environment and track down the cause. Ad-
ditionally, the vendor can maintain a set of
regression tests that reflect how our applica-
tion uses TPS and incorporate those tests into
their QA process. We expect this to dramati-
cally reduce the time it takes to incorporate
additional TPS features into our application.

Distribution: XML data transfer
objects

To facilitate information flow between
tiers, multitiered and distributed enterprise
applications commonly use the Data Transfer
Object design pattern (www.martinfowler.
com/isa/dataTransferObject.html). The DTO
encapsulates a set of parameters that one
component sends or returns to another com-
ponent to complete a single remote method
call (such as a calculation request from our
main application to TPS). In our application,
an object called an XML translator con-
structs an XML-based DTO from data in the
domain object layer and ships it to the ses-
sion or presentation layer, making the data
available in a lightweight package. The ses-
sion and presentation layers can also con-
struct DTOs to be translated back to the do-
main object layer, persisting the data. Using
DTOs prevents the locking problems, net-
work chattiness, and transactional overhead
associated with frequent calls to fine-grained

data-access methods on entity beans.
Earlier in our project, DTOs were ab-

solutely necessary because the production EJB
and servlet containers were in different cities,
and the network latency was far too great to
rely on direct access to the entity beans. Even
now, with servers running in the same JVM,
start-up costs for a new transaction for every
method call make the DTOs valuable.

Our solution
For our DTOs, we chose a hybrid

XML–Java object approach: An XML docu-
ment contains the data and is wrapped by a
Java class for easy access. We write transla-
tors that run in the session layer and perform
the DTO–domain object mapping, both by
building DTOs from the domain object data
and saving modified DTOs back to the do-
main objects.

We can also construct from scratch “up-
date only” DTOs in the presentation layer
with only partially populated element trees.
When the translator persists such a DTO, it
updates only those fields that contain updated
values for domain objects. This lets clients
who only need to update a few fields do so
easily, rather than have to translate a large do-
main object graph to XML, modify one or
two fields, and then save the entire XML
back to the domain objects again.

We generate the Java wrapper classes from
a sample XML document. We do not generate
the translators, because the data often takes a
different form in the presentation layer than in
the domain object layer, and we want to con-
trol the translation. At the individual-field
level, we often use different data types. For ex-
ample, we represent what might be an integer
status ID in the database by its status keyword
in XML (“In Progress,” “Expired,” or “Can-
celed”), and what might be an SQL date in the
database could be in ISO 8601 form in the
XML. On a structural level, we control tra-
versal of the entity bean graph. For example,
if bean Foo is being translated to XML and
contains a reference to bean Bar, we might in-
sert into bean Foo’s XML either bean Bar’s
XML translation or a <barId> element that
lets clients look up bean Bar when required.
Finally, on a semantic level, we occasionally
introduce new, presentation- or session-related
fields into the XML that do not exist on the
domain object layer.

Our translators can also register multiple

M a r c h / A p r i l I E E E  S O F T W A R E 8 5

Ideally, testing
should be

collocated with
the vendor’s
development
work so that
feedback is

more
immediate.



translators of different “flavors” for a single do-
main object. This lets us handle the differing
needs of various application parts. For exam-
ple, if one application part requires only subset
X of the domain object fields, or requires fields
in a certain format, and another part requires
subset Y and a different format, it’s no problem.
We simply write two flavors of the translator.

Experience
Using XML as the DTO representation

was a good choice. Because our application is
an intranet application used by relatively few
employees, we have not had to worry much
about scalability issues, and the XML data
structures’ size in the session has not yet been
a problem. The XML objects make debug-
ging much easier, especially in production or
in environments where debugging tools are
unavailable. We can see the current state of
the business objects clearly, concisely, and in
a human-readable fashion just by looking at
the XML. When we have to integrate with
other software systems, the XML provides a
convenient base for designing the interface.

Our experiences have not been entirely
positive, however. We’ve encountered several
largely unresolved difficulties with using DTOs. 

First, fine-grained access to individual entity-
bean fields is still awkward. When only one
field is required, it’s inefficient to gather all the
data a large DTO requires—which can include
a hundred fields from a dozen different domain
objects. Also, writing an entire XML translator
flavor to return one or two fields is a lot of code
overhead. You can write a session-bean method
to access that field (in our architecture, all pres-
entation-layer calls must be mediated by a ses-
sion bean), but to avoid session-bean bloat—
and to avoid the original network traffic and
transactional issues—we must use such meth-
ods in moderation. An alternate solution that
we’re currently investigating is to pass the trans-
lator one or more XPaths that specify a subset
of the XML DTO’s fields, so that the translator
fetches only those fields from the domain ob-
ject. (For more on XPaths, see www.w3.org/
TR/xpath.)

Second, using DTOs somewhat compli-
cates the modification of domain object
classes. When we add a field to a domain ob-
ject, we must update the sample XML docu-
ment, regenerate the XML DTO class, and
manually update the translators to process
the new data. However, this is not usually dif-

ficult or time consuming.
Third, and perhaps most significant, some-

times the presentation layer requires domain
object business logic, and although DTOs
provide a means of sharing data between the
two layers, they do not help with sharing be-
havior. For example, a domain object might
have an isValid() method that returns true
when the object’s fields satisfy certain require-
ments. If the presentation layer must know
the underlying domain object’s validity, we
must write a session-bean method that ac-
cepts a DTO (or ID) as an argument and del-
egates the call to the existing method on the
corresponding domain object. This works,
but suffers from the same performance issues
as domain object individual-field access. If the
presentation layer modifies our DTO, and we
need to determine the DTO state’s validity, we
have an even more unpleasant situation. In
this case, we must either

� Write a new method that takes the DTO
and duplicates the logic from the do-
main object method, or

� Translate the DTO back to a temporary
“dummy” domain object on which we
can call the isValid() business method 

So far, the problem is not that great in our
case. A possible future solution is to use
method objects1 that represent rules—such as
validity conditions—or other tasks. We could
then run these rules against both domain ob-
jects and DTOs, keeping the business logic in
only one place.

Embracing change: Generated
domain objects

Typically, the core of an enterprise applica-
tion has a domain object layer—a layer of code
that manages persistence and business logic for
the fundamental objects in the business do-
main model. Our domain object layer main-
tains the state of well over 100 domain objects
and also provides some of their basic business
functionality. Such scale and complexity al-
ways provides developers with challenges in
keeping systems flexible and maintainable.

Our solution
In our application, most of the domain

layer consists of code generated using meta-
data files that describe the business objects.
Several factors motivated our decision to use

The XML objects
make debugging

much easier,
especially in
production or 

in environments
where

debugging tools
are unavailable.

8 6 I E E E  S O F T W A R E M a r c h / A p r i l  2 0 0 2



code generation. First, EJBs are a heavy-
weight technology with significant coding
overhead: A bean developer must create at
least four separate files for each domain ob-
ject, which involve a fair amount of code or
declaration duplication. Making the same
change in all these classes slows down devel-
opment. Because our domain objects were
evolving rapidly, especially in early prototyp-
ing stages, we had to be able to quickly up-
date the code, regenerating from the changed
metadata. Later, when we began to separate
persistence and transactional responsibilities
(implemented with EJBs) from business re-
sponsibilities (implemented with plain Java
classes), we required even more classes, fur-
ther increasing the generators’ value.

Second, we’d yet to fully commit to EJB
technology—particularly entity beans, be-
cause at that time they were not even required
by the EJB 1.0 specification (http://java.sun.
com/products/ejb/docs10.html). 

Third, EJB technology was still rather im-
mature; vendor-supplied containers exhibited
many bugs (in fact, they still do). Using gen-
erated domain objects let us centralize the re-
quired hacks and bug workarounds in one
place—the generators—yet still apply them
across all domain object implementations.

We named our domain object inheritance
structure the “martini glass model” for the vi-
sual appearance of its UML diagram (see Fig-
ure 1). Its complex structure is partly a result of
the fact that, unlike many code generation
schemes, generated and handwritten customiz-
ing code never live in the same file, so merges
are never required. Instead, the Java compiler
and virtual machine effectively perform merges
automatically as a result of the inheritance
structure. Our UML model was not designed
up front, but rather evolved as we required
more features and flexibility. This would have
been infeasible without generated code. 

We developed the structure in five stages,
adding features and complexity on an as-
needed basis:

1. We generated the four basic required
classes or interfaces: the remote inter-
face, the home interface, the primary key
class, and the entity bean itself.

2. We generated the finder class (the service
locator), which encapsulated the Java
Naming and Directory Interface (JNDI)
calls and provided a caching service for

the home interface.
3. We split the entity bean into a generated

persistence class (entity bean class) and a
handwritten business-logic class. This let
us regenerate the entity bean (the field
accessors) without obliterating hand-
coded business methods.

4. We then generalized the persistence inter-
face (field accessor signatures), allowing
for persistence implementations other
than entity beans. Because our EJB con-
tainer did not support read-only access
to entity beans, we used our persistence-
layer abstraction to permit two imple-
mentations: fully protected entity bean
read-write access and Java Database
Connectivity (JDBC)-based read-only di-
rect database access. 

5. We added custom mappers and finders. 

Mappers define the mapping between domain
objects and the persisted data that represents
them, and custom mappers let us implement un-
usual persistence requirements. Custom finders
were useful because our container’s query lan-
guage was insufficiently powerful for the domain
object searches we needed. The finders provide
an abstraction layer, delegating to either the bean
home methods for simple searches, such as “find
by primary key,” or custom finders implemented
with SQL queries for more complex searches. 

Experience
One problem we’ve frequently encountered

M a r c h / A p r i l I E E E  S O F T W A R E 8 7

Manually written

Manually written; provides
implementation for business
methods defined in the
DomainObjectBusiness interface

getA(){
return state.a;
}

getA(){
return peer.getA();
}

<<Interface>>

+create()
+remove()
+getFactory()
+isIdentical()
+cloneObject()

DomainObjectPersistentObject

<<Interface>>

DomainObjectBusinessImpl

DomainObjectState DomainObjectEJBAdaptor

DomainObjectBean
DomainObjectJDBCAdaptor

DomainObject

<<Interface>>

+get/set()
+relationalOperation()

DomainObjectBase
<<Interface>>

+businessMethodOne()
+businessMethodTwo()

DomainObjectBusiness

Figure 1. UML diagram of a sample domain object model’s
core. All classes and interfaces shown are generated from
metadata, except for the DomainObjectBusiness interface and
the DomainObjectBusinessImpl class, which are hand-written.



with our domain objects is deadlocking. Our
EJB container uses pessimistic concurrency
control, so if one transaction accesses an en-
tity, it will prevent code running in another
transaction from accessing that same entity.
Deadlock can occur if, for example, an entity
calls a session-bean method—thus starting a
new transaction—and the called method in
turn touches the original entity. To help avoid
deadlocking, we carefully set transactional
contexts and minimized locking through the
use of JDBC-based read-only persistence.

Another problem we faced was perform-
ance. By default, every call on an entity-bean
remote interface must go through the gauntlet
of security context checks, transactional con-
text checks, acquiring locks, and calls through
stubs and skeletons—even if all that the caller
requires is read-access to a field. In such sim-
ple cases, using JDBC-based persistence for
read-only access saves a lot of overhead.

One of the challenges in developing Inter-
net applications is that many of the technolo-
gies are new and many of the implementa-
tions are buggy, incomplete, or not quite
written to specification. This was certainly
true for our EJB container. Many of its bugs
were related to caching and transactional
boundaries and were difficult to find, repro-
duce, and test against because the testing en-

vironment must exactly duplicate the running
system’s transactional context management.
Once we identified the bugs, however, using
code generation facilitated workarounds
throughout our domain object layer.

One disadvantage with code generation is
that to achieve the flexibility of handwritten en-
tity beans, we must use an elaborate structure
of supporting interfaces and classes. This cre-
ates a complex system that takes longer for new
developers to learn. Some of the system’s fea-
tures are esoteric, and few developers know
how to use them. For example, we can set the
persistence implementation to JDBC-based
read-only for all domain objects accessed in a
given transaction. This is useful for avoiding
deadlocks in calculations that access many do-
main objects. Although few developers use or
understand such advanced features, they are
consistent throughout the application. This
would not be true with handwritten code. Also,
the system basics are quite simple. Several other
ThoughtWorks projects have successfully used
the generators, which let developers get an en-
tire domain object layer up and running rapidly.

The greatest benefit of our domain object-
layer generation system is that it lets us main-
tain our agility and frees us from architectural
decisions made years ago. The generators let us
easily modify individual domain objects as well
as make major, global architectural changes,
such as abstracting the persistence interface on
every system domain object, or, in the future,
migrating to the EJB 2.0 specification.

I n the process of developing our system,we learned many lessons. 
� JSPs let us rapidly develop a Web applica-

tion’s user interface and separate Java and
HTML development roles. However, de-
velopers writing applications with highly
dynamic content might find JSPs awk-
ward, making it more difficult to take ad-
vantage of the separation benefits.

� Custom tags are a powerful, clean way to
abstract shared JSP behaviors, such as it-
eration or conditional content inclusion.

� Although conventional wisdom advises
against maintaining a set of regression
tests on third-party code, it is in fact of-
ten useful, especially when those tests
are written in a portable format such as

8 8 I E E E  S O F T W A R E M a r c h / A p r i l  2 0 0 2

About the Authors

Eric Altendorf is an independent consultant. He has worked in both academic settings
and private industry, and was a software developer and architect at ThoughtWorks on the proj-
ect described here. His professional interests include agile development, software architecture
and design, and generative programming. His research interests include neurally plausible the-
ories of thought and language, reinforcement learning approaches to computer-played Go, and
time-series prediction. He received a BS in computer science and mathematics from Oregon
State University, and plans to resume study for a doctoral degree in artificial intelligence. Con-
tact him at EricAltendorf@orst.edu.

Moses Hohman is a software developer for ThoughtWorks, a custom e-business soft-
ware development consulting firm. Currently he works out of the recently opened Bangalore,
India, office, where he mentors new software developers and teaches a course on agile soft-
ware development at the Indian Institute of Information Technology Bangalore. He received
an AB in physics from Harvard University and a PhD in physics from the University of Chicago.
Contact him at mmhohman@thoughtworks.com.

Roman Zabicki is a software developer for ThoughtWorks. He received a BA in com-
puter science from the University of Chicago. Contact him at rfzabick@thoughtworks.com.



XML and can thus be transferred to the
third-party vendor.

� Data transfer objects are a useful, often
necessary part of multitiered applications.
However, owing to the resulting dual ob-
ject representations, developers must care-
fully consider which business methods be-
long in the domain object layer and which
belong in the presentation layer. Doing so
can help avoid excessive code duplication
or cross-tier method call delegation.

� It is possible to change even the most
fundamental architectural decisions—
such as the domain object-layer design—
even when you are well into the project.
Code generators let developers make
sweeping changes easily across the entire
code base and are thus an extremely
valuable tool for the agile developer.

At present we are in the process of interna-
tionalizing our application for France, our first
system-wide language translation. The changes
involved have highlighted some unnecessary
complexity in our presentation layer. Some of
this complexity we can resolve, applying the
lessons discussed above uniformly throughout
the application. Other issues, such as an awk-
ward algorithm for shuttling data between
HTML forms and the XML DTOs, remain
problematic. We have experimented with sev-
eral ideas, from the Apache Struts framework
(http://jakarta.apache.org/struts) to simple cus-
tom-built architectural solutions. We anticipate
that more useful lessons will come from these
investigations.

Reference
1. K. Beck, Smalltalk Best Practice Patterns, Prentice Hall,

Upper Saddle River, N.J., 1997.

MEMBER TECHNICAL STAFF/CABLE
MODEM PROJECT. 

Responsible for design, Develop
and implement software test programs
and test procedures for CM (Cable Mo-
dem), which will verify the quality, per-
formance and compliance to DOCSIS
(Data-Over-Cable Service Interface Spec-
ification). Will provide documentation
and support for trouble shooting and fix-
ing any problems reported during sys-
tem testing of the software. Will also be
responsible for testing and making the
software ready for certification process.
Will understand and apply new and up-
coming technology trends to the current
test software and apply them to new
generation Cable Modems like VoIP
(Voice over Internet Protocol) and others.
Requires M.S in Computer Science with
at least 3 years of professional experience
in job offered or network programming;
Experience must include programming
using SNMPv1/v2/v3, DHCP, TFTP, ToD
Internet Protocols, and using Ethernet
frames, TCP/IP Packets, MAC frames.
Must have current authorization to be
permanently employed in the United
States.  Annual salary of $65,000.  40
hours/week (8 am – 5 pm). Send resume
with Social Security Number to: Indiana
Department of Workforce Development,
10 N. Senate Avenue, Indianapolis, Indi-
ana  46204-2277. Attention: Mr. Tim
Lawhorn, Reference:  I.D.# 8139386.

************
SOFTWARE ENGINEER

Must have 9 months exp in job of-
fered or 9 months in related occupation
as Quality Control Engineer. Implement,

improve and maintain the ISO 9001 cer-
tification and TickIT certification for soft-
ware. Develop and implement the cali-
bration system according to the ISO
17025 and obtain NIST (National Insti-
tute for Standard and Technology) trace-
ability. Using statistical control tech-
niques to monitor and improve
production, test processes and products
in the development of RD family acquire
CE Mark using directives 73/26/EEC and
89/336/EEC. Coordinate the design re-
views, verification and validation of
measuring and test product. Perform
vendor verification for PCB, plastic and
metal parts. Must Possess Certified Qual-
ity Engineer certification ASQ (American
Society for Quality). To work 40 hr/week
from 8am to 5 pm. Salary is $54,000 per
year. Applicant must possess permanent
authorization to work in the U.S. Quali-
fied applicants send resumes with Social
Security Numbers to the Indiana Depart-
ment of Workforce Development, 10 N.
Senate Ave., Indianapolis, IN 46204-
2277, 
Attn: Mr. Gene R. Replogle. 
Refer to ID # 8138653.

***********
RESIDENT SYSTEMS ENGINEER

Must have BS in Computer Science
or equivalent education experience.
Must have 2 yrs exp. in job offered or 2
yrs exp. in related occupation as Com-
puter Engineer or related. Design Release
Satellite Digital Audio Receiver. Electronic
integration of Component with vehicle.
Mechanical integration of Component
with Vehicle. Component/ Vehicle level
Validation/ debugging. Track Program
timing & finance. Write component

technical Spec and Diagnostic Specifica-
tion. Write Class 2 Communication func-
tional specification. Write CAN/GMLAN
Communication functional specification.
To work 40 hrs per week & an 8-5pm
work schedule. Qualified applicants must
send resume to Shannon Miller, Adminis-
trative Assistant at Delphi Delco Electron-
ics Systems, One Corporate Center M/S
CT17A, PO Box 9005, Kokomo, IN
46904-9005. Please refer to PO# 01-
0998.

************
Senior Application Developer 

Needed at Ingersoll-Rand Com-
pany, Security & Safety Business Unit in
Indianapolis, IN.  Duties:  Develop &
maintain Oracle application programs &
systems (business, manufacturing & en-
gineering).  Perform functions of systems
analysis & programming in multiple IT &
S operating systems.  Perform system
analysis, design, documentation, pro-
gramming, testing, implementation,
user training & continuing support of all
applications systems.  Review & recom-
mend IT & S policies.  Evaluate and im-
plement third-party software systems ap-
plications.  Utilize Oracle SQL*PLUS,
PL/SQL, Packages, Forms, Reports, C
Language and UNIX shell programming.
Must possess Master’s Degree or equiv.,
in Computer Science.  Will accept Bach-
elor’s Degree & 5 yrs. of exp. in lieu of
Master’s Degree.  Must also possess 2
yrs. of development exp. either in the job
offered or in related occupations of Soft-
ware Analyst or Programmer. Competi-
tive salary & benefits. Send resume to
Justin Zlotnick at 111 Congressional
Blvd., Carmel, Indiana 46032. 

Career  Opportunities



9 0 I E E E  S O F T W A R E M a r c h / A p r i l  2 0 0 2 0 7 4 0 - 7 4 5 9 / 0 2 / $ 1 7 . 0 0  ©  2 0 0 2  I E E E

For complex applications, we might need to
customize the output generator or build a
new one. How can engineers easily specify or
customize this automatic output generation? I
propose a method where extended UML ob-
ject diagrams specify the translation. This
method is visual and therefore produces spec-
ifications that are clear and concise and easier
to create and maintain. My work focuses on
the modeling domains, whose metamodels
can be defined using basic object-oriented
structural concepts, such as classes and attri-
butes, associations, and generalization.1 My
approach can be used in modeling tools with
different levels of customizability.

Standard translation approaches
Two conceptual levels are important for

the modeling process. The upper level is the
metamodeling level, which generally defines
the modeling domains’ abstractions. That

is, it defines the modeling domain’s vocabu-
lary in terms of its abstractions, their prop-
erties, relationships, semantics, and behav-
ior. This type of domain specification—that
is, its conceptual model—is called the do-
main’s metamodel. 

The lower level is the modeling level. The
user works at this level during the modeling
process. The modeling level represents the
instances of abstractions. In other words,
the elements at the modeling level are gen-
erally instances of the elements at the meta-
modeling level. Therefore, meta is a relative
reference that describes the type-instance di-
chotomy among the elements at different
conceptual levels.

To demonstrate both the problem with
the standard and my proposed solutions, I
use a simple example from telecommunica-
tions software development. In this example,
shown in Figure 1, the user is developing or

feature
Domain Mapping 
Using Extended UML
Object Diagrams

Dragan Milicev, University of Belgrade

Most contemporary
software and other
engineering methods
rely on modeling 
and automatic
translations of
models into different
forms. The author
advocates a UML-
based translation
specification method
that is visual and
therefore easier to
understand and use.

S
oftware tools that support modeling in engineering domains pro-
vide environments for applying modeling methods and notations,
thus making modeling less time consuming and error prone. One
of the most important features of modeling tools is automatic

output generation, or model translation, no matter what the output repre-
sents. It can be documentation, source code (for software systems), a netlist
(for on-chip hardware), or any model other than the one the user created. 

object modeling



customizing output generation of a simple
modeling environment that will generate
C++ code for state machine models. I as-
sume that the user wants to obtain the code
shown in Figure 1b, which is an implemen-
tation of the State design pattern.2 Code
generation for state machines should be cus-
tomizable. If the user needs different execu-
tion models because of performance, con-
currency, distribution, or other issues, he or
she should be able to change the manner in
which the code is generated from the same
initial state machine model.

For this example, several classes will be
generated in the output code. In this exam-
ple, the first class is named FSM. It contains
the methods that correspond to the events
to which the state machine reacts. The sec-
ond class is named FSMState and is ab-
stract. It has one polymorphic operation for

each event. Finally, one class derived from
FSMState is generated for each state. It
overrides the operations that represent
those events on which the state reacts.

Figure 1c shows a simple metamodel of
this modeling domain. It defines the domain
abstractions as the classes State, Transi-
tion, and so on, and their relationships. For
example, a transition is connected to its
source and destination states. The associa-
tions source and target represent the con-
nections in the metamodel. A concrete model
defined by the user in this domain (such as
the one in Figure 1a) consists of instances of
the classes from the metamodel. These are
connected by the links as instances of the cor-
responding associations.1 For example, the
state A in Figure 1a is a notational symbol
for an instance of State. Similarly, the tran-
sition edge from the state A to B (indicated

M a r c h / A p r i l  2 0 0 2 I E E E  S O F T W A R E 9 1

A B

s1 / t1

s2 / t2

s1 / t3
(a)

class FSM {
public:
  FSM ();

  void s1 ();
  void s2 ();

  void t1() {...}
  void t2() {...}
  void t3() {...}

  FSMStateA stateA;
  FSMStateB stateB;

  FSMState* curSt;
};

FSM::FSM () : stateA(this), stateB(this),
  curSt(&stateA) { curSt->entry(); }

void FSM::s1 () {
  curSt->exit();
  curSt = curSt->s1();
  curSt->entry();
}
...

class FSMState {
public:
  FSMState (FSM* fsm) : myFSM(fsm) {}

  virtual FSMState* s1 () {return this;}
  virtual FSMState* s2 () {return this;}

  virtual void entry () {}
  virtual void exit  () {}

protected:
  FSM* myFSM;
};

class FSMStateA : public FSMState {
public:
  FSMStateA (FSM* fsm) : FSMState(fsm) {}

  virtual FSMState* s1 ();
  virtual FSMState* s2 ();
};

FSMState* FSMStateA::s1 () {
  myFSM->t1();
  return &(myFMS->stateA);
}
...

(b)

1 *+mySource +hSourcesource

*+myTarget1 +hTargettarget
0..1*

+myTrigger

trigger

StateMachine

*

1
+myStates

1

states

*

1

+myTransitions

*

1 transitions

*

+myEvents

1 events

(c)

State Transition Event

Figure 1. Code 
generation for state
machines: (a) a 
sample state 
machine model; 
(b) a fragment of the 
generated code; 
(c) the domain’s 
metamodel.



by the arrow from A to B) is a notational
symbol for an instance of Transition,
linked by an instance of the association
source with the instance A, and so on.

In this example, the user specifies the
code generation strategy to apply to each in-
stance of StateMachine created in the
model. In most modeling tools, the standard
practice is to hard-code the output genera-
tion in a procedure. The procedure involves
navigating through the instances in the
model, reading their attribute values, and
producing the textual output following the
C++ syntax and semantics. For this specifi-
cation, I will use C++ for generality. A small
fragment of code that generates the begin-
ning of the class FSMState declaration
might be

void generateStateMachineCode 

(StateMachine* this) {

... // File opening and 

// other preparation actions

// Generate base state class:

output << “class “ << 

(this->name+”State”) << 

“{ \n”;

output << “public: \n”;

output << “    “ << 

(this->name+”State”) << “(“;

output << (this->name) <<  

“*fsm) : myFSM(fsm) {} \n”;

...

The output generation can be viewed as a
creation of a new target model from a
source model. The source model is the
model that the user explicitly creates with
the modeling tool. The target model is the
generated code whose metamodel is im-
plicit—this is the C++ syntax and semantics.
The presented code of the code generator is
actually a specification of the mapping be-
tween these domains. The direct mapping
between these two domains, using the hard-
coded special-purpose generator, is difficult
because the domains are conceptually very
distant. The drawbacks include

� The specification process is extremely te-
dious, time consuming, and error prone.

� The user must work with a complex tar-
get domain (C++ syntax and semantics).

� The user must deal with all technical de-
tails, such as correctness of the output

stream, opening files (.h and .cpp files
must be created), and so on.

� The user cannot easily apply modifica-
tions because the code is unclear and in-
comprehensible.

� The code generator cannot easily be re-
used for other target languages, such as
Java.

� This problem might likely exist in a
broader context of a more general mod-
eling tool, such as a tool for modeling in
UML, which already has a C++ code
generator. This built-in general-purpose
and reusable code generator (for exam-
ple, from UML models to C++) is not
used at all. 

Intermediate approach
Direct mapping between two conceptu-

ally distant domains has the same disadvan-
tages as object-oriented programming in a
target programming language (such as C++)
with no previous modeling at a higher ab-
straction level (such as UML). Instead of di-
rectly generating the textual output, it is
more reasonable to create an intermediate
model from a domain of a higher abstrac-
tion level. This domain might consist of the
basic object-oriented concepts from UML,
such as class, method, and attribute, so that
it can easily be mapped into the target C++
domain. Because the general-purpose C++
code generator from the intermediate mod-
els might already exist in the tool, the trans-
lator could then reuse it for the generated
intermediate model. 

In short, the translator first creates the
needed instances of the intermediate model
using the built-in UML metamodel and then
invokes the built-in code generator to pro-
duce the ultimate output:

// Temporary package for  

// the intermediate model:

Package* pck = Package::create();

// Intermediate model:

// Base state class:

Class* baseState = Class::create(pck);

baseState->name = this->name+

”State”;

// Base state class constructor:

Method* baseStateConstr = 

Method::create(pck);

baseStateConstr->name = this->name+

The translator
first creates
the needed

instances of the
intermediate

model using the
built-in UML

metamodel and
then invokes
the built-in

code generator
to produce the
ultimate output.

9 2 I E E E  S O F T W A R E M a r c h / A p r i l  2 0 0 2



”State”;

createLink(“members”,baseState,

baseStateConstr);

//... and much, much more ...

// Output generation using  

// the built-in generator:

pck->generateCode();

This code fragment creates instances of
the abstractions Class and Method, using
the corresponding intermediate metamodel’s
programming interface. These instances rep-
resent the class FSMState and its construc-
tor in the resulting output. After creating in-
stances, the code fragment sets the values of
their attributes. It then creates the links be-
tween these instances. All instances are
packed into a temporary package for which
the output is finally generated. 

This approach eliminates some of the first
approach’s drawbacks. It eliminates the “im-
pedance mismatching” problem between the
source and the target domains by introducing
an intermediate domain. Output generation
is now split into two smaller steps. The user
does not have to work with complex C++
syntax and semantics, or with the technical
details (output files). Additionally, the built-
in code generator automatically produces
code, instead of the user having to hard-code
it. Moreover, the mapping from the interme-
diate domain into the C++ source code is
more or less standardized, and many model-
ing tools provide code generators that might
be reused. On the other hand, the mappings
between arbitrary user-defined higher-level
domains should be customizable.

Many commercial modeling tools use in-
termediate domains and reusable code gen-
erators for these reasons. Users can enjoy
these tool benefits even if they are not aware
of them. For a good explanation of the ad-
vantages of introducing an intermediate
model and a mapping from the source do-
main, which is independent of the target do-
main, see “Recursive Design of an Applica-
tion-Independent Architecture.”3

This approach works well for simple ap-
plications. However, for more complex ap-
plications, when the mapping from the
source domain into the target output is more
sophisticated, the specification might still be
complex and difficult to maintain. Besides,

the described approaches are solely available
in commercial tools. Moreover, the meta-
models are often not object oriented—that
is, they do not incorporate the abstractions’
generalization or specialization. This makes
programming customized code generators
more difficult than necessary. 

Because the previous specification is ac-
tually the process of creating abstractions’
instances from the intermediate domain (for
this particular mapping, it is the target do-
main), where both domains can be formally
defined by their metamodels, the tool de-
signer can provide this specification in an-
other formal way. Using a visual specifica-
tion, one compatible with UML, is the crux
of my approach.

Domain mapping specification
Figure 2 shows the principle of introduc-

ing an intermediate domain. An intermedi-
ate metamodel is introduced at the meta-
modeling level, and an intermediate model
at the modeling level. Because each trans-
formation is much less complex than the di-
rect transformation, it is easier to specify
and maintain. Besides, each transformation
can be modified independently and reused
separately in different contexts.3

The domain mapping specifies a set of in-
stances of the intermediate metamodel ab-
stractions that should be created during the
source model translation. So, the set is best
represented using UML object diagrams.1

Figure 3a shows an object diagram for a part
of the mapping. The diagram is defined for
fsm, an instance of StateMachine from the
source model. It specifies what should be cre-
ated for each instance of StateMachine. The
diagram also specifies the attribute values
and the links between the created instances.
The attribute values are defined in terms of
expressions that refer to the instances of the
source model and their attribute values, using
the navigation through the source model.
The links are instances of associations from
the intermediate metamodel.

Standard object diagrams are insufficient
for mapping purposes. They also require
repetitive creation. In my example, as Figure 1
shows, one derived state class should be cre-
ated for each machine state. For this purpose,
a stereopackaged package1 with the stereotype
ForEach is used in the mapping specification.
Figure 3b shows an example of this. A ForE-

M a r c h / A p r i l  2 0 0 2 I E E E  S O F T W A R E 9 3

Many
commercial

modeling 
tools use

intermediate
domains and

reusable code
generators.

Users can enjoy
these tool

benefits even if
they are not

aware of them.



ach package represents iteration through a
collection of instances from the source model
and creation of a set of instances and links in
the intermediate model for each of them. The
package contains three tagged values:1

� ForEach is an identifier that is intro-
duced into the package. It is used to re-
fer to the current element of the collec-
tion being iterated.

� OfType specializes the current element’s
type. The iteration is type-sensitive: only
the elements with the specified type
from the collection are processed; the
others are ignored, in case the elements
are polymorphic. The referred type is an
abstraction from the source metamodel.

� InCollection defines an expression
that evaluates to a collection of the
source model elements and indicates
which elements to iterate through.

When a link connects an instance inside a
package with another instance outside that
package, each repetitive instance that the it-
eration created will be linked to the outer
instance. For the expressions that specify at-
tribute values or the collection in a ForEach
package, any formal language for naviga-
tion through the source model can be used
(I use C++ in my example). ForEach pack-
ages also might be nested, as Figure 3b
shows. In this case, a derived class should be
created for each state. This is specified with
the outer ForEach package. For each event
on which this state reacts, an operation

should be generated in this class. This is
specified with the nested package.

Another needed concept is conditional
creation. Any element of the specification
might be tagged with a condition that is a
Boolean expression defined in the context of
the source model. It is specified by the Cond
tagged value. If the expression evaluates to
False when the intermediate model is cre-
ated, the conditional element is not created. 

Following the UML style, a ForEach
package’s contents can be presented in sev-
eral diagrams to improve mapping readabil-
ity. The whole domain mapping specification
is actually a special type of model, packed
into a package stereotyped DomainMapping.
This root package might contain nested pack-
ages, some of them being ForEach packages.
They might iterate through all instances of a
source domain abstraction, using a built-in
operation for that purpose (for example,
StateMachine::getAllInstances()).
Consequently, the diagrams in Figure 3 be-
long to the same ForEach package that iter-
ates through all instances of StateMachine
defined in the source model.

From these specifications, the automatic-
translator code that creates the intermediate
model can be generated automatically.4 For
definitions of the proposed concepts and
their semantics and constraints, along with
some advanced concepts for parameterized,
recursive, and polymorphic substructure cre-
ation, see Automatic Model Transformations
Using Extended UML Object Diagrams in
Modeling Environments.4

9 4 I E E E  S O F T W A R E M a r c h / A p r i l  2 0 0 2

Source
domain

metamodel

Intermediate
domain

metamodel

Source
domain
model

Modeling level

Intermediate
domain
model

Target
domain
model

Output generation

Target
domain

metamodel

Hard-coded
output generator

Implicit instantiation:
Model transformation

Implicit instantiation:
Output generation

Metamodeling level

State machines
metamodel

Extended object
diagrams

State machines Model
transformer

Automatic
generation

Explicit instantiation:
Model specification

UML subset
metamodel

Domain mapping
specification

C++ code
generator

C++ syntax and
semantics

C++ source
code

Figure 2. The 
domain mapping
specification’s role
in code generation
for state machines.
The transformation
from the source into
the target domain is
split into two (or 
generally more)
steps to cope with
the mapping 
specification’s 
complexity.



Translation example

Another simple example deals with the
problem of translating the object-oriented
class model into the relational database model. 

This is a common task in practice, when
persistence of objects is accomplished with a
relational database. In this example, the
source domain is UML. The target domain
is the code that defines the database tables
and fields—that is, SQL declarations. How-
ever, the direct mapping from the class

model into the textual SQL declarations is
difficult to specify. So, an intermediate do-
main is introduced with the metamodel in
Figure 4a. It is a simplified version that en-
compasses only the abstractions of tables
and fields. From this intermediate domain,
it is now easy to specify the generation of
SQL declarations, because the mapping is
almost (if not completely) one to one. 

Implementation of inheritance is the most
difficult task in this process. I assume that

M a r c h / A p r i l  2 0 0 2 I E E E  S O F T W A R E 9 5

baseState : Class

name = fsm.name+"State"

baseStateConstr : Method

name = fsm.name+"State"
isQuery = False
isPolymorphic = False
isAbstract = False
body = " "

baseStateEntry : Method

name = "entry"
isQuery = False
isPolymorphic = True
isAbstract = False
body = " "

baseStateExit : Method

name = "exit"
isQuery = False
isPolymorphic = True
isAbstract = False
body = " "

baseStateHelper : Method

name = "fsm"
isQuery = True
isPolymorphic = False
isAbstract = False
body = " return myFSM; "

baseStateAttr : Attribute

name = "myFSM"
type = fsm.name+"*"
initialValue = "fsm"

baseStateConstrParam : Parameter

name = "fsm"
type = fsm.name+"*"
kind = in
defaultValue = " "

baseStateEntryParam : Parameter

name = " "
type = "void"
kind = return
defaultValue = " "

baseStateExitParam : Parameter

name = " "
type = "void"
kind = return
defaultValue = " "

baseStateHelperParam : Parameter

name = " "
type = fsm.name+"*"
kind = return
defaultValue = " "

: formal parameter : formal parameter : formal parameter

: formal parameter

: members

: members : members

: members : members

{
ForEach = st,
OfType = State,
InCollection = fsm.myStates
}

<<ForEach>>
DerivedStateClass

baseState : Class

name = fsm.name+"State"

: generalization

supertype

subtype

<<ForEach>>
DerivedStateSignal

{
ForEach = tr,
OfType = Transition,
InCollection = st.hSource
}

derivedStateSignal : Method

name = tr.myTrigger.name
isQuery = False
isPolymorphic = True
isAbstract = False
body = "myFSM->" + tr.name + "();\n" +
"return &(myFSM->state" +
tr.myTarget.name + ");\n"

derivedStateSignalP
aram : Parameter

name = ""
type =
fsm.name+"*"
kind = return
defaultValue = ""

: members

: formal parameters

(a)

(b)

derivedState : Class

name = fsm.name+"State"+st.name

Figure 3.
Domain mapping
specifications for the
state machines 
example. (a) A 
simple part of the 
object diagram for
the domain mapping
specification. The 
diagram shows only
the specifications for
the base class 
FFSSMMSSttaattee and its
members that are
generated by default.
The diagram belongs
to the context of the
state machine 
accessible through
the ffssmm identifier. (b)
FFoorrEEaacchh packages
and their nesting.
The diagram shows
both a part of the
specification of the
derived classes for
the states and their
member operations
for the events.



the user can choose between two strategies
of implementing inheritance in relational ta-
bles. The first strategy assumes that a de-
rived class has its own independent table,
with all inherited attributes placed in that
table. In this strategy, a single record in the
table that corresponds to a class represents
one object of that class. In the second strat-
egy, a derived class has a table without in-
herited attributes, but its records are de-
pendent on the records from the table that
represents its base class. Here, a set of
records in the tables that indicates the ob-
ject’s own class and its base class represents
an object. The user can select one of the
strategies for each generalization relation-
ship in the class model by setting a Boolean
property of the generalization named in-
heritFieldsFromCommonTable. This prop-
erty can be defined as a tagged value of a
generalization. If the user sets this value to
True, the second strategy is chosen.

In both strategies, the table should have a
primary key (of type AutoNumber and named
ID) and the field set for the class’s attributes.
In the first strategy, the table should have the
fields for all attributes from the base class for
each inheritance relationship tagged with 
inheritFieldsFromCommonTable = False.
In the second approach, the table should
have only a foreign key (of type Long and

named ID<baseClassName>) to link it to the
base class table. Figure 4b shows the map-
ping scheme.

Apart from this example, a research team
from the University of Belgrade has realized
three more complex and real-world proj-
ects, and several others are in progress.

Research projects
Undergraduate computer science stu-

dents used my visual specification technique
in their projects. The students focused on
three different modeling techniques, with
the goal to define the metamodels and spec-
ify the generators for C++ code that serves
as the executable implementation of the
models. Following the proposed approach,
they used an intermediate domain that is a
subset of UML. We designed this domain to
be conceptually close to common object-ori-
ented programming languages (C++ and
Java), so we easily constructed the C++ code
generator for this domain. We refer to this
as the object-oriented programming lan-
guage (OOPL) domain. Its metamodel has a
dozen classes and associations. For other
summaries, detailed reports, a suggested
methodology for applying the proposed ap-
proach, and simple examples (some of
which show cases when UML is not used as
any of the domains), see “Automatic Model

9 6 I E E E  S O F T W A R E M a r c h / A p r i l  2 0 0 2

Table

+ name : String

Field

+ name : String
+ type : String

fie
ld

s

*

(a)

table : Table

name = cls.name

<<ForEach>>
OwnedAttributes

{
ForEach = attr,
OfType = Attribute,
InCollection = cls.myMembers
}

name = attr.name
type = attr.type

: f
ie

ld
s

<<ForEach>>
BaseClasses

{
ForEach = gen,
OfType = Generalization,
InCollection = cls.supertype
}

: fields primaryKey : Field

name = "ID"
type = "AutoNumber"

{ Cond = gen.inheritFieldsFromCommonTable }
name = "ID"+gen.supertype.name
type = "Long"

<<ForEach>>
InheritedAttributes

{
Cond = ! gen.inheritFieldsFromCommonTable,
ForEach =attr,
OfType = Attribute,
InCollection = gen.supertype.getAllMembers()
}

field : Field

name = attr.name
type = attr.type

: f
ie

ld
s

(b)

: f
ie

ld
s

field : Field

foreignKey : Field

Figure 4. Generation
of the relational 
database scheme
from a UML 
class model (this 
example focuses on 
inheritance): (a) 
the intermediate 
metamodel 
(relational); (b) 
the domain mapping
specification. 
The operation
ggeettAAllllMMeemmbbeerrss(())
returns the collection
of all owned and 
inherited members of a
GGeenneerraalliizzaabblleeEElleemmeenntt.
The source domain’s
metamodel is the
UML metamodel (not
shown here).



Transformations Using Extended UML Ob-
ject Diagrams in Modeling Environments.”4

The first student project focused on state
machine modeling. The source domain
metamodel had four classes and six associa-
tions. The mapping specification had ap-
proximately 30 instances and seven ForEach
packages. It took a student about three days
to implement and test the code generator, al-
though she manually created the translator
from the source domain into OOPL from the
mapping specifications (the supporting tool
was not ready at that time).

The next project dealt with metamodel-
ing the structural part of the real-time ob-
ject-oriented modeling (ROOM) method.5

The source domain metamodel had 19 classes
and 26 associations. The domain mapping
specifications had five diagrams, approxi-
mately 40 instances, and five ForEach pack-
ages. The student developed it in approxi-
mately five days.

The third project dealt with hardware
logic design, in the manner usually sup-
ported by common digital-circuit-modeling
tools. The goal was to support modeling the
circuit structure and behavior and their sim-
ulation using the generated C++ code. The
source domain metamodel had 12 classes
and eight associations. The mapping specifi-
cations consisted of three diagrams, approx-
imately 20 instances, and six ForEach pack-
ages, and took the student about three days
to develop.

These projects confirm that my approach
has specific advantages. For instance, we
started our state machines example using
the conventional hard-coded approaches. It
took us several weeks just to specify the out-
put generation, without even getting to the
tedious process of testing and debugging. By
using the mapping strategy with object dia-
grams, we reduced the working time to sev-
eral days.

Additionally, the projects’ results indi-
cated the relationship of the proposed ap-
proach with other frequently used code-gen-
eration techniques. Namely, the proposed
concepts of ForEach packages, conditional
creation, sequential dependencies, parame-
terized substructures, and recursion4 cover
all fundamental control-flow structures. This
is why even sequential-structures generation
(such as text) can be specified using only the
proposed concepts and diagrams. However,

our experience has proved that textual struc-
tures are more easily generated using the tra-
ditional scripting or template-oriented ap-
proaches,3 but only if the source and the
target domains are conceptually close. 

M y proposed approach is suitablefor bridging the gap between con-ceptually distant domains, while
the traditional approaches are more suitable
for direct generation of textual output from
conceptually simple domains. In other
words, this approach complements the ex-
isting output-generation techniques; it does
not replace them completely. Our current
work focuses on building a fully functional
metamodeling tool that supports the pro-
posed domain mapping method, along with
a repository of reusable metamodels and
mappings for commonly used modeling do-
mains and translations.

Acknowledgments
I am grateful to Dejan Marjanovic, Ljubica Lazare-

vic, Marko Zaric, and Ivan Djordjevic for their contri-
butions to this research and the case study. Thanks to
Dragan Bojic, Igor Tartalja, and Zoran Jovanovic from
the University of Belgrade, Juha-Pekka Tolvanen from
MetaCase Consulting, Bran Selic from Rational, H.S.
Lahman from Pathfinder Solutions, Christof Ebert
from Alcatel, and to the reviewers for their invaluable
comments on this article. 

References
1. Object Management Group, OMG Unified Modeling

Language Specification, v. 1.3, June 1999. 
2. E. Gamma et al., Design Patterns, Addison-Wesley,

Reading, Mass., 1995.
3. S. Shlaer and S.J. Mellor, “Recursive Design of an Ap-

plication-Independent Architecture,” IEEE Software,
vol. 14, no. 1, Jan. 1997, pp. 61–72.

4. D. Milicev, Automatic Model Transformations Using
Extended UML Object Diagrams in Modeling Environ-
ments, tech. report TI-ETF-RTI-00-0042, Univ. of Bel-
grade, School of Electrical Eng., 2000; www.rcub.bg.
ac.yu/~dmilicev. 

5. B. Selic, G. Gullekson, and P. Ward, Real-Time Object-
Oriented Modeling, Wiley & Sons, New York, 1994. 

For more information on this or any other computing topic, please visit our
Digital Library at http://computer.org/publications/dlib.

M a r c h / A p r i l  2 0 0 2 I E E E  S O F T W A R E 9 7

About the Author

Dragan Milicev is an assistant professor at the Department of Computer Science, School
of Electrical Engineering at University of Belgrade. His research interests include object-oriented
software engineering, model-based software development, metamodeling, and information sys-
tems. He has authored or coauthored three books in Serbian on object-oriented software pro-
gramming and modeling. He received his MSc and PhD in Computer Science from the University
of Belgrade. Contact him at dmilicev@rcub.bg.ac.yu; www. rcub.bg.ac.yu/~dmilicev.


