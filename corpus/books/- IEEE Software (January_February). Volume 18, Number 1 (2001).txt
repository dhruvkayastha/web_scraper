***- IEEE Software (January_February). Volume 18, Number 1 (2001)***




































C o p y r i g h t  ©  2 0 0 1  S t e v e n  C .  M c C o n n e l l .  A l l  R i g h t s  R e s e r v e d . J a n u a r y / F e b r u a r y  2 0 0 1 I E E E  S O F T W A R E 5

from the editor
E d i t o r  i n  C h i e f :  S t e v e  M c C o n n e l l  ■  C o n s t r u x  S o f t w a r e  ■   s t e v e m c c @ c o n s t r u x . c o m

T
he traditional distinction between soft-
ware and hardware was that software
was easily changeable and therefore
“soft,” whereas hardware was captured
on a physical medium like a chip, was
hard to change, and was therefore

“hard.” This traditional distinction is break-
ing down today. Software delivered via the In-
ternet is clearly “soft” in the traditional sense,

but software delivered via CD or
DVD is hardly “soft” in the sense
of being “easy to change.”

We now commonly see soft-
ware being delivered on EPROMs;
the electronic control module that
controls my car’s fuel injection is
an example. I can take my car to
my dealer to have the chip repro-
grammed, so in some sense the
program on the chip is soft, but is
it software? Should the chip devel-

opers be using software engineering?
Computer chip designers are now doing

much of their chip development using soft-
ware-engineering-like tools. Only at the last
minute is the code committed to silicon. Do
we really think that committing code to a
CD-ROM makes it software but committing
it to a silicon wafer makes it hardware?
Have we arrived at a point where even com-
puter hardware is really software?

If software and hardware are totally dif-
ferent, then electrical engineers designing
computer chips don’t need to know about
software engineering. But if modern chip de-
sign involves a significant amount of pro-
gramming, then perhaps electrical engineers
should know something about software en-

gineering. Should computer hardware be de-
signed using software engineering?

Throw a few other disciplines into the
mix such as Web programming and games
development, and I think a fundamental
question lurks here: What is software? This
question is important because it leads to a
second question: What is software engineer-
ing today, and who needs it? I recently posed
these questions to several IEEE Software
board members.

Blurred distinctions
Wolfgang Strigel: No doubt, the distinc-

tion between software and hardware has
blurred. When the term software was
coined, there was a clearer distinction, or
nobody cared because it sounded good and
made intuitive sense. Moreover, it is not im-
portant that software is modifiable (or
“soft” once it is completed). Software does
not change its nature by being turned into
something “hard” or unmodifiable. After
all, we have accepted the concept of selling
software on CDs. And RAM can also be
write-protected. What matters is whether
there is a “program” that can be executed
by a device.

The project plan for building a high-rise
is a set of complex instructions, decision
points, and so on that could be interpreted
as a software program. But it is not executed
by a device. But how about multimedia, say
an animated movie? It has instructions for
movement, rendering, and so on, and is ex-
ecuted by a device. It can also be modified.
How about a set of MIDI instructions that
produce music if delivered to an electronic

Who Needs Software 
Engineering?
Steve McConnell



6 I E E E  S O F T W A R E J a n u a r y / F e b r u a r y  2 0 0 1

FROM THE EDITOR

instrument? This does not fundamen-
tally differ from a ladder diagram
that controls the execution of a pro-
grammable logic controller.

Larry Graham: I agree that the
line between software and hardware
is blurry. Patent law has a fairly rich
tradition that equates the two—vir-
tually every hardware device can be
described in terms of a function it
performs and vice versa.

Is software engineering
invariant?

Annie Kuntzmann-Combelles: I
think the basic practices needed to de-
velop software properly are always
the same: get clear and complete re-
quirements from customers; manage
changes to requirements; estimate,
plan, and track the work to be done;
select an adequate life cycle; define
and perform QA activities; and main-
tain product integrity. The software
architecture and coding might differ
from one application to the other, but
the process aspects are invariant.

Tomoo Matsubara: I don’t think
software development is always the
same. My recommendation for im-
proving software process is to apply
domain-specific methodologies and
tools and conduct problem-focused
process improvement. The levels of
importance and priorities are differ-
ent between domains.

For example, one of the most crit-
ical processes for commercial soft-
ware is fixing data flow. For scientific
software, a key to success is choosing
the right algorithm. For COTS, it’s
designing a good human–machine in-
terface. For embedded systems, it’s
pushing instructions into the fewest
memory chips. For maintenance, it’s
rigorous testing with regression tests.
Software development practices should
vary accordingly.

Grant Rule: Think about the
games industry, where the “soft-
ware” is delivered on CD-ROM or
game cartridges. Game development
can take vast amounts of schedule.
Teams can be quite large—25 to 30
people from a variety of disciplines
including analysts, designers, coders,
testers, QA staff, and project man-

agement—and lots of nontraditional
software personnel such as writers,
artists, and so on. Schedules must be
managed carefully to meet holiday
sales seasons. If a game misses its
marketing window, it might be a
commercial failure; there’s no second
chance. Reliability is important: the
game is mass-produced on CD-
ROM, and from that point forward
there is no real chance to correct it—
it is infeasible to recall hundreds of
thousands of copies to fix a defect.

The result seems to be that, in some
cases at least, game developers take a
more rigorous approach to “engineer-
ing” their software than do some de-
velopers of commercial data-process-
ing applications. All this seems to be
“software engineering” to me.

What’s unique about
software?

Robert Cochran: I use the follow-
ing definition to describe what is
unique or special about software:

1. Software is intangible (which I think
is true even if it gets embedded).

2. It has high intellectual content
(many other intangibles have low
intellectual content).

3. It is generally not recognized as an
asset by accountants and so is off
the balance sheet.

4. Its development process is labor
intensive, team based, and project
based. We forget sometimes how
little of the rest of the world re-
gards projects as the normal way
to work.

5. Software doesn’t exhibit any real
separation between R&D and
production.

6. Software is potentially infinitely
changeable. Once you make a
physical widget, there are severe
limits on how you can change it.
In principle, we can keep changing
software forever.

Is there any real distinction between
printing out a source code listing, cre-
ating a binary, or burning software
onto a chip or CD-ROM? In all these
cases, we are just “fixing” a copy of
the software in some form that cannot

DEPARTMENT EDITORS

Bookshelf: Warren Keuffel, 
wkeuffel@computer.org

Country Report: Deependra Moitra, 
d.moitra@computer.org

Design: Martin Fowler, ThoughtWorks,
fowler@acm.org

Loyal Opposition: Robert Glass, Computing Trends,
rglass@indiana.edu

Manager: Don Reifer, Reifer Consultants,
dreifer@sprintmail.com

Quality Time: Jeffrey Voas, Cigital, 
voas@cigital.com

STAFF

Group Managing Editor
Dick Price

Senior Lead Editor 
Dale C. Strok

dstrok@computer.org

Associate Lead Editors
Crystal Chweh, Jenny Ferrero, and 

Dennis Taylor

Staff Lead Editor
Shani Murray

Magazine Assistants
Dawn Craig and Angela Williams

software@computer.org

Art Director
Toni Van Buskirk

Cover Illustration
Dirk Hagner

Technical Illustrator
Alex Torres

Production Artists
Carmen Flores-Garvey and Larry Bauer

Acting Executive Director
Anne Marie Kelly

Publisher
Angela Burgess

Membership/Circulation
Marketing Manager
Georgann Carter

Advertising Assistant
Debbie Sims

CONTRIBUTING EDITORS

Denise Hurst, Kirk Kroeker, Nancy Mead, Kalpana
Mohan, Ware Myers, Paula Powers, Judy Shane, 

Gil Shif, Tanya Smekal, Margaret Weatherford

Editorial: All submissions are subject to editing for clarity,
style, and space. Unless otherwise stated, bylined articles
and departments, as well as product and service descrip-
tions, reflect the author’s or firm’s opinion. Inclusion in
IEEE Software does not necessarily constitute endorsement
by the IEEE or the IEEE Computer Society.

To Submit: Send 2 electronic versions (1 word-processed
and 1 postscript or PDF) of articles to Magazine Assistant,
IEEE Software, 10662 Los Vaqueros Circle, PO Box 3014,
Los Alamitos, CA 90720-1314; software@computer.org. Ar-
ticles must be original and not exceed 5,400 words including
figures and tables, which count for 200 words each. 



J a n u a r y / F e b r u a r y  2 0 0 1 I E E E  S O F T W A R E 7

FROM THE EDITOR

directly or easily be modified. That
does not have any relevance to the na-
ture of the software as such.

Grant: That makes software just
like any written work, art book, or
design drawing. The medium (tech-
nology) might differ—wax tablets,
canvas, paper—but anything that can
have multiple copies made in a muta-
ble medium sounds like it could be
this thing called “software.”

Martin Fowler: Robert’s Number
5 seems to be the key point. Until you
deploy you can build, modify, and
evolve the software, regardless of
whether you eventually deploy to
PROMs or CD. That ability to
evolve while building is a key aspect
of software that has no equivalent in
disciplines where you must separate
design from construction.

This question of how “soft” is
software is quite an important point
and one that gels particularly with
me. One of the reasons that I’m so
much in favor of light methods is
that they try to make software softer
while many methodologies try to
make it harder. In the information
systems world, softness is an impor-
tant and valuable asset.

Making software softer
Steve McConnell: That does seem to

be the age-old challenge: How do you
keep software from becoming brittle?
How do you keep it soft? Whether
you’re creating software, or a com-
puter chip, or even a building, it seems
as though it would be advantageous to
keep the thing you’re building “soft” as
far as possible into the project.

Terry Bollinger: This overall ques-
tion helps to deal with trying to under-
stand the baffling diversity of produc-
tion styles in the software marketplace.
“Hard” software such as that found in
processor chips is catastrophically ex-
pensive to fix after fielding, and so dri-
ves the entire software design process
to be very conservative and validation
intensive. “Fluid” software that can be
changed automatically over the Inter-
net drives the opposite behavior. I think
that understanding these kinds of is-
sues is quite fundamental to software
management.

I do think we need to have a better
overall definition of software. The
very fact that I had to mangle to-
gether a phrase as awful as “hard”
software to describe algorithms en-
coded into silicon shows that the in-
dustry has become more complex
than it was in the days when you had
vacuum tubes and bits on punched
cards, and not much in between.

I think the distinction needs to fo-
cus on the idea of “information ma-
chines”—what we have traditionally
called software—versus the particu-
lar method of distribution and up-
date of such machines. Those are two
separate dimensions, not one. A
bunch of gears is not an information
machine, because it relies primarily
on physical materials for its proper-
ties, even if it happens to do a little
information processing at the same
time (for example, odometers in
cars). A structured algorithm masked
into an addressable array that is an
integral part of a processor chip most
emphatically is an information ma-
chine, because its complete set of
properties can be represented as
structured binary information only,
without any reference to the chip’s
physical properties.

Don Bagert: In defining what’s re-
ally software, my thought is to look at
what the object code does (rather than
where it resides), as well as the source
code. I would define it as follows:
“Software is a set of instructions that
are interpreted or executed by a com-
puter.” The source code is definitely
“soft” not in the sense of the deploy-
ment medium but in that it is a non-
physical entity. It consists of a series of
instructions, by definition nonphysical,
that can be translated into object code
or interpreted by a computer processor.
My colleague Dan Cooke has an inter-
esting view: a Universal Turing ma-
chine corresponds to computer hard-
ware, while an individual Turing
machine defined to solve a particular
problem corresponds to software.

What is software without
programming?

Steve: My original focus on the
medium on which the software hap-

EDITOR-IN-CHIEF: 
Steve McConnell

10662 Los Vaqueros Circle
Los Alamitos, CA 90720-1314

software@construx.com

EDITOR-IN-CHIEF EMERITUS:
Alan M. Davis, Omni-Vista

ASSOCIATE EDITORS-IN-CHIEF

Design: Maarten Boasson, Quaerendo Invenietis
boasson@science.uva.nl

Construction: Terry Bollinger, Mitre Corp.
terry@mitre.org

Requirements: Christof Ebert, Alcatel Telecom
christof.ebert@alcatel.be

Management: Ann Miller, University of Missouri, Rolla
millera@ece.umr.edu

Quality: Jeffrey M. Voas, Cigital
voas@cigital.com

EDITORIAL BOARD

Don Bagert, Texas Tech University
Andy Bytheway, Univ. of the Western Cape
Ray Duncan, Cedars-Sinai Medical Center
Richard Fairley, Oregon Graduate Institute

Martin Fowler, ThoughtWorks
Robert Glass, Computing Trends

Natalia Juristo, Universidad Politécnica de Madrid
Warren Keuffel, independent consultant
Brian Lawrence, Coyote Valley Software

Karen Mackey, Cisco Systems
Stephen Mellor, Project Technology

Deependra Moitra, Lucent Technologies, India
Don Reifer, Reifer Consultants

Wolfgang Strigel, Software Productivity Centre
Karl Wiegers, Process Impact

INDUSTRY ADVISORY BOARD

Robert Cochran, Catalyst Software, chair 
Annie Kuntzmann-Combelles, Q-Labs

Enrique Draier, PSINet
Eric Horvitz, Microsoft Research

David Hsiao, Cisco Systems
Takaya Ishida, Mitsubishi Electric Corp.

Dehua Ju, ASTI Shanghai
Donna Kasperson, Science Applications International

Pavle Knaflic, Hermes SoftLab
Günter Koch, Austrian Research Centers

Wojtek Kozaczynski, Rational Software Corp.
Tomoo Matsubara, Matsubara Consulting

Masao Matsumoto, Univ. of Tsukuba
Dorothy McKinney, Lockheed Martin Space Systems

Susan Mickel, AtomicTangerine
Dave Moore, Vulcan Northwest

Melissa Murphy, Sandia National Laboratories
Kiyoh Nakamura, Fujitsu

Grant Rule, Software Measurement Services
Girish Seshagiri, Advanced Information Services

Chandra Shekaran, Microsoft
Martyn Thomas, Praxis

Rob Thomsett, The Thomsett Company 
John Vu, The Boeing Company

Simon Wright, Integrated Chipware
Tsuneo Yamaura, Hitachi Software Engineering

MAGAZINE OPERATIONS COMMITTEE

Sorel Reisman (chair), James H. Aylor, Jean Bacon,
Thomas J. Bergin, Wushow Chou, George V. Cy-

benko, William I. Grosky, Steve McConnell, Daniel
E. O’Leary, Ken Sakamura, Munindar P. Singh, 

James J. Thomas, Yervant Zorian

PUBLICATIONS BOARD

Rangachar Kasturi (chair), Angela Burgess (pub-
lisher), Jake Aggarwal, Laxmi Bhuran, Lori Clarke,

Mike T. Liu, Sorel Reisman, Gabriella Sannitti
diBaja, Sallie Sheppard, Mike Williams,  Zhiwei Xu



8 I E E E  S O F T W A R E J a n u a r y / F e b r u a r y  2 0 0 1

FROM THE EDITOR

pens to be deployed seems to have
been a red herring. I think software
engineering applies broadly to creat-
ing complex “instructions,” regard-
less of the target media. Years ago,
people argued that the need for soft-
ware engineering had passed because
Fortran had been invented. People
didn’t have to write programs any-
more; they could just write down for-
mulas! Thirty years later, we now
think of Fortran programming as
comparatively low-level software en-
gineering—“writing down formulas”
is harder than it looks.

As the years go by, we see the same
argument repeated time and time
again. The early claims about creat-
ing programs using Visual Basic were
reminiscent of the early press on For-
tran—“No more writing code! Just

drag-and-drop buttons and dialogs!”
And today more new programs are
being written in Visual Basic than any
other language. Ten years from now,
we’ll probably see programming envi-
ronments that are much higher-level
than Visual Basic; people working in
those environments will benefit from
software engineering.

Martin: This is an important
point. Often people talk about things
“without programming” (one of my
alarm-bell phrases). You find this in
phrases like, “Buy our ERP system
and you can customize it for your
business without programming.” So
you end up with people who are ex-
perts in customizing XYZ’s ERP sys-
tem. Are they doing software engi-
neering? I would say yes, even
though their language is putting con-

figuration information into tables.
They still need to know about de-
bugging and testing.

Indeed, whenever we talk about
writing tools so that “users can con-
figure the software without program-
ming,” we run into the same prob-
lems. If you can enter programs
through wizards, you still have to be
able to debug and test the results.

Wolfgang: At the risk of being
simplistic, I would define software as
follows: “Software is a set of instruc-
tions that are interpreted (or exe-
cuted) by a computer.” I did not say
“electronic computer” because it
could well be a chemical computer or
one that operates with light. That
delegates the problem to defining the
term “computer”—and that’s a hard-
ware problem! 



1 6 I E E E  S O F T W A R E J a n u a r y / F e b r u a r y  2 0 0 1 0 7 4 0 - 7 4 5 9 / 0 0 / $ 1 0 . 0 0  ©  2 0 0 1  I E E E

W
hat is software quality assurance,
and where is it headed? Has it be-
come passé as firms embrace rapid
application development tech-
niques, modern tools, and spiral
development models? These are in-

teresting questions to ask in light of SQA’s
history, quality models, and technological
developments.

There is, of course, the old question: is SQA
a discipline or an organization? Matthew

Fisher and I discussed this issue
most recently in a chapter in the
Handbook of Software Quality
Assurance.1 Past and current prac-
tices contradict its status as a disci-
pline; object-oriented design is a
discipline, but it is difficult to cast
SQA in the same light. The fact
alone that many different organi-
zations practice SQA in many dif-
ferent ways makes it difficult to
characterize it as a discipline. 

Discipline or organization?
If we look at organizations in the commer-

cial world, especially those in the information
systems arena, SQA consists of testing—pri-
marily, system testing. In many cases, due to
poor project planning or project management,
little time is available to test adequately. Doc-
umentation of the requirements is often un-
available, so the test program’s ability to de-
tect software defects is suspect. Testing as
SQA is like locking the barn door after the
horse has escaped; it is hardly “assuring prod-
uct quality,” nor is it the discipline of SQA.

Members of organizations that have
adopted the Capability Maturity Model
(CMM) often view SQA as the “process po-

lice.” In this role, SQA determines whether the
developers (and project management) conform
to the process policies, standards, and proce-
dures. Work products are checked for compli-
ance with templates defining their content and
format. In these organizations, those organiza-
tions that have a separate SQA function might
have people with little software background
performing SQA. Deviations from the process
might not be as apparent to them as to more
knowledgeable SQA personnel.

The product assurance department I
managed several years back did a number of
things. It defined the organization’s process,
it checked for compliance with it, evaluated
work products for content quality and con-
formance to format and content require-
ments, moderated inspections, and did some
testing. Many other companies practiced
SQA this way, as well—very differently
from those I described earlier.

Today, a set of practices that tend to vary
quite a bit emerges, hardly fitting the de-
scription of a discipline. SQA is not always
an organization. Nor, when it is an organiza-
tion, is it necessarily an independent one—a
characteristic necessary to ensure objective
evaluations. In many organizations, develop-
ers time-share their activities. They spend
part of the time doing development work
and part doing whatever has been defined as
SQA for that organization. Clearly, there is
ambiguity on that score, as well.

Technological developments
What effect does technology have on the

practice of SQA? In an article on software
management, Walker Royce cites assessing
quality with an independent team and ex-
haustive inspection as principles of conven-

manager

Which Way, SQA?
Emanuel R. Baker

E d i t o r :  D o n a l d  J .  R e i f e r  ■ R e i f e r  C o n s u l t a n t s  ■ d . r e i f e r @ i e e e . o r g



J a n u a r y / F e b r u a r y  2 0 0 1 I E E E  S O F T W A R E 17

MANAGER

tional software management.2 This
approach is an outgrowth, he as-
serts, of the waterfall model of soft-
ware development. Using different
approaches, such as the spiral model
and other iterative development ap-
proaches, lets us use more
modern software manage-
ment techniques. Royce
states, 

Modern software develop-
ment produces the archi-
tecture first, followed by
usable increments of par-
tial capability, and then
completeness. Require-
ments and design flaws
are detected and resolved
earlier in the life cycle,
avoiding the big-bang inte-
gration at the end of a
project. Quality control
improves because system
characteristics inherent in
the architecture (such as
performance, fault tolerance, inter-
operability, and maintainability)
are identifiable earlier in the
process where problems can be
corrected without jeopardizing tar-
get costs and schedules.

He later argues that the conven-
tional principle of a separate quality
assurance results in “projects that iso-
late ‘quality police,’” and that “a bet-
ter approach is to work quality as-
sessment into every activity through
the checks and balances of organiza-
tional teams focused on architecture,
components, and usability.” 

Another fault of conventional soft-
ware management, he suggests, is slav-
ish insistence on requirements trace-
ability to design. Clearly, one of SQA’s
objectives is to ensure that the design
and resultant code implements the
agreed-upon requirements. The prob-
lem is that 

demanding rigorous problem-to-
solution traceability is frequently
counterproductive, forcing the
design to be structured in the
same manner as the requirements.
Good component-based archi-

tectures have chaotic traceability
to their requirements. Tight prob-
lem-to-solution traceability might
have been productive when
100% custom software was the
norm; those days are gone.

The QA role that emerges from these
discussions is one of evaluations
embedded in the development pro-
cess. The process Royce describes de-
emphasizes separate SQA and pro-
cess policing, rightly viewing them as
counterproductive.

E-commerce SQA
It’s interesting to look at Royce’s

approach in contrast to development
practices followed by e-commerce
software houses. We often hear
about doing things “at Internet
speed.” In e-commerce (and the prac-
tices of many shrink-wrap software
houses), that often involves getting
applications out the door as quickly
as possible to beat the competition.
Process is of little concern to many e-
commerce organizations. Few e-com-
merce firms have demonstrated any
interest in adopting quality models
such as the CMM. (For that matter,
few shrink-wrap software houses
have, either.) Consequently, the con-
sumer often gets flawed software. 

For a long time, liability was not
an issue; the consumer merely put
up with flawed software. However,
with the advent of the Internet and

e-commerce, liability is more of a
concern. An e-commerce applica-
tion’s failure to provide adequate
protection against credit card num-
ber theft, for example, raises seri-
ous liability issues. 

Various studies indicate
that 30% to 40% of soft-
ware projects fail, that up to
50% of the failures are due
to inadequate requirements
definition, and that up to
40% of the failures recorded
during a project’s life are at-
tributable to requirements
problems. Yet, e-commerce
firms develop applications at
Internet speed. Undoubtedly,
there is little time for ade-
quate requirements defini-
tion or for testing. These or-
ganizations risk incurring
significant liability losses.
Undoubtedly, they have
much to learn about adopt-
ing some of the quality prin-

ciples discussed above.

CMM and ISO models
What about quality models such as

the CMM or ISO 9000? In their cur-
rent versions, they impose QA over-
sight, with substantial reviews, au-
dits, and record keeping. The CMM
is probably used more widely in the
US than ISO 9001 or the companion
guideline, ISO 9000-3. The CMM re-
quires that an organization establish
an SQA function to achieve Level 2.
The role that it establishes for SQA is
essentially that of the “process po-
lice.” Don Reifer characterized a po-
tential problem in that role: slavish
enforcement of the process, whether
it makes sense or not.3 Adversarial
personnel relationships are still an-
other potential problem. A policing
posture can undermine team spirit.

The CMM also does not provide a
road map for SQA. As noted before,
it sets up SQA as the process police.
In general, the SQA Key Process Area
specifies that SQA performs reviews
and audits of process and product
compliance and identifies specific re-
views and audits under the Verifying
Implementation common feature ap-

Sally Lee



1 8 I E E E  S O F T W A R E J a n u a r y / F e b r u a r y  2 0 0 1

MANAGER

pearing in the remaining KPAs. The
SQA role should expand as the orga-
nization matures, because there are
more codified practices that the orga-
nization should follow as it advances
up the maturity ladder. In the CMM,
there is little sense of an evolving role.
The character of SQA’s role as the or-
ganization matures might be differ-
ent. For example, if process becomes
more a way of life as the organization
moves up the ladder, fewer reviews
and audits should be necessary, and
SQA might add value at Levels 4 and
5 in other ways. 

Does the Integrated CMM pro-
vide better guidance? It does define
an evolving role. It does, for exam-
ple, specify measurement and quanti-
tative process improvement roles at
Levels 4 and 5 for SQA (referred to
as Product and Process Quality As-
surance in the CMMI). However, the
CMMI’s framework makes it appear
to pertain only to measurement and
improvement of the SQA process.
Undoubtedly, to avoid too much
compartmentalization of roles, one
organization could accomplish many
of the measurement and quantitative
assessments of process specified at
Levels 4 and 5 for all the KPAs.

SQA’s future 
What, then, would be a meaningful

role for SQA in the future? It is clearly
desirable to embed reviews and evalu-
ations in the process. However, such
practices probably could not be car-
ried out in a consistent and effective
manner at the CMM’s lower levels;
there is just too little process culture in
most organizations at the lower levels. 

What I would propose, then, is the
scenario that follows, based on the
CMM. To achieve Level 2, the level at
which a project management disci-
pline is institutionalized, typically re-
quires a major shift in the organiza-
tion’s attitude to acceptance of process
discipline. Because organizations tend
to ignore process as much as possible
and focus on getting the software
product out the door, the SQA role
might need to emphasize the process
police and peacekeeper roles to help
instill a process-oriented culture within

the organization. This is probably less
true in going from Level 2 to Level 3,
the level at which a defined process ex-
ists at the organizational level. Because
Level 2 organizations sometimes back-
slide easily, SQA might still need to
pay a great deal of attention to process
compliance. In moving toward Level
3, the organization has come to appre-
ciate that process discipline has, in
fact, made life easier; SQA would be-
gin to shift, requiring the process po-
lice and peacekeeper roles to a lesser
degree. In achieving Level 3, the orga-
nization establishes a process data-
base, and SQA can begin assuming the
responsibility for collecting and ana-
lyzing process metrics.

Once an organization consistently
functions at Level 3 and is moving
ahead to Level 4, the need for
process police diminishes consider-
ably. Process discipline should be a
way of life, and verifying compliance
should be less necessary. Periodic
random checks could verify that
process conformance is still being
maintained. The SQA role could fo-
cus primarily on measuring the
process (and identifying corrective
action when  the process goes awry)
and monitoring the attainment of
numerical quality goals. 

At Level 5, an even further shift to
measurement occurs. The SQA role at
both Levels 4 and 5 can be that of
metrics collector and first-level analy-
sis performer. The first-level analysis
would focus on detection of trends to
be analyzed for identification of the
need for process correction. At Level
5, SQA would perform the first-level
analysis in assessing (undoubtedly, in
pilot applications) the efficacy of pro-
posed changes in technology or the
process. Those who are more expert
in specific processes or technology
would have the responsibility for fi-
nally determining the acceptability of
the changes.

Such a role would not have to be
totally independent. Evaluators could
act in a part-time SQA capacity and
embed evaluations in the process.
Those doing the metrics analyses
could also be developers acting in a
part-time SQA capacity. However,

those defining and organizing the
program and monitoring its perfor-
mance should be independent to en-
sure that it functions properly. This
group of people would not have to be
large at all. The role I’ve described
would result in cost-effective, unob-
trusive, value-added SQA.

The big picture
But this discussion still leaves  an-

other major issue not addressed—
something that might be referred to as
“big picture QA.” Embedding peer re-
views, inspections, audits, and so forth
into the process tends to create a mi-
croscopic view of quality. When we do
this, we are reviewing perhaps the de-
sign or the source code of single units
of code. We still need a macroscopic
view of quality. There are times when
we must step back and see the whole
picture to assess whether all the pieces
fit together properly. All the units
might be coded in accordance with the
design, but does the integrated design
satisfy the user’s or customer’s needs?
Is our drive to embed quality evalua-
tion processes creating other kinds of
quality problems? We still need the
more global project level reviews at
major decision points or milestones in
the project. 

Embedding quality evaluations
into the development process is cer-
tainly a desirable way to go. Let us
hope that the motivation for that
practice is not purely overhead re-
duction and that balanced views of
both macroscopic and microscopic
quality are taken into consideration
in structuring SQA programs.

References
1. E. Baker and M. Fisher, “Software Quality

Program Organization,” Handbook of Soft-
ware Quality Assurance, Schulmeyer and Mc-
Manus, eds., Prentice Hall, Upper Saddle
River, N.J., 1999, pp. 115–145.

2. W. Royce, “Software Management Renais-
sance,” IEEE Software, vol. 17, no. 4, July
2000, pp. 116–19. 

3. D. Reifer, “A Tale of Three Developers,”
Computer, vol. 32, no. 11, Nov. 1999, pp.
128–30.

Emanuel R. Baker is president of Software Engineer-
ing Consultants. Contact him at 10219 Briarwood Drive, Los 
Angeles, CA, 90077, erbaker@swengcon.com.



Introducing Usability

2 0 I E E E  S O F T W A R E J a n u a r y / F e b r u a r y  2 0 0 1 0 7 4 0 - 7 4 5 9 / 0 0 / $ 1 0 . 0 0  ©  2 0 0 1  I E E E

focus

One definition of usability is quality in
use.1 In simple terms, it reflects how easy
the software is to learn and use, how pro-
ductively users will be able to work, and

how much support users will need. A sys-
tem’s usability does not only deal with the
user interface; it also relates closely to the
software’s overall structure and to the con-
cept on which the system is based.

Usability is a difficult attribute to embed
in any system—not only software—and it
requires specific knowledge and a lot of
awareness about the user’s likings, require-
ments, and limitations. However, many
software developers would rather work
with machines than with people; they show
little interest in issues such as how much
data should appear on the screen at one
time. Additionally, many designers do not
realize that their perception of their cre-
ation does not provide much information
about how others will react to it. That is
why we get all those “perfectly obvious to
the designer” creations.

We should regard usability as one more
quality attribute for consideration during
software construction. Of course, we
shouldn’t concentrate on just a single qual-

Natalia Juristo, Universidad Politécnica de Madrid

Helmut Windl, Siemens AG

Larry Constantine, Constantine & Lockwood 

C
ertainly many of you have had enough frustrating experiences us-
ing software to acknowledge that usability strategies, models, and
methods are often not applied adequately during software con-
struction. Usability is not a luxury but a basic ingredient in soft-

ware systems: People’s productivity and comfort relate directly to the us-
ability of the software they use.

guest editors’ introduction



ity attribute when designing systems: com-
bining software characteristics poses the
real challenge.

Usability and software development
Integrating usability into the software de-

velopment process is not easy or obvious. Even
the companies that have usability departments
have difficulty resolving conflicts between us-
ability staff and software developers, based on
the groups’ different perspectives.

In the cases where software practitioners
have applied usability techniques, they tradi-
tionally have done it late in the development
cycle, when fixing major problems is costly.
Ensuring a certain degree of usability based
on the intended user’s work practices is very
important when designing a good system
concept or metaphor and must be integrated
early in the design process. Interaction de-
sign can greatly affect the application’s over-
all architecture. If you consider usability too
late in the life cycle, there is no time left to
really make a difference—you can’t just toss
it in at the last minute, any more than you
could a good database schema. 

However, when you do introduce usabil-

ity concepts into your organization, you can
cost-justify the investment,2,3 reduce devel-
opment time,3,4 increase sales, improve
users’ productivity, and reduce support and
maintenance costs. You can avoid conflicts
with usability staff either by integrating us-
ability experts into the development team or
by making some team members usability ex-
perts. Depending on how much money you
can invest, you can also avoid having to
build costly usability labs.

As IBM has stated, usability “makes
business effective. It makes business effi-
cient. It makes business sense.”5

References
1. ISO/IEC 14598-1, Software Product Evaluation: Gen-

eral Overview, Int’l Org. for Standardization, Geneva,
1999.

2. G. Bias and D. Mayhew, Cost-Justifying Usability, Aca-
demic Press, New York, 1994.

3. T. Gilb, Principles of Software Engineering Manage-
ment, Addison Wesley Longman, Boston, 1988.

4. M. Keil and E. Carmel, “Customer-Developer Links in
Software Development,” Comm. ACM, vol. 38, no. 5,
May 1995, pp. 33–44.

5. IBM, “Cost Justifying Ease of Use,” www-
3.ibm.com/ibm/easy/eou_ext.nsf/Publish/23 (current 2
Jan. 2001).

J a n u a r y / F e b r u a r y  2 0 0 1 I E E E  S O F T W A R E 21

In This Issue
Our aim in this special issue is to encourage software de-

velopers to listen more carefully to usability engineers and to
give usability a more meaningful place in the overall soft-
ware process.

Usability is not an abstract idea; applications already ex-
ist that demonstrate it well. This issue tries to promote the ap-
plication of that knowledge to a wider range of companies
and systems by emphasizing existing techniques rather than
research on new methods or models. Most of the articles fol-
lowing deal with real experiences.

The seven articles we have selected fall into four categories.
First, because of the low level of usability awareness and skills
among software engineers, we include a tutorial, “Usability
Basics for Software Developers” by Xavier Ferre, Natalia Ju-
risto, Helmut Windl, and Larry Constantine. This will help some
readers better understand the rest of the articles.

Second, the links between usability and business competi-
tion or market domains is an especially interesting topic. In
“Usability and the Bottom Line,” George M. Donahue dis-
cusses usability cost-effectiveness and describes how to per-
form a cost–benefit analysis. A company will often make this
the first step toward integrating usability into its processes.

Two articles discuss industrial experiences with usability and
software development. In “Usability in Practice: Three Case
Studies,” Karla Radle and Sarah Young present three real cases
of introducing usability engineering into an organization. Their
article points out common obstacles and describes some lessons

learned. In “Integrating Usability Techniques into the Software
Development Process,” Kathi Garrity, Francie Fleek, Jean An-
derson, and Fred Drake describe how two groups in their com-
pany, the Software Engineering Group and the User Perfor-
mance Group, came to understand each other’s processes,
vocabulary, and approaches. The article discusses the chal-
lenges they faced and the development process that resulted.

Because of the Web’s rapidly increasing significance in
software development, the fourth group of articles addresses
the role of usability engineering for Web applications. The
usability of Web sites and applications continues to be worse
than that of more traditional software. However, to be com-
petitive in e-business, usability is a must. In “A Global Per-
spective on Web Site Usability,” Shirley Becker and Florence
Mottay discuss how to assess Web application usability.
Molly Hammar Cloyd’s article “Designing a User-Centered
Web Application in Web Time” reports on a company’s ex-
perience transforming its development process from a tradi-
tional to a user-driven process.

Beyond usability, we have other issues to consider. As an
example, the one research article in this issue, “Engineering
Joy” by Marc Hassenzahl, Andreas Beu, and Michael
Burmester, talks about the joy of use—the task-unrelated as-
pects of quality called hedonic quality. After a brief overview
of research on the relationship between enjoyment and soft-
ware, this article shows that traditional usability engineering
methods are not suited to analyzing and evaluating hedonic
quality. These authors present promising new approaches.

About the Authors

Natalia Juristo, Helmut
Windl, and Larry 
Constantine’s biographies 
appear on page 29.



2 2 I E E E  S O F T W A R E J a n u a r y / F e b r u a r y  2 0 0 1 0 7 4 0 - 7 4 5 9 / 0 0 / $ 1 0 . 0 0  ©  2 0 0 1  I E E E

Contrary to what some might think, us-
ability is not just the appearance of the user
interface (UI). Usability relates to how the
system interacts with the user, and it includes
five basic attributes: learnability, efficiency,
user retention over time, error rate, and sat-
isfaction. Here, we present the general us-
ability process for building a system with the
desired level of usability. This process, which
most usability practitioners apply with slight
variation, is structured around a design-
evaluate-redesign cycle. Practitioners initiate
the process by analyzing the targeted users
and the tasks those users will perform.  

Clarifying usability concepts
According to ISO 9241, Part 11, usability

is “the extent to which a product can be used
by specified users to achieve specified goals
with effectiveness, efficiency, and satisfaction
in a specified context of use.”2 This definition
ties a system’s usability to specific conditions,
needs, and users—it requires establishing cer-
tain levels of usability based on the five basic
attributes.

Usability engineering defines the target us-
ability level in advance and ensures that the
software developed reaches that level. The
term was coined to reflect the engineering ap-
proach some usability specialists take.3 It is
“a process through which usability character-
istics are specified, quantitatively and early in
the development process, and measured
throughout the process.”4 Usability is an is-
sue we can approach from multiple view-
points, which is why many different disci-
plines, such as psychology, computer science,
and sociology, are trying to tackle it. Unfor-
tunately, this results in a lack of standard ter-
minology. In fact, the term usability engineer-
ing is not universally accepted—other terms
used include usage-centered design, contex-
tual design, participatory design, and goal-
directed design. All these philosophies adhere
to some extent to the core issue of usability
engineering: evaluating usability with real
users from the first stages of development.

Usability attributes
We can’t define usability as a specific as-

focus
Usability Basics for
Software Developers

Xavier Ferré and Natalia Juristo, Universidad Politécnica de Madrid

Helmut Windl, Siemens AG, Germany

Larry Constantine, Constantine & Lockwood

This tutorial
examines the
relationship
between usability
and the user
interface and
discusses how
the usability
process follows a
design-evaluate-
redesign 
cycle. It also
discusses some
management
issues an
organization
must face when
applying usability
techniques. 

I
n recent years, software system usability has made some interesting ad-
vances, with more and more organizations starting to take usability seri-
ously.1 Unfortunately, the average developer has not adopted these new
concepts, so the usability level of software products has not improved.

usability engineering



pect of a system. It differs depending on the
intended use of the system under develop-
ment. For example, a museum kiosk must run
a software system that requires minimum
training, as the majority of users will use it
just once in their lifetime. Some aspects of us-
ability—such as efficiency (the number of
tasks per hour)—are irrelevant for this kind
of system, but ease of learning is critical.
However, a bank cashier’s system would re-
quire training and would need to be highly ef-
ficient to help reduce customer queuing time.

Because usability is too abstract a term to
study directly, it is usually divided into the
attributes we mentioned at the beginning of
the article:5

■ Learnability: How easy it is to learn the
main system functionality and gain pro-
ficiency to complete the job. We usually
assess this by measuring the time a user
spends working with the system before
that user can complete certain tasks in
the time it would take an expert to com-
plete the same tasks. This attribute is
very important for novice users.

■ Efficiency: The number of tasks per unit
of time that the user can perform using
the system. We look for the maximum
speed of user task performance. The
higher system usability is, the faster the
user can perform the task and complete
the job.

■ User retention over time: It is critical for
intermittent users to be able to use the
system without having to climb the learn-
ing curve again. This attribute reflects
how well the user remembers how the
system works after a period of nonusage.

■ Error rate: This attribute contributes neg-
atively to usability. It does not refer to sys-
tem errors. On the contrary, it addresses
the number of errors the user makes while
performing a task. Good usability implies
a low error rate. Errors reduce efficiency
and user satisfaction, and they can be
seen as a failure to communicate to the
user the right way of doing things.

■ Satisfaction: This shows a user’s subjec-
tive impression of the system.

One problem concerning usability is that
these attributes sometimes conflict. For ex-
ample, learnability and efficiency usually in-
fluence each other negatively. A system must

be carefully designed if it requires both high
learnability and high efficiency—for exam-
ple, using accelerators (a combination of keys
to perform a frequent task) usually solves this
conflict. The point is that a system’s usability
is not merely the sum of these attributes’ val-
ues; it is defined as reaching a certain level for
each attribute.

We can further divide these attributes to
precisely address the aspects of usability in
which we are most interested. For example,
performance in normal use and advanced
feature usage are both subattributes of effi-
ciency, and first impression is a subattribute
of satisfaction. Therefore, when analyzing a
particular system’s usability, we decompose
the most important usability attributes
down to the right detail level.

Usability is not only concerned with soft-
ware interaction. It is also concerned with
help features, user documentation, and in-
stallation instructions.

Usability and the user interface
We distinguish between the visible part of

the UI (buttons, pull-down menus, check-
boxes, background color, and so forth) and
the interaction part of the system to under-
stand the depth and scope of a system’s us-
ability. (By interaction, we mean the coordi-
nation of the information exchange between
the user and the system.) It’s important to
carefully consider the interaction not just
when designing the visible part of the UI, but
also when designing the rest of the system.

For example, if a system must provide
continuous feedback to the user, the devel-
opers need to consider this when designing
the time-consuming system operations.
They should design the system so it can fre-
quently send information to the UI to keep
the user informed about the operation’s cur-
rent status. The system could display this in-
formation as a percentage-completed bar, as
in some software installation programs.

Unfortunately, it is not unusual to find de-
velopment teams that think they can design
the system and then have the “usability team”
make it usable by designing a nice set of con-
trols, adding the right color combination, and
using the right font. This approach is clearly
wrong. Developers must consider user inter-
action from the beginning of the development
process. Their understanding of the interac-
tion will affect the final product’s usability.

J a n u a r y / F e b r u a r y  2 0 0 1 I E E E  S O F T W A R E 23

It’s important 
to carefully
consider the
interaction 

not just when
designing 
the visible 
part of the 

user interface, 
but also when
designing the

rest of the
system.



Usability in software development
The main reason for applying usability

techniques when developing a software sys-
tem is to increase user efficiency and satis-
faction and, consequently, productivity. Us-
ability techniques, therefore, can help any
software system reach its goal by helping the
users perform their tasks. Furthermore, good
usability is gaining importance in a world in
which users are less computer literate and
can’t afford to spend a long time learning
how a system works. Usability is critical for
user system acceptance: If users don’t think
the system will help them perform their tasks,
they are less likely to accept it. It’s possible
they won’t use the system at all or will use it
inefficiently after deployment. If we don’t
properly support the user task, we are not
meeting user needs and are missing the main
objective of building a software system.

For a software development organization
operating in a competitive market, failure to
address usability can lead to a loss of market
share should a competitor release a product
with higher usability. Also, a software prod-
uct with better usability will result in re-
duced support costs (in terms of hotlines,
customer support service, and so forth).

Even if a system is being used, it does not
necessarily mean it has a high level of us-
ability. There are other aspects of a software
product that condition its usage, such as
price, possibility of choice, or previous
training. In addition, because users are still
more intelligent than computers, it is usu-
ally the human who adapts to the computer
in human–computer interaction. However,
we shouldn’t force the user to adapt to soft-
ware with poor usability, because this adap-
tation can negatively influence efficiency, ef-
fectiveness, and satisfaction. Usability is a
key aspect of a software product’s success.

The usability process
As we mentioned, a system’s usability de-

pends on the interaction design. Therefore,
we must deal with system usability through-
out the entire development process. Usabil-
ity testing alone is not enough to output a
highly usable product, because usability test-
ing uncovers but does not fix design prob-
lems. Furthermore, usability testing has been
viewed as similar to other types of software
quality assurance testing, so developers of-
ten apply the techniques late in the develop-

ment cycle—when major usability problems
are very costly, if not impossible, to fix.
Therefore, it is crucial to evaluate all results
during the product development process,
which ultimately leads to an iterative devel-
opment process. A pure waterfall approach
to software development makes introducing
usability techniques fairly impossible. 

All software applications are tools that
help users accomplish certain tasks. However,
before we can build usable software tools—
or, rather, design a UI—we need information
about the people who will use the tool:

■ Who are the system users?
■ What will they need to accomplish?
■ What will they need from the system to

accomplish this?
■ How should the system supply what

they need?

The usability process helps user interac-
tion designers answer these questions dur-
ing the analysis phase and supports the de-
sign in the design phase (see Figure 1). 

There are many usability methods—all
essentially based on the same usability
process—so we have abstracted a generic us-
ability process from the different approaches
to usability mentioned earlier. We hope this
makes it easier for the reader to understand
the different usability techniques we will be
describing.

Usability analysis phase
First, we have to get to know the users and

their needs, expectations, interests, behav-
iors, and responsibilities, all of which charac-
terize their relationship with the system. 

User analysis. There are numerous ap-
proaches for gathering information about
users, depending on each individual system
under development and the effort or time
constraints for this phase. The main meth-
ods are site visits, focus groups, surveys,
and derived data.

The primary source for user information
is site visits. Developers observe the users in
their working environment, using the sys-
tem to be replaced or performing their tasks
manually if there is no existing tool. In ad-
dition, developers interview the users to un-
derstand their motivation and the strategy
behind their actions. A well-known method

Usability testing
alone is not
enough to

output a highly 
usable product,

because it
uncovers but
does not fix

design
problems.

2 4 I E E E  S O F T W A R E J a n u a r y / F e b r u a r y  2 0 0 1



for doing user analysis jointly with task
analysis is contextual inquiry.6 This method
provides a structured way for gathering and
organizing information.

A focus group is an organized discussion
with a selected group of users. The goal is to
gather information about their views and
experiences concerning a topic. It is well
suited for getting several viewpoints about
the same topic—for example, if there is a
particular software product to discuss—and
gaining insight into people’s understanding
of everyday system use. 

In a survey, the quality of the information
depends on the quality of the questions.
Surveys are a one-way source, because it is
often difficult or even impossible to check
back with the user. Don A. Dillman’s book
Mail and Internet Surveys provides a struc-
tured method for planning, designing, and
conducting surveys.7

Derived data includes hotline reports,
customer complaint letters, and so forth.
It can be a good source of usability impli-
cations but is often difficult to interpret.
The most important limitation is that such
sources are one-sided. They report only
problems and say nothing about the features
that users liked or that enabled efficient use.

The most important thing about user
analysis is to record, structure, and organize
the findings.

Task analysis. Task analysis describes a set
of techniques people use to get things done.8

The concept of a task is analogous to the
concept of a use case in object-oriented soft-
ware development; a task is an activity
meaningful to the user. User analysis is
taken as input for task analysis, and both
are sometimes performed jointly.

We analyze tasks because we can use the
located tasks to drive and test UI design
throughout the product development cycle.
Focusing on a small set of tasks helps ra-
tionalize the development effort. Therefore,
we suggest prioritizing the set of tasks by

importance and frequency to get a small
task set. This approach guarantees that
you’ll build the most important functionali-
ties into the system and that the product
will not suffer from “featuritis.” These tasks
should be the starting point for developing
the system. One approach to analysis is to
build a task model within the Usage-
Centered Design method, a model-driven
approach for designing highly usable soft-
ware applications, where tasks, described as
essential use cases, are the basis for a well-
structured process and drive UI design.9

Task analysis ends when we evaluate the
discovered task set, which is best done collab-
oratively with users. When the user popula-
tion is already performing a set of tasks, we
perform task analysis during user analysis to
apprehend the tasks the user performs rou-
tinely and how the user perceives these tasks.
After the optional first analysis, we identify
the tasks our system will support, based on a
study of the goals the user wants to attain.
Then, we break the tasks into subtasks and
into particular actions that the user will per-
form and take the identified tasks as the basis
for building the usability specifications. We
then instantiate them to real-world examples
and present them to test participants in a us-
ability test.

Usability benchmarks. We set usability bench-
marks as quantitative usability goals, which
are defined before system design begins.10

They are based on the five basic usability at-
tributes or their subattributes.

We need these benchmarks because, if we
want to assess the value of the usability at-
tributes for the system under development,
we need to have a set of operationally de-
fined usability benchmarks. 

We establish usability benchmarks by
defining a set of benchmarks for each usabil-
ity attribute we want to evaluate—that is, for
each usability attribute we consider impor-
tant for our system. We must define the
benchmarks in a way that makes them calcu-

J a n u a r y / F e b r u a r y  2 0 0 1 I E E E  S O F T W A R E 25

User
analysis

Task
analysis

Usability
benchmarks

Conceptual
design

Visual
design

Evaluation

Analysis phase Design phase

Evaluation Evaluation

Figure 1. The 
usability process.



lable in a usability test or through a user sat-
isfaction questionnaire. Table 1 shows the
format of a usability specification table. (The
“Observed results” column is filled with the
data gathered during the usability tests.)

We take task analysis as an input for this
activity, because most usability benchmarks
are linked to a task specified in task analysis.

Usability design
Once we have analyzed the tasks our sys-

tem will support, we can make a first at-
tempt at the UI’s conceptual design, which
we will evaluate and possibly improve in the
next iteration.

Conceptual design. During the conceptual
design phase, we define the basic user–sys-
tem interaction and the objects in the UI
and the contexts in which interaction takes
place. The findings of the user and task
analysis are the basis for the conceptual de-
sign. The deliverables from this phase are
typically paper prototypes, such as pencil
drawings or screen mockups, and a specifi-
cation, which describes the UI’s behavior.

Conceptual design is the most crucial
phase in the process, because it defines the
foundation for the entire system. Unfortu-
nately, design is a very creative process, and
it can’t be automated with a method. There is
a set of design principles and rules that we
must creatively adapt for a certain design
problem. (A good reading for any designer—
not just software designers—is The Design of
Everyday Things,11 which presents general
design principles by evaluating the design of
everyday objects.)

The main principles of UI design cover
feedback, reuse, simplicity, structure, toler-
ance, and visibility in UIs. Knowing usabil-
ity design principles is the basis for good de-
sign. Compare this to an adult drawing
class. Not everyone will be Picasso by the
end of the course, but the students will be

able to paint reasonable pictures if they use
the principles they learned. Another way to
improve design ability is to examine UIs.
Analyzing the UIs of every software appli-
cation you can access is very helpful and can
sometimes be a source of inspiration for
finding innovative, alternative solutions.

The conceptual design phase also ends
with evaluating the results. It is a good idea to
test the paper prototypes against the defined
task set to check that all the prioritized tasks
can be enacted. The last test in this phase is
run together with users as a usability test or
usability inspection of the paper prototype.

Visual design. Having completed the concep-
tual design, the final step in our process is vi-
sual design, where we define the UI’s appear-
ance. This covers all details, including the
layout of screens and dialog boxes, use of
colors and widgets, and design of graphics
and icons. There are also rules and principles
for visual design, addressing use of color,
text, screen layout, widget use, icon design,
and so forth. It pays to have a professional
screen designer, especially in this phase. Rec-
ommended readings about visual and con-
ceptual design are About Face12 and Soft-
ware for Use,9 which both include numerous
design tips. Designing Visual Interfaces fo-
cuses on screen design and graphics design in
the context of UIs, as well as the underlying
principles of visual design.13

The deliverables of this phase are proto-
types that must be tested, an exact specifi-
cation of the UI appearance, and behavior
plus the specification for new widgets that
must be developed.

Prototyping
Prototypes are not exclusive to UI design,

but they are valuable for performing usabil-
ity testing in early development phases. We
need to build prototypes because abstract
technical specifications are not a good way of

2 6 I E E E  S O F T W A R E J a n u a r y / F e b r u a r y  2 0 0 1

Table 1
A Sample Usability Specification Table4

Usability Measuring Worst acceptable Best possible Observed 
attribute instrument Value to be measured Current level level Planned target level level results

Performance “Answer Length of time taken 2 min, 53 sec 2 min, 53 sec 1 min, 30 sec 50 sec
in normal use request” task to successfully perform 

the task (minutes and 
seconds)

First Questionnaire Average score — 0 1 2
impression (range –2 to 2)



communicating when we want to involve
users in the design process—users understand
tangible system prototypes much better.5

Some prototyping techniques help perform
usability testing and require little implemen-
tation effort. We create prototypes to test
them on the user through usability evaluation
techniques. The prototyping techniques with
which software developers usually are not fa-
miliar include

■ Paper mock-ups: At the beginning of the
design process, the designer creates pa-
per prototypes—usually pencil drawings
or printouts of screen designs—for the
user. The designer will act as the com-
puter, showing the user the next element
when a transition between graphical el-
ements occurs.8

■ “Wizard of Oz” technique:8 A human
expert acts as the system and answers
the user’s requests, without the user’s
knowledge. The user interacts normally
with the screen, but instead of using soft-
ware, a developer sits at another com-
puter (network-connected to the user’s
computer) answering the queries. The user
gets the impression of working with a
real software system, and this method is
cheaper than implementing a real soft-
ware prototype.

■ Scenarios, storyboards, and snapshots:
A scenario describes a fictional story of a
user interacting with the system in a par-
ticular situation; snapshots are visual
images that capture the interaction oc-
curring in a scenario; and storyboards8

are sequences of snapshots that focus on
the main actions in a possible situation.
They make the design team think about
the appropriateness of the design for a
real context of use, and they help make
the process user-centric.

Usability evaluation 
Usability evaluation is a central activity

in the usability process. It can determine the
current version’s usability level and whether
the design works.

Usability testing. The term usability testing
describes the activity of performing usability
tests in a laboratory with a group of users
and recording the results for further analy-
sis. We can’t predict a software system’s us-

ability without testing it with real users.
First, we must decide which groups of

users we want to use to test the system and
how many from each group we will try to
recruit as test participants. Then, we must
design the test tasks we’ll ask the partici-
pants to perform. We usually take them
from the results of the task analysis activity
and apply them to hypothetical real-life sit-
uations. Some characteristics of the test re-
quire consideration, such as

■ whether the participant can ask the
evaluator for help;

■ should two participants jointly perform
each test task to observe the remarks
they exchange in the process;

■ what information participants will receive
about the system prior to the test; and

■ whether to include a period of free sys-
tem access after completing the prede-
fined tasks to get the user’s overall im-
pression of the system.

After we prepare the test and recruit test
participants, we run the tests, optionally
recording them with video cameras or audio
recorders, and log the users’ actions in the
system for further analysis (also optional).
Once we have performed all the tests, we
analyze the data and gather results to apply
them in the next iterative cycle.

Thinking aloud. Formative evaluation seeks
to learn which detailed aspects of the inter-
action are good and how to improve the in-
teraction design.5 This opposes summative
evaluation, which is performed at the end of
the development process, after the system
has been built. The results of summative
evaluation do not help shape the product.

Thinking aloud helps perform formative
evaluation in usability tests. We ask the test
participant to think aloud while using the
system in a usability test,8 to verbalize his or
her actions so we can collect the remarks.
For example, a participant might say, “First,
I open the file, and I click once on the file
icon. Nothing happens. I don’t know why
this is not working like the Web. I press the
Enter key, and it opens. Now I want to
change the color of the label, so I search in
the Tools menu, but I can’t find any option
for what I want to do.” User remarks ob-
tained in usability tests can provide signifi-

J a n u a r y / F e b r u a r y  2 0 0 1 I E E E  S O F T W A R E 27

We can’t predict
a software
system’s
usability

without testing
it with real

users.



cant insight into the best way of designing
the system interaction. By detailing their
mental process, test participants can un-
cover hidden usability problems. 

Formative evaluation is the usual form of
evaluation in a usability process, combining
qualitative data gathered from user com-
ments with quantitative data to check against
previously defined usability benchmarks.

Heuristic evaluation. A usability expert can
perform a heuristic evaluation of the system
to make some development iterations
shorter and to perform more iterations in the
development process. The expert will make
a critique founded on both his or her inter-
action design experience and on generally
accepted usability guidelines, like the ones
by Ben Shneiderman14 and Jakob Nielsen.5

Experts provide a different kind of feed-
back than final users through usability test-
ing. Expert suggestions for modification are

usually more applicable, and they are more
precise about the underlying usability prob-
lems, such as a lack of consistency or poor
navigation. On the other hand, usability
testing must be performed with real users to
identify specific usability problems. Heuris-
tic evaluation can complement but not re-
place usability testing.

Collaborative usability inspection. A collabo-
rative usability inspection is a systematic ex-
amination of a finished system, design or
prototype from the end user’s viewpoint.9 A
team of developers, end users, application
or domain experts, and usability specialists
collaboratively perform the review. Collab-
orative usability inspections (CUIs) use fea-
tures and techniques from heuristic evalua-
tion, pluralistic usability walkthroughs, and
expert evaluations and are less expensive
and faster than usability testing. Behind this
technique is a set of strict rules to avoid the
problems that typically arise if end users dis-
cuss their work together with designers or
developers. CUIs uncover more—albeit dif-
ferent—usability defects (up to 100 defects
per hour) than usability testing. 

Apart from efficiency, one advantage is
that people with multiple perspectives and
expertise examine the test object. Another
advantage is that the participating develop-
ers build skills and know-how about how to
make software more usable.

Management and organizational
issues

When introducing usability, an organiza-
tion must first commit management to the
ideas behind the usability process and con-
vince them of its benefits.4,5 The newest
concepts they need to accept include creat-
ing conceptual design in the first stages of
development and evaluating usability
throughout the development process. Cost-
Justifying Usability presents cost-benefit ar-
guments in favor of performing usability
practices, which can be used when trying to
get management commitment.15 Another
option to convince management is to take a
recently developed system or one that is cur-
rently being developed and to perform
videotaped usability tests with a few users
who are novel to the system. Showing the
results to management and the development
team can produce a change of attitude to-

2 8 I E E E  S O F T W A R E J a n u a r y / F e b r u a r y  2 0 0 1

The following are books about human–computer interaction (HCI) and
usability that are more likely to interest software practitioners with little or no
knowledge about the field (the information for each book is in the Refer-
ences section of this article).

B. Shneiderman, Designing the User Interface: Strategies for Effective Hu-
man–Computer Interaction—This book summarizes all aspects related to
interactive systems from a serious scientific viewpoint, although some
readers might prefer a more engineering-focused approach. It includes a
valuable set of guidelines for designing the user interface. There have
been some interesting additions in the third edition about issues such as
hypermedia and the Web and Computer Supported Cooperative Work.

D. Hix and H.R. Haertsen, Developing User Interfaces: Ensuring Usability
Through Product and Process—Despite its title, this book is not just about
the user interface; it focuses on the process of user interaction design. Writ-
ten in a very practical style, it provides a hands-on approach to designing
the interactive part of a software system. Software practitioners might want
to skip the chapters devoted to the User Action Notation—a technique for
representing interaction designs—which is too formal for non-HCI experts.

L.L. Constantine and L.A.D. Lockwood, Software for Use: A Practical Guide
to the Models and Methods of Usage-Centered Design—The most recent of
the books reviewed here, it presents a process for designing usable soft-
ware systems based on one of the current trends in software engineering:
use cases. The book is written in a practical style that is likely to appeal to
software practitioners.

J. Nielsen, Usability Engineering—This book provides a good introduc-
tion to the issue of usability engineering. It is easy to read and includes
stories of real situations. It deals with a wide variety of issues related to
usability engineering—but none are addressed in depth.

Further Reading



ward usability testing, as the results will
probably show that the system is not as
good in usability terms as expected.

Integrating UI designers into the develop-
ment team isn’t always easy, especially if
they are assigned to several projects at
the same time. One approach to applying
usability techniques in some projects is
to promote one member of each develop-
ment team to usability champion,5 similar to
process improvement champions. Usability
champions learn the basic usability skills
and are coordinated by a user interaction
designer. The user interaction designer then
acts as a consultant in several projects but
can interact with the usability champion in
each group.4

Don’t try to do a full-scale usability process
from the beginning. You can start by setting
a small set of usability specifications with a
simple task analysis of the most prominent
tasks, some conceptual design with paper
prototypes and simple usability tests to be
carried out with a small set of users. You
can also act as usability expert performing
heuristic evaluation on the system using the
guidelines we mentioned (by Shneiderman14

and Nielsen5). Starting with modest objec-
tives will contribute more firmly to the final
success of your endeavor.

D espite increasing usability awarenessin software development organiza-tions, applying usability techniques
in software development is not easy. Soft-
ware engineers and usability engineers have
a different conception of software develop-
ment, and conflicts can arise between them
due to differences in terminology and pro-
cedures. To create acceptable usability con-
cepts, the software engineering community
must integrate usability techniques into a
software engineering process that is recog-
nizable from both fields. Use cases offer a
good starting point, as they are the software
engineering construct closer to a usable soft-
ware development approach. 

References
1. L. Trenner and J. Bawa, The Politics of Usability,

Springer-Verlag, London, 1998. 
2. Ergonomic Requirements for Office Work with Visual

Display Terminals, ISO 9241-11, ISO, Geneva, 1998. 
3. M. Good et al., “User-Derived Impact Analysis as a

Tool for Usability Engineering,” Proc. CHI Conf. Hu-

man Factors in Computing Systems, ACM Press, New
York, 1986, pp. 241–246.

4. D. Hix and H.R. Hartson, Developing User Interfaces:
Ensuring Usability Through Product and Process, John
Wiley & Sons, New York, 1993. 

5. J. Nielsen, Usability Engineering, AP Professional,
Boston, Mass., 1993.

6. H. Beyer and K. Holtzblatt, Contextual Design: A Cus-
tomer-Centered Approach to Systems Design, Morgan
Kaufmann, San Francisco, 1997.

7. D.A. Dillman, Mail and Internet Surveys: The Tailored
Design Method, John Wiley & Sons, New York, 1999.

8. J. Preece et al., Human-Computer Interaction, Addison-
Wesley Longman, Reading, Mass., 1994.

9. L.L. Constantine and L.A.D. Lockwood, Software for
Use: A Practical Guide to the Models and Methods of
Usage-Centered Design, Addison-Wesley Longman,
Reading, Mass., 1999.

10. J. Whiteside, J. Bennett, and K. Holtzblatt, “Usability
Engineering: Our Experience and Evolution,” Hand-
book of Human-Computer Interaction, Elsevier North-
Holland, Amsterdam, 1988.

11. D.A. Norman, The Design of Everyday Things, Dou-
bleday, New York, 1990.

12. A. Cooper, About Face: The Essentials of User Interface
Design, IDG Books Worldwide, Foster City, Calif., 1995.

13. K. Mullet and D. Sano, Designing Visual Interfaces:
Communication Oriented Techniques, Prentice Hall,
Upper Saddle River, N.J., 1994.

14. B. Shneiderman, Designing the User Interface: Strate-
gies for Effective Human-Computer Interaction, Addi-
son-Wesley Longman, Reading, Mass., 1998.

15. R.G. Bias and D.J. Mayhew, Cost-Justifying Usability,
Academic Press, Boston, Mass., 1994.

J a n u a r y / F e b r u a r y  2 0 0 1 I E E E  S O F T W A R E 29

About the Authors
Xavier Ferré is an assistant professor of software engineering at the Universidad Politéc-
nica de Madrid, Spain. His primary research interest is the integration of usability techniques
into software engineering development practices. He has been a visiting PhD student at CERN
(European Laboratory for Particle Physics) and at the HCIL (Human–Computer Interaction Lab-
oratory) at the University of Maryland. He received an MS in computer science from the Uni-
versidad Politécnica de Madrid. He is a member of the ACM and its SIGCHI group. Contact him
at xavier@fi.upm.es. 

Larry Constantine is director of
research and development at Constantine & Lockwood, a training and consulting firm. He is
also an adjunct professor in the School of Computing Sciences at the University of Technology,
Sydney, where he teaches software engineering and managing organizational change, and he
is on the faculty of the Cutter Consortium. He has authored or coauthored 10 books, including
Software for Use: A Practical Guide to the Methods and Models of Usage-Centered Design (Ad-
dison Wesley Longman, 1999). Contact him at Constantine & Lockwood, 58 Kathleen Circle,
Rowley, MA 01969; larry@foruse.com; www.foruse.com.

Helmut Windl leads the User Interface Design Group for Simatic Automation Software at
Siemens’ Automation & Drives Division, where he has helped define and implement a struc-
tured usability process within the software development process. He is an experienced user-
interface and visual designer for large-scale software applications and a project leader for 
usability-focused products. He is also a trainer and presenter in Siemens AG and with 
Constantine & Lockwood. He received a diploma in electrical engineering from the University
of Applied Sciences Regensburg. Contact him at Siemens AG, A&D AS S8, PO Box 4848, 
D-90327 Nuremberg, Germany; helmut.windl@nbgm.siemens.de.

Natalia Juristo is a full professor in
the Computer Science Department at the Universidad Politécnica de Madrid, where she directs
master’s-level courses in knowledge engineering and software engineering. She is also an edi-
torial board member of IEEE Software and the International Journal on Software Engineering
and Knowledge Engineering. She has a BS and PhD in computer science from the Technical
University of Madrid. She is a senior member of the IEEE Computer Society and a member of
the ACM, the American Association for the Advancement of Science, and the New York Acad-
emy of Sciences. Contact her at the Facultad de Informática UPM, Campus de Montegancedo,
s/n, Boadilla del Monte, 28660 Madrid, Spain; natalia@fi.upm.es.



Additionally, it can shorten development
time and improve a product’s marketability.

Here, I will show how you can use a
cost-benefit analysis to sell usability engi-
neering based on the bottom line. I’ll then
discuss the broader benefits a company can
realize from usability engineering.

Cost–benefit analysis 
Although usability’s broad benefits are

impressive, a cost-benefit analysis might be
a necessary first step in introducing usability
into your organization or a particular proj-
ect. In usability cost-benefit analyses, the
goal is to estimate the costs and benefits of
specific usability activities—such as proto-
typing, usability testing, heuristic evalua-
tion, and so on—and contrast them with the
likely costs of not conducting the activities.

The analysis has four steps:

■ selecting a usability technique,

■ determining the appropriate unit of
measurement,

■ making a reasonable assumption about
the benefit’s magnitude, and

■ translating the anticipated benefit into a
monetary figure.

In a usability cost-benefit analysis, it’s
also important to focus on the techniques
and benefits likely to yield the most value
for, and seem most persuasive to, the people
you’re trying to persuade. As Deborah May-
hew and Marilyn Mantei put it, you should
“decide the relevant audience for the analy-
sis and then what the relevant categories of
benefits are for that audience, because not
all potential benefits are relevant to all audi-
ences.”1 For example, a commercial soft-
ware company might be more interested in a
cost-benefit analysis that focuses on usabil-
ity’s potential for reducing development
costs and increasing customer satisfaction

There’s little
debate that
usability
engineering
benefits end
users, but its
benefit for
companies and
the people who
work for them 
is less widely
known. The
author discusses
these broader
usability benefits
and also how 
to use a cost-
benefit analysis
to demonstrate
the value of
usability to 
your company’s
bottom line.

U
sability engineering benefits end users. Few people disagree with
that idea. However, usability’s beneficiaries also include system
developers and the companies they work for. Improving usabil-
ity—whether of IT systems, e-commerce Web sites, or shrink-

wrapped software—is not only highly cost-effective, but it can also re-
duce development, support, training, documentation, and maintenance costs.

George M. Donahue, Sapient

Usability and the 
Bottom Line

focus

0 7 4 0 - 7 4 5 9 / 0 0 / $ 1 0 . 0 0  ©  2 0 0 1  I E E E J a n u a r y / F e b r u a r y  2 0 0 1 I E E E  S O F T W A R E 31

usability engineering



than in an analysis that focuses on its poten-
tial to improve end-user productivity. 

To illustrate, I offer the following scenario,
based on a method developed by Mayhew and
Mantei. While the scenario’s anticipated bene-
fit is a productivity improvement on an inter-
nally developed IT system, you can use the
same methodology to perform usability
cost-benefit analyses for other benefits and
in different organizations, including e-com-
merce and commercial software companies.

For this scenario, assume that you work
at Pretty Good Systems. You are aware that
the human resources department has lodged
complaints against PGS’s internally devel-
oped human resource system, Getting In
Good (GIG). You’re also aware that devel-
opment on GIG Version 2 is about to begin.
You suspect that improving GIG’s usability
would not only make GIG users’ lives easier,
it could save the company money—and PGS
management is very interested in reducing
costs. Before you broach the subject of im-
proving GIG’s usability, you wisely decide
to do some preliminary usability work fol-
lowed by a cost-benefit analysis. 

Preliminary work 
To start, you interview the HR director

and several GIG users. They all say that
GIG is too complicated. They can’t under-
stand why there’s one screen for entering
new applicant data, another for entering
data about applicants who have been inter-
viewed, another if an applicant is hired, and
so on. “There’s simply not that much appli-
cant data,” a GIG user tells you. “I don’t see
why we need so many different screens. It
takes so long to go through them all.” Next
you spend an afternoon observing HR staff
using the system. You then sketch out some
low-fidelity, paper prototypes of how a sin-
gle GIG data-entry screen might look and
try out the prototypes on a few HR staffers
in an informal usability test. Your prelimi-
nary usability work supports your hypothe-
sis: GIG’s user interface is inefficient. 

Usability-aware person that you are, you
also interview other significant GIG stake-
holders—namely, the GIG development
manager and some GIG developers. You
then discuss the option of entering applicant
data on a single screen, as opposed to sev-
eral. The developers tell you that, from a
technical standpoint, it’s easier to “modu-

larize” applicant information according to
the applicant’s status in the hiring process.
Nonetheless, you ask if it’s possible to use a
one-screen-per-applicant approach in GIG’s
next version. The development manager
says that they could do it, but that it would
take at least 30 more person-hours.

Estimating costs
You’re now ready to do the cost-benefit

analysis. First, you estimate how much it
costs to process a job application in GIG’s
current version. Based on your interview
with the HR manager, you know that it
takes an average of four hours per applicant
and that the average loaded salary of a GIG
data-entry person is $25 an hour. You mul-
tiply $25 by four to get the cost of process-
ing a single application: $100.

On average, PGS receives about 1,000
job applications a year. Multiplying $100 by
1,000 gives you the current average annual
cost of processing job applications at PGS:
$100,000.

To be on the safe side, you assume that
making the programming change to GIG
will take 40 hours, rather than the 30 hours
estimated by the development manager. You
then multiply 40 hours by the loaded aver-
age salary of a developer at PGS ($60 an
hour) to determine the cost of making the
change: $2,400.

Estimating benefits
Your preliminary usability work suggests

that adopting a one-screen-per-applicant ap-
proach would cut application-processing
time in half. This is your unit of measure-
ment for usability. However, to be cautious,
you assume a 25-percent reduction in pro-
cessing time. Given this, on average, a person
could process an application in three hours
rather than four. Given that the average
loaded salary of a data entry person is $25 per
hour, you estimate a cost of $75 to process a
single job application in the new system.

To estimate the overall savings in the av-
erage annual cost of processing job applica-
tions at PGS, you multiply $75 by 1,000.
The result is $75,000—$25,000 a year less
than the current average processing costs.
You then factor in the cost of making the
changes ($2,400) and subtract it from the
anticipated first-year savings, giving you a
first-year benefit of $22,600. 

In usability
cost-benefit
analyses, the

goal is to
estimate the

costs and
benefits of

specific
usability

activities and
contrast them
with the likely

costs of not
conducting 

the activities.

3 2 I E E E  S O F T W A R E J a n u a r y / F e b r u a r y  2 0 0 1



However, you’re not quite finished. The
typical lifespan of a PGS system is three
years. Because the cost of making the
changes will be incurred once, you need only
deduct that cost for one year, whereas the
benefit will be realized each year that the new
version is used. Thus, you add the first-year
benefit ($22,600) to the benefit for the sec-
ond and third years ($50,000), and you get a
total lifetime usability benefit of $72,600.

The overall result is a cost-benefit ratio
of 1:301/4. That is, $72,600 ÷ $2,400 =
$30.25. 

In this example, the benefits to the HR de-
partment are different than the benefits to
PGS as a whole. Because HR’s data-entry
process will be streamlined, GIG2’s users
should be able to get more work done and
their main benefits are higher productivity
and better job satisfaction. For PGS’s devel-
opment managers (and presumably also its
executives and shareholders), the main bene-
fit is decreased costs. Although managers, ex-
ecutives, and shareholders may be happy to
hear that usability engineering will improve
job satisfaction, your cost-benefit analysis
should focus on the cost savings, because
that’s the most relevant benefit to develop-
ment managers—and their buy-in is crucial.

Broader usability benefits
According to the above analysis, every

dollar spent on usability offers a return of
$30.25. That’s a nice return-on-investment.
Nonetheless, considering usability solely
from an ROI perspective does not give it its
full due. 

Usability costs are typically seen as addi-
tional; that is, if development doesn’t include
any formal usability activities—such as us-
ability testing or prototyping—there won’t
be any usability costs. That assumption is
wrong. Every software product has a user in-
terface, whether it’s usability-engineered or
not. The interface takes time to build, re-
gardless of whether it’s consciously engi-
neered for usability. Time is money. In other
words, user-interface costs are inevitable and
intrinsic to development. Many systems also
have support, documentation, maintenance,
and other costs. These are also usability ex-
penditures, and regardless of how such costs
appear on the company’s books, usability
engineering can help manage them. 

Although the ROI argument is compelling,

your usability case can be bolstered by point-
ing out that the company always spends
money on usability, even though it may not
see it this way. In his article, Arnold M. Lund
cost-justifies the existence of a permanent us-
ability group within an organization over car-
rying out discrete usability activities.2 I’ll now
examine several broader usability benefits in
more detail.

Reduced development and maintenance costs 
Software development projects typically

overrun their budgets and schedules. Such
overruns are often caused by overlooked
tasks and similar problems, which tech-
niques such as user analysis and task analy-
sis can address.

When you focus on real user needs and
understand the people you’re designing for,
the result is often fewer design arguments
and fewer iterations. Usability techniques,
such those described in the sidebar, “Basic
Usability Engineering,” are also highly effec-
tive in helping you detect usability problems
early in the development cycle, when they’re
easiest and least costly to fix. By correcting
usability problems in a project’s design
phase, for example, American Airlines re-
duced the cost of those fixes by 60 to 90 per-
cent.1 One frequently referenced study
found that correcting a problem once a sys-
tem is in development costs 10 times as
much as fixing the same problem in the de-
sign stage. Once a system is released, the cost
to fix a problem increases to 100 times that
of a design-stage fix. This study also found
that 80 percent of software life-cycle costs
occur during the maintenance phase; many
maintenance costs are associated with user
requirements and other problems that us-
ability engineering can prevent.3

Whether your company does usability
testing or not, your customers will, in effect,
usability-test the system. Ultimately, of
course, relying on such “usability testing by
default” risks angering customers, and, as
the studies above show, post-release prob-
lems cost much more to fix. A printer man-
ufacturer, for example, released a printer
driver that many users had difficulty in-
stalling. More than 50,000 users called the
support desk for assistance, costing the com-
pany nearly $500,000 a month. To correct
the situation, the manufacturer sent out let-
ters of apology and patch diskettes (at a cost

J a n u a r y / F e b r u a r y  2 0 0 1 I E E E  S O F T W A R E 33

Whether your
company does

usability testing
or not, your

customers will,
in effect,

usability-test
the system.
Ultimately,

relying on such
“usability
testing by

default” risks
angering

customers.



of $3 each). In all, they spent $900,000 on
the problem. The company ran no user test-
ing of the driver before its release. As one re-
searcher put it, “The problem could have
been identified and corrected at a fraction of
the cost if the product had been subjected to
even the simplest of usability testing.”1

Improved productivity and efficiency 
People tend to be more productive using

usability-engineered systems. This benefit
can be especially important in the context of
IT software systems. For example, a major
computer company spent $20,700 on usabil-
ity work to improve the sign-on procedure in
a system used by several thousand people.
The resulting productivity improvement
saved the company $41,700 the first day the
redesigned system was used. On a system
used by more than 100,000 people, for a us-
ability outlay of $68,000, the same company
recognized a benefit of $6,800,000 within
the first year of the system’s implementation.
A cost-benefit analysis of such figures results
in a cost-benefit ratio of $1:$100.1

Working with systems that have not been
usability engineered is often stressful. Alan
Cooper, “the father of Visual Basic,” worked
on a project to improve the usability of an air-
line in-flight entertainment system. IFEs are
devices connected through an onboard local
area network that provide movies and music
to travelers on transoceanic routes. One air-
line’s IFE was so frustrating for the flight at-
tendants that many of them were bidding to
fly shorter, local routes—which are usually
considered highly undesirable—to avoid hav-
ing to learn and use the difficult systems. “For
flight attendants to bid for flights from Den-
ver to Dallas just to avoid the IFE indicated a
serious morale problem.”4

When possible, people avoid using stress-
ful systems; if people must use such systems,
stress tends to undermine their productivity.
And, as Cooper’s anecdote illustrates, poor
usability can undermine morale. 

Reduced training costs
Usability-engineered systems can reduce

training needs. When user interface design is
informed by usability data and expertise, the
resulting interfaces often facilitate and rein-
force learning and retention, thereby reduc-
ing training time. At one company, only one
hour of end-user training was needed on a

3 4 I E E E  S O F T W A R E J a n u a r y / F e b r u a r y  2 0 0 1

Applying usability techniques—even if only in an informal, “guerilla”
manner—can offer many usability benefits. Here, I outline a few basic tech-
niques that are fundamental to usability engineering. For more information,
see books such as Jakob Nielsen’s Usability Engineering (Academic Press,
Boston, 1993), Ben Shneiderman’s Designing the User Interface (third edi-
tion, Addison Wesley Longman, Reading, Mass., 1997), or Mark Pearrow’s
Web Site Usability Handbook (Charles River Media, Rockland, Mass.,
2000). There are also many good usability Web sites, including www.stc.
org/pics/usability/topics/index.html, www.useit.com, and www.usableweb.
com.

User and task analysis
The focus in this technique is on interviewing the actual or intended

users. If the system does not yet exist, you can ask your marketing depart-
ment for customer profiles and use them to guide your recruitment effort.
(Because marketing departments typically think of people as customers,
rather than users, it’s important that you ask specifically for “customer
profiles, as asking for “user profiles” will most likely produce only quizzi-
cal looks.) Once you’ve recruited users, ask them to explain what they use
the system for, what they most like about it, what they don’t like about it,
and so on. If there is no digital system in place, ask users questions about
the current, manual process of completing the tasks. Next, observe them
using the system (or completing the tasks manually). You can then ask
them questions based on your observations, such as, “When you were us-
ing the XYZ Screen, you said, ‘I always get mixed up here.’ Could we go
back to that screen now so you can show me exactly where it is you get
mixed up?”

Low-fidelity prototyping
With this technique, the focus is on user interaction with designs, screens,

or pages. You begin by sketching system screens or site pages, preferably
on paper. The less “finished-looking” your designs are, the better. Users are
typically more candid with rough, high-level interaction designs that look as
if they didn’t take much work. Focus your prototypes on the screens or
pages that are commonly used or that you think users might find difficult to
work with. It’s unlikely that you’ll be able to prototype and test the entire
user interface.

Usability-testing the prototype
Once you’ve designed your low-fidelity prototype, usability-test it with

three to five users or intended users. To do this, you first write a usability test
script with tasks for the test participants to perform using the prototyped
pages. Although the prototype is low-fidelity, ask users to interact with it as
if it were a functional system. For example, if it’s a paper prototype, have
them use their finger for the mouse and say “I’d click here to display my
most recent transactions,” and so on. If they have problems completing the
task, note what the problems are.

Once you’ve completed testing, review the findings for patterns and to
understand why people had the problems they did. Next, think of alternative
designs that might eliminate the problems discovered in testing. Do this as
many times as necessary or possible until you have a design that facilitates
good user performance. Once you’ve done low-fidelity prototype usability
testing, you might want to conduct usability testing with an interactive,
higher fidelity system prototype. However, the main idea is to try to identify
usability bugs as soon as possible, which is why low-fidelity prototyping—
which you can do prior to coding—is so important.

Basic Usability Engineering



J a n u a r y / F e b r u a r y  2 0 0 1 I E E E  S O F T W A R E 35

When user
interface
design is

informed by
usability data
and expertise,
the resulting

interfaces often
facilitate and

reinforce
learning and

retention. 

usability-engineered internal system, in con-
trast to the full week of training for a prede-
cessor system that had no usability work. At
AT&T, usability improvements saved the
company $2.5 million in training expenses.1

Lower support costs
Providing telephone support for com-

puter software is estimated to cost compa-
nies between $12 and $250 per call, de-
pending on the organization.5 Such support
costs can add significantly to a system’s to-
tal cost of ownership and erode profits for
both the developing company and pur-
chaser alike. When a software product is
understandable and easy to learn, users
don’t need to call support as often. As a re-
sult, commercial software companies may
need fewer people to work the support lines
(and perhaps fewer DJs to entertain those
on hold). At Microsoft several years ago,
Word for Windows’s print-merge feature
was generating a lot of lengthy support calls
(45 minutes each, on average). As a result of
usability testing and other techniques, the
user interface for the feature was adjusted.
In the next release, support calls dropped
dramatically, and Microsoft’s savings were
substantial.1

Reduced documentation costs
Because usability-engineered systems

tend to have predictable and consistent in-
terfaces, they are relatively easy to docu-
ment. As a former technical writer, I can at-
test that user manuals and online help for
such systems are completed more quickly
and are less susceptible to inaccuracies than
those of difficult-to-document systems.
Also, usability-engineered systems often re-
quire less documentation, and that docu-
mentation tends to cost less to produce than
documentation for systems developed with-
out usability engineering. For example, one
company saved $40,000 in a single year
when usability work eliminated the need to
reprint and distribute a manual.1

Litigation deterrence 
Although software makers aren’t neces-

sarily subject to the same sorts of litigation
as, for example, a manufacturer of medical
equipment might be, poor usability is a po-
tential element in lawsuits and other litiga-
tion. For example, in July 2000, a US-based

Web consultancy was sued by a client com-
pany that accused it of creating a site-com-
ponent interface that was “unusable.”6

Even if the lawsuit was spurious, as the Web
consultancy contends, the situation points
to a new liability for software development
firms. Although usability engineering may
not prevent such lawsuits, companies that
can demonstrate that they applied usability-
engineering techniques during product de-
velopment might be less vulnerable should
such litigation occur. The US government’s
recent case against Microsoft hinged on a
usability question: Are users well-served
when the browser and operating system are
closely integrated? Although no usability
experts were called in to testify, they are
likely to be included in the future as usabil-
ity awareness increases.

Increased e-commerce potential
Though usability can benefit all develop-

ment organizations, perhaps nowhere is the
relationship between usability and prof-
itability as direct as in e-commerce, as For-
rester Research suggests:

Usability goals are business goals. Web sites
that are hard to use frustrate customers,
forfeit revenue, and erode brands. Execu-
tives can apply a disciplined approach to
improve all aspects of ease-of-use. Start
with usability reviews to assess specific
flaws and understand their causes. Then fix
the right problems through action-driven
design practices. Finally, maintain usability
with changes in business processes.7

Usability-engineered sites let users be
more efficient and productive. However, it’s
important that you interpret efficiency and
productivity in relation to online shopping.
Ideally, online shopping should be enjoy-
able, rather than frustrating: Users should
not have to waste time searching for mer-
chandise or figuring out how to buy it; nor
should they have any doubt that their
credit-card numbers and other personal in-
formation are secure. Buying a product or
service online should be superior to making
a purchase in a brick-and-mortar shop.  

More than 44 million people in the US
have made online purchases; 37 million
more say they expect to do so soon.8

However, many of these would-be online-
shoppers won’t succeed in making a Web



Usability is
important for
all Web sites,

but for 
e-commerce
sites, having 
a competitive

edge in
usability is

crucial. 

3 6 I E E E  S O F T W A R E J a n u a r y / F e b r u a r y  2 0 0 1

purchase, because e-commerce sites are,
for the most part, too difficult for the av-
erage user to navigate. Moreover, the ex-
perience of some online shoppers has been
so bad that they don’t want to buy online
again.3,7

Finally, online shoppers spend most of
their time and money at sites with the best
usability.9 Good navigation and site design
make it easier for users to find what they’re
looking for and to buy it once they’ve found
it. Usability can significantly improve the e-
commerce bottom line: According to Jakob
Nielsen, usability efforts can increase sales
by 100 percent.10

Competitive edge 
Users always cite ease-of-use as high on

their list of software system demands.1

Thus, giving users usability is giving them
what they want. Users appreciate software
and Web sites that don’t waste their time or
try their patience with complicated user in-
terfaces. Building usability into your soft-
ware tells users that you value their time
and don’t take them for granted. 

Usability is important for all Web sites,
but for e-commerce sites, having a competi-
tive edge in usability is crucial. Such sites
commonly drive away nearly half of repeat
traffic by making it hard for visitors to find
what they need.11 And, repeat customers are
the most valuable: New users at one e-com-
merce site spent an average of $127 per pur-
chase, while repeat users spent nearly twice
that.12

Usable e-commerce sites also build good-
will. Users recognize the effort put into
making their e-commerce experience easy
and efficient by returning to usable sites.
Moreover, one of the biggest obstacles to e-
commerce is trust. Consumers must trust a
site before they will disclose the personal
and financial information typically required
for online purchases. A study of e-com-
merce trust found that navigation and pres-
entation—both usability concerns—were
essential for creating trust.13

Advertising advantages 
High usability can garner attention for

your company’s Web site and help distin-
guish it from other sites. Improved usability
can also help differentiate commercial soft-
ware applications. Compaq, Microsoft, and

Lotus have all made usability part of their
advertising campaigns, for example.5 More
recently, a Swedish consultancy announced a
“usability guarantee” for sites it develops,
requiring large-scale projects to undergo
testing in a usability lab before they are re-
leased. If the project fails the test, the con-
sultancy promises to “improve the solution
without any additional costs to the client.”14

As another example, MacroMedia recently
issued a press release describing its “usabil-
ity initiative.” Though the initiative seems to
consist of posting basic usability tips on its
Web site, the fact that a large software com-
pany took such action suggests that compa-
nies might be realizing the advantage of be-
ing perceived as usability-aware.

These examples notwithstanding, and de-
spite its great potential, usability’s advertising
value remains largely unexploited. This seems
especially so in e-commerce, where users are
increasingly nontechnical consumers who
won’t suffer technical difficulties gladly. It
may behoove usability proponents to try to
increase advertising departments’ awareness
of usability’s value.

Better notices in the media
People in the media have discovered the

connections among usability, productivity,
and cost-effectiveness, especially on the In-
ternet. Companies are regularly taken to
task about usability in business publications
and on e-business sites. For example, CIO
Business Web Magazine pointed out, “On a
corporate intranet, poor usability means
poor employee productivity; investments in
making an intranet easier to use can pay off
by a factor of 10 or more, especially at large
companies.”15 The question arises: If those
in the media see increased productivity and
cost-effectiveness, can shareholders be far
behind?

In 1993, Nielsen studied the coverage of
usability issues in trade press reviews of new
software products and found that approxi-
mately 18 to 30 percent of the accounts were
usability-related.16 A good review in an in-
dustry publication can be worth millions in
advertising. Such reviews increasingly include
usability as a criterion. One of Internet
Week’s most popular columns features user-
interface design and usability specialists dis-
cussing the relative usability of various e-com-
merce and e-business sites, for example.



P erforming usability cost-benefitanalyses in your company could be afirst step toward introducing usabil-
ity engineering techniques, and thus the first
step toward realizing the benefits I outlined.
Working to improve usability can bring sig-
nificant economic benefits to companies that
develop IT applications, e-commerce sites,
and commercial software. Of course, users
of these systems benefit as well. 

References
1. R.G. Bias and D. J. Mayhew, eds., Cost-Justifying Us-

ability, Harcourt Brace & Co., Boston, 1994.
2. A.M. Lund, “Another Approach to Justifying the Cost

of Usability,” Interactions, vol. 4, no. 3, May/June
1997, pp. 48–56.

3. R.S. Pressman, Software Engineering: A Practitioner’s
Approach, McGraw Hill, New York, 1992.

4. A. Cooper, The Inmates Are Running the Asylum: Why
High-Tech Products Drive Us Crazy and How to Re-
store the Sanity, SAMS, Indianapolis, 1999.

5. M.E. Wiklund, Usability In Practice: How Companies
Develop User-Friendly Products, Academic Press,
Boston, 1994.

6. B. Berkowitz and D. Levin, “IAM Sues Razorfish for
Poor Design,” The Standard, 14 July 2000, www.
thestandard.com/article/display/0%2C1151%2C16831

%2C00.html (current 3 Jan. 2001).
7. H. Manning, “Why Most Web Sites Fail,” Forrester 

Research, Sept. 1998; view an excerpt or purchase the
report at www.forrester.com/ER/Research/Report/
Excerpt/0,1338,1285,FF.html (current 3 Jan. 2001). 

8. S. Wildstrom, “A Computer User’s Manifesto,” Busi-
ness Week, Sept. 28, 1998.

9. J. Nielsen, “The Web Usage Paradox: Why Do People
Use Something This Bad?” Alertbox, 9 Aug. 1998;
www.useit.com/alertbox/980809.html (current 3 Jan.
2001).

10. J. Nielsen, “Web Research: Believe the Data.” Alertbox,
11 July 1999; www.useit.com/alertbox/990711.html
(current 3 Jan. 2001).

11. H. Manning, “The Right Way To Test Ease-Of-Use.”
Forrester Research, Jan. 1999; view an excerpt or pur-
chase the report at www.forrester.com/ER/Research/
Brief/Excerpt/0,1317,5299,FF.html (current 3 Jan.
2001). 

12. J. Nielsen, “Loyalty on the Web,” Alertbox, 1 Aug.
1997, www.useit.com/alertbox/9708a.html (current 3
Jan. 2001).

13. Cheskin Research and Studio Archetype/Sapient, “E-com-
merce Trust Study,” 1999; www.studioarchetype.com/
cheskin/index.html (current 3 Jan. 2001).

14. “Icon Medialab Announces Usability Guarantee,” Icon
Media Lab, 29 Nov. 2000; www.iconmedialab.se/
default/news/press_releases/right.asp?id=310 (current 
3 Jan. 2001).

15. S. Kalin, “Mazed and Confused,” CIO Web Business
Magazine, 1 Apr. 1999; www.cio.com/archive/
webbusiness/040199_use.html (current 3 Jan. 2001).

16. J. Nielsen, “Is Usability Engineering Really Worth It?”
IEEE Software, vol. 10, no. 6, Nov. 1993, pp. 90–92. 

About the Author

George M. Donahue is a senior
experience
modeler at
Sapient. In
addition to
conducting
scores of us-
ability tests of
e-commerce,
financial, in-

teractive television, and new media Web
sites, he has designed user interfaces and
written user interface design guides, user
manuals, and online help guides. His cur-
rent research interests include strategic
usability and cross-cultural user interface
design. Before joining Sapient, Donahue
was a senior usability specialist at the
Compuware Corporation. He holds de-
grees from the University of Delaware
and Clemson University and is a member
of the ACM Special Interest Group on
Computer-Human Interaction. He is also a
member of the Usability Professionals’
Association. Contact him at Sapient, 250
Williams St., Ste. 1400, Atlanta, GA
30303; gdonahue@sapient.com; www.
sapient.com.

CALL FOR ARTICLES

As the market for software and related services be-
comes increasingly competitive and global, organi-
zations must benchmark themselves—their prac-
tices and performance—against other
organizations. Yet benchmarking is not widely
practiced in software engineering, so neither its po-
tential nor problems are well understood.  More-
over, some have questioned the quality of reported
benchmarks. This special issue will discuss alter-
native benchmarking approaches, their strengths,
and their weaknesses. Topics of interest include

• Case studies of partner-based benchmarking
• Cross-industry comparisons of organizational 

or project performance
• Descriptions, evaluations, critiques of 

different approaches
• Lessons learned, benefits and uses of 

benchmarking results
• Techniques for ensuring objective and 

accurate results

SEPT./OCT. 2001

Authors of articles describing results of quantita-
tive benchmarking must be willing to make their
data and methods available for review (but not 
necessarily published).

In addition to regular papers, we will also consider
short news articles describing unique benchmark-
ing resources. We also invite software benchmark-
ing product and service providers to participate in
a survey, whose results will be published.

Guest Editors:
David N. Card
Software Productivity Consortium
card@software.org
(contact for regular articles)

Dave Zubrow
Software Engineering Institute
dz@sei.cmu.edu
(contact for news articles and survey participation)

Submit articles by 28 February 2001 to
IEEE Software
10662 Los Vaqueros Circle, P.O. Box 3014
Los Alamitos, CA  90720-1314
software@computer.org

B E N C H M A R K I N G
S O F T W A R E  O R G A N I Z A T I O N S
B E N C H M A R K I N G
S O F T W A R E  O R G A N I Z A T I O N S



3 8 I E E E  S O F T W A R E J a n u a r y / F e b r u a r y  2 0 0 1 0 7 4 0 - 7 4 5 9 / 0 0 / $ 1 0 . 0 0  ©  2 0 0 1  I E E E

As today’s business environment focuses
increasingly on productivity, companies that
make their products easy to use—and quick
to benefit the user—gain a key advantage.
By identifying the users and determining
their needs and expectations, companies can
incorporate usability factors early in the
product’s life cycle. The result: a company
can increase its competitiveness, productiv-
ity, customer and user satisfaction, and prof-
itability while decreasing support costs. 

But, how do you begin to address usabil-
ity, especially in an organization that might
not have a basic understanding of this disci-
pline or the human factors engineering re-
sources required? There isn’t one best way
to initiate usability, or HFE, into an organi-
zation because each situation is unique.
There are, however, common obstacles and
issues that challenge any HFE practice. The
following case studies describe successful in-
troductions of HFE into organizations.
While the different organizations shared
common problems, each creative solution

met the needs of specific circumstances.
(The “Situations” sidebar describes the
overall conditions facing each organization.)

Organization A
In this situation, the information prod-

ucts department manager believed the de-
partment’s future was tied to usability. She
believed that all IP departments should try
to put themselves out of business, because
many manuals are a byproduct of poor us-
ability. The IP department was a service
center to all development organizations.
None of the employees had formal or in-
dustry training in usability.

How did it start?
The IP manager educated her team and

earned their support. She brought in an exter-
nal resource and provided user-centered de-
sign training to her staff. Next, she created a
presentation explaining how usability relates
to creating publications. She looked for op-
portunities to present the mission statement to

focus
Partnering Usability with Development:
How Three Organizations
Succeeded

Karla Radle, iXL 

Sarah Young, NCR

Improving
product usability
enhances an
organization’s
productivity,
competitiveness,
and profitability.
However,
integrating
usability
practice into 
an organization
is challenging.
These case
studies examine
how three
organizations
succeeded.

M
ost people using the computer as a tool are more interested in
accomplishing their objective than getting to know their com-
puter. The essence of usability is the degree to which people can
accomplish their goals without the tool hindering them. The

science of engineering usability into a product is human factors engineering. 

usability engineering



other managers. Her approach was not to lec-
ture but rather ask: “What do you think? Is
this the right direction for our company? How
would it affect your department?”

What were the obstacles?
There was a general lack of awareness

about HFE or how it could improve products.
HFE expertise did not yet exist internally.
Time was a driving factor because of extreme
pressure to be first to market, and meeting de-
livery dates was the primary criterion for per-
formance reviews and bonuses. Developers
were concerned about adding time to the
schedule for HFE activities. Their current
source of feedback was from testing and oc-
casional customer visits. No executive sup-
port existed for the development of HFE ac-
tivities, so this had to be a grassroots effort.

What strategy was used?
Each January, the IP department scoped the

projects and resource needs for the year. Our
consulting team contacted development teams
to obtain product roadmaps and release dates.
We began assessing which products should re-
ceive HFE resources, focusing first on high-
revenue products, activities within the scope of
the current HFE skill set, product managers
that were most open to HFE participation,
and potential impact on products.

Become part of the team. We worked hard to
become a part of each development team. We
attended the process and development team
meetings. We listened for developers’ pain,
which helped us formulate a strategy to use
when negotiating changes later. For example,
one team was coding in a language for which
few of them had expertise. Another team was
meeting an extremely tight deadline driven
by executive management. Still another team
was trying to provide additional deliverables
in order to obtain a bonus. Attending meet-
ings helped us identify opportunities to add
value with HFE, provided openings for im-
promptu explanations of HFE, and helped us
understand the team’s dynamics. 

Aim for awareness. We did not slam develop-
ers’ heads on the table every time they re-
fused to change the interface. In fact, we
greatly soft-pedaled the results of our HFE
activities—at first. We took a long-term ap-
proach of raising awareness and gaining buy-

in. Although it was painful at times, we did
not insist on perfect or even tolerable usabil-
ity of any one product. After the first few
projects, momentum began to build and we
gained greater acceptance of our suggestions.

Start with parallel activities. For each prod-
uct, we reviewed development and testing
schedules, listened for key pain points, and
considered our own HFE resources. We
chose the most appropriate activities (such
as usability testing, heuristic review, or
needs analysis) and looked for a way to do
them in parallel with the existing develop-
ment schedules. Becoming the critical path
in a schedule-driven environment would not
help us gain acceptance. At first, this meant
that many recommendations were held until
a later release. Meanwhile, we were honing
our skills and teams began to respect our
methodology; within a short time, we began
to have an impact on product releases.

J a n u a r y / F e b r u a r y  2 0 0 1 I E E E  S O F T W A R E 39

Organization A
Company size One site within a global Fortune 500 company 
Originating department The information products department
Management support Front-line management, but not executive, 

support
Existing HFE skills No formal or industry training in usability 
Types of products Middleware, front- and back-office software, 

Intranet and e-commerce Web sites
Third-party products Partners’ products were occasionally incorp-

orated into deliverables

Organization B
Company size Global Fortune 500 company developing 

consumer products in a highly competitive 
market

Originating department Newly created HFE department
Management support Direct management; some executive support
Existing HFE skills Formal and industry training in usability
Types of products Software for recovery, CD computer hard-

ware, desktops, laptops, and convergence 
systems, DVD/CD-ROM, corporate Web site, 
diagnostics, and setup

Third-party products Included in the product offerings

Organization C
Company size One division within a global Fortune 500 

company 
Originating department An application software development group
Management support Some management support
Existing HFE skills A few seasoned human factors engineers
Types of products Hardware, peripherals, and point-of-sale systems
Third-party products Many software partners; a few hardware 

partners

Situations



Educate the campus. We took every opportu-
nity to talk about what HFE is and why it is
needed. We spent most of this time one-on-
one with project team leads before meet-
ings. We asked to be on the agenda occa-
sionally, to explain what we were doing and
why. We invited developers to observe us-
ability tests, which makes the feedback per-
sonal and concrete in a way that no presen-
tation can replicate.

Develop the HFE skill set. Our manager
brought HFE training in-house throughout
the year. We attended conferences, read
books, and learned by doing. For example,
one of our usability evaluations lasted four
hours per participant. We learned that even
the most well-intentioned user would hit the
wall after two hours. It was slow going at
first, but as we built expertise in some core
areas—such as usability testing and heuristic
reviews—we routinely were able to provide
those services while learning new techniques.

Did you have a lab?
We began to conduct short usability evalu-

ations at our workstations, but quickly moved
to a conference room. Later, we found space in
an existing lab where we installed chairs and a
table. Our equipment was an old PC and a
box of paper prototyping supplies. The biggest
advantage was that we could control the room
schedule and conduct studies on short notice.

When the building was remodeled, we
found a one-way mirror and installed it be-
tween our lab and conference room. We
wheeled the PC into the conference room and
used the lab for recording and observation.
Our equipment consisted of a video camera
(borrowed from the graphics lab) and a $100
microphone threaded through the wall.

What techniques were used?
We started by conducting usability tests

of our own publications, both printed and
CD. This gave us a low-pressure opportu-
nity to develop our skills and obtain useful
feedback. We conducted short, 15-minute
usability tests at our own workstations and
gave chewing gum as a thank you gift! 

Over time, we developed a strong list of
HFE services that we could offer (see the “Us-
ability Techniques” sidebar) and found ways
around common obstacles. For example, we
asked the quality group to incorporate usabil-
ity objectives into the formal quality require-
ments. If circumstances prevented us from
contacting actual users, we would document
assumptions about the users and their tasks.
It became obvious to developers how many
assumptions were being made and the need to
at least start validating them. 

We began creating personas (see the “Us-
ability Techniques” sidebar) for every project.
We found a picture in a magazine that “looked
like” our persona user, and printed color copies
of this one-page description for the develop-
ment team. Whenever design discussions di-
gressed into personal opinions, we would sim-
ply ask “what would {persona’s name} want?”
This was very effective at returning focus to
pleasing the primary user group.

What were the results?
Developers were skeptical at first, but

they quickly recognized the benefits of our
services. We discovered some developers
who had usability concerns but did not
know how to support them within their
own groups. There are several reasons why
developers want HFE assistance, once they
understand the HFE role:

■ HFE provides feedback unavailable
through common software testing.

■ The feedback is usually based on exter-
nal users.

■ HFE uncovers details that the require-

4 0 I E E E  S O F T W A R E J a n u a r y / F e b r u a r y  2 0 0 1

Most organizations face similar obstacles when first trying to incorporate
HFE activities and guidelines.

■ Awareness level. How many people on site have heard of “user-cen-
tered design” or “human factors engineering”? Is there a lack of
awareness and knowledge about these activities and their benefits? 

■ HFE resources. How much HFE expertise exists internally? How many
people and what range of skills do they have? Are they formally
trained or do they have work experience in HFE?

■ Performance measurements. What performance measures are in place
for developers? Are they rewarded for meeting schedules, achieving
quality, or both? These measures have a profound impact on the
processes used and decisions made. 

■ Feedback sources. What current sources of feedback exist for the develop-
ment teams? Is feedback limited to comments from the testing department?

■ Management support and communication. How many managers sup-
port the HFE effort? Are they front-line managers or executives? How
are HFE efforts communicated within the organization?

■ Market positioning. Which marketing pressures are influencing the
products most? Is the product functionality greatly needed and so new
that users will overlook usability issues at first? Are third-party products
an integral part of the deliverable?

Common Obstacles



ments document leaves out.
■ HFE does not compete with developers.

Our job is to define the user needs and
interaction requirements; the developers
still handle the functional and architec-
tural requirements.

■ We make the developer’s decisions eas-
ier by providing adequate information.
Their job is to write good code, and all
too often they find themselves involved
in speculative discussions instead. We
provide the information that reduces
those discussions and allows them to do
their jobs more effectively.

After a few months, we began to see a
shift in the developers’ mindset, and within a
year we started experiencing the following:

■ Developers sought us out to evaluate
their interface designs.

■ Teams included our activities in their de-
velopment schedules.

■ Developers asked us to log our findings
as regular bug reports and created spe-
cial usability categories.

■ Teams incorporated recommendations
from previous usability evaluations into
the requirements document. 

■ Requests for HFE assistance began to
outnumber resources. We started rotat-
ing our assistance among the key rev-
enue-generating products.

Organization B
In the competitive computer industry, this

global, Fortune 500 company decided that
the usability of their products would provide
a competitive advantage. The company hired
a director to create the Customer Experience
Department. Its mandate was to ensure that
the consumer market would find the com-
pany’s products usable. This department was
responsible for the usability of the product’s
hardware, software, and documentation.

How did it start?
The director of customer experience de-

veloped a department of human factors
engineers, usability specialists, and experi-
mental psychologists. Some of the profes-
sionals had been successful in their field
for several years, while others were just
beginning their careers in the usability
field. The vice president of product devel-

opment requested that these usability pro-
fessionals be integrated into the product
development teams. Management clearly
communicated that usability would be the
key differentiator between the products of
this company and the competition’s prod-
ucts. However, management gave no for-
mal presentation to explain the role of the

J a n u a r y / F e b r u a r y  2 0 0 1 I E E E  S O F T W A R E 41

Human factors engineers and usability specialists use many different
techniques to analyze, evaluate, design, and test. The needs of the product
and situation dictate the techniques chosen. This is not a comprehensive list
but an overview of the breadth of techniques and examples of deliverables.

■ User needs analysis. Accumulate raw data on users and their experi-
ences, mental models, backgrounds, skill levels, work activities, and ex-
pectations for the product. (This data serves as a basis for many of the
other techniques mentioned below.)

■ Competitive evaluation. Evaluate and compare usability of competitive
products. This information helps identify usability gaps, refine usability
objectives, prevent inclusion of inadequate third-party products, and
identify alternatives users might seek if product usability is poor.

■ Focus groups. Conduct moderated group discussions with users to de-
termine features, functionality, and user experiences with the usability of
competitive products. 

■ Usability objectives. Create measurable objectives for product usability.
This requires an in-depth understanding of the business objectives and
user objectives for the product.

■ User profiles. Define user groups and identify demographics and skill
levels that will affect how users interact with the product. Profiles are
essential for making design decisions.

■ User personas. Create a short description of a fictitious representative
of a user group, including a name and a photograph. Serves as a con-
crete reference point when making design decisions, and provides a
mechanism to bypass opinionated discussions.

■ User task analysis. Identify a list of prioritized user tasks. This should
be developed from in-depth interviews and observations with actual
users. At a minimum, it can be helpful to document what assumptions
the development staff is making regarding user tasks. A clear under-
standing of user tasks and priorities is essential for making design
decisions.

■ Interface prototyping. Create interface prototypes with storyboards
and flowcharts. These serve as first drafts for interface design discus-
sions, rapid prototyping, and usability testing.

■ Rapid paper prototyping. Conduct rapid prototyping sessions with
users. Working with paper designs allows rapid prototype evolution
without an investment in coding.

■ Heuristic reviews. Review products against design standards. This gives
insight into interface design flaws and how to correct them.

■ Task scenarios. Create user scenarios to test product features. Scenar-
ios reflect common user tasks for the product evaluated.

■ Usability testing. Conduct usability tests with users via paper or online,
interactive prototypes. Gives specific feedback on design flaws or vali-
dates design approaches. This can be used for benchmarking purposes
against usability objectives.

Usability Techniques



Customer Experience Department. Al-
though there was executive- and director-
level support, the product teams were un-
informed about the existence of our
human factors group. As a result, mem-
bers of the Customer Experience Depart-
ment received blank stares at the product
meetings because the teams had no knowl-
edge of who we were or what we did.

What were the obstacles?
Although there was support at an execu-

tive level, there still were several obstacles to
overcome in the integration of usability. The
most problematic issue to overcome was the
lack of awareness by the company as a whole.
There was no formal process for integration
into the development teams. Management
gave no formal presentations or other com-
munication that HFE professionals were to
become part of the team. Most teams felt the
integration would increase the length of the
development cycle. This company also felt the
“first-to-market” time pressure. There was no
time to do usability studies of any kind. The
only feedback came from test engineering and
as long as the product functioned, the attitude
was “ship it.” One last obstacle to overcome
was developing the “fresh-out-of-school”
professionals. There were no formal struc-
tures, guidelines, or mentor programs to
guide the new professionals. They were on
their own to fight the political battles in this
corporate environment. Many opportunities
were lost because of inexperience, lack of
support and guidance, and the need to get the
product out the door on schedule.

In addition to these internal obstacles,
there were external obstacles to face when
working with third-party vendors. HFE pro-
fessionals commonly worked with third-party
vendors to complete a software project be-
cause this particular organization lacked the
resources to design and develop all the soft-
ware for their products. The obstacles experi-
enced in the external projects were unique in
some ways; for example, the development
team often was not on-site. It took several
email messages, faxes, or conference calls to
agree on design changes, which increased the
time it took for feedback. The developers
would make the changes and send the revised
product, and then the HFE professional
would review it and send feedback. Had de-
velopment been on-site, the HFE professional

could have sat down with the developer to
make and approve changes in real time. 

The last obstacle the HFE professionals
faced was dealing with software developed
by certain third-party vendors where the
company had no control or interaction dur-
ing the design phase. Products shipped with
software over which this organization had
no control. This software often had major
usability problems, which reflected poorly
on the organization’s products. Unfortu-
nately, we had little ability to recommend or
control any changes to these particular
third-party vendor products.

What strategies were used?
The HFE professionals were assigned to

specific projects or product lines to address
any software need. From there, strategies
varied depending on the type of project and
the amount of awareness of human factors
each individual team had.

Integration into the team. We were proactive in
contacting the project leads, obtaining prod-
uct roadmaps, meeting schedules, and attend-
ing design reviews. This integration allowed
the HFE professionals to determine the skills
needed in the design of the product. It also let
us demonstrate how HFE could make the
product more usable through discussions, ex-
amples, prototypes, and usability tests.

Integration into the development process. Based
on the meetings and design reviews, the HFE
professional identified what aspects of the
product could be improved and worked with
the development team to integrate these
changes. We delayed implementation for
some areas until future releases, but placed
them on the requirements list immediately.

Awareness. The best way we made the com-
pany and a product team aware of our serv-
ices was to get involved in every aspect of
the product. This included product develop-
ment, marketing, documentation, and tech-
nical support meetings. We told all individ-
uals about our existence and the value we
could add to the team. We usually achieved
this awareness by introducing who we are,
what we do, why we do it, and what value
we add for the user. This was informal. Be-
yond the initial introduction, we increased
awareness by speaking out about issues or

Members of 
the Customer
Experience
Department

received blank
stares at 

the product
meetings

because the
teams had no
knowledge of
who we were

or what we did.

4 2 I E E E  S O F T W A R E J a n u a r y / F e b r u a r y  2 0 0 1



designs that we could easily enhance. This
might be as simple as the placement of icons
or menu items to more complex issues, such
as actual navigation of the system.

Human factors skill development. Manage-
ment wanted to keep the HFE group abreast
of all the latest technologies, software pack-
ages, coding languages, or whatever else re-
quired to do their job. With management’s
support and an annual budget, the HFE
professionals took classes, went to training
sessions, and attended conferences. Each in-
dividual determined what skills or educa-
tions he or she needed to succeed.

What techniques were used?
We started usability testing by identifying

the user population. This user profile defined
demographic information such as age, sex,
education, computer experience, profession,
skill level, previous experience, and training.
This information helped us determine and
complete usability objectives. The produc-
tion of benchmark data on how long it
should take a user to complete a particular
task helped in the definition. We captured
this information during several trials with
many users, through competitive analyses,
and definition of the user’s perceptions of
successful completion of a task. Another ob-
jective was whether a particular task required
instructions for successful completion. Once
we defined the study’s users and objectives,
we defined the tasks the users would per-
form. With this list of tasks, we created user
scenarios for use in the usability study.

HFE also was involved in the identifica-
tion of features. Based on focus sessions con-
ducted in conjunction with the marketing
group, HFE created a features list for the
product. This list influenced design decisions
and provided feedback about the usability of
current and competitive product features.

We regularly used heuristic evaluations to
measure existing software products against
design standards or usability objectives.
These evaluations were helpful in competi-
tive analyses and investigation of third-party
products as well. They helped determine
what features to include and what user in-
terface changes to make to produce a more
usable and competitive product. The third-
party evaluations assisted in selecting the
products we would sell to our customers and

determining whether they met the usability
objectives we defined. If not, we would not
sell the third-party product with our prod-
ucts. The heuristic evaluations also identified
whether the usability problem related solely
to the software from the third-party vendor
or resulted in combination with the hard-
ware and software solution.

Competitive evaluations helped us deter-
mine how usable our product was compared
to our direct competitors. We identified task
scenarios, and each user tested all the prod-
ucts. The result was a comparison of comple-
tion time, accuracy, and if and how the task
was completed across all products tested.
This again was useful in determining product
features and usability changes to the interface.

Did you have a lab?
We had a lab built to our specifications, in-

cluding a testing room, control room, and ex-
ecutive viewing room (see Figure 1). The test-
ing room had a one-way mirror, two
moveable cameras mounted on large tripods,
a couch, coffee table, large-screen TV, and
two PCs on adjustable tables. The controls
consisted of high-end video and mixing
equipment, microphones, a scan converter,
speakers, and several TV monitors to view the
different camera angles. A wall and a glass
window directly behind the control room sep-
arated the executive viewing room. This room
included speakers, a monitor, and high, bar-
like chairs with a writing surface. The visitors
in the executive viewing room could hear and
see into the testing room through the clear
glass and one-way mirror in the control
room, or by looking at the monitor.  

What were the results?
Over time, the product teams saw the

J a n u a r y / F e b r u a r y  2 0 0 1 I E E E  S O F T W A R E 43

Figure 1. Organization
B’s usability lab:
from within the
executive viewing
room, the testing
room appears in the
background (yellow
lighting).



value that HFE could provide, especially
those teams that were heavily concentrated
on hardware. The HFE could take the lead
and run with the development of the user
interface that would operate that hardware
piece. This was particularly true of the de-
velopment of the DVD user interface. As the
product team concentrated on the hard-
ware, the HFE worked closely with the ven-
dor to develop the user interface. The HFE
provided an update at each product meeting
and showed prototypes of the user interface
as well as the functionality it supported.
The HFE became the expert on DVD and
consultant to the product team when ques-
tions arose.

In addition to the product team integra-
tion, HFE began to play a key role in com-
petitive studies. The HFE group proposed the
value of competitive analysis and how it
could improve the products delivered. The
studies identified what users found easy and
difficult to use, desired features, where we fell
short, and where the competition excelled.

As time went on, the HFE group gained
many more successes. The product teams be-
gan integrating HFE into the product life cycle,
asking questions when they felt HFE expertise
was necessary for product development, and
even requesting usability studies be performed
on their product and the competition. This led
to the development of a competitive “petting
zoo” comprised of the competition’s hardware
and software solutions for review.

Organization C
Human factors engineers and psycholo-

gists worked at the corporate office in pri-
marily a research capacity. During a 10-year
period, the company decentralized this
group to create pockets of expertise without
any central directive or support for HFE ac-
tivities. Three of these specialists wound up
in a software development group together.
They conducted software user interface de-
sign and testing for products but did not
hold the title “human factors engineer.”

How did it start?
An experienced HFE manager relocated

to the division for the purpose of starting an
HFE department. He brought key customer
contacts where he had been engaged in ex-
ternal HFE consulting and a number of
marketing techniques aimed at understand-

ing business cultures in decision making. He
pulled the existing HFE experts into the
new department.

What were the obstacles?
There was some management support

and a limited departmental budget. Insuffi-
cient funding existed to support HFE efforts
for all product lines. There was a small pool
of very experienced and skilled HFEs. Their
job descriptions, however, did not accu-
rately reflect their interface design activities.
The group needed a lot of motivation to
reenergize into a dynamic new department.
There was a wide gap between the experi-
enced engineers and new graduates. 

What strategy was used?
The group worked with marketing to con-

duct HFE activities with external customers.
This provided financial backing for advanced
development activities with internal develop-
ment groups. We developed marketing col-
lateral for external engagements, such as suc-
cess stories, formal brochures of services, and
templates for contracts and statements of
work. We wrote white papers given to—and
often requested by—customers. 

Our access to and successes with cus-
tomers gave us credibility with development
teams and product marketing. Interpersonal
skills were critical to creating relationships
with the development staff. We created a
web site to explain our purpose, individual
backgrounds, and how to request assistance.
We focused on developing tools to test with
customers instead of testing in a laboratory.
We reclassified jobs to reflect HFE expertise
and added performance measures to reflect
continued external customer contact while
maintaining internal HFE activity levels.

The HFE department had a substantial
impact on a few product lines through its ad-
vanced development work. We leveraged the
results of ergonomic and usability studies to
assist with some new product designs. These
designs eventually became very successful
product lines for the company, resulting in
greater credibility for the HFE department.

Did you have a lab?
We typically worked at the customer site,

so we studied real environments in lieu of
using a lab. We had access to many univer-
sity labs, and after a few years we gained ac-

4 4 I E E E  S O F T W A R E J a n u a r y / F e b r u a r y  2 0 0 1

Each of 
these three

organizations
took a different
approach, and
yet they were
all successful 

in initiating HFE
to address

product
usability.



cess to a state-of-the-art usability lab that a
nearby sister organization had recently in-
stalled. Our focus from the beginning, how-
ever, was to develop tools for use at the cus-
tomer site instead of in the lab.

What techniques were used?
To make the HFE department successful,

we developed new tools that showed value to
the customer. One such tool was a time-and-
motion study to analyze client performance.
In addition, the opportunity to work with
and receive support from advanced develop-
ment research helped us develop prototypes.
Prototypes defining a product and addressing
a particular need continued to drive next-gen-
eration product designs and support for HFE.
The HFE group also generated support by
producing white papers and journal and con-
ference publications based on the research the
group conducted. This visibility within the
company and among other HFE profession-
als and other companies in the industry
helped spread the word about the HFE
work’s value. HFE expertise became an asset
in sealing the deal for many sales individuals
because it provided proven results and was a
key differentiator from the competition.

What were the results?
Internal development teams began to set

aside annual funding for HFE services. Each
year, they planned the general services that
should apply and then the HFE specialist re-
fined those plans as work proceeded. We
had a growing base of external customers
and large corporations were soliciting our
services. The HFE department had a busi-
ness manager and high visibility with upper
management because of our customer in-
volvement. The department was growing
rapidly and published papers each year,
both internally and externally.

Lessons learned
As we reviewed the different approaches

taken by these three organizations, we
learned some lessons about partnering HFE
with development.  These lessons will serve
as valuable reminders to any organization
that is just beginning to address usability:

■ Excellent interpersonal skills are crucial
to developing relationships with devel-
opment teams.

■ Applying the results of HFE activities to
thought leadership in product develop-
ment makes the company more success-
ful and raises HFE’s credibility.

■ Working directly with the customer cre-
ates high visibility with management,
marketing, and product teams.

■ Even when schedule pressures are in-
tense, HFE is possible. At a minimum,
HFE activities will raise awareness and
understanding, and will set the stage for
the future.

■ Expensive labs and equipment are not
necessarily a prerequisite for HFE in-
volvement; this depends on the product
and what is measured.

■ Most resistance to HFE comes from
other pressures (such as schedule) and a
lack of information. 

■ There is no substitute for observing user
interactions first hand.

E ach of these three organizations tooka different approach, and yet theywere all successful in initiating HFE
to address product usability. Even if your or-
ganization faces common obstacles, such as a
lack of management support, HFE skills, or
customer focus, you can still begin to address
the usability issues of your product. Consider
the situation, assess the temperament of the
group, and choose a course of action that is
appropriate to the existing conditions.

Of course, each success story raises the
global awareness of the value of HFE.  How-
ever, there is more to do. As businesses ad-
dress procedures and standardization, such
as ISO, the Capability Maturity Model, and
Six Sigma, the challenge is to clearly define
the most effective role for HFE within each
process. And as this engineering practice of
addressing the human need matures, we
must strive to constantly improve our meth-
ods and research. We will find better ways to
integrate the different aspects of user needs,
such as physical and cognitive, and to deal
with the constantly changing dynamics of
the human population. 

References
1. R.G. Bias and D.J. Mayhew, Cost-Justifying Usability,

Academic Press, San Diego, 1994. 
2. J. Nielsen, Usability Engineering, Morgan Kaufmann,

San Francisco, 1994.
3. D.A. Norman, The Design of Everyday Things, Cur-

rency Doubleday, New York, 1998.

J a n u a r y / F e b r u a r y  2 0 0 1 I E E E  S O F T W A R E 45J a n u a r y / F e b r u a r y  2 0 0 1 I E E E  S O F T W A R E 45

About the Authors

Karla Radle is a human factors engi-
neer at iXL.
Karla has over
six years of
experience 
designing and
conducting us-
ability studies
on hardware,
software, and
documentation. She has worked in com-
puter, Internet, and retail industries. She
earned a BA in psychology and sociology
and an MS in industrial engineering from
the University of Wisconsin, Madison. Con-
tact her at iXL, 1600 Peachtree St., NW,
Atlanta, GA 30309; karla@radle.com or
kradle@ixl.com.

Sarah Young is a human factors 
engineer in
NCR’s Retail
Solution Divi-
sion. She con-
ducts user-
centered
development
for kiosks,
Web applica-

tions, and point-of-sale systems. She
earned a BS in statistics from the Univer-
sity of South Carolina. Contact her at NCR
Corporation, 2651 Satellite Blvd., Duluth,
GA 30096; sarah.young@ncr.com.



4 6 I E E E  S O F T W A R E J a n u a r y / F e b r u a r y  2 0 0 1 0 7 4 0 - 7 4 5 9 / 0 1 / $ 1 0 . 0 0  ©  2 0 0 1  I E E E

After much study and consideration, senior
management committed to implementing a
universal OO development methodology.

Senior management recognized the need
to improve customer satisfaction, which had
always been high but needed to be better in
an increasingly competitive market. Man-
agement saw the introduction of usability
practices as a prime means to achieve this
objective. So they began to place greater
emphasis on usability—even to the point of
building and staffing dual state-of-the-art
usability labs.

Our goal throughout the projects we de-
scribe here was to combine the best OO
analysis and design practices and usability
techniques to create a powerful, unified 
way to develop software. We wanted user-
centered design and evaluation to be a core
component of the development process in-
stead of an afterthought. Given the diversity
and number of entrenched methods used

within the company, implementing a univer-
sal methodology presented quite a challenge.

The company chartered a process coordi-
nation group to create the best possible
process and to act as a change agent by edu-
cating and consulting with the various devel-
opment groups. The process coordination
group included the software engineering
team responsible for the OO development
process and tools. It also included the usabil-
ity team responsible for the usability labs
and evaluation processes as well as the com-
pany’s user interface standards.

Although the usability team made signifi-
cant progress in introducing their evaluation
process, they were frustrated to see it continu-
ally left until the end of the development
process—at which point most changes are too
costly to implement. The team wanted to
bring usability into the earliest possible phases
where it could have the most impact by im-
proving initial design and eliminating rework.

focus
Integrating Usability
Techniques into Software
Development

Jean Anderson, Francie Fleek, Kathi Garrity, and Fred Drake, 
Shared Medical Systems

Focusing on the
user early in the
development
process goes a
long way toward
improving
product quality
and eliminating
rework. In this
article, the
authors show
how their
company is
working toward
this goal.

N
ow merged into Siemens Medical Solutions Health Services, our
company, formerly called Shared Medical Systems, creates clinical,
financial, and administrative software for the healthcare industry.
Like other medium to large companies, SMS had reached a scale

and maturity level that required the development process to be documented,
predictable, repeatable, measurable, and usable by the development groups.1

usability engineering



The team members realized they had to inte-
grate usability testing fully into the software
development process rather than continue to
support it as a complementary process.

A small group of representatives from the
software engineering and usability teams
took on the task of integrating user-centered
design and evaluation into the company’s
new OO development process. This task
proved more challenging than we expected.

Identifying the challenges
The usability team promoted a user-

centered design process that combined the
contextual-inquiry design techniques devel-
oped or made popular by Hugh Beyer and
Karen Holtzblatt2 and the usage-centered
design processes derived from the work of
Larry Constantine and Lucy Lockwood.3

An overall design framework created by
Charles B. Kreitzberg and Whitney Quesen-
bery guided the usability process through
the software development phases.4

The software engineering team chose the
Rational Unified Process as the knowledge
base to represent the company’s development
processes. RUP is a comprehensive tool that
provides information, guidance, templates,
and examples for software engineering devel-
opment activities. Unfortunately, although it
acknowledges usability as a component of
good software development, it inadequately
supports such activities as the collection of us-
ability requirements or usability evaluations.

So, one of our first challenges was to repre-
sent the usability activities within RUP. RUP’s
process guide is customizable. However, be-
cause it employs extensive cross-referencing
and provides for frequent updates, we had to
execute modifications carefully to avoid diffi-
cult maintenance later on.

For the two teams involved, understanding
each other’s processes, vocabulary, tools, and
perspective was crucial. We spent considerable
time trying to bridge the gaps, with each team
participating in education classes sponsored
by the other team. We couldn’t make real
progress until we started to focus on the intent
of each activity in the two processes. This let
us move beyond both terminology and se-
quence differences. By happy coincidence, the
focus on intent is also a key usability tech-
nique for identifying usability requirements.

To further complicate the situation, not
all the development groups began using RUP

immediately. In particular, some strategic
projects used Princeton Softech Enterprise as
the case tool for entering use cases and other
OO artifacts. The developers used a variety
of other tools, such as Visio and Caliber, for
capturing the models, requirements, and di-
agrams created during the process.

Aligning these tools from a technological
perspective and reconciling the differences
in terminology presented further challenges.
Both teams needed to be able to address 
developers, regardless of their background
and tool experience. The usability team par-
ticularly needed to be able to explain how
to integrate user-centered design into any of
the development processes.

Dealing with organizational and cultural
changes within the company proved to be a
challenge equal to—if not greater than—the
technical ones. Our older systems encour-
aged analysts to be more system-focused
than user-focused. Business process model-
ing, or understanding our customers’ needs,
although always a priority, really hadn’t been
well structured. So, design decisions tended
to put the burden on the user to learn a com-
plex system rather than on the development
team to produce an easy-to-use system.

Initially, these development groups be-
came concerned that user-centered design in
the early stages of development would
lengthen the development process. Because
user-centered design was so new to most
people at the company, many had difficulty
trusting this approach’s ability to reduce the
overall development time by eliminating re-
work and improving design specifications.
Clearly, user-centered design was not a nat-
ural way of thinking among many of the nu-
merous development groups that shared re-
sponsibility for software products.

Until the development groups became 
accustomed to and proficient with these
processes, they resisted implementing them.
We determined that education and manage-
ment support for the transition was vital.

Agreeing on process
Figures 1 and 2 illustrate the various ac-

tivities in the software development process
and the software development phases using
the RUP nomenclature. The phases and steps
don’t completely synchronize but are a gen-
eral guideline. These figures also illustrate
several key elements of the process:

J a n u a r y / F e b r u a r y  2 0 0 1 I E E E  S O F T W A R E 47

Dealing with
organizational
and cultural

changes within
the company
proved to be 
a challenge

equal to—if not
greater than—

the technical
ones.



■ The usability activities and their deliver-
ables should emerge during the require-
ments gathering, analysis, and visioning
steps. The better you execute these tech-
niques, the better the product will be.
Avoid the tendency to jump too early to
coding.

■ You gather requirements, analyze them,
and design the product vision iteratively
during the inception and elaboration
phases. The first iteration, during the in-
ception phase, is a high-level requirements
analysis for the project business plan.
Later iterations, during the elaboration
phase, are performed in a more detailed
manner to produce the product design.
Throughout all iterations of requirements
definition, potential users provide insight.

■ The requirements and visioning activi-
ties feed both the system and user inter-
face design.

Agreeing on a requirements process
All software development processes em-

phasize the importance of gathering require-
ments. However, many processes do not de-
scribe this step in detail. Ad hoc methods of
defining requirements abound. Develop-
ment teams commonly spend considerable
time discussing and analyzing requirements.
They also commonly argue about require-
ments late in the development process.

We recognized that the requirements
process was the stage where both the OO
development process and user-centered de-
sign process could have a major impact on

improving the development process. So, we
made requirements a priority and researched
and analyzed requirements-gathering meth-
ods to select a best-practice direction.5–10

The user role models—or profiles—de-
scribed by Constantine and Lockwood are of
great importance for understanding and inter-
preting the project requirements. Contextual
inquiry provides an effective, detailed, and
structured requirements-gathering technique.
Defining the user and learning about the work
process at the place where the work occurs,
and then analyzing the findings in a structured
way, can considerably reduce the time a com-
pany normally takes to define requirements.

We have divided requirements definition
into four steps (see Figure 1):

1. Creating concepts. This step is the initial
business-modeling process. High-level
requirements gathering becomes neces-
sary at a project’s inception to define the
business plan properly, because the busi-
ness plan is the main deliverable for this
first step. To the business plan, we add a
concept statement for the product that
includes an outline of the intended end
user, what the product should do, and
the initial usability goals.

2. Gathering requirements. This step is best
done through site visits to see the actual
work taking place. After each visit, we
capture the findings and collect them in
detailed user profiles and a variety of
models illustrating the observed work-
flow sequence, the communication pat-

4 8 I E E E  S O F T W A R E J a n u a r y / F e b r u a r y  2 0 0 1

• Create concept statement
• Define goals
• Set objectives
• Start business terms glossary

Inception phase

Elaboration phase

Create concept

• Plan and perform site visits
• Define user profiles
• Define user models (workflow,
   artifacts, cultural, and physical)

Gather requirements

• Create affinity diagram
• Consolidate workflow, artifacts,
   cultural, and physical models

Analyze requirements

• Create vision picture
• Build vision storyboards or
   business use cases
• Create user environment design

Design product vision

Figure 1. A high-level
view of requirements
definition during 
software development.



terns, the artifacts used (such as docu-
ments and equipment), the cultural cli-
mate, and the physical environment.
Beyer and Holtzblatt’s contextual-
inquiry technique uses five models. We
condensed these models down to a more
flexible set that we can use as needed, de-
pending on the particular product’s
scope and activity.

3. Analyzing requirements. After an appro-
priate number of site visits, the require-
ments team interprets the data and com-
piles a user profile that represents the
common traits of all the users observed
and interviewed. The team also consoli-
dates the workflow, artifact, cultural, and
physical models. In addition, the team de-
fines detailed usability objectives for the
project and creates an affinity diagram of
all the issues still needing investigation.

4. Designing the product vision. In this
step, the team develops a model envi-
sioning the final product’s strategy for
meeting the requirements.

Beyer and Holtzblatt call the product vi-
sion the user environment design model,
Cognetics calls it a roadmap, and Constan-
tine and Lockwood call it the contextual
model. The product vision provides the struc-
tural blueprint for the product and how the
end user will interact with and navigate
through the system. This model is a key com-
ponent of the user-centered design process.

Developing essential-use cases with ex-
tended-use cases to illustrate requirements is
part of this vision model; this enables evalua-
tion of all stages of the design. By agreeing
that development teams could use a combina-
tion of context diagramming, use cases, and
storyboarding to create this vision model, we
were able to integrate this key component
into the software development process.

Agreeing on design and execution processes
With a good product vision well docu-

mented—both visually and with use cases—the
rest of the software development process falls
into place. Four steps remain (see Figure 2):

J a n u a r y / F e b r u a r y  2 0 0 1 I E E E  S O F T W A R E 49

• Build system use cases
• Map to domain 
   model/business objects

• Build class diagrams
• Build collaboration diagrams
• Build sequence diagrams
• Build state diagrams
• Create software design model

Analyze and design

• Refine software design models
• Create implementation model
• Create component diagram
• Build source code
• Test

Implement design
• Participate in all design reviews
• Implement changes
• Perform final UI evaluation

During build

• Produce software
• Package
• Distribute
• Install/implement at sites

Transition project

Support product environment

Construction
phase

• Build prototype storyboards
   or use system use cases
• Create paper prototype
• Evaluate the design/
   refine/evaluate/refine
• Create design spec

Design UISystem modeling

• Build vision storyboards 
   and/or business use cases
• Create user environment design

Design product vision

• Create UI
• Conduct evaluation

Implement UI

Working together...

User interface pathSystem path

Elaboration
phase

Transition
phase

Evolution
phase • Support, train

• Improve processes
• Perform studies at production 
   sites

Figure 2. A high-level
view of design and
execution during 
software development.



1. Designing the user interface. Building UI
prototypes lets the team test its design
with potential end users. Iteratively test-
ing the design, refining it, and retesting it
until the team is certain the design works
ensures the product’s future success.11

You accomplish the testing in this step
with a series of prototypes, starting with
rough, hand-drawn paper sketches and
ending with detailed mock-ups simulat-
ing the functionality.

2. Modeling the system—analysis and design.
You map use cases with system responses
to domain models; the business objects
lead to OO models, including class, collab-
oration, sequence, and state diagrams.12,13

3. Implementing UI design. Now, and only
now, is the product coded. All the require-
ments analysis, as well as the previous it-
erative testing and refinement of the de-
sign, ensures that the design specifications
have greater detail and that this stage in-
volves less rework. At this point we hope
to see the development teams recognize
and appreciate the process’s cost-effective-
ness, because they find that coding is more
efficient than it has traditionally been.

4. Transitioning the project and supporting
the product environment. These steps
cover the product’s rollout and production.
During production is the time to perform
usability studies at user sites to both com-
plete the cycle and begin the next cycle.

All software development process activi-
ties are potentially iterative. To mitigate
project risk, iteration is critical to the proj-
ect’s success. Some iteration must occur
even within a cycle. Revisiting and reconcil-
ing the high-level models and design con-
cepts created during a project’s initial steps
is vital as a project moves into the construc-
tion phase. This activity keeps the project
focused on its vision, within the scope of its
requirements, and on track with its budget.

Flexibility in project development is also es-
sential. The software development process we
developed might seem a generic solution. But
good judgment in adapting the process and
choosing activities appropriate for any specific
project and its time frames is always necessary.

Usability roles
While clarifying the process changes for

our company’s developers, we identified

some new roles and modified some existing
ones on our development teams. A role
doesn’t necessarily equate to a person. More
than one person can perform a single role; a
single person can perform more than one
role. We based these roles loosely on those
defined by Deborah Mayhew.14

The usability engineer has primary respon-
sibility for gathering and analyzing user re-
quirements and for expressing those require-
ments in the product vision and use cases.
These responsibilities extend beyond those of
the typical business analyst. The business an-
alyst gathers requirements to determine what
the product should do. The usability engineer
studies the potential end users to determine
how the product should do those things. This
means collecting details about end users, such
as their typical level of education and level of
experience with computers, as well as details
about their work, such as the average
turnover rate and their criteria for perform-
ance evaluations. This role’s goal is to capture
the end users’ mental model of the work.

The user interface designer sculpts the end
users’ interactions with the product, develop-
ing the early prototypes of the product design.
The UI designer also evaluates the usability of
the design prototypes. In addition, this role
creates the design specifications. This role’s
goal is to express the end users’ mental model
as closely as possible within the constraints of
the information and technical architecture.

The usability evaluator has primary re-
sponsibility for testing the product design,
analyzing and documenting the results, and
presenting the results to the development
team. The bulk of this testing takes place
during the initial stages of design before you
code the product.

The user has no primary responsibilities,
but is key to the product’s success. Users
help define the requirements and design the
user interface. They also evaluate the UI
during usability testing.

Workflow models
Along the way, we attempted to simplify

the process. While consulting on a develop-
ment project with extremely tight deadlines,
one of the authors, a UI designer, found the
number of models necessary for reporting
field observations too time consuming. Be-
cause she still needed to document and
share the knowledge gained from her field

All software
development

process
activities are

potentially
iterative. To

mitigate project
risk, iteration

is critical to the
project’s
success.

5 0 I E E E  S O F T W A R E J a n u a r y / F e b r u a r y  2 0 0 1



studies with Unified Modeling Language-
trained analysts on her team, she created a
workflow model that condensed the Beyer
and Holtzblatt sequence and communica-
tion models. She also designed it to resem-
ble UML so that the analysts would under-
stand it more easily. It was a success.

We incorporated the new workflow model
into our interpretation session models, which

drive the analysis following site visits. Within
one model, we show both the communica-
tion relationships (or transfer of data) and
the activity’s sequence, which helped us re-
duce the number of models that we created
to represent the users’ work. Figure 3 shows
a simple workflow model from a site visit.

In Figure 3, stick figures represent the
user being studied (in boldface) and other

J a n u a r y / F e b r u a r y  2 0 0 1 I E E E  S O F T W A R E 51

Office

Trigger: Patient 
is present to
receive care

Intent: Obtain 
information 
to aid
physician in
diagnosis.

Med
assistant

Task 1
Interviews patients and measures vital signs

Patient

Patient

Patient

Documents vitals and chief complaint
Patient
chart

1

Diagnoses patient; explains diagnosis
3

2

Picks up and reviews patient chart
Patient
Chart1

Interviews and examines patient
2

Places patient chart and facesheet on door
3

Obtains instruction sheet

Intent: 
Obtain patient 
instructions;
document 
visit; fill out 
charge info.

Task 3

Intent:
Provide
diagnosis and
treatment of
problem to
patient.

Task 2

Goes to preceptor room

Completes physician portion of facesheet

1

Informs physician patient is ready
4

Patient
instructions2

3

Issue: Distance
between rooms

Issue: 
Paper charting
is difficult to

share information
and organize it

Preceptor
room

Facesheet

Physician

Facesheet

Takes patient chart to office

Intent: 
Provide
billing
information.

Task 4
Returns to give instructions and facesheet to patient

1

2Physician

Intent: 
Record the
visit.

Task 5
Completes documentation before next visit or at end of day

1 Patient chart

Front desk clerk

Patient chart

Physician

Physician

Physician

Facesheet

Figure 3. A workflow
model for interpreting
a field study.



human actors involved in the task. Commu-
nication arrows extend from the principal
actor to the other people (and artifacts) with
whom that person interacts. We label each
arrow with an action or message. The activ-
ity is broken down into tasks and task steps
that we number to show the activity se-
quence. Each task’s principal actor is on the
left, letting the model show where the user
is a recipient of an interaction. At least one
intent must be identified for each task.

When we move to the next stage of analy-
sis, in which we consolidate the lessons of each
field study (in preparation for designing the
product vision), we study the intents from all
the workflow models. Then we create a con-
solidated workflow model to depict the task
structure and the strategies common to our
various customers. The other contextual de-

sign models—artifact, cul-
tural, and physical—are con-
solidated as well, and we
build an affinity diagram of
the issues and insights from
the field studies. Figure 4
shows a consolidated work-
flow model from another set
of field studies pertaining to
patient bed location.

Reaching agreement
early

Reaching a high-level
agreement on the essential
processes was the first step to-
ward integrating usability into
our company’s process. Once
we achieved this agreement,
working through the process
details became easier. We
chose to use key strategic proj-
ects to incorporate the main
techniques before rolling the
process out to more estab-
lished development teams.

We have kept in mind
that rolling out a company-
wide process needs to be
done over time. Upper man-
agement must stand behind
the process, and the devel-
opment teams must also
buy in. Maintaining the big
picture is critical to a suc-
cessful implementation.

We found that teams need to focus on
several steps early in the process:

■ Refine the product’s user profiles to en-
sure that the entire team has a thorough
understanding of the users.

■ Prioritize site visits to gather usability
and functional requirements.

■ Have key usability engineers provide
thorough and structured interpretation
of the data collected from site visits.

■ Build the software blueprints—the 
user environment designs—and provide
roadmaps to guide the rest of the process.

The user interface designers must be versed
in usability principles and employ these princi-
ples as they work. Also, usability testing must
take place during the early stages of design.

5 2 I E E E  S O F T W A R E J a n u a r y / F e b r u a r y  2 0 0 1

I'm looking for a bed for
these patients.

Here's the bed info.
Use these beds.

Trigger:
Today's 
OR schedule

Intent: 
Have room
assignment
and artifact
ready when
patient arrives.

Bed
coordinator

Task 1
Who needs a bed today?

Unit clerk

May I give them these beds?

Nurse manager

Patient
registration

log1

Writes bed # on paper log
Patient

registration
log3

Revises patient's registration info
4

2

Patient's
record

Completes registration info,
gets signatures

Trigger: Patient
presents self
at window

Intent: 
Complete all
necessary
patient
information.

Admitting
clerk

Task 2
Greets patient

Directs patient to room

Notifies that patient arrived

Trigger: 
Admitting
clerk

Intent: 
Record start
time of room
occupancy.

Task 3

1

Records room/bed assignment,
arrival time

Patient
registration

log

2

3

4

Patient's
record

Patient

goes to room

is taken
to OR

is taken to
recovery room

is taken
back

to room

Room/bed

Operating roomRecovery

Bed
coordinator

Bed coordinator

Prints
• Encounter facesheet
• ID bracelet
• Patient ID card

Figure 4. A 
consolidated 
workflow model 
compiled from a 
series of field 
studies.



W e embrace the observations of Al-istair Cockburn,15 who stressesthe people side of software devel-
opment. He points out that the human fac-
tors have dominance over any other factor
and that the development process must con-
sider the human factors within the develop-
ment team, as well as those of the end users.

No matter what our company’s ultimate
software development process turns out to
be, it must address Cockburn’s very real 14
tenets, including these four:

■ People act according to their reward.
■ Recognize the presence of dominant

personalities.
■ People work in certain ways better than

others.
■ The communications load can soon

dominate the project.

Until a company understands, accepts, and
finds ways to address Cockburn’s tenets,
every software development process will
face challenges that could be more easily
solved by applying some of the principles
we’ve discussed here.

We’re not suggesting that usability tech-
niques are a panacea to every software de-
velopment ill, but we are making an appeal
to implement some of the principles we’ve
discussed. Focusing on the user early in the
process—indeed, throughout every develop-
ment stage—goes a long way toward
achieving two of the holy grails of develop-
ment: improved product quality and elimi-
nation of rework.

No product will ever be “perfect.” And
eliminating rework completely will of course
never be possible, because—after all—soft-
ware development is almost endlessly itera-
tive, with shifting user needs and the resulting
requisite upgrades. But we can still measure
several benefits in very tangible terms: user
satisfaction. Injecting the user’s voice early in
the process is our main objective. 

References
1. Software Eng. Inst., Carnegie Mellon Univ., The Capabil-

ity Maturity Model: Guidelines for Improving the Soft-
ware Process, Addison-Wesley, Reading, Mass., 1995.

2. H. Beyer and K. Holtzblatt, Contextual Design: Defin-
ing Customer-Centered Systems, Morgan Kaufmann,
San Francisco, 1998.

3. L.L. Constantine and L.A.D. Lockwood, Software for
Use: A Practical Guide to the Models and Methods of
Usage Centered Design, ACM Press, New York, 1999.

4. “How to Stop Computer Waste: An Interview with Dr.
Charles B. Kreitzberg, President, Cognetics Corp.,”
Leaders Magazine, vol. 20, no. 4, 1998.

5. D.C. Gause and G.M. Weinberg, Exploring Require-
ments: Quality before Design, Dorset House Publishing,
New York, 1989.

6. J.T. Hackos and J.C. Redish, User and Task Analysis for
Interface Design, John Wiley & Sons, New York, 1998.

7. IEEE Std. 830-1993, Recommended Practice for Soft-
ware Requirements Specifications, IEEE, Piscataway,
N.J., 1993.

8. L.C. Kubeck, Techniques for Business Process Redesign:
Tying It All Together, John Wiley & Sons, New York,
1995.

9. D. Leffingwell and D. Widrig, Managing Software Re-
quirements: A Unified Approach, Addison-Wesley,
Reading, Mass., 2000.

10. D.A. Norman, The Invisible Computer: Why Good
Products Fail, the Personal Computer Is So Complex,
and Information Appliances Are the Solution, MIT
Press, Cambridge, Mass., 1998.

11. J. Rubin, Handbook of Usability Testing: How to Plan,
Design, and Conduct Effective Tests, John Wiley &
Sons, New York, 1994.

12. S.H. Spewak, Enterprise Architecture Planning: Devel-
oping a Blueprint for Data, Applications and Technol-
ogy, QED Publishing Group, Boston, Mass., 1993.

13. E. Yourdon et al., Mainstream Objects: An Analysis
and Design Approach for Business, Prentice-Hall, Up-
per Saddle River, N.J., 1995.

14. D.J. Mayhew, The Usability Engineering Lifecycle,
Morgan Kaufmann, San Francisco, 1999.

15. A. Cockburn, “Growth of Human Factors in Applica-
tion Development,” http://members.aol.com/
acockburn/papers/adchange.htm (current 17 Jan. 2001).

J a n u a r y / F e b r u a r y  2 0 0 1 I E E E  S O F T W A R E 53

About the Authors

Jean Anderson is a senior usability analyst and user interface designer at Siemens Health
Services (formerly Shared Medical Systems). She authored the first edition of SMS’s User-Centered
Design Handbook. Her interests include designing Web application interfaces and helping develop-
ment teams commit to the use of usability principles. Anderson is a member of ACM SIGCHI. Contact
her at Siemens Health Services, XO6, 51 Valley Stream Parkway, Malvern, PA 19355; jean.ander-
son@ smed.com.

Francie Fleek is a usability consultant. While working at SMS,
she pioneered the creation and establishment of the usability processes. Her interests include clinical
and security software for hosipitals and ambulatory care. She received an MS in technical and sci-
ence communication from Drexel University. She is a member of the IEEE, the Usability Profession-
als’ Association, and ACM SIGCHI. Contact her at franciefleek@compuserve.com.

Kathi Garrity is a lead systems analyst at the Vanguard Group.
While at SMS, she was instrumental in the effort to develop a com-
pany-wide software process. Her interests include software development, all forms of methods
and requirements analysis, usability testing, and business applications. Contact her at
klgarr@hotmail.com.

Fred Drake is a management consultant. While at SMS, he led
the usability effort. His interests include improving the creation and

quality of product information and moving it from an extrinsic support to a part of the prod-
uct. He also is interested in technical documentation, online help, and technical training. He re-
ceived his BS and MS in aerospace engineering from the University of Virginia. Contact him at
freddrake@earthlink.net.



5 4 I E E E  S O F T W A R E J a n u a r y / F e b r u a r y  2 0 0 1 0 7 4 0 - 7 4 5 9 / 0 0 / $ 1 0 . 0 0  ©  2 0 0 1  I E E E

the demise of e-commerce sites when sites
are too late, too buggy, or too complex for
ease of use.

Many Internet analysts correctly pre-
dicted that a significant number of busi-
ness-to-consumer sites would fail during the
year 2000 due to a lack of customer reten-
tion and repeat sales. Webmergers esti-
mated that 150 dot-coms failed during
2000 and more will follow this year.3 Those
sites that continue to succeed have and will
expend significant resources modifying
their sites to improve customer retention.

Many of the dot-com statistics do not
take into account the global aspect of on-
line marketing. The potential for financial

gain in a global market is great, yet little is
known about global ventures’ success rates
in terms of meeting customer needs on a lo-
cal level. On a global scale, we could argue
that cultural diversity and sensitivity must
be considered to ensure that the online
shopping experience is the same for each
customer regardless of locality. The fierce
online competition that has led to the de-
mise of poorly designed online sites nation-
ally may occur globally if nothing is done to
address global usability.

What can be done strategically to reach
out to a global market? We propose the use
of a Web-based usability assessment model
that promotes customer satisfaction as an

focus
A Global Perspective on
Web Site Usability 

Shirley A. Becker and Florence E. Mottay,

Florida Institute of Technology 

online business failures are increasing as customers turn away from Online

Online business
failures are
increasing as
customers turn
away from
unusable or
unfriendly sites.
From a global
perspective,
usability
requires cultural
sensitivity in
language
translation,
along with the
appropriate use
of color, design,
and animation.

“There is a widening customer experience gap online. Companies
who bridge this gap will win.”1

A
lthough many companies have succeeded in developing online
business applications, numerous others have failed. Many of the
failures resulted from a lack of corporate vision by not taking
Web usability into account. A study by Deloitte and Touche

stated that approximately 70 percent of retailers lack a clearly articulated e-
commerce strategy and considered their site as testing the waters for online
demand.2 This corporate “build it and they will come” mentality has led to

usability engineering



integral part of online business application
development. This usability assessment
model is an outgrowth of our collaboration
with industry in the pursuit of more effec-
tive online development efforts. From a
global perspective, our work is in an ex-
ploratory phase. However, with the current
expansion of online business applications in
the global market, we believe our assess-
ment findings can be useful.

Strategic usability factors
Thomas Powell4 formally describes Web

usability as allowing the user to manipulate
the site’s features to accomplish a particular
goal. The targeted customer assesses usabil-
ity for simplicity, understandability, and
ease of use. The perception of usability is in-
fluenced by user characteristics, such as gen-
der, age, educational level, and technology
skills. Usability perception is also affected
by cultural differences associated with, for
example, design layout, use of color and an-
imation, and information content.

We developed the usability assessment
model, which Figure 1 shows, to identify
and measure usability factors that impact
a customer’s online experience. We’ve ex-
panded these factors into more than 100
usability elements, not shown for space
reasons, that have been used during us-
ability assessments of commercial sites.5

The following usability factors are briefly
defined.

Page layout
Page layout is the visual presentation of

the Web page by means of background
color, white space, horizontal and vertical
scrolling, font size and color, and other de-
sign elements. The layout affects ease of use
and quick identification of page compo-
nents. Layout can be influenced by cultural
differences in usability, such as the signifi-
cance of a particular color, use of graphics
(for example, country flags or symbols), or
textual organization (left to right or top
down).

Navigation
Navigation is the navigational schema in

terms of breadth and depth of search paths
and traversal mechanisms. Simplicity is pro-
moted through the effective use of links,
frames, buttons, and text. Navigational

considerations, from a global perspective,
include ready access to other country sites
from a home page (understandable in any
native language) or via a navigational
schema on each page. Figure 2 illustrates
global aspects of navigation on a Web site.

Design consistency
Design consistency is the consistent loca-

tion of page components within and across
pages. Various components requiring con-
sistency include textual descriptions, labels,
prompts, and messages. Consistency of
color is required for links, background, and
text, among others. Design consistency pro-
motes ease of use by applying a common
look and feel to each page in a particular
site or across global sites. Figure 3 shows a
high level of design consistency in Yahoo’s
various country Web sites.

Information content
Information content includes timely and

correct error messages, prompts, button la-
bels, textual descriptions, help, and cus-
tomer service information. From a global
perspective, information translated from
one language to another should be gram-
matically correct, not archaic, and appro-
priate for cultural differences. Local termi-
nology for a shopping cart, for example,
includes shopping trolley and shopping bag.
Figure 4 shows an example of effective in-
formation content with buttons appropri-
ately labeled for local use.

J a n u a r y / F e b r u a r y  2 0 0 1 I E E E  S O F T W A R E 55J a n u a r y / F e b r u a r y  2 0 0 1 I E E E  S O F T W A R E 55J a n u a r y / F e b r u a r y  2 0 0 1 I E E E  S O F T W A R E 55J a n u a r y / F e b r u a r y  2 0 0 1 I E E E  S O F T W A R E 55

Strategic goals
• Customer satisfaction
• Financial
• Business process effectiveness
• Learning and innovation Design

layout
Navigation Design

consistency

Information
content

Clearly
labeled
fields

. . .
Facilitation

of data
entry

Performance

Customer
service

Reliability

Security

Environment
• Browser
• Monitor
• Modem
   . . .

User profile
• Age
• Gender
• Computing skills
• Native language
   . . .
Localization factors
• Reading
• Language
• Custom
   . . .

Usability
assessment

Figure 1. The usability
assessment model
incorporates usability
factors as well 
as the user profile
and computing
environment. All 
of these affect a
customer’s perception
of Web site usability.



Performance
Performance is measured according to

consumer wait and system response times.
Currently, there is significant global dispar-
ity in terms of modem speed and personal
access to the Internet. Cultural sensitivity
translates into sensitivity concerning down-
load time. Performance-related cultural in-
sensitivity is demonstrated by the high use
of animation in many Asian and South
American Web sites affiliated with US com-
panies (we found animation disparity for
European and Japanese-based companies as
well). Yet their North American and Euro-
pean sister sites, where Internet access with

higher modem speeds is more readily avail-
able, minimize the use of animation. 

Customer service
Customer service is additional informa-

tion and support mechanisms that are read-
ily available from the organization to en-
hance the shopping experience. This
includes, for example, email and mail ad-
dresses, phone numbers, and interactive
chat rooms. It can also mean that help is
available in a native language.

Reliability
Reliability is defined in terms of site

5 6 I E E E  S O F T W A R E J a n u a r y / F e b r u a r y  2 0 0 1

Figure 2. Illustration
of navigational
aspects of global
usability. The world
map supports global
navigation by show-
ing available country
Web sites for a
selected area on the
map. The second
Web page illustrates
inconsistent global
navigation. In terms
of global usability,
not all country Web
sites navigate
consistently to other
country Web sites.
(It’s possible that the
Web sites cited in
this article have
since changed.)

Figure 3. Illustration
of design consis-
tency. Note that the
German and English
site designs look
very similar.

National Schema:
This page allows a user to
highlight a region to display a
list of countries for that 
region.

Navigation inconsistency:
There is a link from the Swedish 
to the German and US pages but
no link from the German to the
Swedish page.



crashes, downtime, error messages, and con-
sistent response times. A common usability
problem related to reliability results when
SQL, JavaScript, and other cryptic error
boxes are displayed to the end user. Another
common problem results from a miscalcula-
tion in the number of hits during peak peri-
ods of Web use. In terms of global-related re-
liability, these problems will have a major
effect on customer usability.

Security
Security is concerned with privacy and

limited access to personal information. The
security issues facing American consumers
extend to customers worldwide regarding
the misuse and unauthorized distribution of
credit card numbers, addresses, phone num-
bers, income, and other personal data.

Other usability components
Our usability assessment model includes

a user profile of the targeted customer base
and the customer’s computing environment,
which is important in ensuring that modem
speed, browser type, and screen size are
taken into account during the assessment
process. A usability assessment also consid-
ers other environmental factors. Moreover,
the user profile and environment data might
need to be localized based on a particular
country’s or region’s characteristics.

The usability assessment model also in-
cludes the organization’s strategic goals to

ensure that these are weighed during usabil-
ity decision making. Typically, strategic
goals require a balance of financial, cus-
tomer, business process, and internal learn-
ing perspectives.6 Strategic goals will dictate
whether cultural sensitivity (driven chiefly
by customer satisfaction goals) or cultural
insensitivity (driven chiefly by financial,
time-to-market goals) take priority in the de-
velopment of online business applications.

Country-centricity and usability
As a result of our study of usability asso-

ciated with US companies, we discovered
that organizations tend to develop country-
centric sites to support their global market.
(We limited our study, and so our discus-
sion, to US-centric usability, although the
usability concept could apply to any coun-
try.) US-centricity is imposing a Web usabil-
ity look and feel from an American perspec-
tive onto localized Web sites. The result
might be an emphasis of English as a pri-
mary language on all international Web
sites with little regard for native-language
support. The result might also include a
lack of concern (or awareness) for gram-
matical inconsistencies or incorrect transla-
tions to a native language.

US-centricity can come about unknow-
ingly, for example, when an English-
language Web site is directly translated into
native-language Web sites. Other possible
reasons for US-centricity are when a com-

J a n u a r y / F e b r u a r y  2 0 0 1 I E E E  S O F T W A R E 57

Figure 4. Button
labels are appropriate
for local use of a
given Web site.

The user has the option of an 
English or Chinese version of this
Web site. In either case, the button
label for the other selection is 
written in the appropriate language
for ease of use.



pany deems it economically feasible to
maintain only English-supported country
sites, translates one US-based Web design
into many international sites, or uses im-
plied design standards regardless of cultural
differences. Figure 5 illustrates this concept
of cultural insensitivity whereby site pages
for global use are written in English.

Usability problems that we encountered
range from simple grammatical mistakes to
the overuse of animation, which severely
slows download time. A number of US-
centric usability issues can negatively affect
a local customer’s online experience:

■ The use of culture-specific icons may be
inappropriate, confusing, or unknown
at a local level. A common example is
the shopping cart icon. Other countries
use different terminology to represent
the shopping container, such as a trolley
or a bag.

■ The use of a particular color for back-
grounds, error messages, or textual in-
formation may be inappropriate, con-
fusing, or misleading. A color might
have different meanings in different
countries. The color red means error or
warning in the US although this isn’t the
case in Asian countries. One or more
colors might represent nationalism for a
country. Yellow, for example, is found
on many German sites, as this is a na-
tional color.

■ Commonly used English words and
phrases, as well as trademarks, are often
not translated into the native language. Lo-
cally, these words might be misunderstood,
difficult to pronounce, or their meaning
might be unknown (see Figure 6).

■ Direct translation of English to a native
language can result in unintuitive or
confusing labels and instructions. On
one particular site, the English word
“map” was translated directly into the
French word “plan,” which is not self-
explanatory in French. Plan du site—
plan of the site—would have been a bet-
ter phrase for improved readability.
Figures 7 and 8 show examples of Web
sites in which the direct translation
might affect local usability.

■ A main or home page for accessing
country or regional sites is in English.
The user must select a country or option
from a list of English words with no
translation support for the native lan-
guage. (Some sites have remedied this in
part by providing a visual map of the
world, as Figure 2 shows).

■ The use of animation varies by country
site. For several US companies, their
Asian and Central and South America
sites have significant animation when
compared to North American and Euro-
pean sites. For several European and
Japanese sites, the US site contained
more animation. Figure 9 shows an ex-
ample of a European company with
varying degrees of animation associated

5 8 I E E E  S O F T W A R E J a n u a r y / F e b r u a r y  2 0 0 1

Figure 5. Several
international sites
that are in English.
These sites illustrate
the reliance on the
English language for
international sites.
The user would 
have to understand
English, for example,
to ask for directions
in a native language.

Figure 6. An example of a site with potential for confusion:
Selected English words are not translated into the native
language. In this case, the English words are difficult to
pronounce and may not be understood in French.

The France site is in
English.

English words
hard to pronounce
in French.

The site is in English.
Directions provided for Euro-
pean travel may be selected
in many different languages.



with its country sites.
■ Navigational schema varies by country

site. Inconsistencies in navigation make
it difficult to traverse consistently across
sites. Some country sites allow access to
a home page; others allow access to a
particular region of the country, while
others access all countries (Figure 2
shows this limitation).

Usability strategies
In pursuing a global market, organizations

should be sensitive to cultural differences
that might impact usability. Several strategies
are available that can help with usability, de-
pending on the organization’s goals.

Common design
A general design layout, with little or no

customization for particular country sites,
might reduce the cost of upgrades and main-
tenance associated with multiple sites. For
customers accessing more than one country
site, it provides design consistency for ease
of use. It is also easier to enforce global de-
sign standards in terms of the site’s look and
feel. Figure 3 illustrates this concept for Ya-
hoo sites, which have a high level of design
consistency.

The risk associated with this strategy is
that usability can be degraded when gram-
matical mistakes, missing translations, and
inappropriate colors, for example, are intro-
duced during site construction and mainte-
nance. Usability assessments uncover these
problems before they reach the customer.

Customization
A lot could be learned about cultural

sensitivity, concerning global site deploy-
ment, from the international marketing
strategies of McDonald’s and Coca-Cola.
When visiting a McDonald’s in Aruba, for
example, there is a localized food item—
barbeque chicken—not found on the North
American menu. Similarly, Coca-Cola lo-
calizes the flavor of its products to maxi-
mize global sales. This localization concept
could be applied in the development of
global online business applications to en-
hance global usability. The downside to de-
veloping customized Web sites for each
country, however, includes higher develop-
ment and maintenance costs when each site
is built and maintained separately.

Combined common and customized design
This middle-of-the-road strategy supports

design consistency across all Web sites while
customizing a particular Web site to meet the
locality’s cultural needs. By standardizing
corporate logos, nav bars, graphics, and
other standard look-and-feel components
across all sites, companies support the us-
ability goals of simplicity and ease of use. By
customizing colors, icons, graphics, and
other Web components to meet a given coun-
try’s needs, companies promote understand-
ability and ease of use. Perhaps most impor-
tant, however, is the appropriate use of the
native language for each respective Web site.

Applying the customization or the com-

J a n u a r y / F e b r u a r y  2 0 0 1 I E E E  S O F T W A R E 59

Figure 7. Direct
translation with
potential negative
connotation. In this
case, the French
translation of the
English word has a
negative connotation.
Though its meaning
is explained to the
user, there may still
be a negative
impression.

Figure 8. English words may cause confusion when inter-
preted in a native language.

The US name “Escrow” may be interpreted as 
“fraud” in French. Though the French version of 
escrow is escroc, it is pronounced the same. The 
company explains the meaning of the word, but one
still has to question whether this will overcome the
negative connotation of the word.

Academic initiative is English but is also 
composed of two French words in reverse 
order. For a non-English-speaking French 
person, academic initiative can be understood
to have meaning but is grammatically incorrect.



bined strategy instead of a common design re-
sults in higher development and maintenance
costs. The higher costs are justified, however,
by customer satisfaction achieved with cultur-
ally sensitive sites. Although more research is
needed, the national fallout of business-to-
consumer Web sites to date tells us that fierce
competition and customer satisfaction both
play a critical role in online success.

Usability assessments: A study
Much of our work on usability assess-

ments of US sites has focused on user profile
data that included age, gender, computing
skills, and other commonly used marketing
data. When profiling the consumer for a
particular country, however, there is addi-
tional information that would assist in de-
veloping an effective online business appli-
cation. From a global standpoint, a user
profile for a country should include the level
of understanding (or popularization) of
commonly used icons (such as a shopping
cart), words (such as the GO button label),

6 0 I E E E  S O F T W A R E J a n u a r y / F e b r u a r y  2 0 0 1

Figure 9. Animation
and performance
issues. These two
examples illustrate
country centricity in
terms of animation
and the impact on
performance. In each
case, the country-of-
origin Web site has
less animation than
the other country
site.

Table 1
Comparison of country Web sites for a software company. The study was 
conducted using a 56K modem, 15-inch monitor on a notebook computer, 

and Microsoft’s Internet Explorer browser.
Country Use of animation Horizontal scrolling Oversized graphics English content

(Scale 1 – 5)1 (Yes or no) (Yes or no) 2 (Scale 1 – 5)1 

US No animation Yes No Not applicable
Australia No animation Yes No Not applicable
Sweden 3 No No 1
France 3 No No 1
Japan No animation No No 2
China 3 No Yes 1 (button label GO)
Brazil 1 Yes Yes 4

1 Likert scale where 1 is the lowest point of allocation and 5 is the highest. A 1 indicates low significance; 5, high significance.
2 Oversized graphics waste valuable information space and require more vertical or horizontal scrolling to find information. 

The download time for the US site versus 
the Argentina site is significant. The Argentina
site has significant animation, which the US
site does not have.

US site navigation is complex 
because of the extensive use of
animation and frames. The Dutch
site has little animation and is very
simple in design.



and colors (such as red). A usability assess-
ment, based on the model in Figure 1, can
uncover this information.

To illustrate the importance of usability
assessments in uncovering design flaws, we
compared seven country sites for a US-
based, global software company. Table 1
summarizes the results. The usability ele-
ments included animation, horizontal scroll-
ing, graphics, and English content. 

It’s interesting that although these sites
were customized, each had usability prob-
lems. The US and Australian sites did not
have animated components, thus minimiz-
ing download time. However, both sites
made use of horizontal scrolling, which neg-
atively impacted readability. The China and
Brazil sites had oversized graphics, which
wasted valuable information content space.
All non-English sites had various amounts
of English embedded in the text.

The company that we studied and summa-
rized in the table is a large, well-established
software company selling multiple products
in an international market. Common aspects
of all the company’s sites included consistent
use of background colors, fairly consistent
page design, mixed English with native lan-
guage, good use of vertical white space, and
the use of the folder design standard (popu-
larized by Amazon.com design). 

T he number of non-US customers us-ing online business applicationscontinues to increase very rapidly.
To take advantage of this opportunity, com-
panies must understand the target market in
terms of localized and common online
needs. In this respect, we have only just be-
gun to understand the usability issues that
influence short- and long-term use of online
business applications.

We are developing a tool, an automated
environment, that will let users enter their
assessment of a particular Web page or site.
The tool implements the usability model
shown in Figure 1. It supports data entry for
one or more selected usability elements in
order to analyze the user’s perspective on

Web site usability. The tool’s report genera-
tor allows for data analysis based on user
profile or environmental selection criteria.
Our future endeavors will expand our tool
to incorporate our findings on global us-
ability for more effective assessments.

Acknowledgment
We would like to thank Anthony Berkemeyer and

Natalie Roberts for their usability expertise and their
invaluable assistance in uncovering global usability
issues.

References
1. M. Hurst and E. Gellady, “Building a Great Customer

Experience to Develop Brand, Increase Loyalty and
Grow Revenues,” www.creativegood.
com/creativegood-whitepaper.pdf (current 16 Jan 2001).

2. R. Spiegel, “Report: 70 Percent of Retailers Lack E-
Commerce Strategy,” E-Commerce Times; www.
ecommercetimes.com/news/articles2000/000126-1.shtml
(current 16 Jan 2001).

3. J. Weisman, “E-Commerce 2000: The Year of Living
Dangerously,” E-Commerce Times, 29 Dec. 2000;
www.ecommercetimes.com/perl/story/6380.html 
(current 16 Jan 2001).

4. T. Powell, Web Design: The Complete Reference, Os-
borne McGraw-Hill, Berkeley, Calif., 2000.

5. S. Becker, A. Berkemeyer, and B. Zou, “A Goal-Driven
Approach to Assessing the Usability of an E-commerce
System,” Cutter Information Technology J., Apr. 2000,
pp. 25–34.

6. R.S. Kaplan and D.P. Norton, The Balanced Score-
card—Translating Strategy into Action, Harvard Busi-
ness School, Boston, 1996.

J a n u a r y / F e b r u a r y  2 0 0 1 I E E E  S O F T W A R E 61

About the Authors

Shirley A. Becker is a professor of computer science at the Florida Institute of Technol-
ogy, Melbourne, and codirector of its Software Engineering Research Center. Her funded re-
search includes Web usability and testing, Web-enabling tools and technologies, e-commerce
systems development, and database systems. She recently served as editor of the Journal of
Database Management and serves on several editorial review boards. Becker received her MS
and PhD in information systems from the University of Maryland, College Park. She is a
member of the IEEE, the ACM, and the Association for Women in Computing.

Florence E. Mottay is a graduate student in software engineering and a research as-
sistant at the Center for Software Engineering Research, Florida Institute of Technology, Mel-
bourne. Her research interests are in software testing, formal languages, mathematical mod-
els, and e-commerce. She was awarded for excellence in mathematics by the United States
Achievement Academy (1997) and for academic excellence by the American Association of
University Women (1998). Mottay received a BS in applied mathematics from Florida Institute
of Technology.

Contact the authors at the Florida Inst. of Technology, 150 West University Blvd., Melbourne,
FL 32901; abecker@cs.fit.edu; fmottay@fit.edu.



6 2 I E E E  S O F T W A R E J a n u a r y / F e b r u a r y  2 0 0 1 0 7 4 0 - 7 4 5 9 / 0 0 / $ 1 0 . 0 0  ©  2 0 0 1  I E E E

focus

The growth of e-commerce and business-
to-business applications has created an un-
precedented emphasis on knowing our
users and designing usable applications.
However, backing corporate commitments
to usability with user-driven development
processes is a challenge. Designers struggle
to design new applications, defining Web
user interface standards as they go, all the
while under pressure to deliver applications
faster—in “Web time.” These problems are
compounded in many start-ups, which have
little design process infrastructure, much
less human factors methodology, in place.

With little historical data about Web ap-
plication user interface and usability stan-
dards, human factors engineers are search-
ing for ways to balance three different
approaches to Web-based usability engi-
neering: transferring traditional application
design techniques to the Web environment,
relying on emerging Web design standards,
and conducting new research into what

Web application users want and need. De-
velopers frequently ask usability profession-
als, “What’s the difference between a Web
site and a Web application?” “Should I con-
form to Web site standards or Windows
standards when designing Web application
screens?” and “What should the Cancel
button on a Web form do?”

To complicate matters, developers focusing
on getting Web applications to market in Web
time often means they cut back on planning
and design in the development process. Over-
all, the use of software engineering processes
is in decline.3 The result: human factors engi-
neers are pressured to provide unprecedented
usability in a fraction of the time they need.

This article presents a case study of how
Decisionism, an analytic-applications com-
pany, redefined its software development
process to design usable Web applications
in Web time. In the midst of these process
changes, Broadbase Software acquired the
company. The development process that

Designing User-Centered
Web Applications in Web
Time

Molly Hammar Cloyd, Broadbase Software

As designers
struggle to
develop Web
applications “in
Web time,” they
are under the
added pressure
of delivering
usability. 
This author
describes her
company’s
successful
transformation
to user-driven
processes for
designing 
e-commerce
applications. 
She also offers
strategies for
introducing
human factors
methods into a
reluctant
development
organization.

U
sability has moved from a “nice to have” to a “must have” com-
ponent of e-commerce application design.1 In the past, customers
purchased desktop applications and then struggled to learn how
to use them or called for technical support. Now, they shop in a

try-before-you-buy model. If they can’t navigate your site, they are a few
clicks away from your competition. Even if you’re in an industry with few
competitors, users’ time and attention are at a premium.2

usability engineering



Decisionism pioneered is now the basis for
the user-centered design group at Broadbase
Software.

Our challenge
Our organization’s decision to enter the

B2B Web application arena shifted us from
being a traditional software developer to a
Web application provider. Specifically, we
were faced with these challenges: 

■ Shifting the development organization’s
mindset from a feature-driven approach
to a user-goal-driven one.4 Rather than
generate lists of product features (what
our product would do), we wanted to
set requirements based on what users
would be doing with our product.

■ Changing the organization’s view to
human factors methods. Prior to enter-
ing the Web application arena, Deci-
sionism did not have a human factors
group, so its addition represented a
change in the corporate culture.

■ Introducing a design process in an organ-
ization in which team members were re-
luctant to be bound by procedures or
heavy project documentation require-
ments. Our challenge was to design a
process comprehensive enough to be re-
peatable and to support introducing new
team members and technologies, without
being cumbersome.

■ Defining an all-new product, starting
with very little knowledge about poten-
tial users and no concrete information
about how users would perform tasks
with the new application.

■ Having a limited design, development,
and quality assurance staff along with a
corporate goal to be first to market with
a B2B analytic application.

■ Designing a development process that
would allow for the thorough investiga-
tion of users’ characteristics and goals
yet would facilitate a rapid application
development life cycle.

■ Making an architectural shift from a
user interface that is tightly bound with
functional components to a flexible one
that could be changed with minimal im-
pact to the underlying code.

Our Web application design process
Decisionism redefined its development

process by placing human factors methods
at the core. We stripped away our existing
software development process and started
over. In a matter of days, we outlined the
human factors methods and deliverables
that would be required to

■ determine who our new application’s
users would be, what their goals are,
and how they work;

■ establish overall Web application user
interface standards;

■ identify usability goals for the new
application;

■ communicate usability architecture re-
quirements to the developers;

■ determine the application’s overall flow;
■ design the user interface, including site

maps, prototypes, and usability tests; and
■ produce a user interface specification

that supports developers in program-
ming the interface but does not take
months to write.

With this process outlined, we asked the
development, quality assurance, documenta-
tion, and marketing leads to add their pieces
to the process. We sequentially added each
functional group’s tasks until all the people
on the team were satisfied that the process
met their needs. The resulting development
process is centered on human factors meth-
ods. Every human factors deliverable is a crit-
ical input to other functional teams’ work.

We defined five phases for the require-
ments and design process:

1. Condensed user and user-goal analysis.
2. Proof of concept (prototyping).
3. Combined site maps and storyboard 

content.
4. Use cases with screen mockups.
5. Hand-off of use cases and screen mock-

ups to development.

Condensed user and user-goal analysis
Prior to the design phase, the business de-

velopment group completed a market analysis
of prospective customer companies. Using
this information, we spent one week sketch-
ing out a preliminary picture of our prospec-
tive users, identifying such factors as their
goals, skill level, and measures of job success.
Everyone on the team—the lead architect,
programmers, services members, the quality

J a n u a r y / F e b r u a r y  2 0 0 1 I E E E  S O F T W A R E 63

Decisionism
redefined its
development
process by

placing human
factors methods
at the core. We
stripped away
our existing

software
development
process and
started over. 



assurance and documentation lead, and the
vice presidents of engineering and business
development—participated in creating user
profiles. This initial look at users gave the
team a starting point for identifying the tasks
that users would perform with the application
and for creating a prototype to use in subse-
quent user analysis and feedback sessions.

In Mastering the Requirements Process,
James and Suzanne Robertson describe the
requirements process as determining “the
business problem to be solved ... and what
the product will do to contribute to a solu-
tion.”5 Unfortunately, in many companies,
this process is abbreviated because of tight
deadlines. The functional requirements doc-
ument becomes merely a shopping list of
features that engineers prioritize and iden-
tify trade-offs to determine which features
can be implemented in a given release.

In taking a user-centered approach, we
steered away from feature lists and focused
on a handful of real-life user problems or
goals that our product would accomplish.
For example, instead of listing requirements
such as “Display of multiple analytics on a
single page,” one of our requirements was
“Enable users to determine the best auction
starting price for a commodity.” This re-
quirement led to an Offer Optimizer soft-
ware module that not only displayed multi-
ple analytics on one page but also supported
users in making smart buying and selling
decisions in a B2B market.

With our preliminary user profiles and
user goals in place, we started an ongoing
process of meeting with potential customers,
watching them work, and asking for their
feedback on user interface prototypes. In ad-
dition to soliciting feedback from a variety of

B2B companies, we created a close develop-
ment partnership with 20tons.com, a market-
place information provider for the plastics in-
dustry. They acted as subject matter experts
and provided us with ongoing feedback and
input into user profiles and use cases through-
out our design and development processes.

Proof of concept
Because we were introducing a brand-

new product idea and starting with so little
user information, we created a paper proto-
type to convey our initial product vision to
team members. This served as a starting
point for gathering requirements and usabil-
ity feedback from prospective users.

We chose paper prototyping rather than
functional prototyping for three reasons:

■ It was faster to mock up and revise de-
signs than coding screens.

■ The designs clearly had not yet been
coded, so reviewers did not hesitate to
suggest changes.

■ Developers were not tempted to use al-
ready written code.

Once we were satisfied with our initial
paper prototypes, we created PowerPoint
slides of the proposed user interface (see
Figure 1). We used these to gather feedback
about our overall product requirements and
interface design approach. The PowerPoint
prototype conveyed our overall vision for
the product yet was general enough to spur
design conversations with users.

Because we wanted to gather require-
ments as well as usability feedback, we
used cognitive walkthrough to evaluate our
prototype design. In a cognitive walk-

6 4 I E E E  S O F T W A R E J a n u a r y / F e b r u a r y  2 0 0 1

Figure 1. We used a
paper prototype to
convey our initial
product vision to
team members and
from this developed
a PowerPoint demo
screen to test with
users.



through, prospective users tell the facilita-
tor what their goals would be for using the
product, and then they guess where each
navigation path will take them and explain
how they would expect to perform certain
tasks using the prototype design.6 These
methods expose the users’ goals and expec-
tations and identify potential navigation
pitfalls in user interface designs. Cognitive
walkthrough proved to be a valuable tech-
nique for gathering usability data on proto-
types that were not fully functional. We
used it to evaluate our prototype with five
users at two net market companies. Ideally,
we would have gathered feedback from a
larger sample of users. However, with our
time constraints and the difficulties we had
in finding users, we collected as much in-
formation as we could before moving on to
the next phase.

Combined site maps and storyboard content
Armed with a better understanding of our
users, we were ready to build a site map, an
aerial view of the application showing how
the user interface screens would flow from a
user’s perspective. To save time and make
the site map easier for reviewers to concep-
tualize, we built storyboarding components
directly into our site map. Whereas many
site maps only contain representations of
each screen and the navigation between
screens, our site map included lists of each
screen’s content. By presenting user goals,
navigation, and screen content in the con-
text of the overall application flow, the site
map was the converging point for user-
driven and technical product requirements
(see Figure 2).

We conducted a series of intensive review
sessions to get input and approval from every

J a n u a r y / F e b r u a r y  2 0 0 1 I E E E  S O F T W A R E 65

Usage | Reports | Documents | Users | Desktops 

Usage reports (determined by
Decisionism)
News or important information.

Administrator Home Page

Report 1

User clicks Change Report

User clicks Save

Report 2

Report displays in a new browser window,
(without browser buttons or navigation
panel—this is just a window for working
with the report). User can drill down,
change dimensions, etc.

View/Change Report

Grid/list of reports, with columns: Report
name, report category, price, active/inactive.
User can sort by columns, change report
attributes directly in the grid or click ‘Change’
to bring up change form, click the report name
to view the report, or click Delete next to a
report.

Manage Reports

Usage | Reports | Documents | Users | Desktops 

Import list
of Reports

Browse for spreadsheet of reports attributes
Specify delimiter
Specify file to write errored records to

Import List of Reports Select File for Import

Usage | Reports | Documents | Users | Desktops 

Import

The report list was imported successfully

Confirm Import

Usage | Reports | Documents | Users | Desktops 

Return to
Reports

The report list was imported, with the
exception of the following reports:

Report Name—why it failed

Tell user to check records in error file and
rerun the import with just those errors.

Notify User of Import Errors

Usage | Reports | Documents | Users | Desktops 

User specifies:
Location
Name
Private or shared (if shared, must specify
who can see it)

Save Report

Return to
Reports

View error
file

Export list
of Reports

Add
Report

Manage
categories

Windows browse dialog

Figure 2. Combined
site map and 
storyboards made it
easier for reviewers
to conceptualize.



Having the
overarching
site map in

place made it
possible for us

to hand off
sections of the
user interface

to be coded
without losing

continuity
across

incremental
designs.

member of the development, marketing, serv-
ices, and executive teams. We also gathered
feedback from our 20tons.com development
partner. This feedback and approval process
was critical to our ability to develop the ap-
plication quickly. As a group, we walked
through every screen of the application, con-
sidering the task flow and functional require-
ments from a user’s perspective. The process,
though tedious, ensured that everyone in-
volved in the application’s design, develop-
ment, and marketing was in full agreement
about its scope and flow. This process also
identified and forced us to resolve contradic-
tory visions of the application scope or flow
early in the design process.7

After the development, marketing, and
business development teams signed off on
the prototype, we created detailed designs
for each screen and included them in a mod-
ified use case document. Together, the site
map and the modified use case document
took the place of the traditional user inter-
face specification document.

Use cases with screen mock-ups
We expanded each user goal identified in

the condensed user and user-goal analysis
phase to include use case information. The
lead architect, lead developer, and human
factors engineer jointly contributed to use
case documents. With slight modifications of
the Rational Unified Process use case tem-
plate,8 our use cases embodied users’ goals
and motivations and functioned as develop-
ers’ guidelines for implementation.

Our use case document was organized by
user task (for example, “Viewing a Report”).
For each process, the document provided de-
tails about the look and feel, task flow, and
technical requirements for implementing the
use case in the application. For each user goal
or task, the document included the following: 

■ the users’ goals and, if applicable, how
users would know when they met each
goal;

■ frequency and criticality of tasks;
■ usability requirements of the user inter-

face supporting each use case;
■ a picture of the screen (this was a place-

holder section in early versions of the
document, later filled in with a design
diagram);

■ a list of data elements (such as buttons,

links, or display-only items) and how
they would respond to users’ actions;

■ descriptions of how interactions with
the data elements would be validated;

■ requirements for entering and exiting
each screen; and

■ requirements for future releases that might
affect how a use case is implemented.

Figure 3 shows an example use case and
screen mock-up that we developed using the
modified use case template.

Critics of use cases argue that it is a time-
consuming, arduous task that can delay im-
plementation. Others argue that there is no
way of knowing when the set of use cases is
complete.9 However, our team subscribed to
the view that in rapid development environ-
ments, designers should select a small num-
ber of users and use cases that represent the
entire product and then develop a user in-
terface architecture that can extend to the
whole product.10 We questioned the cost-
effectiveness of creating an exhaustive set of
use cases with such limited time. Our aim
was to identify the users’ most important
goals and then develop an application that
would enable users to meet those goals,
meet the product requirements, and be ex-
tensible to outlying goals and tasks.

We generated use cases for each product
requirement. We focused on the activities
users would perform most frequently with
the application and activities most critical to
the users’ success with the product. These use
cases gave us the framework we needed to
develop the application’s core functionality.

Combining use cases, screen mock-ups,
and screen descriptions into a single docu-
ment saved time and also ensured that use
cases, user interface designs, and functional
requirements were kept in sync. Jointly devel-
oping use cases put human factors’ influence
into a context that was already familiar to de-
velopers. Using the modified use case tem-
plate ensured that user goals were viewed as
integral to every use case. This meant that
technical requirements in use cases were
driven by the flow of events from a user’s per-
spective. Also, efficiencies were gained by cre-
ating a use case model that incorporated both
developers’ and users’ needs instead of em-
barking on separate activities to define hu-
man factors requirements and development
requirements.

6 6 I E E E  S O F T W A R E J a n u a r y / F e b r u a r y  2 0 0 1



Hand-off of use cases and screen mock-ups
to development

The design and development process was
iterative. Once we identified the big picture
of the application flow in the site map, we
created use case documents for specific ar-
eas of functionality. When a use case or
group of use cases was complete, we handed
those off to engineers for development. For
each set of use cases, the director of devel-
opment produced an architecture design for
that iteration of the product.

As engineers coded one set of use cases,
the design team created the next set of use
cases. If engineers encountered implementa-
tion issues that required user interface
changes, we responded by quickly mocking
up alternative screen designs. Having the
overarching site map in place made it possi-
ble for us to hand off sections of the user in-
terface to be coded without losing continu-
ity across incremental designs.

The entire product design process, from
user analysis to hand-off of the design to de-
velopers for coding, took about 12 weeks.

Implementing human factors
processes in a reluctant
organization

Several members of our development
team were reluctant to adopt development
processes, let alone one grounded in human
factors methods. Some had come from large
companies where they’d had negative expe-
riences with ISO or slow-moving waterfall
processes. Others were concerned that the
human factors engineer would design the ap-
plication in isolation and hand down designs
that the developers would have no control
over. Most were concerned that following a
process would prevent us from meeting our
time-to-market goal. With these concerns in
mind, we worked to create a process that
would help, rather than hinder, developers.
Strategies included the following:

■ Combining phases of traditional human
factors processes to ensure that our
process required minimal documenta-
tion and was not cumbersome.

J a n u a r y / F e b r u a r y  2 0 0 1 I E E E  S O F T W A R E 67

Figure 3. An enhanced use case and
screen mock-up.



■ Completing design phases in parallel
and handing off designs for coding in-
crementally. We emphasized that our
process is an iterative process, not a tra-
ditional waterfall process.11

■ Focusing on getting the developers’ buy-
in to the new design process. We did this
by involving them in every step of the
design. In The Elements of User Inter-
face Design, Theo Mandel discusses the
importance of creating a multidiscipli-
nary product design team.10 Involving a
wide range of people not only provides
the full spectrum of skills needed for
good design, but it also increases the
team’s buy-in to the design.

■ Creating a shared vision among all team
members.7 Our proof-of-concept proto-
type produced early in the design
process conveyed the product vision to
the entire company. This gave every
team member a vision of what the prod-
uct would do to help B2B users. It took
the product from seeming like something
too large and impossible to produce to
something we could actually design and
build within our time constraints.

■ Distributing articles and Web site infor-
mation to developers pertaining to Web
application usability and design. This in-
creased the developers’ awareness of the
need for usability in Web-based products.

Accruing benefits from the process
Involving the entire development team in

the design phases had a number of benefits.
It gave developers a say in what they would
be developing, and it showed them the vol-
ume of work that had to be done before
coding could begin. It gave us more com-
plete requirements and designs because of
collaborative input from multiple disci-
plines. It also shortened the calendar time
spent on each design phase. This enabled us
to do user analysis and detailed design while
staying on schedule.

We were also able to demonstrate to the
whole team the importance of identifying
our users and understanding their experi-
ences. This was the beginning of a user-cen-
tered culture at Decisionism.

Our process helped dissolve communica-
tion barriers between human factors and de-
velopment personnel. Because team members
were involved in developing user profiles and

task analyses from the start, we lost no time
communicating user research findings and
convincing developers of what users needed.
Team members stopped viewing user require-
ments as something imposed on them and
started viewing them as the purpose for the
project. Design meetings emphasized how
the application should work from a user’s
perspective.

Finally, by distributing the human factors
workload, we were able to accomplish human
factors activities in the time permitted. Instead
of the classic problem of not enough human
factors people to do the work, one human fac-
tors engineer was able to oversee all human
factors activities and keep a big-picture per-
spective of working toward usable design.

R eleased in December 2000, our ap-plication, called E-Marketplace,was the first B2B analytic applica-
tion of its kind in its market. When Broad-
base Software approached Decisionism about
acquisition, Decisionism illustrated the via-
bility of getting a B2B product to market us-
ing the proof-of-concept’s prototypes, user
profiles, site maps, and enhanced use cases.
Since Decisionism had not yet released a
B2B analytic product, this demonstration
enhanced our appeal as an acquisition can-
didate. The human factors and user inter-
face design team, now part of Broadbase
Software, is implementing the processes de-
scribed in this article for Broadbase, along
with the three other software companies
Broadbase recently acquired.

Beyond the business benefits, the devel-
opment team reported several positive re-
sults from this process. Team commitment
improved in getting the product to market.
The marketing, human factors, and devel-
opment teams worked closely together to
create a product vision and design. Creating
the project plan was greatly simplified.

Developers saw the product as a whole
instead of focusing only on the individual
features or components they were coding.
They also understood the interdependencies
between features and worked together to
make a cohesive product.

Released in
December 2000,
our application,

called 
E-Marketplace,

was the first
B2B analytic

application of
its kind in its

market.

6 8 I E E E  S O F T W A R E J a n u a r y / F e b r u a r y  2 0 0 1



Developers had time to focus on solving
implementation issues and coding the prod-
uct. Having clear site maps and screen de-
signs meant that they didn’t have to spend
time deciphering requirements documents
or worrying about details of screen flow
and layout. The application flow in the site
map expedited identifying and resolving
business and presentation logic issues in the
technical architecture and made it easy for
them to identify dependencies among fea-
tures.

Identifying navigation and application
flow problems at the site map phase mini-
mized the number of defects that our QA
engineer found during final testing. The QA
engineer used the site map as a reference in
planning test cases. Moreover, the detailed
site map and use case documents controlled
scope creep by clearly outlining what
needed to be developed.

Developers accepted human factors as a
key part of the design process and began
seeking out human factors and user interface
design team members for design guidance.

At the time of our acquisition by Broad-
base, the development team was required to
completely change the underlying technolo-
gies, development language, and third-party
components. The technology-independent na-
ture of the site maps and use cases made this
change possible. In fact, the development
team was able to make the required changes
and still deliver the product three weeks be-
fore the deadline.

Most importantly, E-Marketplace hit the
mark with B2B net markets and their cus-
tomers. While we have not yet completed
formal usability tests, we gathered subjective
feedback and cognitive walkthrough data
throughout our design and development
process. We responded to customer problems
and suggestions, and customers successfully
navigated our user interface during cognitive
walkthroughs. Ultimately, we provided cus-
tomers with a targeted analytic application
for doing business in online markets.

The first release of E-Marketplace was a
stake in the ground that redefined our design
processes and development culture. Future
plans for E-Marketplace include formal us-
ability testing and integration into the Broad-
base analytic application suite. Like the user
interface, our Web application development
process will be iterative. We also plan to inte-

grate a few more techniques into our process
for future product releases:

■ While group design and storyboarding
sessions helped us generate a broad
range of design ideas, we plan to exper-
iment with parallel design, in which de-
signers sketch screens separately before
coming together to combine efforts. We
hope this will expedite the initial screen
mock-ups and facilitate generating more
design options for the team to choose
from.

■ We plan to conduct formal usability test-
ing at multiple points along the design
process. We have received funding for us-
ability testing equipment and resources
so we can gather quantitative usability
data, identify specific areas for design im-
provements, and measure improvements
against baseline usability results.

■ We will iterate user interface designs
based on usability test results, user feed-
back, market requirements, and new
Web application technologies.10

References
1. S. Ward and P. Kroll, “Building Web Solutions with the

Rational Unified Process: Unifying the Creative Design
Process and the Software Engineering Process,” www.
rational.com/products/whitepapers/101057.jsp (current
2 Jan. 2001).

2. J. Nielsen, Designing Web Usability: The Practice of
Simplicity, New Riders, Indianapolis, Ind., 2000.

3. R. Reddy, “Building the Unbreakable Chain,” In-
telligent Enterprise Magazine, vol. 3, no. 3, www.
intelligententerprise.com/000209/feat3.shtml (current 2
Jan. 2001).

4. A. Cooper, The Inmates Are Running the Asylum,
Macmillan USA, Indianapolis, Ind., 1999.

5. S. Robertson and J. Robertson, Mastering the Require-
ments Process, ACM Press, New York, 1999.

6. J.S. Dumas and J.C. Redish, A Practical Guide to Us-
ability Testing, Ablex, Norwood, N.J., 1993.

7. J. McCarthy, Dynamics of Software Development, Mi-
crosoft Press, Redmond, Wash., 1995.

8. G. Schneider and J.P. Winters, Applying Use Cases: A
Practical Guide, Addison-Wesley Longman, Reading,
Mass., 1998.

9. B.L. Kovitz, Practical Software Requirements, Man-
ning, Greenwich, Conn., 1999.

10. D. Mayhew, The Usability Engineering Lifecycle, Mor-
gan Kaufmann, San Francisco, 1999.

11. T. Mandel, The Elements of User Interface Design,
John Wiley & Sons, New York, 1997.

J a n u a r y / F e b r u a r y  2 0 0 1 I E E E  S O F T W A R E 69

About the Author

Molly Hammar Cloyd is the hu-
man factors
engineer at
Broadbase
Software. She
joined the
company after
starting the
human factors
function at De-

cisionism, an analytic applications company
recently acquired by Broadbase. She is now
building a user-centered design group at
Broadbase. Contact her at Broadbase Soft-
ware, 4775 Walnut St., Ste. 2D, Boulder,
CO 80301; molly@broadbase.com;
www.broadbase.com.



7 0 I E E E  S O F T W A R E J a n u a r y / F e b r u a r y  2 0 0 1 0 7 4 0 - 7 4 5 9 / 0 0 / $ 1 0 . 0 0  ©  2 0 0 1  I E E E

engineering is still a first-generation field,
some of its basic ideas are widespread and
reach back to practices in industrial design2

and the “golden rules” of John D. Gould
and Clayton Lewis.3 The key principles in-
clude the analysis of a product’s intended
context of use (user skills and needs, task
requirements, and physical, social, and or-
ganizational context) at the beginning of
development; user participation throughout
the development process; early prototyping;
usability evaluation; and continuous revi-
sion based on evaluation data.4 As the
field’s methods have evolved, they have
changed the concept of usability from a
narrow product-oriented quality attribute
to the broad concept of quality of use, that
is, “that the product can be used for its in-
tended purpose in the real world.”5

However broad the latest definition of
usability is, it recently acquired a new asso-
ciate, the so-called joy of use. The notion of

joy of use is instantly appealing, though its
actual meaning is hard to grasp. In 1997,
Bob Glass said, “If you’re still talking about
ease of use then you’re behind. It is all
about the joy of use. Ease of use has become
a given—it’s assumed that your product will
work.” However, joy of use is extremely
hard to define. As Glass said, “You don’t
notice it, but you’re drawn to it.”6

The way the term joy of use is employed
in general computer and human–computer-
interaction literature reveals three perspec-
tives on the issue: 

■ Usability reductionism supposes that joy
of use simply results from usable soft-
ware and that the answer to the question
of how to design for enjoyment is al-
ready known. The only problem is how
to put usability engineering into practice.
So, joy of use appears to be just a natu-
ral consequence of excellent usability.

focus
Engineering Joy

Marc Hassenzahl, Andreas Beu, 
and Michael Burmester, User Interface Design GmbH

Joy of use 
has become a
buzzword in
user interface
design although
serious attempts
at defining it
remain sparse.
The authors
propose
systematic
methods of
taking into
account one 
of its main
determinants,
hedonic quality,
and its complex
interplay with
usability and
utility as a step
toward truly
engineering the
user experience.

O
ver the last 30 years, usability has become an acknowledged
quality aspect of a wide variety of technical products, ranging
from software to washing machines. The concept of usability
has been accompanied by the assumption that usability can 

be engineered.  Clearly, the aim of usability engineering is to devise
processes to assure that products are usable.1 Although usability 

usability engineering



This perspective discounts the qualitative
differences between simply doing a job
and enjoying doing a job.

■ Design reductionism reduces joy of use to
a quality that graphical and industrial de-
signers add to software. Designers “pos-
sess the ... skills that combine science and
a rich body of experience with art and in-
tuition. Here is where ‘joy’ and ‘pleasure’
come into the equation: joy of ownership,
joy of use.”7 This perspective assumes
that joy of use is concerned more with su-
perficial than with deeper qualities, such
as interaction style and functionality.
Therefore, it fails to acknowledge the
complex interplay of visual, interactional,
and functional qualities.

■ Marketing reductionism reduces joy of
use to a simple marketing claim. This
opinion is comparable to the perception
of usability at its advent: user-friendliness.
It is mainly a claim with no substance.

None of these perspectives seems satisfac-
tory. Given that our aim is to design enjoy-
able software systems, we should take the
analysis of joy of use as seriously today as
we took ease of use yesterday.  

Why consider enjoyment in
software design?

The most basic reason for considering
joy of use is the humanistic view that enjoy-
ment is fundamental to life. Glass said, “I
believe that products of the future should
celebrate life! They should be a joy to use. I
predict that joy of use will become an im-
portant factor in product development and
product success.”8

Although some might readily agree with
this view, others object on the grounds that
there is a radical difference between leisure
and work. The former calls for idle enjoy-
ment, the latter for concentrated work. Erik
Hollnagel has voiced a perspective against
connecting emotions (such as enjoyment)
with software design.9 He argues that hu-
man–computer interaction is basically about
efficiency and control, and that emotions in-
terfere with these attributes. For example,
one might make decisions based on highly
subjective, emotional criteria not suitable for
the rational work domain. Somewhat cyni-
cally he states, “Affective interfaces may
serve a therapeutic purpose, [to] make the

user feel better.”9 We, on the other hand, be-
lieve that the users’ well-being always mat-
ters, especially in a work domain. Technol-
ogy acceptance research has demonstrated
the positive effects of perceived enjoyment or
fun in work settings. For example, in one
study, when people enjoyed a software prod-
uct, their acceptance and satisfaction in-
creased.10 The impact of user-perceived en-
joyment on acceptance (one important
determinant of productivity) nearly equaled
that of user-perceived usefulness. In another
example, providing an enjoyable workplace
for call center agents was assumed to sustain
the quality of customer service and even in-
crease it throughout the day.11 So, in certain
work positions (those requiring “emotion
work,” such as a call center agent or hotel re-
ceptionist), enjoyment might have an impor-
tant effect on work quality instead of solely
serving a therapeutic purpose. There are
other cases where joy or fun plays a role as a
software requirement, for example, where
learning is the main system function.12

Acknowledging the positive effects of
enjoyment does not necessarily imply
knowledge of how to design enjoyable soft-
ware. The primary question is: What do we
actually have to do to design for joy of use?
Advocates of usability reductionism would
answer: “Nothing! Just provide useful
functionality so the users can easily operate
the software.” This view emphasizes soft-
ware’s role as a tool for accomplishing a
task and focuses on task-related qualities
(usability and utility). It has been shown,
however, that hedonic qualities, that is,
task-unrelated qualities, can also play a
role. For example, including hedonic com-
ponents (task-unrelated graphics, color,
and music) increased an information sys-
tem’s enjoyment and usage.13 Similarly, the
perception of hedonic quality (task-unre-
lated aspects such as novelty or originality)
substantially contributed to the overall ap-
peal of software prototypes for process
control tasks14 and different visual display
units—a standard CRT, an LCD flat screen,
and a computer image projected on the
desktop.15 Both studies demonstrate that
task-related and -unrelated quality aspects
seem to compensate for each other from the
user’s perspective. In other words, ex-
tremely usable but tedious software might
be as appealing to a user as an extremely

J a n u a r y / F e b r u a r y  2 0 0 1 I E E E  S O F T W A R E 71

The most basic
reason for

considering joy
of use is the
humanistic
view that

enjoyment is
fundamental 

to life.



unusable but thrilling one. Thus, exploring
and understanding the nature of hedonic
quality as a software requirement and, fur-
thermore, the dependence between hedonic
quality and task-related quality (utility and
usability) is a valuable road toward design-
ing for joy of use.

The driving forces behind hedonic
quality

The definition of hedonic quality as task-
unrelated quality is clearly too broad to
guide design. The driving forces behind the
scene might be more specific qualities such
as the need for novelty and change and the
need to communicate and express oneself
through objects.16

Though these may not be the only needs,
they exemplify the two-edged nature of the
forces behind hedonic quality. One part is
directed inward, concerning the individ-
ual’s personal development or growth; the
other part is directed outward, concerning
social and societal issues. If users perceive a
software product as potentially capable of
satisfying the need for personal growth and
status, it has hedonic quality. The percep-
tion of hedonic quality (or lack of it) will
affect the user’s preference for a given soft-
ware product. 

Need for novelty and change
Several areas of research have found evi-

dence of a general human need for novelty
and change. Daniel Berlyne, for example,
states that our central nervous system is de-
signed to cope with environments that pro-
duce a certain rate of stimulation and chal-
lenge to its capacities.17 We reach best
performance at a level of optimal excite-
ment, where neither overstimulation nor
monotony are present. The same notion ex-
ists in Mihaly Csikszentmihalyi’s optimal-
experience concept.18 Optimal experience
or flow describes the state when somebody
is completely wrapped up in an activity. The
crucial determinant is the certainty that the
activity is challenging but attainable—it has
the optimal level of excitement. In a home
automation system evaluation study,19 we
found that individuals with a technical job
background reported the system to be of
low hedonic quality compared to individu-
als with a nontechnical job background. We
suppose that technically educated individu-

als are more likely to possess knowledge
about existing home automation system
functionality, so they don’t find the func-
tionality excitingly new. Conversely, for in-
dividuals with nontechnical job back-
grounds, the system provided the means to
do things they could not do before. Indeed,
the focus during system design was on us-
ability and visual design rather than on
adding exciting new functionality. In this as-
pect, the experiment did not address the
technically oriented users’ need for chal-
lenge and stimulation. Strikingly, taking the
need for novelty and change into account
might unavoidably imply a reduction of us-
ability. Usability and joy of use might be
partially incompatible, because the former
requires consistency and simplicity, whereas
the latter requires surprise and a certain
amount of complexity.20

Designers need to introduce novelty with
care. User interfaces that are too novel and
unfamiliar are likely to evoke strong ade-
quacy concerns instead of hedonic quality
perceptions.21 What is needed is a way to
determine an optimal level of novelty.

Need to communicate and express oneself
through objects

This need addresses the social dimension
of using software. Robinson states that the
artifacts people choose to use can be inter-
preted as statements in an ongoing “dialog”
people have with other people in their envi-
ronment.22 We should not underestimate
the fact that using a product can evoke a
state of importance. Being an expert at
something that others do not understand,
being able to afford something that others
cannot afford, or possessing something that
others desire are strong driving forces. To
give an anecdotal example from our experi-
ence: The home automation system men-
tioned earlier had a user interface designed
to be as nonintimidating as possible in order
to encourage use by people with low com-
puter expertise. The strategy succeeded. Us-
ability tests with elderly, non-computer-lit-
erate individuals showed an astonishingly
low number of severe usability problems.
However, one participant with a more so-
phisticated technical background com-
plained about the visual design. He said it
looked like a “children’s book” and that his
friends would laugh at the system’s appar-

Designers need
to introduce
novelty with
care. User

interfaces that
are too novel

and unfamiliar
are likely to
evoke strong

adequacy
concerns
instead of

hedonic quality
perceptions.

7 2 I E E E  S O F T W A R E J a n u a r y / F e b r u a r y  2 0 0 1



ent lack of professionalism. Thus, designers
need to develop user interfaces with status
needs in mind.

Techniques for engineering
hedonic quality

There is an explicit difference between
knowing that hedonic quality could play a
role in designing interactive systems and ac-
tively accounting for it. The latter requires
practical methodical support for both de-
sign (techniques for gathering and analyzing
hedonic requirements) and evaluation (met-
rics and techniques to measure hedonic
quality). As long as you understand their
advantages and disadvantages, the follow-
ing techniques can fit into a design process
for interactive systems. 

A semantic differential for measuring 
perceived hedonic quality 
A well-known technique for measuring how
people perceive and evaluate objects is the se-
mantic differential. The differential we em-
ploy comprises seven pairs of adjectives that
characterize hedonic quality’s presence or ab-
sence, evaluated on a seven-point rating
scale. Each pair of extremes corresponds to
opposing adjectives, such as good–bad, inter-
esting–boring, or clear–confusing. Once the
participants rate the software on each char-
acteristic, we calculate a hedonic quality
“value” by summing or averaging the rat-
ings.  Figure 1 shows the semantic differential
we typically use for measuring hedonic qual-
ity14,15,19 (note that the verbal anchors used
in these studies were originally in German). 

We can apply the differential throughout
the design process for interactive systems,
from the evaluation of early mock-ups or
prototypes to fully operational systems. It
has various advantages: the usability engi-
neer does not require special training for us-
ing the differential, the participants can
quickly and easily fill it in, and the statisti-
cal analysis is straightforward. The charac-
teristics are high-level and deal with subjec-
tive user perceptions—that is, that “quality
is in the eye of the beholder.” This makes
the differential applicable to various soft-
ware products without needing to adjust it
to the product’s special features.

The differential’s general applicability is
also one of its major disadvantages. Although
it can show the extent to which users regard a

piece of software as hedonic, the underlying
reasons (determinants of hedonic quality or
lack thereof) remain unknown. However, it
is exactly the understanding of the underly-
ing reasons that proves to be most impor-
tant for stimulating and improving a soft-
ware product design, especially when it
comes to a premature construct such as he-
donic quality. Another problem associated
with the nature of hedonic quality is the
solely operational definition that the differ-
ential provides. Without a theoretically
solid definition, there is always the danger
of missing an important facet of hedonic
quality. 

Repertory grid technique
A way to overcome the differential’s

problems is the repertory grid technique
(RGT).23,24 Georg Kelly assumes that indi-
viduals view the world (persons, objects,
events) through personal constructs. A per-
sonal construct is a similarity–difference di-
mension comparable to a semantic differen-
tial scale. For example, if you perceive two
software products as being different, you
might come up with the personal construct
“too colorful—looks good” to name the op-
posed extremes. On the one hand, this per-
sonal construct tells something about you,
namely that too many colors disturb your
sense of aesthetics. On the other hand, it
also reveals information about the products’
attributes. From a design perspective, we
are interested in differences between soft-
ware products rather than differences in in-
dividuals, so we focus on what the personal
constructs of a group of users might tell us
about the products they interact with.

RGT deals with systematically extracting
personal constructs. It consists of two steps:
construct extraction and product rating. For
construct extraction, we present individuals
with a randomly drawn triad from a soft-
ware products set, marking the “design
space” we are interested in. They must an-
swer in what way two of the three products
are similar to each other and different from

J a n u a r y / F e b r u a r y  2 0 0 1 I E E E  S O F T W A R E 73

Outstanding
Exclusive

Impressive
Unique

Innovative
Exciting

Interesting

Second-rate
Standard
Nondescript
Ordinary
Conservative
Dull
Boring

Figure 1. 
Semantic 
differential for meas-
uring 
hedonic quality. 



the third. This procedure produces a con-
struct that accounts for a perceived differ-
ence. The people then name the construct
(for example, playful–serious, two-dimen-
sional–three-dimensional, ugly–attractive)
indicating which of the two poles they per-
ceive as desirable (having positive value).
We repeat the process until no further novel
construct arises. The result is a semantic dif-
ferential solely based on each individual’s
idiosyncratic view. In the product rating
step, we ask people to rate all products on
their personal constructs. The result is an
individual-based description of the products
based on perceived differences.

Designers can apply RGT in various
forms throughout the user-centered design
process for interactive systems. A promising
application might be “diagnostic bench-
marking,” for example, comparing your
current, future, and competitors’ Web
sites.25 We recently used RGT to explore the
differences between design studies for con-
trol room software resulting from a parallel
design session.21 Table 1 shows some exam-
ple constructs from this study. These con-
structs illustrate that the participants were
concerned about the adequacy of some of
the designs for a work domain. At least two
different views became apparent: some par-
ticipants believed that control room soft-
ware must look serious (constructs 1–4),
maybe to induce trustworthiness (constructs
1 and 4) and perceived control (construct
3). Other participants acknowledged the he-
donic quality of some designs (and the en-
joyment they derived from them) (construct
5) but emphasized the dichotomy between
leisure and work (construct 5 and 6). This
illustrates the rich information that we can
obtain by RGT. In the study just mentioned,
we extracted 154 constructs from 10 partic-
ipants, covering topics such as quality of in-
teraction and presentation, hedonic quality,
and adequacy concerns (participants’ belief
about the extent to which the prototype is
suitable for the task).

RGT has a number of advantages: 

■ It is a theoretically grounded23 and struc-
tured approach, but nevertheless open to
each participant’s individual view. The
focus on individual (personally meaning-
ful) constructs is a clear advantage over
the semantic differential. The differential
can only measure what we define to be
hedonic quality. In other words, the par-
ticipants must use our constructs (the
scales we provide), regardless of whether
they are meaningful to them and cover
the topics relevant to them. 

■ RGT is more efficient than comparable
open approaches such as unstructured
interviews. Focusing on the personal
constructs as data denotes a significant
reduction in the amount of data to be
analyzed compared to transcribing and
analyzing unstructured interviews. This
is especially important in the context of
parallel design, or benchmarking, when
many alternatives are under study. 

■ Personal constructs have the potential
to be design-relevant. The whole ap-
proach is likely to generate different
views on software products, embodying
various individual needs and concerns
in relation to the product and its context
of use. This again is something the se-
mantic differential neglects. 

■ The basic method can be applied to al-
most any set of software products.

The method’s main disadvantage is the
amount of effort invested. While we can use
the semantic differential as an add-on to a
regular usability test or as an online ques-
tionnaire, an RGT study is a self-contained
method in which the experimenter needs
considerable training. Another disadvan-
tage is that RGT relies on comparisons and
its application is therefore confined to situ-
ations where at least four alternatives are
available.

Shira interviewing 
Rainer Wessler, Kai-Christoph Hamborg

(University of Osnabrück), and Marc Has-
senzahl have recently developed a new
analysis method that avoids at least the mul-
tiple-alternative problem associated with
RGT. Structured hierarchical interviewing
for requirement analysis (Shira)26 is an in-

7 4 I E E E  S O F T W A R E J a n u a r y / F e b r u a r y  2 0 0 1

Table 1
Example Constructs

Pole A Pole B

1 Does not take the problem seriously Takes the problem seriously
2 Inappropriately funny Serious
3 Non-expert-like Technically appropriate
4 All show, no substance Technology-oriented
5 Playful Expert-like
6 Has been fun Serious (good for work)



terviewing technique that seeks to explore
the meaning of product attributes such as
“controllable,” “simple,” “impressive,” or
“innovative” for a specific software applica-
tion in a specific context of use.

Shira starts from a pool of attributes cov-
ering usability aspects (such as “control-
lable”) and hedonic qualities (such as “in-
novative”). We first introduce participants
to a possible software application and its in-
tended context of use—for example, a home
automation system or software for writing
one’s diary. In a second step, we ask the par-
ticipants to select an attribute from the pool
that is important to them with regard to the
software (Figure 2 shows an example deal-
ing with the attribute “simple”). Starting
from the attribute, they then list software
features that would justify attaching that at-
tribute. By repeatedly answering questions
such as “what makes a home automation
system seem innovative to you,” they will
generate a list of features that contain con-
text and the attribute’s software-specific de-
terminants (for example, “user-friendly”
and “not patronizing”). The resulting list
comprises the context level. In the third
step, the participants must produce recom-
mendations for each entry in the context
level suggesting how the actual design could
address the feature (for example, “adaptive,
learning, intelligent system that works more
or less independently and requires little at-
tention from the user”). We call this the de-
sign level. The result is a hierarchical, per-
sonal model of attributes that are important
to the participants with regard to a specified
software product, what these attributes ac-
tually mean to them, and how they can be
addressed by the design.

Shira is a systematic way to get in-depth
data and detailed insights into an individ-
ual’s expectations of a specified software
system. Its hierarchical representation facil-
itates getting a better idea of central and pe-
ripheral aspects (attributes, features, or de-
sign recommendations). In particular, Shira
has the power to gather hedonic require-
ments by using hedonic attributes as stimu-
lation. By integrating personal models into a
group model, we obtain a rich body of in-
formation about the system’s design space.

Shira is especially suited to gather infor-
mation at early stages of the design process
for interactive systems. However, it might

also be possible to evaluate software at a
later stage regarding how it fits the user’s
expectations.

Shira is still at an early development
stage. It is too early to assess advantages
and disadvantages. However, from our pre-
liminary experience with the technique, it
seems to provide detailed design-relevant
data in a structured form that facilitates in-
terpretation and integration of multiple per-
sonal perspectives.

U sability and utility are basicallyabout how well software supportspeople in getting their jobs done.
However, task-unrelated qualities can play a
crucial role. Traditional usability engineer-
ing methods are not adequate for analyzing
and evaluating hedonic quality and its com-
plex interplay with usability and utility. The
techniques we have suggested might signifi-
cantly broaden usability engineering prac-
tices by shifting the focus to a more holistic
perspective on human needs and desires. In

J a n u a r y / F e b r u a r y  2 0 0 1 I E E E  S O F T W A R E 75

Simple

User-friendly

Attribute level Context
level

Design
level

Not patronizing

System remembers previous interactions

System adapts to my habits

I only have to specify exceptions to the rule

Common sense (for example, system 
automatically excludes Saturdays and 
Sundays from the daily morning wake-up call)

The system may remind me but must not 
order me

I do not want to feel like the system knows 
everything and I know nothing

Figure 2. Portion of a
personal model gath-
ered by using Shira.



the future, we might see usability engineer-
ing evolving toward more complete user ex-
perience design—one that encompasses the
joy of use. 

Acknowledgments
This article was partly funded by the German

Ministry for Research (BMBF) in the context of IN-
VITE (01 IL 901 V 8). See www.invite.de for further
information. We are grateful to Uta Sailer for her
helpful comments on an earlier draft of the article.

References
1. J. Nielsen, Usability Engineering, Academic Press,

Boston, San Diego, 1993.
2. H. Dreyfuss, Designing for People, Simon & Schuster,

New York, 1995.
3. J.D. Gould and C.H. Lewis, “Designing for Usability:

Key Principles and What Designers Think,” Comm.
ACM, vol. 28, no. 3, Mar. 1985, pp. 300–311. 

4. ISO-13407 Human-Centred Design Processes for Inter-
active Systems,, Int’l Organization for Standardization,
Geneva, 1999. 

5. N. Bevan, “Usability is Quality of Use,” Proc. HCI Int’l
95,  Lawrence Erlbaum Associates, Mahwah, N.J.,
1995, pp. 349–354.  

6. R. Glass, “The Looking Glass,” www.sun.com.au/news/
onsun/oct97/page6.html (current 15 Jan. 2001).

7. D.A. Norman, The Invisible Computer, MIT Press,
Cambridge, Mass., 1998.

8. B. Glass, “Swept Away in a Sea of Evolution: New
Challenges and Opportunities for Usability Profession-
als,” Software-Ergonomie ’97. Usability Engineering: 
Integration von Mensch-Computer-Interaktion und 
Software-Entwicklung, R. Liskowsky, B.M. Velichkovsky,
and W. Wünschmann, eds., B.G. Teubner, Stuttgart,
Germany, 1997, pp. 17–26. 

9. E. Hollnagel, “Keep Cool: The Value of Affective Com-
puter Interfaces in a Rational World,” Proc. HCI Int’l
99, vol. 2, Lawrence Erlbaum Associates, Mahwah,
N.J., 1999, pp. 676–680. 

10. M. Igbaria et al., “The Respective Roles of Perceived
Usefulness and Perceived Fun in the Acceptance of Mi-
crocomputer Technology,” Behaviour & Information
Technology, vol. 13, no. 6, 1994, pp. 349–361.

11. N. Millard et al., “Smiling through: Motivation at the
User Interface,” Proc. HCI Int’l ’99, vol. 2, Lawrence
Erlbaum Associates, Mahwah, N.J., 1999, pp. 824–828. 

12. S.W. Draper, “Analysing Fun as a Candidate Software
Requirement,” Personal Technology, vol. 3, no. 1,
1999, pp. 1–6.

13. N. Mundorf et al., “Effects of Hedonic Components
and User’s Gender on the Acceptance of Screen-Based
Information Services,” Behaviour & Information Tech-
nology, vol. 12, no. 5, 1993, pp. 293–303.

14. M. Hassenzahl et al., “Hedonic and Ergonomic Quality
Aspects Determine a Software’s Appeal,” Proc. CHI
2000 Conf. Human Factors in Computing Systems,
ACM Press, New York, 2000, pp. 201–208. 

15. M. Hassenzahl, “The Effect of Perceived Hedonic Qual-
ity on Product Appealingness,” Int’l J. Human–Com-
puter Interaction, submitted for publication. 

16. R.J. Logan et al., “Design of Simplified Television Re-
mote Controls: A Case for Behavioral and Emotional
Usability,” Proc. 38th Human Factors and Ergonomics
Soc., Santa Monica, Calif., 1994, pp. 365–369. 

17. D.E. Berlyne, “Curiosity and Exploration,” Science, vol.
153, 1968, pp. 25–33. 

18. M. Csikszentmihalyi, Beyond Boredom and Anxiety,
Jossey-Bass, San Francisco, 1975.

19. M. Hassenzahl et al., “Perceived Novelty of Func-
tions—A Source of Hedonic Quality,” Interfaces, vol.
42, no. 11, 2000, p. 11. 

20. J.M. Carroll and J.C. Thomas, “Fun,” ACM SIGCHI
Bull., vol. 19, no. 3, 1988, pp. 21–24. 

21. M. Hassenzahl and R. Wessler, “Capturing Design
Space from a User Perspective: The Repertory Grid
Technique Revisited,” Int’l J. Human-Computer Inter-
action, vol. 12, no. 3–4, 2000, pp. 441–459.

22. L. Leventhal et al., “Assessing User Interfaces for Di-
verse User Groups: Evaluation Strategies and Defining
Characteristics,”  Behaviour & Information Technology,
vol. 15, no. 3, 1996, pp. 127–137, and references therein.

23. G.A. Kelly, The Psychology of Personal Constructs,
vols. 1–2, Norton, New York, 1955 (reprinted by Rout-
ledge, 1991).

24. F. Fransella and D. Bannister, A Manual for Repertory
Grid Technique, Academic Press, London, 1977.

25. M. Hassenzahl and T. Trautmann, Analysis of Web
Sites with the Repertory Grid Technique, submitted for
publication.

26. R. Wessler et al., Orientation, Understanding and Deci-
sion-Making—a User-Centred Approach to Guide the
Design of Prototypes, submitted for publication.

7 6 I E E E  S O F T W A R E J a n u a r y / F e b r u a r y  2 0 0 1

About the Authors

Marc Hassenzahl works at User Interface Design GmbH in Munich. He is involved in
projects ranging from usability evaluation of automation software to user interface design for
computer chip design tools. His research interests are appealing user interfaces, especially he-
donic qualities, and related new analysis and evaluation techniques. He studied psychology and
computer science at the Technical University, Darmstadt. Contact him at User Interface Design
GmbH, Dompfaffweg 10, 81827 Munich, Germany; marc.hassenzahl@uidesign.de.

Andreas Beu works at User Interface Design GmbH in Mu-
nich. He is interested in user interface design for small displays,

wearable computers, and augmented reality systems. He studied mechanical engineering at
the University of Stuttgart. Contact him at User Interface Design GmbH, Dompfaffweg 10,
81827 Munich, Germany; andreas.beu@uidesign.de.

Michael Burmester is head of the Munich office of User
Interface Design GmbH, a software and usability consultancy com-
pany. Results and experiences of his research and consultancy
work are published in over 40 scientific and technical papers. He studied psychology at the Uni-
versity of Regensburg in southern Germany. Contact him at User Interface Design GmbH,
Dompfaffweg 10, 81827 Munich, Germany; michael.burmester@uidesign.de.



E d i t o r :  D e e p e n d r a  M o i t r a  ■ L u c e n t  Te c h n o l o g i e s  ■ d m o i t r a @ c o m p u t e r . o r g

country report

0 7 4 0 - 7 4 5 9 / 0 0 / $ 1 0 . 0 0  ©  2 0 0 1  I E E E J a n u a r y / F e b r u a r y  2 0 0 1 I E E E  S O F T W A R E 77

O
n my way to New Jersey recently, I
was seated next to a businessman. To
avoid boredom, I grabbed the earli-
est opportunity to introduce myself.
When I told him that I was from In-
dia traveling to the US on business,

he exclaimed, “India! You must be a soft-
ware engineer.” Such is the reputation of In-
dia’s software industry that the world has

taken notice, and India today has
a distinct identity as a software
superpower.

With the world’s second largest
pool of English-speaking scientific
and technical professionals, India
boasts a US$5.7 billion software
industry with an annual growth
rate of more than 50 percent. As
the software industry increasingly
becomes a major driver of the na-
tion’s economy and policymakers

devise ways to fuel its growth, India’s soft-
ware industry is poised for massive expan-
sion. As a matter of fact, policymakers and in-
dustry leaders envision this industry’s growing
to more than US$80 billion by 2008 (with
US$50 billion worth of software exports).

India: The land of contrasts
Is India a developing nation with more

than 40 percent illiteracy and a large popula-
tion living below the poverty line, or is it the
land of world-class technical brains and en-
trepreneurs? The answer is both, and much
more. India is a land of such contrasts, para-
doxes, and inconsistencies that the answer re-
ally depends on the viewer’s perspective. It is
one of the world’s oldest civilizations, with a

very rich cultural heritage. Home to more
than one billion people, it is the fifth largest
economy and the largest democracy in the
world. In the last two decades, India’s image
has transformed from the land of snake
charmers to one of top-notch software sol-
diers. India has been primarily an agrarian
economy but, because of the fast pace of
growth in the high-tech sector, software and
IT are fast becoming a critical component of
India’s economic growth.

Historically, the Indian software industry
began by providing onsite contracting services
(“body shopping”) to US and European orga-
nizations. Gradually, the trend shifted from
onsite services to offshore development,
which accounts for approximately 50 percent
of the total revenue. One reason for this shift
relates to restrictions on US visas. The US gov-
ernment used to limit the number of visas for
foreigners and require that foreign nationals
working in the US be paid comparable wages
to that of their American counterparts. It also
imposed a local tax for contracting foreign
nationals, resulting in serious cost implica-
tions for onsite services and thus giving birth
to the offshore development model as the only
economically viable option.

India’s competitive advantage
The low cost of onsite services was the

original growth driver for the Indian soft-
ware industry. As the offshore model became
increasingly popular, the industry has experi-
enced rapid growth and global recognition—
so much so that the market capitalization of
Indian software companies has skyrocketed
from approximately US$4 billion in January

India’s Software Industry

Deependra Moitra



78 I E E E  S O F T W A R E J a n u a r y / F e b r u a r y  2 0 0 1

COUNTRY REPORT

1999 to nearly US$55 billion at the
beginning of June 2000. Today, India
exports software to about 100 coun-
tries around the globe, with North
America being the major market (62
percent). Nearly 200 Fortune 500
companies either have their develop-
ment centers in India or outsource de-
velopment to India. So, what really is
India’s competitive advantage? Five
factors contribute to the growth of
India’s software industry:

■ availability of a highly competent
and large talent pool,

■ world-class quality and high pro-
cess maturity,

■ competitive cost structures,
■ rapid delivery capability, and
■ no language barrier (English-

speaking resource pool).

People—the raw material 
As mentioned earlier, India has the

world’s second largest pool of English-
speaking scientific and technical pro-
fessionals. The Indian software indus-
try employs approximately 300,000
people, 80 percent men and 20 percent
women. However, to reach US$80 bil-
lion by 2008, the industry must add
approximately 200,000 people a year.
Indian universities annually churn out
nearly 90,000 engineering graduates,
with many private training houses pro-
ducing similar numbers (with varying
degrees of skill and quality of output).
Although the government has set up a
new generation of institutes, called In-
dian Institutes of Information Technol-
ogy, in response to the pressure for
high-quality resources, it still needs to

do much to address the projected gap
between supply and demand. 

A real war for talent is underway.
With increasing competition for re-
sources, the ability to attract, develop,
and retain high-quality employees is in-
deed the name of the game. Companies
are using a plethora of approaches:
market hiring (hiring experienced peo-
ple from other companies), advertise-
ments, job portals, campus recruitment,
headhunters, joint industry–academia
programs, employee referrals, and re-
taining freelance headhunters to work
exclusively for a particular company.
Still, no single approach adequately
meets the need. Many companies use
an approach that could be termed
“catch them young,” creating mind
share even at the high school level.
Some companies hire qualified house-
wives to work from home on a part-
time basis. Interestingly, reverse “brain
drain” is also taking place; increasingly,
qualified Indians abroad are returning
home to be part of an exciting and hap-
pening industry.

However, the competition for tal-
ent has also given rise to a dis-
turbingly high employee turnover.
With rising internal competition and
continued overseas demand, the aver-
age industry attrition ranges between
12 and 35 percent, leading to a very
high cost of hiring and employee de-
velopment. For knowledge-intensive
activities, such as high-tech product
development, attrition means not only
losing people to competitors but also
knowledge walking out of the organi-
zation. The competitive landscape for
the talent pool is worsening, with

many European countries opening
their gates to Indian software profes-
sionals. One recent example is the
German government’s move to open
the IT sector to Indian software pro-
fessionals to meet their widening sup-
ply–demand gap for qualified human
resources.

Clearly, talent acquisition and re-
tention is fast becoming the center-
piece of companies’ competitive strat-
egy. Many firms are recruiting based
on learnability, and many large com-
panies—such as TCS, Wipro, Satyam,
and Infosys—have established their
own training hubs and learning cen-
ters. Then there are those such as
NIIT and Pentafour that follow a
“horse and hay approach”—running
expensive software and IT training
programs and spotting talented peo-
ple whom they eventually absorb into
their workforce. To combat the reten-
tion issue, they use many strategies,
including providing

■ continuous learning opportunities
and increasing employability, 

■ high quality of work and work life,
■ overseas assignments,
■ competitive compensation and

pay for performance,
■ perks, loans, and recreation facilities,
■ wealth creation opportunities such

as employee stock options,
■ support for distance learning, and
■ career progression and management.

The key to solving the retention is-
sue, however, is capitalizing on the ex-
isting emotional reservoir in each or-
ganization and effectively managing

A New Department
As software becomes more and more critical to products and services, and as most businesses become software depen-

dent, developing and capitalizing on a strong software capability is increasingly the mantra for competing in the new econ-
omy. As a result, different nations are competing for huge software business opportunities to fuel their economic growth.

IEEE Software will publish a column periodically to bring its readers a comprehensive account of the software industry in
various countries. These columns will cover all the dimensions of the software industry in both major and emerging “software
nations,” along the lines of this article.

The next issue will carry an account of the Irish software industry—another strongly emerging software nation gaining
prominence on the world map.

We’d like to hear from you about this new department. Please send your comments and suggestions to dmoitra@acm.org.
—Deependra Moitra



J a n u a r y / F e b r u a r y  2 0 0 1 I E E E  S O F T W A R E 79

COUNTRY REPORT

employee expectations. With the In-
dian software professional’s median
age being only 26.5 years, managing
the industry’s raw material—the peo-
ple—is indeed a complex undertaking.
Younger employees have different ex-
pectations and priorities from their
older counterparts and are often un-
sure as to their goals. To stay compet-
itive, many companies have estab-
lished human-resources differentiators
who go beyond the work environment
and look at employees’ personal, so-
cial, and family needs.

The quality bandwagon
As the overseas companies con-

tracting work to India needed some
assurance of the quality of deliver-
ables, Indian firms responded by
launching major quality initiatives to
boost confidence. Over time, as the
number of players competing for a
share of the global software business
multiplied and competition among
the software development houses be-
came cutthroat, companies increas-
ingly focused on quality as a way to
create a strategic differentiator in the
marketplace. In addition, as some
level of quality certification became a
prerequisite for doing business, the
trend toward model-based quality
and process improvement began. To-
day, however, the situation is chang-
ing. As Indian software organizations
face stiff competition from other fast-
emerging software nations such as
Ireland, China, and Israel, a focus on
quality is a competitive necessity as
opposed to a strategic advantage—
and that is for the better.

Today, more than 175 software
companies have ISO 9001 certifica-
tion and nearly 50 boast of an SW-
CMM level 3 or higher ranking. In-
terestingly, more than 55 percent of
the CMM level 5 companies in the
world are in India, and many compa-
nies have embraced P-CMM and Six
Sigma. Unfortunately, a focus on
quality and certification is still being
used as a marketing instrument. The
drive for attaining a particular certi-
fication or CMM level has led to a
predominantly compliance-based ap-
proach instead of really driving busi-

ness excellence and innovation
through quality and processes. With
the average business productivity per
employee so low (US$35K–50K per
person per year) and the same prob-
ably true of software development
productivity, we need a strategic fo-
cus on processes combined with im-
proved technology integration. 

India’s software competence
The Indian software industry has

focused primarily on providing soft-
ware services in almost all possible
areas: systems software, telecommu-
nications, e-commerce, medical sys-
tems, automotive software, Web-
based development and multimedia
applications, and applications soft-
ware for the insurance, banking, and
retail industries. The focus on devel-
oping products has almost been
nonexistent, and only recently have a
new breed of high-tech entrepreneurs
and multinationals launched major
product-based organizations in In-
dia. While software design and devel-
opment skills are really on par with
the best by global standards, India’s
capabilities in project management
and new product innovation must
grow to retain its hard-earned com-
petitive edge.

Enabling initiatives
In May 1998, the government of

India formed an IT task force to pro-
vide thrust to India’s software sector.

This thrust has become more strategic
with the formation of the Ministry of
Information Technology this year.
With continued efforts to improve the
infrastructure and simplify policies
and procedures, the government is
helping accelerate the industry’s
growth. It uses such measures as the
establishment of software technology
parks and export processing zones of-
fering tax holidays on software ex-
ports, new working capital guide-
lines, liberalization of telecom policy,
and relaxing the taxation policy for
employee stock options. 

India’s software cities
Bangalore is by far the hottest of

all the software cities in India and is
often termed India’s software hub
or the Silicon Valley of India. Hy-
derabad—also popularly called Cy-
berabad—is another city with a lot
of action. Its chief minister wooed
Bill Gates to open a Microsoft de-
velopment center there, and it
earned prominence as a high-tech
city when US President Bill Clinton
visited it on his trip to India this
year. Mumbai (formerly Bombay)
and Pune in the western part of In-
dia and Delhi, India’s capital in the
north, are also teeming with soft-
ware professionals. Chennai (for-
merly Madras) and Calcutta are
quickly catching up and attracting
investments from entrepreneurs and
large companies.

I ndia has come a long way towardprominence as a software super-power, but it must address certain
issues to continue to enjoy its current
reputation. Innovative thinking and
practices to retain knowledge work-
ers will be critical to the industry’s
success in the long run. Also, a cata-
lyst for the industry’s growth clearly
has been low costs; this advantage
must be sustained through continued
productivity and infrastructure im-
provement. The rising cost of salaries,
the cost of attrition, and the lack of a
world-class infrastructure seem to be
weakening the cost advantage. Other
important issues requiring immediate

While software 
design and development

skills are really 
on par with the best 
by global standards,
India’s capabilities 

in project management
and new product

innovation must grow.



80 I E E E  S O F T W A R E J a n u a r y / F e b r u a r y  2 0 0 1

COUNTRY REPORT

attention include brand-building, sig-
nificantly improving the quality of
training, securing global parity in
telecom infrastructure, and creating
an ideal regulatory framework. Also,
in the last two years the industry’s
real “cash cows” were projects in
Y2K and Euro conversion, but now it
must reposition itself with a renewed
set of capabilities to maintain its
growth rate. More focus on product
development to move up the value
chain and ensure higher revenue gen-
eration requires a serious and imme-
diate effort. Many multinational
companies such as Texas Instruments,
Novell, Oracle, and Parametric Tech-
nologies are getting their strategic
products developed at their R&D
centers in India, but Indian compa-
nies lag far behind in R&D. The lat-
ter must significantly increase R&D
spending to about 3 percent of the to-
tal budget from the current approxi-
mate of 1.5 percent. 

Huge opportunities lie in such ar-
eas as e-commerce, Web-based tech-
nologies, convergence technologies,
mobile Internet devices, and applica-
tion service providers—but to exploit
them the industry must focus contin-
uously on upgrading skills, especially
in project execution and manage-
ment and high-quality, rapid delivery.
The ability to innovate and respond

to changing market needs and ever-
increasing customer expectations will
certainly be a key factor. Also, inno-
vative outsourcing models and a sig-
nificantly improved ability to orga-
nize and execute geographically
distributed software engineering will
distinguish India and keep it ahead as
other nations compete to become
software superpowers.

The Indian software industry has
come a long way from being a small
US$50 million industry in 1990 to
nearly a US$5.7 billion industry in
the financial year 1999–2000 (see
Table 1). With several Indian compa-
nies globalizing and some already
listed on NASDAQ, and with a con-
tinuous flow of venture capital in-
vestments, the sector is really hot.
The recent visits of Jack Welch (Gen-
eral Electric) and Bill Gates, during
which they announced their mega-
plans for India, have given further
impetus to the industry. Similarly,
many other high-profile CEOs and
entrepreneurs are bullish on India.
However, to be able to reciprocate
and capitalize on such opportunities,
the Indian software industry must re-
vitalize itself and innovate relent-
lessly to meet the expectations of
people as demanding as Welch and
Gates. Indian software community,
are you listening?  

Acknowledgments
Most of the data presented here is based

primarily on NASSCOM (National Associa-
tion of Software and Services Companies) re-
ports and its Web site, www.nasscom.org. I
have made every effort to ensure the accuracy
of the information presented here, but neither
I nor IEEE Software is responsible for any er-
ror. The views expressed in this report are
mine and do not in any way reflect the views
of my organization.

I thank Asit Pant for his help in preparing
this report.

Deependra Moitra is general manager of engineering
at Lucent Technologies’ India R&D Program in Bangalore and an
adjunct faculty member at the Indian Institute of Information
Technology. His current interests are in software engineering man-
agement, business and technology strategy, management of tech-
nology and innovation, new product innovation management,
knowledge management, and entrepreneurship in software and
the high-tech industry. He has a BTech in instrumentation and con-
trol engineering from the University of Calcutta. He serves on the
editorial boards of several international journals, including Re-
search & Technology Management Journal, Technology Analysis
and Strategic Management Journal, The International Journal of
Entrepreneurship and Innovation, The Journal of Knowledge Man-
agement, Knowledge and Process Management Journal, and IEEE
Software. Contact him at d.moitra@computer.org.

For More Information
The best source of information on the Indian software industry is the Web

site of the National Association of Software and Services Companies (NASS-
COM), www.nasscom.org. It provides a comprehensive account of the industry,
ways of doing business in India, regulatory mechanisms and government poli-
cies, and so forth. It also offers a database of Indian software companies and
growth areas and an executive summary of the NASSCOM-McKinsey 1999 
report on India’s software industry.

The Web site for Dataquest magazine (www.dqindia.com), currently under
construction, will be another good place to find information about what is hap-
pening in the Indian software and IT sector. This magazine publishes a compre-
hensive analysis of trends and performance each year in July and August. 

The December 2000 issue of CIO magazine (www.cio.com) carries a de-
tailed account of the Indian software industry based on a study and series of in-
terviews done by senior editors Tom Field and Cheryl Bentsen. The diary pertain-
ing to their India visit can be found at www.cio.com/forums/global/edit/
082100_indialetter.html.

Table 1
Financial Snapshot of

India’s Software
Powerhouses 

(export revenues)
Companies Software Exports in 1999–2000

(in million US$, assuming 
US$1 = Rs. 44.00)

Tata Consultancy Services 413.72
Wipro Technologies 237.30
Infosys Technologies 197.66
Satyam Computer Services 150.67
HCL Technologies 143.85
NIIT 125.41
Silverline Technologies 98.83
Cognizant Technology Solutions 94.15
Pentamedia Graphics 88.42
Pentasoft Technologies 80.23
Patni Computer Systems 67.16
IBM Global Services India 61.62
DSQ Software 59.63
Mastek 54.80
Mahindra British Telecom 53.45
HCL Perot Systems 48.28
i-Flex Solutions 43.78
Tata Infotech 43.39
Zensar Technologies 42.12
Birlasoft 37.31



feature

0 7 4 0 - 7 4 5 9 / 0 0 / $ 1 0 . 0 0  ©  2 0 0 1  I E E E J a n u a r y / F e b r u a r y  2 0 0 1 I E E E  S O F T W A R E 81

ultimate system quality. Implementing the
function test as a formal process lets testers
cope better with the functional complexity
of the software application under test. Test-
plan documents and test-case specifications
are important deliverables from the formal
test process.2

For complex systems, test cases are criti-
cal for effective testing. However, the mere
fact that testers use test-case specifications
does not guarantee that systems are suffi-
ciently tested. Numerous other factors also
determine whether testers have performed
well and whether testing was effective. 

Ultimately, testers can only evaluate
complete testing effectiveness when a sys-
tem is in production. However, if this eval-
uation finds that a system was insufficiently
tested or that the test cases were ineffective,
it is too late to benefit the present project.
To reduce such a risk, the project team can
assess testing effectiveness by performing
in-process evaluation of test-case effective-
ness. This way, they can identify problems

and correct the testing process before re-
leasing the system.

This article describes a technique for in-
process validation and improvement of test-
case effectiveness. It is based on a new metric
and an associated improvement framework.
These work together to improve system qual-
ity before its release into production.

Evaluation =
verification + validation

The evaluation process certifies that a
product fits its intended use. For software
project deliverables in general, and test cases
in particular, evaluation commonly consists
of verification and validation tasks.3

Verification 
To start, a project team must first verify

test-case specifications at the end of the test-
design phase. Verifying test cases before test
execution is important; it lets the team assess
the conformance of test-case specifications to
their respective requirements. However, such

Validating and Improving
Test-Case Effectiveness

Yuri Chernak, Valley Forge Consulting

Effective
software testing
before release
is crucial for
product success.
Based on a new
metric and an
associated
methodology for
in-process
validation of
test-case
effectiveness,
the author
presents an
approach to
improving the
software testing
process.

M
anagers of critical software projects must focus on reducing the
risk of releasing systems whose quality is unacceptable to
users. Software testing helps application developers manage
this risk by finding and removing software defects prior to re-

lease. Formal test methodology defines various test types, including the
function test.1 By focusing on a system’s functionality and looking for as
many defects as possible, this test bears most direct responsibility for

software testing



conformance does not mean that the test
cases will automatically be effective in find-
ing defects. Other factors also determine
whether test cases will be effective in the test
cycle. These include design of test cases using
incomplete or outdated functional specifica-
tions, poor test-design logic, and misunder-
standing of test specifications by testers. 

Verification activities commonly used in-
clude reviews or inspections and traceability
analysis. Reviews or inspections let us evaluate
test-case specifications for their correctness
and completeness, compliance with conven-
tions, templates, or standards, and so forth.
Traceability matrices or trees, on the other
hand, let testers trace from the functional spec-
ifications to the corresponding test-case speci-
fications, which ensures that all functional re-
quirements are covered by the given test cases. 

Nevertheless, test cases that passed verifi-
cation could have weak failure-detecting
ability and, therefore, should be required to
pass validation as well.

Validation 
Validation can proceed as soon as testers

have executed all test cases, which is at the
end of the test process’s test-execution
phase. As its main objective, test-suite vali-
dation determines whether the test cases
were sufficiently effective in finding defects. 

If a test suite was effective, the system un-
der test will likely be of high quality and
users will be satisfied with the released prod-
uct. But, if a test suite was not effective, there
is a high risk that the system was not suffi-
ciently tested. In such cases, the users will
likely be dissatisfied with the system’s quality. 

If test-case effectiveness has not proved
satisfactory, it is not too late to analyze the
reasons and correct the test process. Using
the proposed improvement framework,
testers can revise and improve the test suite,
and then execute the tests again. This, in
turn, can help them find additional defects
and thus deliver a better software product. 

The test-case effectiveness metric
To perform validation objectively, testers

need a metric to measure the effectiveness of
test cases. When testing online mainframe
systems, and especially client–server systems,
a certain number of defects are always found
as a side effect. By side effect, I mean the sit-
uation where testers find defects by executing

some steps or conditions that are not written
into a test-case specification. This can hap-
pen either accidentally or, more frequently,
when the tester gets an idea on the fly. 

When defining a metric for test-case ef-
fectiveness, we can assume that the more de-
fects test cases find, the more effective they
are. But, if test cases find only a small num-
ber of defects, their value is questionable.
Based on this logic, I propose a simple test-
case effectiveness metric, which is defined as
the ratio of defects found by test cases (Ntc)
to the total number of defects (Ntot) reported
during the function test cycle:

TCE = Ntc / Ntot ∗ 100%

More precisely, Ntot is the sum of defects found
by test cases and defects found as a side effect.

The proposed TCE metric might resemble
Jones’ defect removal efficiency metric,4

which is defined as the ratio of defects found
prior to production to the total number of
reported defects. The important distinction
is that DRE has the purpose of evaluating
user satisfaction with the entire test process.
It measures the test-process effectiveness and
reflects a production or users’ perspective on
the test process. In contrast, my TCE metric
serves specifically to validate the effective-
ness of functional test cases. Unlike DRE,
the TCE metric evaluates test cases from the
test-cycle perspective, which provides in-
process feedback to the project team on how
well a test suite has worked for testers. 

As I’ve discussed, validation serves pri-
marily to determine whether a test suite was
sufficiently effective in the test cycle. We can
conclude this by comparing the actual TCE
value, calculated for the given test cycle,
with a baseline value. The project team
selects the latter in advance, possibly ob-
taining it by analyzing previous successful
projects that are considered appropriate as
models for current and future projects. My
experience with successful client-server
projects delivering business applications
suggests 75 perecent to be an acceptable
baseline value. However, the goal for test-
case effectiveness can be different for vari-
ous application categories, such as commer-
cial, military, or business applications. 

When the TCE value is at the baseline
level or above, we can conclude that the test
cases have been sufficiently effective in a test

8 2 I E E E  S O F T W A R E J a n u a r y / F e b r u a r y  2 0 0 1

Validation
serves

primarily to
determine

whether a test
suite was

sufficiently
effective in the

test cycle. 



cycle. In this case, the project team can an-
ticipate user satisfaction with the system in
production. But, the further the TCE value
falls below the baseline level, the higher is
the risk of user dissatisfaction. In such cases,
the project team can correct the test process
based on the framework, as I’ll describe. 

Improving test-case effectiveness 
If in-process validation finds test-case ef-

fectiveness to be less than acceptable, the
project team should analyze the causes and
identify areas for test process improvement. 

My proposed improvement framework
stems from the defect-prevention concept
developed at IBM.5 The IBM approach im-
proves the test process on the basis of causal
analysis of defects, so-called test escapes,
that were missed in testing. Test escapes are
“product defects that a particular test failed
to find, but which were found in a later test,
or by a customer [in production].” 

I further evolve IBM’s concept and suggest
that the analysis of defects missed by test cases
can help us improve test-case effectiveness.
Hence, my improvement framework is based
on test-case escapes, defined as software de-
fects that a given suite of test cases failed to find
but that were found as a side effect in the same
test cycle. Once they are found by chance, we
can view test-case escapes as a manifestation of
deficiencies in the formal test process. There-
fore, their causal analysis can help us identify
areas for test process improvement. 

In brief, the proposed improvement
framework consists of the following steps:
1. Understand and document the test 

process used by the project team.
2. Make assumptions about the factors af-

fecting test-case effectiveness.
3. Gather defect data and perform causal 

analysis of test-case escapes.
4. Identify the main factors.
5. Implement corrective actions.

Following these steps, either a revised
part or the entire test suite (as I’ll discuss
later) should be executed again. As a result,
testers should find additional defects that
justify the improvement effort. Below I dis-
cuss each of the five steps in detail.

Clearly, my approach relies entirely on the
analysis of defects missed by test cases. Con-
sequently, it requires that a sufficient number
of such defects be available. This fact can
limit the applicability of the approach for

some projects, for example, in the testing of
mainframe batch systems. Here, testers gen-
erally exercise only preplanned conditions,
and the number of defects found as a side ef-
fect is usually very low in the test cycle. But,
for client–server projects that implement for-
mal testing, the share of such defects could be
from 20 to 50%, which provides a valuable
source of information for test-suite valida-
tion and test-process improvement.

Let’s look at the five steps.

Step 1. Understand, document the test process
When a project team uses written test-case

specifications and focuses on their evaluation
and improvement, this already indicates that
a certain test process has been established
and followed. The test process should be
planned at the beginning of the software
project and documented in a test plan. Com-
monly, testers define the test process in terms
of the following phases: test planning, test
design, test preparation and execution, and
test evaluation and improvement.6–8 Each
phase should be planned and defined in
terms of tasks and deliverables. For example,
we can define the test process as follows:
■ Test planning. In this phase, the main

tasks are the definition of the scope, ob-
jectives, and approach to testing. The
main deliverable is a test-plan document.

■ Test design. This involves the design of
test cases, with the main deliverables be-
ing the test-case specifications.

■ Test preparation and execution. In this
phase, preparation of the test environ-
ment, executing test cases, and finding
defects are the necessary tasks, and the
main deliverables are defect reports. 

■ Test evaluation and improvement. Here,
the main task is analyzing the results of
testing and the main deliverable is a test
summary report.

In all phases, except the last, there are a
number of factors that determine the effec-
tiveness of functional test cases in a given
project. Hence, the following steps of my
improvement framework focus on identify-
ing and evaluating these factors.

Step 2. Make assumptions
Once it understands and documents the

test process, the project team should analyze
each phase and identify factors that can af-
fect test-case effectiveness.

J a n u a r y / F e b r u a r y  2 0 0 1 I E E E  S O F T W A R E 83

Clearly, my
approach relies
entirely on the

analysis of
defects missed
by test cases.

Consequently, it
requires that a

sufficient
number of such

defects be
available.



Test planning. The main deliverable of the test-
planning phase is a test-plan document that,
among other things, defines the scope and ob-
jectives of testing. We can define test objectives
as features to be tested2 that, in turn, should be
traced to functional specifications. If the func-
tional specifications do not completely define
functional features, the test-plan document will
not be complete either. Hence, the test cases
will not completely cover a system’s function-
ality, thereby reducing their effectiveness. 

Test design. When writing a test-case speci-
fication, we usually begin by understanding
and analyzing the corresponding business
rule that is the object of the test. Then, we
consider the test logic required for testing
this functional feature. To identify necessary
test cases, we can use test design techniques
such as decision tables, equivalence parti-
tioning, boundary analysis, and so forth.1,7,8

The test-design phase can give rise to other
factors that affect test-case effectiveness.
First, the test suite might be incomplete and
some of the business rules in the functional
specifications might not be covered by test
cases. Second, test-design logic could be in-
complete and some of the necessary test
conditions could be missing in test-case
specifications. A common example of this
situation is a lack of negative test cases. By
definition, a test case is negative if it exer-
cises abnormal conditions by using either
invalid data input or the wrong user action.
Finally, a third factor is that the test-case
specifications could simply be incorrect. For
example, a source document—a correspon-
ding functional specification—could be in-
correct or unclear, or there might be an er-
ror in the test-case specification itself.

All the deficiencies identified in the test-
planning and test-design phases will ultimately
require addition of new and revision of exist-
ing test-case specifications and their retesting.

Test preparation and execution. The test-
execution phase itself can be a source of fac-
tors that reduce test-case effectiveness. For ex-
ample, some test cases might not be executed

or might be executed incorrectly. In addi-
tion, a tester might overlook defects, espe-
cially when the verification of expected re-
sults is not straightforward. Based on our
experience, only a small number of test-case
escapes stem from test-execution factors.
Therefore, these factors will probably not
be central to the test-case effectiveness im-
provement effort. However, further analysis
at Step 4 might show that the proportion of
defects in this category is significant. In such
cases, a detailed evaluation of test-execution
factors should be performed. 

Figure 1 shows these factors in the form of
a cause–effect diagram. I have grouped the
factors according to the test-process phases
in which they originate. However, at this
point, they are just assumptions that should
be evaluated using the following steps to
identify the factors that are mostly responsi-
ble for insufficient test-case effectiveness.

Step 3. Gather defect data and perform
causal analysis

To perform causal analysis of test-case
escapes at the end of the test-execution
phase, testers must select the defects missed
by test cases. This requires the use of a de-
fect-tracking system. Also, the testers must
identify which defects were found as a result
of test-case execution and which were found
as a side effect—that is, as test-case escapes.
Once identified and selected, the test-case
escapes should be classified according to
one of the factors based on the causal analy-
sis logic shown in Figure 2. 

This analysis is used to evaluate each
test-case escape and understand why the test
suite missed the corresponding defect dur-
ing test execution. We can begin causal
analysis by verifying that a functional spec-
ification has a business rule related to the
given defect. If it does not, we have deter-
mined that the cause of this test-case escape
is an incomplete functional specification.
However, if it does, we need to check
whether the test suite has a test specification
that should have found this test-case escape. 

If a test-case specification does not exist,
this means that the test suite does not cover
all business rules. Therefore, an incomplete
test suite is the reason this defect was
missed. If a test specification does exist, we
need to check the defect against test cases in
the specification. If none of them were de-

8 4 I E E E  S O F T W A R E J a n u a r y / F e b r u a r y  2 0 0 1

Incorrect test
specifications

Test-design phase factors

Test-execution
phase factors

Test-planning
phase factors

Incomplete
test design

Incomplete
test suite

Test-case effectiveness

Test
execution
problems

Incorrect
functional

specifications

Incomplete
functional

specifications

Figure 1. Factors 
affecting test-case
effectiveness.



signed to catch such a defect, this indicates
that the test specification is incomplete. In-
deed, all test inputs and expected results in
the test-case specification might be correct.
However, the specification might include,
for example, only positive test cases. 

A lack of negative test cases in test specifi-
cations is a common cause of missed defects.
This is a case of deficiency in test design that
was used to derive test cases. Hence, we can
specify that the cause of such test-case escapes
is incomplete test design. But, if the test spec-
ification includes conditions related to a given
defect, we need to verify that these test condi-

tions and the corresponding expected results
are correct. If they are correct, we should con-
clude that test-execution problems are the
likely reason that the defect was missed. 

If the test conditions or expected results
were not correct, we need to understand
why the test specification is incorrect. First,
we should check the source document and
see if the corresponding business rule is also
incorrect. If this is the case, we should clas-
sify the cause of this test-case escape as an
incorrect functional specification. Other-
wise, the cause is incorrect test specification. 

As a result of defect causal analysis, all

J a n u a r y / F e b r u a r y  2 0 0 1 I E E E  S O F T W A R E 85

This project was a banking application intended for external
clients—financial institutions. The system had a three-tier
client–server architecture with a Windows NT front-end developed
in Visual Basic and Visual C++. The second tier was implemented
in a Unix environment with Oracle 7 as a database engine. The
third tier was a data feed from a mainframe COBOL/DB2 system.
The project team consisted of 10 developers and three testers.

Because the application was intended for external clients, soft-
ware quality was of great importance to project management. To
ensure high quality, the project team implemented a formal test
process with a focus on functional testing. The development team
was responsible for functional specifications, and the test team was
responsible for the test-plan document and test-case specifications.
Management defined the functional testing exit criteria as follows:
■ 100% of test cases are executed.
■ No defects of high and medium severity remain open.
■ Test-case effectiveness not less than 75%.

By the end of the test-execution phase, testers had executed all
test specifications and reported 183 defects. Defects were man-
aged using the PVCS-based defect tracking system. In reporting de-
fects, testers classified them either as test-case escapes or as being
found by conditions in test-case specifications. Testers reported 71
test-case escapes and 112 defects found by test cases. Based on
these numbers, the calculated TCE metric value was 61%, which
was considerably lower than the acceptable level of 75%. As a re-
sult, the project team concluded that functional testing did not pass
the exit criteria and the system was likely not sufficiently tested.
Hence, test-process correction and system retesting were needed.

The project team performed the test process improvement ac-
cording to the framework described above. First, they analyzed all
test-case escapes and classified them by appropriate causes.
Next, they built a distribution of causes (see Figure A). Analysis of
the distribution showed incomplete test design and incomplete
functional specifications to be the main factors causing missed de-
fects by test cases. To improve the test process, the project team
began by correcting and completing the functional specifications
and reviewing them with the users. A subsequent review of test-
case specifications showed that the main deficiency of the test de-
sign was a lack of negative test cases. Therefore, the existing test-
case specifications were completed with negative test cases. 
By definition, negative test cases focus on abnormal workflow

and are intended to break a system. However, the test
suite initially used by the testers was not sufficiently “de-
structive.” A significant number of defects were found as
side effects as opposed to being found by conditions in
test specifications. In addition, the team created a number
of new test-case specifications to completely cover the
business rules in the revised functional specifications. To
verify test suite completeness, this time the project team
used a traceability matrix, which was not done in the first
test cycle. Test suite incompleteness was one of the factors
that reduced test-case effectiveness (see Figure A).

After these corrections, the testers executed the revised part of
the test suite. As a result, they found 48 additional defects that oth-
erwise would have been released into production. At this point, the
number of defects found during the test cycles had grown to 231.
After two months in production, the rate of defects, reported by
users, had noticeably declined. By the end of the second month the
number of production defects was 23. The DRE metric calculated at
this time was 91%, which is 231/(231+23) = 0.91, and indicated
sufficient effectiveness of the test process.4 Indeed, none of the de-
fects reported from production by the users were of critical severity,
and the users were fairly satisfied with the system quality.

35
30
25
20
15
10

5
0

Inc
om

ple
te 

tes
t d

es
ign

Inc
om

ple
te 

fun
cti

on
al 

sp
ec

ific
ati

on
s

Inc
om

ple
te 

tes
t s

uit
e

Inc
orr

ec
t fu

nc
tio

na
l s

pe
cif

ica
tio

ns

Inc
orr

ec
t te

st-
ca

se
 sp

ec
ific

ati
on

s

Te
st 

ex
ec

uti
on

 pr
ob

lem
s

Description of causes

Nu
m

be
r o

f t
es

t-c
as

e 
es

ca
pe

s

Figure A. A Pareto chart.

Case Study



test-case escapes should be classified ac-
cording to one of the possible causes pre-
sented in Figure 1.

Step 4. Identify the main factors
At this point, all test-case escapes have

been classified according to their respective
causes. We now need to identify those “vital
few” factors that are responsible for the ma-
jority of the defects being missed by test cases. 

For this, we can build a Pareto chart,9

which displays frequency bars in descending
order, convenient for analyzing types of
problems. Once identified, the most impor-
tant causes will be the focus of the next
step—implementation of corrective actions.

Step 5. Implement corrective actions
After identification of the main causes of

test-case escapes, the project team should
implement corrective actions and repeat the
test execution cycle. For the factors shown
in Figure 1, corrective actions could be any
of the following:
■ Incomplete or incorrect functional spec-

ifications—inspect and rework func-
tional specifications, then rework test-
case specifications.

■ Incomplete test suite—use a traceability
matrix to ensure complete coverage of
business rules by test cases.

■ Incomplete test design—implement
training of testers on test-design tech-
niques; use checklists or templates to de-
sign test-case specifications; rework test-
case specifications.

■ Incorrect test-case specifications—inspect
and rework test-case specifications.

■ Test-execution problems—implement
training of testers on test execution, de-
velop and use procedures for test execu-
tion and verification of test results.

When functional specifications or test cases
must be corrected, the project team should
revise the test suite and execute the revised
part again. However, if correction is required
only due to the test-execution problems, the
same test can be used for retesting. The main
objective of retesting is to find additional
defects. If additional ones are found, this
fact can justify the whole improvement effort. 

The “Case Study” box illustrates how my
proposed approach to test-case effectiveness
validation and in-process improvement was
implemented in a client–server project.

T his technique for in-process valida-tion of test cases is intended to giveproject teams better visibility into
test-process effectiveness before their sys-
tems are released into production. The pro-
posed technique can be applied within any
project management model, including incre-
mental or evolutionary models, where it can
be used for assessment of test-process effec-
tiveness and its tuning from one incremental
build to another.

A project team has to decide in advance
what level of test-case effectiveness is ac-
ceptable for their project. Such a require-
ment can vary depending primarily on the
project’s criticality. Future work will focus
on developing a formal approach to select-
ing a baseline value for the TCE metric. 

Acknowledgments
I am grateful to Vladimir Ivanov for his help in

preparing this material. I thank Richard Reithner for
editing the article. Finally, I am grateful to the IEEE
Software reviewers for their helpful feedback and
comments.

References
1. G. Myers, The Art of Software Testing, John Wiley & Sons,

Upper Saddle River, N.J., 1979.
2. IEEE Std. 829-1983, Software Test Documentation, IEEE,

Piscataway, N.J., 1983.
3. IEEE Std. 1012-1986, IEEE Standard for Software Verifica-

tion and Validation Plans, IEEE, Piscataway, N.J., 1986.
4. C. Jones, Applied Software Measurement, McGraw-Hill,

New York, 1991.
5. R. Mays et al., “Experiences with Defect Prevention,” IBM

Systems J., vol. 29, no. 1, 1990, pp. 4–32.
6. Y. Chernak, “Approach to the Function Test Decomposition and

Management,” Proc. 15 Pacific Northwest Software Quality
Conf., PNSQC/Pacific Agenda, Portland, 1997, pp. 400–418.

7.  E. Kit Longman, Software Testing in the Real World, Addi-
son-Wesley, Reading, Mass., 1995.

8.  P. Goglia, Testing Client–server Applications, QED Publish-
ing Group, Wellesley, Mass., 1993. 

9. L.J. Arthur, Improving Software Quality, John Wiley &
Sons, Upper Saddle River, N.J., 1993.

8 6 I E E E  S O F T W A R E J a n u a r y / F e b r u a r y  2 0 0 1

About the Author
Yuri Chernak is president and

founder of
Valley Forge
Consulting,
Inc., a consult-
ing firm that
specializes in
the field of
software qual-
ity assurance

and systems testing. He has over 20
years of experience in the software indus-
try. As a consultant, he has worked for
various clients, primarily for the broker-
age firms in New York. He has a PhD in
computer science and is a member of the
IEEE. His research interests cover systems
test methodology, software metrics, and
process improvement. He has been a
speaker at international conferences on
software quality. Contact him at 
ychernak@idt.net.

Incomplete
test suite

Incomplete
functional

specification

Does a
test specification

exist?

Does a
business rule

exist?

Test-case escape

No

Yes

No

Yes

No
Incomplete
test design

Incorrect
functional

specification

Test
execution
problems

Incorrect
test

specification

Yes

Yes

Yes

No

Is the
test specification

complete?

Is the
test specification

correct?

Is the business
rule correct?

No

Figure 2. Test-case
escape classification
logic.



0 7 4 0 - 7 4 5 9 / 0 0 / $ 1 0 . 0 0  ©  2 0 0 1  I E E E J a n u a r y / F e b r u a r y  2 0 0 1 I E E E  S O F T W A R E 87

estimation practices are the lack of necessary
information at the beginningof the project,
the specificity of the domain addressed, the
effort and time required, and the need to in-
troduce a vocabulary foreign to stakeholders
without a software background.

However, as Figure 1 shows, ad hoc size
estimates have problems of their own. Their
accuracy (the closeness of a measured value
to the true one) and precision (indicating
how repeatable a measurement is) leave
much to be desired. The problem is not ac-
ademic: inaccurate size estimates automati-
cally translate into questionable project
budgets and schedules.

This article presents a method based on
paired comparisons, which social science re-
searchers use for measuring when there is no
accepted measurement scale or when a
measurement instrument does not exist. Al-
though not new, the idea has received little
attention in the literature. Earlier work in-

cludes Target Software’s software sizing
method4 and more recent articles by Focal
Point AB5 and by Bournemouth University’s
Empirical Software Engineering Research
Group,6 which uses the analytic hierarchical
process to prioritize requirements relative to
their cost and estimate effort respectively.7

Overall approach
The idea behind paired comparisons is to

estimate the size of n entities by asking one
or more experts to judge the entities’ relative
largeness rather than to provide absolute size
values. (Entities can be requirements, use
cases, modules, features, objects, or anything
else relevant to all stakeholders and for
which it is possible to know the number of
lines of code, hours, or any other magnitude
that could later be used for planning pur-
poses.) By requiring multiple and explicit de-
cisions about the relative size of every two en-
tities and by using easily available historical

feature
Improving Subjective 
Estimates Using 
Paired Comparisons

Eduardo Miranda, Ericsson Research Canada

Despite the
existence of
structured
methods for
software sizing
and effort
estimation, 
the so-called
“expert”
approach 
seems to be 
the prevalent
way to produce
estimates in 
the software
industry. 
The paired-
comparisons
method offers 
a more accurate
and precise
alternative to
“guesstimating.”

M
ost practitioners and project managers still produce estimates
based on ad hoc or so-called “expert” approaches, even though
several software sizing methods—counting source lines of
code,1 function points,2 full function points,3 and object points,

to name a few—are well known and have been available for a long time.
Among the most common explanations given for not adopting more formal 

estimation



data—rather than a single comparison to
some vague notion of size buried in the es-
timator’s mind—the paired-comparisons
method improves both the accuracy and the
precision of estimates, as shown in Figure 1.
These findings are consistent with the con-
clusions of Albert L. Lederer and Jayesh
Prasad’s study, which shows that using his-
torical data and documented comparisons
produce better estimates than those based on
intuition and guessing.8

As Figure 2 shows, with the proposed ap-
proach we start by arranging the entities to
be sized according to their perceived large-
ness. We then assess the relative size of each
one with respect to all the others and record
this information in what is called a judg-
ment matrix. From the judgments made, we
derive a ratio scale using a simple mathe-

matical procedure and then calculate the ab-
solute size of the entities using the ratio
scale and a reference value. Should the need
arise, judgments can be reviewed for inter-
nal consistency.

The method is independent of the type of
entities chosen. It is important, however,
that the sizes of the entities being estimated
do not differ by more than one order of
magnitude, because our ability to accurately
discriminate size diminishes as the difference
between the entities becomes larger.7,9,10

Judgment matrices
A judgment matrix is a square matrix of

size n, where n is the number of entities being
compared; and each element aij captures the
relative size of entity i with respect to entity j.
The elements of the matrix are defined as

How much bigger (smaller)
entity i is with respect to entity j

Every entity has the same size        (1)
as itself

If entity i is aij times bigger (smaller)
than entity j, then entity j is1/aij
times smaller (bigger) than entity i

In practice, as Table 1 shows, the judges must
estimate only the relative sizes of the upper

8 8 I E E E  S O F T W A R E J a n u a r y / F e b r u a r y  2 0 0 1

500
450
400
350
300
250
200
150
100
50
0

Stack Queue Binary
tree

Linked
list (a)

Reference
(string)

Linked
list (b)

Balanced
tree

Hash
table

Li
ne

s 
of

 c
od

e
Actual Ad hoc Paired comparisons

Figure 1. The accuracy and precision of different estimation
approaches (results of a study involving over 30 software 
professionals and graduate students): actual measurements,
ad hoc estimates, and paired-comparison estimates.

Rank artifacts
from largest to

smallest

Compare
the artifacts pairwise

establishing their
relative size

Review
internal

inconsistencies

Calculate ratio
scale and

inconsistency
index

Calculate
absolute sizes

Artifacts to
be sized

Verbal scale
(optional)

Judgment
matrix

Ratio
scale

Reference
value(s)

Sized
artifactsFigure 2. The 

paired-comparisons
estimation process.

A a

a
s

s

a

a
a

n n
ij

ij
i

j

ii

ji
ij

× = [ ]

=

=

=

=

















     

  

 

1

1



J a n u a r y / F e b r u a r y  2 0 0 1 I E E E  S O F T W A R E 89

diagonal elements of the matrix, because all
the other values can be derived from them.

The element “a12 = 4” in the example in
Table 1 expresses the fact that entity D has
been judged four times bigger than B. No-
tice that as shown by the relations D / C =
6, D / A = 7.5, and C / A = 2, the judgments
recorded in the matrix do not need to be
perfectly consistent. After all, who knows
which is the true value? Remember that we
are estimating things that have not been
built yet.

Although not a mandatory step, arrang-
ing the entities in descending order accord-
ing to their size makes the rest of the process
much easier. As Table 1 shows, when we sort
the rows of a judgment matrix in descending
order, the comparisons flow in one direction
only. For example, entity D will be either
equal to or larger than any of the other enti-
ties against which it is compared; it will
never be smaller. Notice also that within a
row, the values to the left of any given col-
umn are always smaller than or equal to
those to the right. While these properties are
irrelevant from the mathematical point of
view, they diminish the strain put on the
judges by the large number of decisions they
must make.

The paired-comparisons method requires
the existence of at least one reference entity
whose size is known, for example, from a
previous development project. First, we rank
this entity, as we would any other, by com-
paring it to every other entity to be sized.
Later, we use its size to calculate the absolute
size of the entities being estimated.

The choice of a reference entity is an im-
portant decision. Entities with sizes in either
extreme of the scale tend to amplify any bias
that might affect the judgments. To minimize
this risk, it is better to choose as reference an

entity that will divide
the population being es-
timated into halves or
to use two or more ref-
erences instead of one.

The verbal scale
Using a verbal scale simplifies and speeds

up the estimation process without jeopard-
izing the accuracy of the results. Although
not an essential part of the methodology,
having a shared understanding of how small
is “smaller” and how big is “bigger” helps
the participants reach consensus in the siz-
ing process. A predefined value scale keeps
us from wasting time discussing values
down to the second decimal when our judg-
ment error is one or two orders of magni-
tude bigger than that.

Quoting an earlier work from Ernest H.
Weber, Thomas L. Saaty proposes using a
scale from 1 to 9 and their reciprocals to
pass judgment on the entities being evalu-
ated.7 Table 2 lists the equivalence between
verbal expressions and relative sizes.

Suspecting that the values proposed by
Saaty could be different for the software 
domain, I conducted an informal survey
among colleagues; 30 people from different
countries and from both industry and aca-
demia provided input for the scale. The re-
sults suggest that the correspondence be-
tween size and verbal description in the
software domain is closer to the one shown
in Table 3 than to Saaty’s.

Calculating a ratio scale and an
inconsistency index

A ratio scale is a vector [r1, r2, …, rn] in
which each number ri is proportional to the
size of entity i. An inconsistency index is a
number that measures how far away our

Table 1
Judgment Matrix Example

Entities D B C A

D 4 6 7.5
B 1.5 2
C 2
A

Table 2
Saaty’s Verbal Scale

Definition Explanation Relative value Reciprocal

Equal size The two entities are roughly the same size. 1 1.00
Slightly bigger (smaller) Experience or judgment recognizes one entity as being somewhat bigger (smaller). 3 .33
Bigger (smaller) Experience or judgment recognizes one entity as being definitely bigger (smaller). 5 .20
Much bigger (smaller) The dominance of one entity over the other is self-evident; very strong difference in size. 7 .14
Extremely bigger (smaller) The difference between the entities being compared is of an order of magnitude. 9 .11
Intermediate values between When compromise is needed. 2, 4, 6, 8 .5, .25, .16, .12  
adjacent scales



judgments are from being perfectly consis-
tent. (A perfectly consistent judgment ma-
trix is one in which all its elements satisfy
the condition aij × ajk = aik for all i, j, k.)

There are several ways to derive ratio scales
and inconsistency indexes from paired-com-
parisons data, among them Saaty’s eigenval-
ues,7 averaging over normalized columns,7

and Gordon Crawford and Cindy Williams’
geometric mean procedure.11 Here, I use
Crawford and Williams’s approach because of
its simplicity and good results. I first calculate
the geometric mean of the matrix’s rows as

, (2)

then I calculate the ratio scale as

,
(3)

and finally the inconsistency index as

.
(4)

Thus, given the ratio scale [r1, r2, …, rn], we
can calculate the absolute sizes of the enti-
ties being estimated using the expression

.  (5)

If more than one reference value is stipu-
lated, the regression line of the references
provided can replace the reference size. 

A numerical example
Let’s look at a complete numerical exam-

ple using as the departure point the judg-
ments stated in Table 1. First, using the rules
for creating a judgment matrix and the rel-
ative size of the entities given earlier as ex-
amples, we derive values for the matrix:

.

Applying Equation 2, we calculate the vec-
tor of the row’s geometric means:

.

We sum the geometric means

and then normalize the vector just calculated
by dividing it by the sum of the means:

.

Assuming that entity C is the reference point
and its size is 1.7 KLOC, we can calculate
the absolute size of the other entities using
the relationship in Equation 5:

.

The absolute sizes—SizeD = 9.07 KSLOC,
SizeB = 2.3 KSLOC, and SizeA = 1.06
KSLOC—with an inconsistency index of
3% are the final outputs of the process.

Implementation
Successful implementation of the paired-

comparisons method requires the selection
of qualified judges and a tool capable of au-
tomating the calculations.

When the number of entities to evaluate is
large, you can divide the work among multi-
ple judges. You can also use this approach to
minimize the bias introduced by a single
judge and to get buy-in to the results. The
number of judges used to evaluate n entities

.

.
.

.

.
.

.

.
.

.

.
.

64

12
1 7

16

12
1 7

12

12
1 7

07

12
1 7

∗

∗

∗

∗





























.

.

.

.

64

16

12

07



















vi =∑ 5 7.

3 6

93

68

42

.

.

.

.



















1 4 6 7 5
25 1 1 5 2
16 7 1 2
13 5 1 1

.
. .
. .
. .



















Size
r

r
Sizei

i

reference
reference= ∗

ln lna
v

v

n n

ij
i

jj i

n

i

n

−










−( ) −( )
>=
∑∑

2

1

1 2

2

r
v

v
i

i

l
l

n=

=
∑

1

v ai ij
j

n

n=
=

∏
1

9 0 I E E E  S O F T W A R E J a n u a r y / F e b r u a r y  2 0 0 1

Table 3
Verbal Scale for the Software Domain

Definition Explanation Relative value Reciprocal

Equal size Ei  /Ej ≤ 1.25 (0–25%) 1.00 1.00

Slightly bigger (smaller) 1.25 < Ei /Ej ≤ 1.75 (26–75%) 1.25 .80

Bigger (smaller) 1.75 < Ei /Ej ≤ 2.275 (76–275%) 1.75 .57

Much bigger (smaller) 2.275 < Ei /Ej ≤ 5.75 (276–575%) 4.00 .25

Extremely bigger (smaller) 5.75 < Ei /Ej ≤ 10 (576–1000%) 7.00 .13



should not exceed n / 3, otherwise the ad-
vantage of the method will be lost because
each judge will not get the opportunity to
make multiple comparisons for a given en-
tity. A simple way to allocate comparisons to
judges is to assign every other comparison to
a different judge in a sequential fashion.

At Ericsson, we use the home-grown tool
MinimumTime to support the paired-com-
parisons method. Figure 3a shows Minimum-
Time’s interface, which was designed to reduce
the strain put on judges by the large number
of comparisons required by the method.

MinimumTime displays all the completed
decisions in a matrix structure using a sym-
bolic or numeric format, according to the
user’s preferences. In keeping with the idea of
providing range rather than point estimates,
the tool calculates a confidence interval
based on the scale dispersion. The tool also
provides an analysis capability, shown in Fig-
ure 3b, to detect judgment inconsistencies
and thus to iteratively refine the initial esti-
mate. The sensibility of this tool can, and
should, be adjusted to find only the major
discrepancies. Since the true value of the re-
lation is unknown, a certain degree of incon-
sistency could be considered beneficial.

S oftware sizing using the paired-com-parisons method is especially wellsuited to the early stages of a develop-
ment project, when the knowledge available
to project team members is mostly qualitative.

The mathematics behind the method are
foolproof, but the judgments on which the cal-
culations are based are not. For the method to
work, those making the comparisons must un-
derstand both the functional and the techno-
logical dimensions of the things being sized.

Although not conclusive, the results ob-
served so far are promising. Further experi-
mentation is necessary to establish the validity
of the verbal scale for the software domain and
to verify that the method scales up when used
with larger and more complex entities.

Acknowledgments
I thank Tamara Keating and Gaetano Lombardi

from Ericsson, Alain Abran from the Université du
Québec à Montréal, Norma Chhab-Alperin from Sta-
tistics Canada, and Raul Martinez from RMyA for
their valuable comments.

References
1. R. Park, Software Size Measurement: A Framework for

Counting Source Statements, tech. report CMU/SEI-92-

TR-20, Software Eng. Inst., Carnegie Mellon Univ.,
Pittsburgh, 1992.

2. A. Albrecht and J. Gaffney, “Software Function, Source
Lines of Code, and Development Effort Prediction: A
Software Science Validation,” IEEE Trans. Software
Eng., vol. SE-9, no. 6, 1983, pp. 639–648.

3. COSMIC—Full Function Points, Release 2.0, Software
Engineering Management Research Lab, Montreal,
Sept. 1999; www.lrgl.uqam.ca/cosmic-ffp/manual.html
(current 11 Dec. 2000).

4. G. Bozoki, “An Expert Judgment Based Software Sizing
Model,” Target Software, www.targetsoft-ware.com
(current 8 Jan. 2001).

5. J. Karlsson and K. Ryan, “A Cost-Value Approach for
Prioritizing Requirements,” IEEE Software, vol. 14, no.
5, Sept./Oct. 1997, pp. 67–74.

6. M. Shepperd, S. Barker, and M. Aylett, “The Analytic
Hierarchy Processing and Almost Dataless Prediction,”
ESCOM-SCOPE ’99, Proc. 10th European Software
Control and Metrics Conf., Shaker Publishing, Maas-
tricht, The Netherlands, 1999. 

7. T. Saaty, Multicriteria Decision Making: The Analytic
Hierarchy Process, RWS Publications, Pittsburgh, 1996.

8. A. Lederer and J. Prasad, “Nine Management Guide-
lines for Better Cost Estimating,” Comm. ACM, vol.
35, no. 2, Feb. 1992, pp. 51–59.

9. E. Miranda, “Establishing Software Size Using the
Paired Comparisons Method,” Proc. 9th Int’l Work-
shop Software Measurement, Université du Québec à
Montréal, 1999, pp. 132–142; www.lrgl.uqam.ca/
iwsm99/index2.html (current 11 Dec. 2000).

10. E. Miranda, “An Evaluation of the Paired Comparisons
Method for Software Sizing,” Proc. 22th Int’l Conf.
Software Eng., ACM, New York, 2000, pp. 597–604.

11. G. Crawford and C. Williams, The Analysis of Subjec-
tive Judgment Matrices, tech. report R-2572-1-AF,
Rand Corp., Santa Monica, Calif., 1985, pp. xi, 34;
www.rand.org/cgi-bin/Abstracts/ordi/getabbydoc.
pl?doc=R-2572-1 (current 12 Dec. 2000).

J a n u a r y / F e b r u a r y  2 0 0 1 I E E E  S O F T W A R E 91

Req f      is

25 % tolerance

<< < = > >>

Ratio
scale

+ / –Estimated
value

Total 9 . 5
3 %

793.4
Inconsistency Index

Reference
value

Artifact
name

= =
=

=
=
=

>
=
<
> 

>
>
>
=
>  

>
>
>
>
>
<

>
>>
>>
>
=
=
>

>>
>>
>
>
>
=
=
>
=

90

Re
q 

a

Re
q 

b

Re
q 

c

Re
q 

d

Re
q 

e

Re
q 

f

Re
q 

g

Re
q 

h

Re
q 

i

Re
q 

j

Calculate Clear

Analyze

0.12
0.13
0.11
0.11
0.11
0.09
0.09
0.08
0.08
0.08

1.1
1.2
1.0
1.0
1.0
0.8
0.8
0.8
0.7
0.7

96.5
101.6
86.0
90.0
85.0
69.5
73.6
65.9
63.0
63.0

Req a
Req b
Req c
Req d
Req e
Req f
Req g
Req h
Req i
Req j

>
>>
<
>
>
>
>
>

Inconsistency Diagnostic

REQ A is equal to REQ C. REQ C is 0.8 times smaller than REQ I. So REQ A 
should be 0.8 times smaller than REQ I, but its value is 1.3.
In a perfectly consistent relationships A[i,j]*A[j,k]/A[i,k]=1. The current value 
is: 0.56.
Review the relationships between the artifacts named above
NOTE:
 A[i,j] is red, A[j,k] blue and A[i,k] purple OK

×

Req a
Req b
Req c
Req d
Req e
Req f
Req g
Req h
Req i
Req j

Req c      is

2 % tolerance

<< < = > >>

Ratio
scale

+ / –Estimated
value

Total 9 . 5
3 %

793.1
Inconsistency Index

Reference
value

Artifact
name

= =
=

=
=
=

>
=
<
> 

>
>
>
=
>  

>
>
>
>
>
<

>
>>
>>
>
=
=
>

>
>>
<
>
>
>
>
>

>>
>>
>
>
>
=
=
>
=

90

Re
q 

a

Re
q 

b

Re
q 

c

Re
q 

d

Re
q 

e

Re
q 

f

Re
q 

g

Re
q 

h

Re
q 

i

Re
q 

j

Calculate Clear

Analyze

0.12
0.13
0.11
0.11
0.11
0.09
0.09
0.08
0.07
0.08

1.2
1.2
1.1
1.1
1.0
0.8
0.9
0.8
0.7
0.7

96.5
101.6
91.1
90.0
85.0
69.5
73.6
65.9
59.5
60.5

than      Req 1

Figure 3. (a) The
MinimumTime tool’s
graphical interface.
(b) The consistency
analyzer.

About the Author

Eduardo
Miranda is
a senior special-
ist at Ericsson
Research
Canada and an
industrial re-
searcher affili-
ated with the

Research Laboratory in Software Engineer-
ing Management at the Université du
Québec à Montréal. He is in charge of in-
vestigating new management techniques
for planning and tracking projects. He re-
ceived a BS in system analysis from the
University of Buenos Aires and an MEng.
from the University of Ottawa. He is a
member of the IEEE Computer Society and
the ACM. Contact him at Ericsson Research
Canada, 8400 Decaire Blvd., Town of
Mount Royal, Quebec H4P 2N2, Canada;
eduardo.miranda@lmc.ericsson.se.

(a)

(b)



9 2 I E E E  S O F T W A R E J a n u a r y / F e b r u a r y  2 0 0 1 0 7 4 0 - 7 4 5 9 / 0 0 / $ 1 0 . 0 0  ©  2 0 0 1  I E E E

While such techniques1 form the founda-
tion for many contemporary software engi-
neering practices, requirements analysis has
to involve more than understanding and
modeling the functions, data, and interfaces
for a new system. In addition, the require-
ments engineer needs to explore alternatives
and evaluate their feasibility and desirabil-
ity with respect to business goals.

For instance, suppose your task is to build
a system to schedule meetings. First, you
might want to explore whether the system
should do most of the scheduling work or
only record meetings. Then you might want
to evaluate these requirements with respect to
technical objectives (such as response time)
and business objectives (such as meeting effec-
tiveness, low costs, or system usability). Once

you select an alternative to best meet overall
objectives, you can further refine the meaning
of terms such as “meeting,” “participant,” or
“scheduling conflict.” You can also define the
basic functions the system will support.

The need to explore alternatives and eval-
uate them with respect to business objectives
has led to research on goal-oriented analy-
sis.2,3 We argue here that goal-oriented analy-
sis complements and strengthens traditional
requirements analysis techniques by offering
a means for capturing and evaluating alterna-
tive ways of meeting business goals.

The remainder of this article details the five
main steps that comprise goal-oriented analy-
sis. These steps include goal analysis, softgoal
analysis, softgoal correlation analysis, goal
correlation analysis, and evaluation of alter-

feature
Exploring Alternatives
during Requirements
Analysis

John Mylopoulos, University of Toronto 

Lawrence Chung, University of Texas, Dallas

Stephen Liao and Huaiqing Wang, City University of Hong Kong

Eric Yu, University of Toronto

Goal-oriented
requirements
analysis
techniques
provide ways 
to refine
organizational
and technical
objectives, to
more effectively
explore
alternatives
during
requirements
definition. After
selecting a set of
alternatives to
achieve these
objectives, you
can elaborate on
them during
subsequent
phases to make
them more
precise and
complete.

T
raditionally, requirements analysis consisted of identifying rele-
vant data and functions that a software system would support.
The data to be handled by the system might be described in terms
of entity-relationship diagrams, while the functions might be de-

scribed in terms of data flows. Or you could use object-oriented analysis
techniques that offer class, use case, state chart, sequence, and other dia-
grammatic notations for modeling.

requirements engineering



natives. To illustrate the main elements of the
proposed analysis technique, we explore a typ-
ical scenario that involves defining require-
ments for a meeting scheduling system.

Goal analysis
Let’s suppose that the meeting-scheduling

task is a generic goal we want to achieve. We
can then use AND/OR decompositions to ex-
plore alternative solutions. Each alternative
reflects a potential plan for satisfying the goal.
Figure 1 presents the decomposition of the
Schedule Meeting goal. We mark the AND
decompositions with an arc, which indicates
that satisfying the goal can be accomplished
by satisfying all subgoals. We mark the OR
decompositions, on the other hand, with a
double arc; these decompositions require only
one of their subgoals to be satisfied.

In Figure 1, the goal Schedule Meeting is
first AND-decomposed to two subgoals: Col-
lect Constraints and Generate Schedule. The
Generate Schedule subgoal is OR-decom-
posed into three subgoals that find a schedule
manually, automatically (by the system being
designed), or interactively. Other decomposi-
tions explore alternative ways of fetching the
necessary information, including timetable
information, which might or might not be
publicly available for each potential partici-
pant. Even in this simple example, there are
literally dozens of alternative solutions.

Generally, AND/OR diagrams system-
atize the exploration of alternatives within a
space that can be very large. These diagrams
make it possible to conduct a simple form of
analysis. In particular, suppose we choose
three leaf nodes—marked with a checkmark
in Figure 1—as requirements for the system.
With a simple, well-known algorithm, we
can explore whether we’ve achieved the
root goal or not. In particular, the algorithm
propagates a check upwards if all the sub-
goals of an AND-goal are checked or one of
the subgoals of an OR-goal is checked.

Softgoal analysis
Unfortunately, basic goal analysis can

only help us with goals that can be defined
crisply, such as having or not having a meet-
ing scheduled. Not all goals, however, can
be so clearly delineated. Suppose we want
to represent and analyze less clear-cut re-
quirements such as “the system must be
highly usable” or “the system must improve

meeting quality.” Such requirements, often
called qualities or nonfunctional require-
ments, can’t be defined generically nor can
they have clear-cut criteria to determine
whether they’ve been satisfied. For these re-
quirements, we need a looser notion of goal
and a richer set of relationships so that we
can indicate, for example, that a goal sup-
ports or hinders another one without being
limited to strict AND/OR relationships.

To model this looser notion of goal, we
use the notion of softgoal, presented in de-
tail in Nonfunctional Requirements in Soft-
ware Engineering.4 Softgoals represent ill-
defined goals and their interdependencies.
To distinguish softgoals from their hard
goal cousins, we declare a softgoal as being
satisfied when there is sufficient positive ev-
idence and little negative evidence against it;
a softgoal is unsatisfiable when there is suf-
ficient negative and little positive evidence. 

Figure 2 illustrates an example that focuses
on the quality “highly usable system,” which
might be as important an objective as any of
the functional goals encountered earlier. The
softgoal Usability in Figure 2 represents this
requirement. Analyzing this softgoal consists
of iterative decompositions that involve
AND/OR relationships or other more loosely
defined dependency relations. Figure 2 shows
a number of such relationships labeled with a
+ sign, which indicates that one softgoal sup-
ports—or “positively influences”—another.

In Figure 2, for instance, User Flexibility is
clearly enhanced not only by the system qual-
ity Modularity, which allows for module sub-
stitutions, but also by the system’s ability to
allow setting changes. These factors, however,
are not necessarily sufficient to satisfy User
Flexibility. This is why we use + and – labels
to describe relationships instead of AND/OR
labels. Figure 2 provides only a partial de-
composition of the softgoal. In particular, the

J a n u a r y / F e b r u a r y  2 0 0 1 I E E E  S O F T W A R E 93

Generate
schedule

Schedule meeting

Collect
constraints

Collect
timetables

Share
timetables

Interactively

AutomaticallyManually

System
collects

Person
collects

From
all From

initiator only

By email By all means

Collect other
constraints

Figure 1. An AND/OR
decomposition that
illustrates alterna-
tives for achieving
the meeting-schedul-
ing goal.



softgoals Error Avoidance, Information Shar-
ing, and Ease of Learning have their own rich
space of alternatives, which might be elabo-
rated through further refinements.

What is the relevant knowledge for identi-
fying softgoal decompositions and dependen-
cies? Some of the relevant knowledge might
be generic and related to the softgoal being
analyzed. For instance, general software sys-
tem qualities—such as reliability, perform-
ance, and usability—all have generic decom-
positions into a number of finer-grain quality
factors. There can also be generic rules for
decomposing finer-grain softgoals. Here is
one example: “A Speed performance softgoal
can be AND-decomposed into three soft-
goals: Minimize User Interaction, Use Effi-
cient Algorithms, and Ensure Adequate CPU
Power.” In certain situations, however, you
can use project-specific or task-specific de-
composition methods after agreement among
the project’s stakeholders. 

For any given software development
project, you will always initially set several
softgoals as required qualities. Some of
these might be technical—such as system
performance—because they refer specifi-
cally to qualities of the future system. Oth-
ers will be more organizationally oriented.
For instance, it is perfectly reasonable for
management to require that introduction of
the new system should improve meeting
quality (by increasing average participation
and/or effectiveness measured in some way)
or that it should cut average cost per meet-
ing (where costs include those incurred dur-
ing the scheduling process). Softgoal analy-
sis calls for each of these qualities to be
analyzed in terms of a softgoal hierarchy
like the one shown in Figure 2.

Softgoal correlation analysis

You can build softgoal hierarchies by re-
peatedly asking what can be done to satisfy
or otherwise support a particular softgoal.
Unfortunately, quality goals frequently con-
flict with each other. Consider, for example,
security and user friendliness, performance
and flexibility, high quality and low costs.
Correlation analysis can help discover posi-
tive or negative lateral relationships be-
tween these softgoals.

You can begin such analysis by noting
top-level lateral relationships, such as a neg-
atively labeled relationship from Perfor-
mance to Flexibility. This relationship can
then be refined to one or more relationships
of the same type from subgoals of Perfor-
mance (such as Capacity or Speed) to sub-
goals of Flexibility (such as Programmabil-
ity or Information Sharing). You repeat this
process until you reach the point where you
can’t refine relationships any further down
the softgoal hierarchies.

As with the construction of softgoal hier-
archies, you can discover correlations by us-
ing generic rules that state conditions under
which softgoals of one type—say, Perfor-
mance—can positively or negatively influ-
ence softgoals of another type. For example,
a correlation rule might state that complete
automation can prevent users from customiz-
ing or modifying output. This rule negatively
correlates certain automation-related per-
formance softgoals to certain flexibility ones.

Figure 3 shows diagrammatically the
softgoal hierarchy for Security, with correla-
tion links to other hierarchies developed
during the softgoal analysis process.

Goal correlation analysis
We next need to correlate the goals

shown in Figure 1 with all the softgoals
identified so far, because we propose to use
the latter in order to compare and evaluate
the former. For example, alternative sub-
goals of the goal Schedule Meeting will re-
quire different amounts of effort for sched-
uling. With respect to the softgoal of
Minimal Effort, automation is desirable,
while doing things manually is not. On that
basis, we can set up positively or negatively
labeled relationships from subgoals to
choose the schedule Automatically or Man-
ually, as shown in Figure 4. On the contrary,
if we determine that meeting quality will be

9 4 I E E E  S O F T W A R E J a n u a r y / F e b r u a r y  2 0 0 1

Ease of learning

User
tailorability

Programmability

Usability

+ +
+

+
+

+
+

Error
avoidance

Allow change
of settings

User flexibility

Modularity

Information
sharing

User-defined
writing toolUse

components

Allow change
of languageAllow change

of state

Allow change
of colors

Figure 2. A partial
softgoal hierarchy for
Usability. We adopted
this diagram from
coursework by Lisa
Gibbens and Jennifer
Spiess, prepared for
a graduate course
taught by Eric Yu.



the criterion, doing things manually is desir-
able because, presumably, it adds a personal
touch to the scheduling process.

Figure 4 shows a possible set of correla-
tion links for a simplified version of the
Schedule Meeting goal in terms of the soft-
goals Minimal Effort and Quality of Sched-
ule. The goal tree structure in the lower
right half of the figure shows the refinement
of the Schedule Meeting goal, while the two
quality softgoal trees at the upper left of the
figure represent the softgoals that are in-
tended to serve as evaluation criteria.

Figure 4 illustrates a major advantage of
distinguishing between goals and softgoals: It
encourages the separation of the analysis of a
quality sort (such as Flexibility) from the ob-
ject to which it is applied (such as System).
This distinction lets you bring relevant knowl-
edge to bear on the analysis process—from
very generic knowledge (“to achieve quality
X for a system, try to achieve X for all its
components”) to very specific (“to achieve ef-
fectiveness of a software review meeting, all
stakeholders must be present”). Knowledge
structuring mechanisms such as classification,
generalization, and aggregation can be used
to organize the available know-how for sup-
porting goal-oriented analysis. 

Evaluating of alternatives
The final step of goal-oriented analysis

calls for an evaluation of alternative func-
tional goal decompositions in terms of the
softgoal hierarchies we’ve already con-
structed. You can evaluate alternatives by
selecting a set of leaf goals that collectively
satisfy all given goals. For our single given
goal, Schedule Meeting, we might want to
choose the two leaf goals labeled with
checkmarks in Figure 4. These leaf goals
clearly satisfy the Schedule Meeting goal. In
addition, we might want to choose sets of
leaf softgoals that collectively either satisfy
or at least provide the best overall support
for top-level softgoals.

Satisfying softgoals might be impossible
because of conflicts. Accordingly, our search
for alternatives might involve finding a set
of leaf softgoals that maximize their positive
support for top softgoals while minimizing
negative support. The result of this softgoal
evaluation will lead to additional design de-
cisions, such as using passwords or allowing
setting changes.

Of course, generally there will be several
possible solutions to the satisfaction of
given goals and satisfying or finding the best
overall support for softgoals. These alterna-
tives all need to be evaluated and compared
to each other.

Putting together the pieces
In summary, goal-oriented analysis amounts

to an intertwined execution of the different
types of analysis we’ve outlined in this article.
The following steps summarize the process:

■ Input: A set of functional goals, such as
Schedule Meeting, and also a set of qual-
ities, such as Improve Meeting Participa-
tion or Reduce Meeting Costs.

■ Step 1: Decompose each functional goal
into an AND/OR hierarchy.

■ Step 2: Decompose each given quality
into a softgoal hierarchy.

■ Step 3: Identify correlations among soft-
goals.

■ Step 4: Identify correlations between goals
and softgoals. Select a set of leaf softgoals
that best satisfy all input qualities.

■ Step 5: Select a set of goals and softgoals

J a n u a r y / F e b r u a r y  2 0 0 1 I E E E  S O F T W A R E 95

Availability

Extra testing

Security

–

–

–

+ +

+

+

+

+Integrity

Confidentiality

Do backups

Accuracy

Minimize external
communication

Minimize
redundancy

Use
passwords

Maintainability

Flexibility
Performance

Usability

Recoverability

Figure 3. A partial
softgoal hierarchy for
Security, with corre-
lations to and from
other hierarchies.

Degree of
participation

Quality of
schedule

+
Schedule
meeting

Matching
effort

Minimal effort

Collection effort

Minimal
conflicts

By all means By email Manually
AutomaticallyHave updated

timetables
Collect
them

–
+

–
+

–
+

– Collect
timetables

By
system

Choose
scheduleBy

person

Figure 4. The result
of goal correlation
analysis for Schedule
Meeting.



that satisfy all given functional goals and
best satisfy all given qualities.

■ Output: A set of functions to be per-
formed by the system that collectively
meet the functional goals set out.

Output can also reflect a set of design deci-
sions, such as to use back-ups that are in-
tended to address particular quality require-
ments. In addition, output can be a set of
requirements on the system’s operational envi-
ronment, such as the requirement that their
owners update timetables at least once a week.

W e hope you agree that the stepswe’ve outlined here can precedeand augment conventional re-
quirements analysis. Indeed, defining data
and functions can begin with the functional
requirements produced by a goal-oriented
analysis.

For our example, once we select a set of
leaf nodes for satisfying the Schedule Meet-

ing goal, we can proceed to define how
meeting scheduling will be done and what
the role of the new system will be. In other
words, conventional requirements analysis
assumes that we have already settled on a
particular solution that meets predefined
organizational and technical objectives
through the introduction of the new system.

The framework we presented here is es-
sentially a simplified version of a proposal
we’ve already fleshed out in detail else-
where.4 Moreover, this framework is only
one sample among many proposals. For ex-
ample, the KAOS methodology employs a
rich and formal notion of goal-identifica-
tion as the central building block during re-
quirements acquisition.2 There is also em-
pirical evidence that goal-oriented analysis
leads to better requirements definitions.5

Different conceptions of meeting a goal
have led to different ways of handling con-
flict and evaluating alternatives.6 In addi-
tion, goals have been used as an important
mechanism for connecting requirements to
design. The “composite systems” design ap-
proach, for instance, used goals to construct
and later prune the design space.7

While there are indeed several different
approaches to goal-oriented requirements
analysis, we believe the technique proposed
here systematizes the search for a solution
that can characterize early phases of soft-
ware development, rationalizes the choice
of a particular solution, and relates design
decisions to their origins in organizational
and technical objectives.

References
1. A. Davis, Software Requirements: Objects, Functions,

and States, Prentice Hall, Old Tappan, N.J. 1993.
2. A. Dardenne, A. van Lamsweerde, and S. Fickas,

“Goal-Directed Requirements Acquisition,” Science of
Computer Programming, vol. 20, 1993, pp. 3–50.

3. J. Mylopoulos, L. Chung, and E. Yu, “From Object-
Oriented to Goal-Oriented Requirements Analysis,”
Comm. ACM, vol. 42, no. 1, Jan. 1999, pp. 31–37.

4. L. Chung et al., Non-Functional Requirements in Soft-
ware Engineering, Kluwer Publishing, Dordrecht, the
Netherlands, 2000.

5. A.I. Anton, “Goal-based Requirements Analysis,” Proc.
2nd IEEE Int’l Conf. Requirements Engineering, CS
Press, Los Alamitos, Calif., Apr. 1996, pp. 136–144. 

6. A. van Lamsweerde, R. Darimont, and P. Massonet,
“Goal Directed Elaboration of Requirements for a
Meeting Scheduler: Problems and Lessons Learnt.,”
Proc. 2nd IEEE Int’l Symp. Requirements Engineering,
CS Press, Los Alamitos, Calif., Mar. 1995, pp. 194–203.

7. M. S. Feather, “Language Support for the Specification
and Development of Composite Systems,” ACM Trans.
on Programming Languages and Systems, vol. 9, no. 2,
Apr. 1987, pp. 198–234.

9 6 I E E E  S O F T W A R E J a n u a r y / F e b r u a r y  2 0 0 1

About the Authors

John Mylopoulos is a professor of computer science at the University of Toronto. His
research interests include requirements engineering, databases, knowledge-based systems, and
conceptual modeling. He received a PhD from Princeton University. He was a corecipient of the
“most influential paper” award at the International Conference on Software Engineering, is the
president of the VLDB Endowment, and is a fellow of the AAAI. He currently serves as coeditor
of the Requirements Engineering Journal and is a member of the editorial board of the ACM
Transactions on Software Engineering and Methodology. Contact him at jm@cs.toronto.edu.

Lawrence Chung is an associate
professor of computer science at the University of Texas, Dallas. His research interests include
requirements engineering, software architecture, electronic business systems, and conceptual
modeling. He received a PhD in computer science from the University of Toronto and serves on
the editorial board of the Requirements Engineering Journal. He is the principal author of Non-
Functional Requirements in Software Engineering. Contact him at chung@utdallas.edu.

Huaiqing Wang is an associate professor at the Department of Information Systems,
City University of Hong Kong. His interests include intelligent systems, Web-based intelligent
agents, and their e-business applications, such as multiagent supported financial monitoring
systems and intelligent agent-based knowledge management systems. He received a PhD in
computer science from the University of Manchester. Contact him at jswang@is.cityu.edu.hk.

Stephen Liao is an as-
sistant professor in the Information Systems Department, City University of Hong Kong. His re-
search interests include OO modeling, systems, and technology; user profiling in e-business;
and data mining techniques and applications. He received a PhD in information systems from
Aix Marseille III University. Contact him at ishongko@is.cityu.edu.hk.

Eric Yu is associate professor in the faculty of Information Studies at the University of
Toronto. His research interests include conceptual modeling, requirements engineering, knowl-
edge management, and organizational modeling. He has served as cochair of workshops on
agent-oriented information systems held at the Conference on Advanced Information Systems
Engineering (CAiSE) and the AAAI conference. Contact him at yu@fis.utoronto.ca.



J a n u a r y / F e b r u a r y  2 0 0 1 I E E E  S O F T W A R E 97

design
E d i t o r :  M a r t i n  F o w l e r  ■ T h o u g h t Wo r k s  ■ f o w l e r @ a c m . o r g

S
oftware design is not easy—not easy
to do, teach, or evaluate. Much of
software education these days is
about products and APIs, yet much
of these are transient, whereas good
design is eternal—if only we could

figure out what good design is. 

Searching for design principles
One of the best ways to capture and pro-

mulgate good design is to learn
from the patterns community.
Their work, especially the famous
book Design Patterns (E. Gamma
et al., Addison Wesley, Reading,
Mass., 1994), has become a cor-
nerstone for many designers of
object-oriented software. Pat-
terns are not easy to understand,
but they reward the effort of
study. We can learn from the spe-
cific solutions they convey and

from the thinking process that leads to their
development. The thinking process is hard to
grasp, but understanding it helps us discover
principles that often generate these patterns. 

Over the last year, I’ve been struck by one
of the underlying principles that leads to
better designs: remove duplication. It’s also
been highlighted by mantras in a couple of
recent books: the DRY (don’t repeat your-
self) principle in the Pragmatic Programmer
(A. Hunt, and D. Thomas, Addison Wesley,
1999) and “Once and Only Once” from Ex-
treme Programming Explained: Embrace
Change (K. Beck, Addison Wesley, 1999). 

The principle is simple: say anything in
your program only once. Stated blandly like
that, it hardly bears saying. Yet identifying

and removing repetition can lead to many
interesting consequences. I have an increas-
ing sense that a pig-headed determination to
remove all repetition can lead you a long
way toward a good design and can help you
apply and understand the patterns that are
common in good designs. 

A simple case: subroutine calls
Take a simple example: subroutine calls.

You use a subroutine when you realize that
two blocks of code, in different places, are
or will be the same. You define a subroutine
and call it from both places. So, if you
change what you need to, you don’t have to
hunt down multiple repetitions to make the
change. Granted, sometimes the duplication
is just a coincidence, so you wouldn’t want
a change in one to affect the other—but I
find that is rare and easy to spot. 

So what if the blocks are similar but not
identical? Maybe some data is different in
the two cases. In that case, the answer is ob-
vious: you parameterize the data by passing
in arguments to a subroutine. One block of
code that multiplies by five and another
that multiplies by 10 become one block that
multiplies by x, and you replace x with the
right number. 

That’s a simple resolution but it illus-
trates a basic principle that carries over into
more complicated cases. Identify what is
common and what varies, find a way to iso-
late the common stuff from the variations,
then remove the redundancy in the common
stuff. In this case, separating the commonal-
ity and the variability is easy. Many times it
seems impossible, but the effort of trying
leads to good design. 

Avoiding Repetition
Martin Fowler



9 8 I E E E  S O F T W A R E J a n u a r y / F e b r u a r y  2 0 0 1

DEPT TITLE

What’s the same and what’s
different?

What if two routines have the
same basic flow of behavior but dif-
fer in the actual steps (see Figure 1)?
These two routines are similar, but
not the same. So, what is the same
and what is different? 

The sameness is the routine’s over-
all structure, and the differences are in
the steps. In both cases, the structure is 

■ print some header for the invoice, 
■ loop through each item printing a

line, and 
■ print a footer for the invoice. 

As Figure 2 shows, we can separate
the two by coming up with some kind
of printer notion with a common in-
terface for header, footer, and lines
and an implementation for the ASCII
case. Figure 3a shows that the com-
mon part is then the looping structure,
so we can wire the pieces together as
shown in Figure 3b.

There’s nothing earth-shattering
about this solution; just apply a poly-
morphic interface—which is common
to any OO or component-based envi-
ronment that lets you easily plug in
multiple implementations of a com-
mon interface. Design Patterns
junkies will recognize the Template
Method pattern. If you are a good de-
signer and are familiar with polymor-
phic interfaces, you could probably
come up with this yourself—as many
did. Knowing the pattern just gets
you there quicker. The point is, the
desire to eliminate duplication can
lead you to this solution.

Duplication and patterns
Thinking in terms of duplication

and its problems also helps you un-
derstand the benefits of patterns.
Framework folks like patterns be-
cause they easily let you define new
pluggable behaviors to fit behind the
interface. Eliminating duplication
helps because as you write a new im-
plementation, you don’t have to
worry about the common things that
need to be. Any common behavior
should be in the template method.
This lets you concentrate on the new

DESIGN

Figure 1. Two similar routines with different steps. 

class Invoice...

String asciiStatement() {
StringBuffer result = new StringBuffer();
result.append(“Bill for “ + customer + “\n”);
Iterator it = items.iterator();
while(it.hasNext()) {

LineItem each = (LineItem) it.next();
result.append(“\t” + each.product() + “\t\t” 

+ each.amount() + “\n”);
}
result.append(“total owed:” + total + “\n”);
return result.toString();

}

String htmlStatement() {
StringBuffer result = new StringBuffer();
result.append(“<P>Bill for <I>” + customer + “</I></P>”);
result.append(“<table>”);
Iterator it = items.iterator();
while(it.hasNext()) {

LineItem each = (LineItem) it.next();
result.append(“<tr><td>” + each.product() 

+ “</td><td>” + each.amount() + “</td></tr>”);
}
result.append(“</table>”);
result.append(“<P> total owed:<B>” + total + “</B></P>”);
return result.toString();

}

Figure 2. (a) A common interface for header, footer, and lines and
(b) an implementation for the ASCII case (try the HTML case as an
exercise on your own).

interface Printer {
String header(Invoice iv);
String item(LineItem line);
String footer(Invoice iv);

}

(a)

static class AsciiPrinter implements Printer {
public String header(Invoice iv) {
return “Bill for “ + iv.customer + “\n”;

}
public String item(LineItem line) {

return “\t” + line.product()+ “\t\t” + line.amount() +“\n”;
}
public String footer(Invoice iv) {
return “total owed:” + iv.total + “\n”;

}
}

(b)



DESIGN

J a n u a r y / F e b r u a r y  2 0 0 1 I E E E  S O F T W A R E 99

behavior rather than the old. 
The principle of duplication also

helps you think about when to apply
this pattern. As many people know,
one of the problems with people who
have just read a pattern is that they
insist on using it, which often leads to
more complicated designs. When you
insist on using a pattern, ask, “What
repetition is this removing?” Remov-
ing repetition makes it more likely
that you’re making good use of the
pattern. If not, perhaps you shouldn’t
use it. 

Often, the hard part of eliminat-
ing duplication is spotting it in the
first place. In my example, you can
spot the two routines easily because
they are in the same file and located
close to each other. What happens
when they are in separate files and
written by different people in differ-
ent millennia?

This question leads us to think
about how we construct our soft-
ware to reduce the chances of this
happening. Using abstract data types
is a good way of doing this. Because
you have to pass data around, you
find that people are less likely to du-
plicate data structures. If you try to
place routines next to their data

structures, you are more likely to
spot any duplication in the routines.
Much of the reason why objects are
popular is because of this kind of so-
cial effect, which works well when
reinforced by a culture that encour-
ages people to look around and fix
duplications when they do arise. 

S o, avoiding repetition is a simpleprinciple that leads to good de-sign. I intend to use this column
to explore other simple principles
that have this effect. Have you no-
ticed simple principles like this in
your work? If so, please contact
me—I’m always happy to repeat
good ideas. 

Martin Fowler is the chief scientist for ThoughtWorks, an
Internet systems delivery and consulting company. For a decade,
he was an independent consultant pioneering the use of objects
in developing business information systems. He’s worked with
technologies including Smalltalk, C++, object and relational
databases, and Enterprise Java with domains including leasing,
payroll, derivatives trading, and healthcare. He is particularly
known for his work in patterns, UML, lightweight methodologies,
and refactoring. He has written four books: Analysis Patterns,
Refactoring, Planning Extreme Programming, and UML Distilled.
Contact him at fowler@acm.org.

Figure 3. (a) The common part of the routine and (b) the pieces
wired together.

class Invoice...
public String statement(Printer pr) {

StringBuffer result = new StringBuffer();
result.append(pr.header(this));
Iterator it = items.iterator();
while(it.hasNext()) {

LineItem each = (LineItem) it.next();
result.append(pr.item(each));

}
result.append(pr.footer(this));
return result.toString();

}

(a)

class Invoice...

public String asciiStatement2() {

return statement (new AsciiPrinter());

}

(b)

How to 
Reach Us

Writers
For detailed information on submitting articles,
write for our Editorial Guidelines (software@
computer.org), or access computer.org/
software/author.htm.

Letters to the Editor
Send letters to

Letters Editor
IEEE Software
10662 Los Vaqueros Circle
Los Alamitos, CA 90720
dstrok@computer.org

Please provide an e-mail address or 
daytime phone number with your letter.

On the Web
Access computer.org/software for information
about IEEE Software.

Subscription Change of Address
Send change-of-address requests for magazine
subscriptions to address.change@ieee.org. 
Be sure to specify IEEE Software.

Membership Change of Address
Send change-of-address requests for the mem-
bership directory to directory.updates@
computer.org.

Missing or Damaged Copies
If you are missing an issue or you received 
a damaged copy, contact membership@ 
computer.org.

Reprints of Articles
For price information or to order reprints, send
e-mail to software@computer.org or fax +1
714 821 4010.

Reprint Permission
To obtain permission to reprint an article, con-
tact William Hagen, IEEE Copyrights and Trade-
marks Manager, at whagen@ieee.org.

How to 
Reach Us


